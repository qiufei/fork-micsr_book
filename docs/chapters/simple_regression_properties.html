<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Microeconometrics with R - 2&nbsp; Statistical properties of the simple linear estimator</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/multiple_regression.html" rel="next">
<link href="../chapters/simple_regression.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Microeconometrics with R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../OLS.html" class="sidebar-item-text sidebar-link">Ordinary least squares estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression_properties.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/multiple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coefficients.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../beyond_OLS.html" class="sidebar-item-text sidebar-link">Beyond the OLS estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/maximum_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/non_spherical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/endogeneity.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/treateff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/spatial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Spatial econometrics</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../special_responses.html" class="sidebar-item-text sidebar-link">Special responses</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/binomial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Binomial models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/tobit.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Censored and truncated models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/count.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Count data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/duration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Duration models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discrete choice models</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-exact_prop_ols" id="toc-sec-exact_prop_ols" class="nav-link active" data-scroll-target="#sec-exact_prop_ols"><span class="toc-section-number">2.1</span>  Exact properties of the OLS estimator</a>
  <ul class="collapse">
<li><a href="#errors-have-0-expected-value" id="toc-errors-have-0-expected-value" class="nav-link" data-scroll-target="#errors-have-0-expected-value">Errors have 0 expected value</a></li>
  <li><a href="#conditional-expectation-of-the-errors-is-0" id="toc-conditional-expectation-of-the-errors-is-0" class="nav-link" data-scroll-target="#conditional-expectation-of-the-errors-is-0">Conditional expectation of the errors is 0</a></li>
  <li><a href="#estimator-for-the-variance-of-the-ols-estimator" id="toc-estimator-for-the-variance-of-the-ols-estimator" class="nav-link" data-scroll-target="#estimator-for-the-variance-of-the-ols-estimator">Estimator for the variance of the OLS estimator</a></li>
  <li><a href="#sec-simple_ols_blue" id="toc-sec-simple_ols_blue" class="nav-link" data-scroll-target="#sec-simple_ols_blue">OLS estimator is BLUE</a></li>
  </ul>
</li>
  <li>
<a href="#sec-asymp_prop_ols" id="toc-sec-asymp_prop_ols" class="nav-link" data-scroll-target="#sec-asymp_prop_ols"><span class="toc-section-number">2.2</span>  Asymptotic properties of the estimator</a>
  <ul class="collapse">
<li><a href="#convergence-in-probability" id="toc-convergence-in-probability" class="nav-link" data-scroll-target="#convergence-in-probability">Convergence in probability</a></li>
  <li><a href="#sec-clt" id="toc-sec-clt" class="nav-link" data-scroll-target="#sec-clt">Convergence in distribution: central-limit theorem</a></li>
  <li><a href="#simulations" id="toc-simulations" class="nav-link" data-scroll-target="#simulations">Simulations</a></li>
  </ul>
</li>
  <li>
<a href="#sec-confint_test_slm" id="toc-sec-confint_test_slm" class="nav-link" data-scroll-target="#sec-confint_test_slm"><span class="toc-section-number">2.3</span>  Confidence interval and tests</a>
  <ul class="collapse">
<li><a href="#testing-hypothesis" id="toc-testing-hypothesis" class="nav-link" data-scroll-target="#testing-hypothesis">Testing hypothesis</a></li>
  <li><a href="#sec-confint_simple_ols" id="toc-sec-confint_simple_ols" class="nav-link" data-scroll-target="#sec-confint_simple_ols">Confidence interval</a></li>
  <li><a href="#exact-distribution-the-student-distribution" id="toc-exact-distribution-the-student-distribution" class="nav-link" data-scroll-target="#exact-distribution-the-student-distribution">Exact distribution, the Student distribution</a></li>
  <li><a href="#inference-with-r" id="toc-inference-with-r" class="nav-link" data-scroll-target="#inference-with-r">Inference with R</a></li>
  <li><a href="#delta-method" id="toc-delta-method" class="nav-link" data-scroll-target="#delta-method">Delta method</a></li>
  <li><a href="#confidence-interval-for-the-prediction" id="toc-confidence-interval-for-the-prediction" class="nav-link" data-scroll-target="#confidence-interval-for-the-prediction">Confidence interval for the prediction</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-stat_prop_slm" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><!-- see the sentence of Davidson McKinnon for inconsistency --><!-- The second term is the ratio of covariance and variance, not exactly --><p>To analyze the statistical properties of the OLS estimator, we use <a href="simple_regression.html#eq-ols_lin_comb_errors">Equation&nbsp;<span>1.16</span></a> that indicates that the difference between the estimated slope and the true value is a linear combination of the errors:</p>
<p><span id="eq-linest"><span class="math display">\[
\hat{\beta}=\beta + \sum_{n=1}^N c_n \epsilon_n, \mbox{ with } c_n = (x_n - \bar{x}) / S_{xx}
\tag{2.1}\]</span></span></p>
<p>The properties of <span class="math inline">\(\hat{\beta}\)</span> are therefore directly deduced from those of <span class="math inline">\(\epsilon\)</span>. We’ll consider two sets of properties:</p>
<ul>
<li>
<strong>exact</strong> properties that apply whatever the size of the sample is (<a href="#sec-exact_prop_ols"><span>Section&nbsp;2.1</span></a>),</li>
<li>
<strong>asymptotic</strong> properties that indicate approximate results, the approximation being better and better as the sample size grows (<a href="#sec-asymp_prop_ols"><span>Section&nbsp;2.2</span></a>).</li>
</ul>
<p><a href="#sec-confint_test_slm"><span>Section&nbsp;2.3</span></a> explains how these properties can be used to construct confidence intervals and tests.</p>
<section id="sec-exact_prop_ols" class="level2" data-number="2.1"><h2 data-number="2.1" class="anchored" data-anchor-id="sec-exact_prop_ols">
<span class="header-section-number">2.1</span> Exact properties of the OLS estimator</h2>
<p>The OLS estimator is a random variable, for which we observe one value, obtained with a given sample. The exact properties of the OLS estimator concern:</p>
<ul>
<li>its expected value: if the true value is <span class="math inline">\(\beta_o\)</span>, what is the expected value of <span class="math inline">\(\hat{\beta}\)</span>, <span class="math inline">\(\beta_o\)</span> or another value?</li>
<li>its variance (or standard deviation): is the variance small (the estimation is precise) or large?</li>
</ul>
<p>The computation of the expected value indicates the presence or absence of a <strong>bias</strong>. Therefore, we check here whether there is a systematic error (called the bias) while performing the estimation. The variance indicates the <strong>efficiency</strong> (or the precision) of the estimator. It measures the amount of the <strong>sampling error</strong>, i.e., the average distance between the value of the estimator and its expected value.</p>
<p>To analyze the properties of the OLS estimator, we’ll make different hypotheses and we’ll see that if these hypotheses are satisfied, the OLS is the best (most efficient) linear unbiased estimator. To illustrate the results of this chapter, we’ll use the price-time model, with the same GDP as previously: <span class="math inline">\(\alpha = -0.2\)</span>, <span class="math inline">\(\beta = 0.032\)</span> and <span class="math inline">\(\sigma_\epsilon = 0.08\)</span> and we’ll consider different departures from this reference case.</p>
<section id="errors-have-0-expected-value" class="level3"><h3 class="anchored" data-anchor-id="errors-have-0-expected-value">Errors have 0 expected value</h3>
<p>The reference model being <span class="math inline">\(y_n = \alpha + \beta x_n + \epsilon_n\)</span> with <span class="math inline">\(\mbox{E}(\epsilon_n) = 0\)</span>, consider the alternative model: <span class="math inline">\(y_n = \gamma + \beta x_n + \eta_n\)</span>, for which the slope is the same and the error term is <span class="math inline">\(\eta_n\)</span>, with <span class="math inline">\(\mbox{E}(\eta) = \mu_\eta \neq 0\)</span>. We have therefore: <span class="math inline">\(y_n = \alpha + \beta x_n + (\eta_n + \gamma - \alpha)\)</span> or: <span class="math inline">\(\eta_n = \epsilon_n + \alpha - \gamma\)</span> and finally: <span class="math inline">\(\mbox{E}(\eta) = \mu_\eta = \mbox{E}(\epsilon) + \alpha - \gamma = \alpha - \gamma\)</span>.</p>
<p>Therefore, the alternative model is: <span class="math inline">\(y_n = \gamma + \mu_\gamma + \beta x_n + \epsilon\)</span>, which is the same model as the initial model with <span class="math inline">\(\alpha\)</span> replaced by <span class="math inline">\(\gamma + \mu_\gamma\)</span>. Therefore, it is impossible to discriminate between the initial and the alternative model, as what can be estimated is the sum of the intercept and the expected value of the errors (<span class="math inline">\(\gamma + \mu_\gamma\)</span>) and the two elements of this sum can’t be estimated separately. This is an illustration of a very important problem in econometrics called <strong>identification</strong> that we’ll encounter in subsequent chapters. We can say here that <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\mu_\gamma\)</span> are not identified, but that their sum is. Therefore, we can set one of the two parameters to any value. For example, we can simply set <span class="math inline">\(\mu_\gamma=0\)</span>, i.e., suppose that the expected value of the errors is 0 and the other parameter <span class="math inline">\(\gamma\)</span> became identified, i.e., it can be estimated using the data.</p>
<p><a href="#fig-everrors">Figure&nbsp;<span>2.1</span></a> illustrates the “reference” model (plain line and <span class="math inline">\(\epsilon_n\)</span> represented by plain vectors) and the alternative model (dashed line and <span class="math inline">\(\eta_n\)</span> represented by dashed vectors).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-everrors" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-everrors-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.1: Intercept and the expected value of the error</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="conditional-expectation-of-the-errors-is-0" class="level3"><h3 class="anchored" data-anchor-id="conditional-expectation-of-the-errors-is-0">Conditional expectation of the errors is 0</h3>
<p>As we have seen, the hypothesis that <span class="math inline">\(\mbox{E}(\epsilon)=0\)</span> can always be stated if the model contains an intercept. On the contrary, the hypothesis that the expected value of <span class="math inline">\(\epsilon\)</span> conditional on <span class="math inline">\(x\)</span> is 0 (<span class="math inline">\(\mbox{E}(\epsilon |x)=0\)</span>) is much more problematic and the violation of this hypothesis has dramatic consequences for the OLS estimator. It is important to understand that this condition actually implies that there is no correlation between the error and the covariate. Starting with the expression of the covariance between the error and the covariate: <span class="math inline">\(\mbox{cov}(x, \epsilon) = \mbox{E}\left((x - \mu_x)(\epsilon-\mu_\epsilon)\right)\)</span>, with <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_\epsilon\)</span> the expected values of <span class="math inline">\(x\)</span> and <span class="math inline">\(\epsilon\)</span>, we can rewrite this covariance using conditional expectation, using the rule of repeated expectations:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mbox{cov}(x, \epsilon) &amp;=&amp; \mbox{E}_x\left[\mbox{E}\left((x - \mu_x)(\epsilon-\mu_\epsilon)| x\right)\right]\\
&amp;=&amp;\mbox{E}_x\left[(x - \mu_x)\left(\mbox{E}(\epsilon-\mu_\epsilon)| x\right)\right]\\
&amp;=&amp;\mbox{E}_x\left[(x - \mu_x)\mbox{E}(\epsilon| x)\right]
\end{array}
\]</span></p>
<p>The covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(\epsilon\)</span> is therefore equal to the covariance between <span class="math inline">\(x\)</span> and the conditional expectation of <span class="math inline">\(\epsilon\)</span>. If <span class="math inline">\(\mbox{E}(\epsilon|x)\)</span> is a constant (equal to <span class="math inline">\(\mu_\epsilon\)</span>, the unconditional expectation), the covariance is:</p>
<p><span class="math display">\[
\mbox{cov}(x, \epsilon)=\mbox{E}\left[(x - \mu_x)\mu_\epsilon\right] = \mu_\epsilon\mbox{E}\left[x - \mu_x\right]=0
\]</span></p>
<p>Therefore, a constant conditional expectation of <span class="math inline">\(\epsilon\)</span> (not necessarily 0 but we have seen previously than we can safely suppose that it is 0) implies that the covariance between the errors and the covariate is 0 or, stated differently, that the errors are uncorrelated with the covariate. From <a href="#eq-linest">Equation&nbsp;<span>2.1</span></a>, the conditional expectation of the estimator is: </p>
<p><span id="eq-condexp"><span class="math display">\[
\mbox{E}(\hat{\beta}\mid x) = \beta + \sum_{n = 1} ^ N
\mbox{E}(c_{n}\epsilon_{n} \mid x_n)=
\beta + \sum_{n = 1} ^ N
c_{n}\mbox{E}(\epsilon_{n} \mid x_n)
\tag{2.2}\]</span></span></p>
<p>If the conditional expectation of the errors is constant (<span class="math inline">\(\mbox{E}(\epsilon_n \mid x_n) = \mu_\epsilon\)</span>), <span class="math inline">\(\sum_{n=1}^N c_n\mbox{E}(\epsilon_n | x) = \mu_\epsilon\sum_{n=1}^N c_n = 0\)</span> as <span class="math inline">\(\sum_n c_n = 0\)</span>, so that <span class="math inline">\(\mbox{E}(\hat{\beta} | x) = \beta\)</span>, which means that the expected value of the estimator is the true value. In this case, the estimator is <strong>unbiased</strong>. Therefore, the hypothesis of constant conditional expectation of the errors is crucial.</p>
<p>It is very important to understand why, in practice, this hypothesis may be violated. As an illustration, consider the wage / education model. It is well documented in a lot of countries that, for a given value of education, women earn less than men on average. This means that the conditional expectation of wage is lower for women or, graphically, that in a scatterplot, points for women will be in general below the line which indicates the conditional expectation of wages and that points for men will be above this line. To see whether this can induce a bias in the OLS estimator, rewrite <a href="#eq-condexp">Equation&nbsp;<span>2.2</span></a> as:</p>
<p><span class="math display">\[
\mbox{E}(\hat{\beta}\mid x) = \beta  + \frac{\sum_{n=1} ^ N (x_n - \bar{x})\mbox{E}(\epsilon_n | x_n)}
{\sum_{n=1} ^ N (x_n - \bar{x}) ^ 2}
\]</span></p>
<p>The second term is the ratio of the covariance between <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(x\)</span> and the variance of <span class="math inline">\(x\)</span>. If not zero, this ratio is the bias of the OLS estimator. In our example, the key question is whether being a male is correlated with education:</p>
<ul>
<li>with no correlation, the OLS estimator is unbiased,</li>
<li>with a negative correlation, the OLS estimator is downward biased,</li>
<li>with a positive correlation, the OLS estimator is upward biased.</li>
</ul>
<p>These three situations are depicted in <a href="#fig-sex_educ">Figure&nbsp;<span>2.2</span></a>. The common feature of the three rows of the figure is that women (depicted by circles) are in general below the expected value line (plain line) and men (depicted by triangles) are above. For row 1 of the figure, considering the horizontal position of the points, women and men are approximately uniformly disposed, which indicates the absence of correlation between education and being a male. The regression (dashed) line is then very close to the expected value line, the OLS estimator is unbiased. For row 2 of the figure, males have in general a lower level of education than females. There is therefore a negative correlation between education and being a man and the consequence is that the OLS estimator is downward biased. Finally, for row 3 of the figure, the correlation is positive and the OLS estimator is upward biased.</p>
<div class="cell" data-layout-align="center" data-fig.asp="1.5">
<div class="cell-output-display">
<div id="fig-sex_educ" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-sex_educ-1.png" class="img-fluid figure-img" style="width:45.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.2: Education, sex and wage</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Consider the latter case in details. Women have a lower wage than men for two reasons: they are less educated and, for a given value of education, they receive a lower wage than males. Increasing the education level from, say, 4 to 5 years will have two effects on the expected wage:</p>
<ul>
<li>direct positive effect of education on wage,</li>
<li>indirect positive effect: as being a man is positively correlated with education, considering a higher level of education, we’ll get a subpopulation with a higher share of males, and therefore a higher wage.</li>
</ul>
<p>The OLS estimator estimates the sum of these two effects and is therefore in this case upward biased. </p>
</section><section id="estimator-for-the-variance-of-the-ols-estimator" class="level3"><h3 class="anchored" data-anchor-id="estimator-for-the-variance-of-the-ols-estimator">Estimator for the variance of the OLS estimator</h3>
<p>Consider now the conditional variance of the OLS estimator:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mbox{V}(\hat{\beta}\mid x )&amp; =
&amp;\mbox{E}\left(\left(\hat{\beta}-\beta\right)^ 2 \mid x\right)\\
&amp; = &amp;\mbox{E}\left(\left(\sum_{n = 1} ^ N c_{n}\epsilon_{n}\right)^
2 \mid x \right)\\
&amp; = &amp; \frac{1}{S_{xx}^2}\mbox{E}\left(\left(\sum_{n = 1} ^ N (x_n - \bar{x})\epsilon_{n}\right)^
2  \mid x\right)
\end{array}
\]</span></p>
<p>To compute the variance, we therefore have to take the expected value of <span class="math inline">\(N ^ 2\)</span> terms, <span class="math inline">\(N\)</span> of them being of the form: <span class="math inline">\((x_n - \bar{x}) ^ 2 \epsilon_n ^ 2\)</span> and the <span class="math inline">\(N ^ 2 - N\)</span> other of the form: <span class="math inline">\((x_n - \bar{x})(x_m - \bar{x})\epsilon_n\epsilon_m\)</span>. This is best understood by arranging the <span class="math inline">\(N^2\)</span> terms in a square matrix of dimension <span class="math inline">\(N\)</span>. With <span class="math inline">\(N=4\)</span>, we have<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[
\scriptsize{\left(\begin{array}{cccc}
(x_1-\bar{x})^2 \mbox{E}(\epsilon_1^2) &amp;
(x_1-\bar{x}) (x_2-\bar{x}) \mbox{E}(\epsilon_1\epsilon_2) &amp;
(x_1-\bar{x}) (x_3-\bar{x}) \mbox{E}(\epsilon_1\epsilon_3) &amp;
(x_1-\bar{x}) (x_4-\bar{x}) \mbox{E}(\epsilon_1\epsilon_4) \\
(x_2-\bar{x}) (x_1-\bar{x}) \mbox{E}(\epsilon_2\epsilon_1) &amp;
(x_2-\bar{x})^2 \mbox{E}(\epsilon_2^2) &amp;
(x_2-\bar{x}) (x_3-\bar{x}) \mbox{E}(\epsilon_2\epsilon_3) &amp;
(x_2-\bar{x}) (x_4-\bar{x}) \mbox{E}(\epsilon_2\epsilon_4) \\
(x_3-\bar{x}) (x_1-\bar{x}) \mbox{E}(\epsilon_3\epsilon_1) &amp;
(x_3-\bar{x}) (x_2-\bar{x}) \mbox{E}(\epsilon_3\epsilon_2) &amp;
(x_3-\bar{x})^2 \mbox{E}(\epsilon_3^2) &amp;
(x_3-\bar{x}) (x_4-\bar{x}) \mbox{E}(\epsilon_3\epsilon_4) \\
(x_4-\bar{x}) (x_1-\bar{x}) \mbox{E}(\epsilon_4\epsilon_1) &amp;
(x_4-\bar{x}) (x_2-\bar{x}) \mbox{E}(\epsilon_4\epsilon_2) &amp;
(x_4-\bar{x}) (x_3-\bar{x}) \mbox{E}(\epsilon_4\epsilon_3) &amp;
(x_4-\bar{x})^2 \mbox{E}(\epsilon_4^2) \\
\end{array}
\right)
}
\]</span></p>
</div>
<section id="uncorrelation-and-homoskedasticity" class="level4"><h4 class="anchored" data-anchor-id="uncorrelation-and-homoskedasticity">Uncorrelation and homoskedasticity</h4>
<p> <span class="math inline">\(\mbox{V}(\hat{\beta}\mid x)\)</span> is obtained by taking the sum of these <span class="math inline">\(N ^ 2\)</span> terms, <span class="math inline">\(N\)</span> terms depending on conditional variances (<span class="math inline">\(\mbox{E}(\epsilon_n ^ 2 \mid x_n) = \mbox{V}(\epsilon_n \mid x_n)\)</span>) and <span class="math inline">\(N\times(N -1)\)</span> on conditional covariances (<span class="math inline">\(\mbox{E}(\epsilon_n\epsilon_m\mid x_n, x_m) = \mbox{cov}(\epsilon_n, \epsilon_m\mid x_n, x_m)\)</span>). The resulting estimator has a very simple form if two hypothesis are made:</p>
<ul>
<li>the errors are <strong>homoskedastic</strong>, which means that their variances don’t depend on <span class="math inline">\(x\)</span>, the <span class="math inline">\(N\)</span> terms that contain the conditional variances are then equal to <span class="math inline">\((x_n - \bar{x})^2\sigma_\epsilon ^ 2\)</span>,</li>
<li>the errors are <strong>uncorrelated</strong>, the <span class="math inline">\(N\times(N-1)\)</span> terms that involve the covariance are then equal to 0.</li>
</ul>
<p>With these two hypotheses in hand, only the diagonal terms are not zero and their sum is <span class="math inline">\(\sigma_\epsilon ^ 2 \sum_{n=1}^N (x_n - \bar{x}) ^ 2 = \sigma_\epsilon ^ 2 S_{xx}\)</span>, which finally leads to the simplified formula of the variance of <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span id="eq-var_hbeta"><span class="math display">\[
\mbox{V}(\hat{\beta}\mid x) = \sigma_{\hat{\beta}} ^ 2 =  \frac{\sigma_\epsilon ^ 2S_{xx}}{S_{xx} ^ 2}=
\frac{\sigma_\epsilon ^ 2}{S_{xx}}
= \frac{\sigma_\epsilon ^ 2}{N\hat{\sigma}_x^2}
\tag{2.3}\]</span></span></p>
<p>Note that this is the “true” variance of <span class="math inline">\(\hat{\beta}\)</span> if the two hypotheses are satisfied, and that it can’t be computed as it depends on the unknown parameter <span class="math inline">\(\sigma_\epsilon\)</span>. The square root of <a href="#eq-var_hbeta">Equation&nbsp;<span>2.3</span></a> is the standard deviation of <span class="math inline">\(\beta\)</span>, is measured in the same unit as <span class="math inline">\(\beta\)</span> and is commonly called the <strong>standard error</strong> of <span class="math inline">\(\hat{\beta}\)</span>. It is therefore a convenient indicator of the precision of the estimator:</p>
<p><span id="eq-stder_hat_beta"><span class="math display">\[
\sigma_{\hat{\beta}} = \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}
\tag{2.4}\]</span></span></p>
<p>It is clear from <a href="#eq-stder_hat_beta">Equation&nbsp;<span>2.4</span></a> that the precision of the estimator depends on three components which will be described in details in the next subsection. </p>
</section><section id="determinants-of-the-precision-of-the-ols-estimator" class="level4"><h4 class="anchored" data-anchor-id="determinants-of-the-precision-of-the-ols-estimator">Determinants of the precision of the OLS estimator</h4>
<p>First consider the “size” of the error, measured by its standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>. <a href="#fig-sigmaeps">Figure&nbsp;<span>2.3</span></a> presents a scatterplot for six samples which use the same DGP, except that samples on the second line are generated with a smaller value of <span class="math inline">\(\sigma_\epsilon\)</span>. The “true model” (<span class="math inline">\(\alpha + \beta x\)</span>) is represented by a plain line and the regression line is dashed. Obviously, the estimation is much more precise on the second line of <a href="#fig-sigmaeps">Figure&nbsp;<span>2.3</span></a>, because of small-sized errors.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-sigmaeps" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-sigmaeps-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.3: Size of the error and the precision of the slope estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Next consider the sample size. In <a href="#fig-smplsize">Figure&nbsp;<span>2.4</span></a>, we take the same value of <span class="math inline">\(\sigma_\epsilon\)</span> as in row 1 of <a href="#fig-sigmaeps">Figure&nbsp;<span>2.3</span></a>, but we increase the sample size to 40 for the samples of the second line. In large samples (second line in <a href="#fig-smplsize">Figure&nbsp;<span>2.4</span></a>) the slope is very precisely estimated, which means that the value of <span class="math inline">\(\hat{\beta}\)</span> is almost the same from a sample to another. On the contrary, with a small sample size (first line in <a href="#fig-smplsize">Figure&nbsp;<span>2.4</span></a>), the slopes of the regression lines are very different for the three samples, which indicates that the standard error of <span class="math inline">\(\hat{\beta}\)</span> is high (or that the estimator is imprecise).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-smplsize" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-smplsize-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.4: Sample size and precision of the estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Finally, in <a href="#fig-varx">Figure&nbsp;<span>2.5</span></a>, we consider a variation of the variance of <span class="math inline">\(x\)</span>. For samples on the second line, the variance of <span class="math inline">\(x\)</span> is much smaller than for samples on the first line. The larger the variance of <span class="math inline">\(x\)</span> is, the more precise is the estimator of the slope. Obviously, it is difficult to estimate the effect of education on wage if all the individuals in the sample have almost the same level of education. Consider the extreme case of no variation of <span class="math inline">\(x\)</span> in a sample; in this case it is impossible to estimate the effect of <span class="math inline">\(x\)</span>, as all the observations are characterized by the same value of <span class="math inline">\(x\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-varx" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-varx-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.5: Variance of <span class="math inline">\(x\)</span> and precision of the estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">

</div>
<p> All these results can be illustrated by simulations, using the <code>price_time</code> data set. For convenience, we replicate in the following code the operations we performed on this data set in <a href="simple_regression.html"><span>Chapter&nbsp;1</span></a>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="va">price_time</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_names</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span> sr <span class="op">=</span> <span class="va">qr</span> <span class="op">/</span> <span class="op">(</span><span class="va">qr</span> <span class="op">+</span> <span class="va">qa</span><span class="op">)</span>,</span>
<span>         h <span class="op">=</span> <span class="op">(</span><span class="va">pa</span> <span class="op">-</span> <span class="va">pr</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="va">tr</span> <span class="op">-</span> <span class="va">ta</span><span class="op">)</span> <span class="op">/</span> <span class="fl">60</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">&lt;</span> <span class="fl">0.75</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We start with our reference case (<span class="math inline">\(\sigma_\epsilon = 0.08\)</span>, <span class="math inline">\(N = 9\)</span> and <span class="math inline">\(x\)</span> is the vector of the threshold value of time for the nine selected cities of the <code>prtime</code> data set, with a sample standard deviation <span class="math inline">\(\hat{\sigma}_x = 6.085\)</span>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fl">0.2</span> ; <span class="va">beta</span> <span class="op">&lt;-</span> <span class="fl">0.032</span></span>
<span><span class="va">seps</span> <span class="op">&lt;-</span> <span class="fl">0.08</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">pull</span><span class="op">(</span><span class="va">prtime</span>, <span class="va">h</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We generate <span class="math inline">\(R = 100\)</span> samples using each time the same vector of covariate (<span class="math inline">\(x\)</span>) and drawing the errors in a normal distribution:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">dataref</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>smpls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>                  x     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">R</span><span class="op">)</span>,</span>
<span>                  eps   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">R</span> <span class="op">*</span> <span class="va">N</span>, sd <span class="op">=</span> <span class="va">seps</span><span class="op">)</span>,</span>
<span>                  y     <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To illustrate the influence of <span class="math inline">\(\sigma_\epsilon\)</span> on the precision of the estimator, we take a value of <span class="math inline">\(\sigma_\epsilon =0.04\)</span>, i.e., we divide <span class="math inline">\(\sigma_\epsilon\)</span> by 2 compared to the reference case:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dataseps</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>smpls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>                   x     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">R</span><span class="op">)</span>,</span>
<span>                   eps   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">R</span> <span class="op">*</span> <span class="va">N</span>, sd <span class="op">=</span> <span class="va">seps</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>                   y     <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we increase the sample size to <span class="math inline">\(N = 36\)</span>, i.e., we multiply the sample size by 4. More specifically, for every sample each value of <span class="math inline">\(x\)</span> is repeated four times:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">*</span> <span class="fl">4</span></span>
<span><span class="va">xN</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">datasN</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>smpls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>                   x     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">xN</span>, <span class="va">R</span><span class="op">)</span>,</span>
<span>                   eps   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">R</span> <span class="op">*</span> <span class="va">N</span>, sd <span class="op">=</span> <span class="va">seps</span><span class="op">)</span>,</span>
<span>                   y     <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we increase the variation of <span class="math inline">\(x\)</span>, simply by multiplying all the values by 2. In this case, the standard deviation of <span class="math inline">\(x\)</span> is also multiplied by 2.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">xv</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">*</span> <span class="fl">2</span></span>
<span><span class="va">datasvx</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>smpls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>                   x     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">xv</span>, <span class="va">R</span><span class="op">)</span>,</span>
<span>                   eps   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">R</span> <span class="op">*</span> <span class="va">N</span>, sd<span class="op">=</span> <span class="va">seps</span><span class="op">)</span>,</span>
<span>                   y     <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The standard deviation for the reference case is:</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\beta}} = \frac{\sigma_\epsilon}{\sqrt{N} \hat{\sigma}_x}=
\frac{0.08}{\sqrt{9} \times
     6.085}
= 0.0044
\]</span></p>
<p>which is very close to the standard deviation of <span class="math inline">\(\hat{\beta}\)</span> for our <span class="math inline">\(R = 100\)</span> samples:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dataref</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">smpls</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span>slope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">)</span> <span class="op">/</span> </span>
<span>              <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">summarise</span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
    mean      sd
   &lt;dbl&gt;   &lt;dbl&gt;
1 0.0316 0.00420</code></pre>
</div>
</div>
<p>Note that we used twice the <code>summarise</code> function. The first time, it is used with <code>group_by</code> so that a tibble with <code>R</code> lines is returned, containing the values of the estimator <code>slope</code>. The second time, a one-line tibble is returned containing the mean and the standard deviation of the <code>R</code> values of the estimator.</p>
<p>When <span class="math inline">\(\sigma_\epsilon\)</span> is divided by 2 (from 0.08 to 0.04), the standard deviation of <span class="math inline">\(\hat{\beta}\)</span> should also be divided by 2 (from <span class="math inline">\(0.00438\)</span> to <span class="math inline">\(0.00219\)</span>), which is approximately the value of the standard deviation of the values of <span class="math inline">\(\hat{\beta}\)</span> for our <span class="math inline">\(R = 100\)</span> samples:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dataseps</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">smpls</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>hbeta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">)</span> <span class="op">/</span> </span>
<span>              <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">hbeta</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">hbeta</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
    mean      sd
   &lt;dbl&gt;   &lt;dbl&gt;
1 0.0318 0.00222</code></pre>
</div>
</div>
<p>When the sample size is multiplied by 4, <span class="math inline">\(\hat{\sigma}_\beta\)</span> should also be divided by 2:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datasN</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">smpls</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>hbeta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">)</span> <span class="op">/</span> </span>
<span>              <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">hbeta</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">hbeta</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
    mean      sd
   &lt;dbl&gt;   &lt;dbl&gt;
1 0.0323 0.00191</code></pre>
</div>
</div>
<p>Finally, when every value of <span class="math inline">\(x\)</span> is multiplied by 2, <span class="math inline">\(x\)</span>’s standard deviation is also multiplied by 2 and <span class="math inline">\(\hat{\sigma}_\beta\)</span> should be divided by 2:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datasvx</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">smpls</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>hbeta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">)</span> <span class="op">/</span> </span>
<span>              <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">hbeta</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">hbeta</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
    mean      sd
   &lt;dbl&gt;   &lt;dbl&gt;
1 0.0324 0.00247</code></pre>
</div>
</div>
<p></p>
</section><section id="variance-of-hatalpha-and-covariance-between-hatalpha-and-hatbeta" class="level4"><h4 class="anchored" data-anchor-id="variance-of-hatalpha-and-covariance-between-hatalpha-and-hatbeta">Variance of <span class="math inline">\(\hat{\alpha}\)</span> and covariance between <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>
</h4>
<p>To get the variance of the estimator of the intercept, consider the “true” and the fitted model for one observation:</p>
<p><span id="eq-true_estimated_model"><span class="math display">\[
\left\{
\begin{array}{rcl}
y_n &amp;=&amp; \alpha +\beta x_n + \epsilon_n\\
y_n &amp;=&amp; \hat{\alpha} + \hat{\beta} x_n+ \hat{\epsilon}_n\\
\end{array}
\right.
\tag{2.5}\]</span></span></p>
<p>Equating the two expressions in <a href="#eq-true_estimated_model">Equation&nbsp;<span>2.5</span></a>, we get:</p>
<p><span id="eq-diff_true_estimated_model"><span class="math display">\[
(\hat{\alpha} - \alpha) + (\hat{\beta} - \beta) x_n + (\hat{\epsilon}_n - \epsilon_n) = 0
\tag{2.6}\]</span></span></p>
<p>Summing <a href="#eq-diff_true_estimated_model">Equation&nbsp;<span>2.6</span></a> for the whole sample and dividing by <span class="math inline">\(N\)</span>:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><span id="eq-diff_true_estimated_mean"><span class="math display">\[
(\hat{\alpha} - \alpha) + (\hat{\beta} - \beta) \bar{x} - \bar{\epsilon} = 0
\tag{2.7}\]</span></span></p>
<p>Finally, subtracting <a href="#eq-diff_true_estimated_mean">Equation&nbsp;<span>2.7</span></a> from <a href="#eq-diff_true_estimated_model">Equation&nbsp;<span>2.6</span></a>:</p>
<p><span id="eq-diff_true_estimated_dev"><span class="math display">\[
(\hat{\beta} - \beta)(x_n - \bar{x}) + \hat{\epsilon}_n - (\epsilon_n - \bar{\epsilon}) = 0
\tag{2.8}\]</span></span></p>
<p>From <a href="#eq-diff_true_estimated_mean">Equation&nbsp;<span>2.7</span></a>, the variance of <span class="math inline">\(\hat{\alpha}\)</span> is the expected value of the square of:</p>
<p><span class="math display">\[
\begin{array}{rcl}
(\hat{\alpha} - \alpha) &amp;=&amp;  - (\hat{\beta} - \beta) \bar{x} +
\bar{\epsilon}\\
&amp;=&amp; -\sum_n c_n \epsilon_n \bar{x} + \bar{\epsilon} \\
&amp;=&amp; -\sum_n \left(\bar{x} c_n - \frac{1}{N}\right)\epsilon_n
\end{array}
\]</span> With the hypothesis of homoskedastic and uncorrelated errors, the variance simplifies to:</p>
<p><span class="math display">\[
\sigma_{\hat{\alpha}} ^ 2 = \sigma_\epsilon^2
\sum_n \left(\bar{x} ^ 2 c_n ^ 2 + \frac{1}{N ^ 2} -
\frac{2\bar{x}}{N} c_n\right)
\]</span></p>
<p>As <span class="math inline">\(\sum_n c_n = 0\)</span> and <span class="math inline">\(\sum_n c_n ^ 2 = \frac{1}{N \hat{\sigma}_x ^ 2}\)</span>, we finally get:</p>
<p><span class="math display">\[
\sigma_{\hat{\alpha}} ^ 2 = \frac{\sigma_\epsilon ^ 2}{N \hat{\sigma}_x ^
2}(\hat{\sigma}_x ^ 2 + \bar{x} ^ 2)
\]</span></p>
<p>Finally, to get the covariance between the slope and the intercept, we take the product of the two estimators in deviation from their expected values.</p>
<p><span class="math display">\[
(\hat{\alpha} - \alpha)(\hat{\beta} - \beta) =
-\left[\sum_n \left(\bar{x} c_n - \frac{1}{N}\right)\epsilon_n\right]
\left[\sum_n c_n \epsilon_n\right]
\]</span></p>
<p>Taking the expected value, we get:</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\alpha}\hat{\beta}} = - \sigma_\epsilon ^ 2 \sum_n
\left(\bar{x} c_n ^ 2  - \frac{1}{N} c_n\right) = -
\bar{x}\frac{\sigma_\epsilon ^ 2}{N \hat{\sigma}_x ^ 2}
\]</span></p>
<p>We can then compactly write the variances and the covariance of the OLS estimator in matrix form:</p>
<p><span id="eq-covariance_gamma"><span class="math display">\[
\left(
\begin{array}{cc}
\sigma_{\hat{\alpha}} ^ 2 &amp; \sigma_{\hat{\alpha}\hat{\beta}} \\
\sigma_{\hat{\alpha}\hat{\beta}} &amp; \sigma_{\hat{\beta}} ^ 2
\end{array}
\right)
=
\frac{\sigma_\epsilon ^ 2}{N\hat{\sigma}_x ^ 2}
\left(
\begin{array}{cc}
\bar{x} ^ 2 + \hat{\sigma}_x ^ 2 &amp; - \bar{x} \\
-\bar{x} &amp; 1
\end{array}
\right)
\tag{2.9}\]</span></span></p>
</section><section id="estimation-of-the-variance-of-the-errors" class="level4"><h4 class="anchored" data-anchor-id="estimation-of-the-variance-of-the-errors">Estimation of the variance of the errors</h4>
<p>The standard deviation of the OLS estimator can’t be computed because it depends on an unknown parameter <span class="math inline">\(\sigma_\epsilon\)</span>. To get an estimation of <span class="math inline">\(\sigma_{\hat{\beta}}\)</span>, we therefore need to estimate first <span class="math inline">\(\sigma_\epsilon\)</span>. If the error were observed, a natural estimator would be obtained by computing the empirical variance of the errors in the sample: <span class="math inline">\(\frac{1}{N}\sum_{n=1} ^ N (\epsilon_n -\bar{\epsilon}) ^ 2\)</span>. As the errors are not observed, this estimator cannot be computed, but a feasible estimator is obtained by replacing the unobserved errors by the residuals:</p>
<p><span class="math display">\[
\hat{\sigma}_\epsilon ^ 2 = \frac{\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2}{N}
\]</span></p>
<p>To analyze the properties of this estimator, we first compute the variance of one residual. From <a href="#eq-diff_true_estimated_dev">Equation&nbsp;<span>2.8</span></a>, a residual can be written as:</p>
<p><span class="math display">\[
\hat{\epsilon}_n = \epsilon_n - \bar{\epsilon}- (\hat{\beta} - \beta)(x_n - \bar{x})=
\epsilon_n - \frac{1}{N}\sum_n \epsilon_n - (x_n - \bar{x})\sum_n c_n\epsilon_n
\]</span></p>
<p>Taking the expected value of the square of this expression and noting that <span class="math inline">\(\mbox{E}\left(\bar{\epsilon} (\hat{\beta} - \beta) (x_n - \bar{x})\right) = \frac{1}{N}(x_n - \bar{x})\sigma_\epsilon ^ 2\sum_n c_n = 0\)</span>, we get the following variance:</p>
<p><span class="math display">\[
\mbox{V}(\hat{\epsilon}_n) = \sigma_\epsilon ^ 2 +
\frac{1}{N}\sigma_\epsilon ^ 2 + \frac{(x_n - \bar{x}) ^ 2}{S_{xx}}
\sigma_\epsilon ^ 2 -
2  \frac{1}{N}\sigma_\epsilon ^ 2 - 2  \frac{(x_n - \bar{x}) ^
2}{S_{xx}}\sigma_\epsilon ^ 2
\]</span></p>
<p>Re-arranging terms:</p>
<p><span class="math display">\[
\sigma_{\hat{\epsilon}_n}^2= \sigma_\epsilon ^ 2 \left(1 -
\frac{1}{N} -
\frac{(x_n- \bar{x}) ^
2}{S_{xx}}\right)
\]</span></p>
<p>Note that <span class="math inline">\(\sigma_{\hat{\epsilon}_n} &lt; \sigma_\epsilon\)</span>, which means that residuals are on average “smaller” than errors; this is a direct consequence of the fact that we minimize the sum of the squares of the residuals (see <a href="simple_regression.html#sec-geometry_ols"><span>Section&nbsp;1.4.2</span></a>). Summing for all the observations, we get the expected value of the sum of squares residuals:</p>
<p><span class="math display">\[
\mbox{E}\left(\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2\right) =
\sigma_{\epsilon} ^ 2 (N - 2)
\]</span></p>
<p>Therefore, the previously computed estimator of the variance of the errors <span class="math inline">\(\hat{\sigma}_\epsilon ^ 2\)</span> is biased:</p>
<p><span class="math display">\[
\mbox{E}(\hat{\sigma}_\epsilon ^ 2) =
\frac{\mbox{E}\left(\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2\right)}{N} = \sigma_\epsilon ^ 2 \frac{N-2}{N}
\]</span> More precisely, <span class="math inline">\(\hat{\sigma}_\epsilon\)</span> is downward biased, by a factor of <span class="math inline">\(\sqrt{\frac{N-2}{N}}\)</span>. For example, for <span class="math inline">\(N=10, 20, 100\)</span>, we get <span class="math inline">\(\sqrt{\frac{N-2}{N}} = 0.89, 0.95, 0.99\)</span>, which means a 11, 5, 1% downward bias for the estimated standard deviation. As the factor <span class="math inline">\(\frac{N-2}{N}\)</span> tends to 1 for <span class="math inline">\(N\rightarrow +\infty\)</span>, the bias will be negligible for large samples, but can be severe in small samples. We’ll from now denote <span class="math inline">\(\dot{\sigma}_\epsilon\)</span> the unbiased estimator:</p>
<p><span class="math display">\[
\dot{\sigma}_\epsilon = \sqrt{\frac{N}{N-2}}\hat{\sigma}_\epsilon
\]</span></p>
<p><span class="math inline">\(\dot{\sigma}_\epsilon\)</span> is often called the <strong>residual standard error</strong>. </p>
</section><section id="sec-exact_ols_distribution" class="level4"><h4 class="anchored" data-anchor-id="sec-exact_ols_distribution">Exact distribution of the OLS estimator with normal errors</h4>
<p> If the distribution of the errors is normal, as the OLS estimator is a linear combination of the errors, its exact distribution is also normal. Therefore:</p>
<p><span class="math display">\[
\hat{\beta}_N \sim \mathcal{N}\left(\beta, \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}\right)
\]</span></p>
<p>Subtracting the mean and dividing by the standard deviation, we get a standard normal deviate:</p>
<p><span class="math display">\[
\frac{\hat{\beta}-\beta}{\sigma_\epsilon / (\sqrt{N}\hat{\sigma}_x)} \sim \mathcal{N}(0, 1)
\]</span> <span class="math inline">\(\sigma_\epsilon\)</span> is unknown; replacing it by its unbiased estimator <span class="math inline">\(\dot{\sigma}_\epsilon\)</span> induces an increase of the uncertainty and the distribution changes from a normal to a Student t distribution:</p>
<p><span class="math display">\[
\frac{\hat{\beta}-\beta}{\dot{\sigma}_\epsilon / (\sqrt{N}\hat{\sigma}_x)} \sim t_{N-2}
\]</span></p>
<p>The Student distribution is symmetric, has fatter tails than the normal distribution and converges in distribution to the normal distribution. Actually, it is worth considering the Student and not the normal distribution as an approximation only for small-sized samples. For example, for sample sizes of 10, 20, 50 and 100, 5% critical values for a Student are 2.31, 2.1, 2.01 and 1.98, as the critical value is 1.96 for the normal distribution. </p>
</section><section id="computation-of-the-variance-of-the-ols-estimator-with-r" class="level4"><h4 class="anchored" data-anchor-id="computation-of-the-variance-of-the-ols-estimator-with-r">Computation of the variance of the OLS estimator with R</h4>
<p>We go back to the price-time model estimated in the previous chapter. Remember that the fitted model (called <code>pxt</code>) was obtained with the following code:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="va">price_time</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_names</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span> sr <span class="op">=</span> <span class="va">qr</span> <span class="op">/</span> <span class="op">(</span><span class="va">qr</span> <span class="op">+</span> <span class="va">qa</span><span class="op">)</span>,</span>
<span>         h <span class="op">=</span> <span class="op">(</span><span class="va">pa</span> <span class="op">-</span> <span class="va">pr</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="va">tr</span> <span class="op">-</span> <span class="va">ta</span><span class="op">)</span> <span class="op">/</span> <span class="fl">60</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">&lt;</span> <span class="fl">0.75</span><span class="op">)</span></span>
<span><span class="va">pxt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span>, <span class="va">prtime</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first compute “by hand” the standard error of the OLS estimator and we then use the relevant methods for <code>lm</code> objects to do so. We first extract the <span class="math inline">\(x\)</span> vector, its length <span class="math inline">\(N\)</span>, its mean <span class="math inline">\(\bar{x}\)</span> and its sample standard deviation <span class="math inline">\(\hat{\sigma}_x\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">pull</span><span class="op">(</span><span class="va">prtime</span>, <span class="va">h</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">bx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">sx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">bx</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then get the sum of square residuals, and the residual standard error:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">heps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="va">SSR</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">heps</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">seps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">SSR</span> <span class="op">/</span> <span class="op">(</span><span class="va">N</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">seps</span></span>
<span><span class="co">## [1] 0.07177</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which finally leads to the estimators of the standard deviation of the OLS coefficients:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sbeta</span> <span class="op">&lt;-</span> <span class="va">seps</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">*</span> <span class="va">sx</span><span class="op">)</span></span>
<span><span class="va">salpha</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">bx</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">sx</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="va">seps</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">*</span> <span class="va">sx</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">salpha</span>, <span class="va">sbeta</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.073262 0.003931</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>All this information can be retrieved easily with <strong>R</strong> using specific functions. To get the sample size and the number of degrees of freedom (which is, in the simple linear model, <span class="math inline">\(N-2\)</span>), we have:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="co">## [1] 9</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="co">## [1] 7</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="math inline">\(\dot{\sigma}_\epsilon\)</span> is computed using:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.07177</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The matrix of variance-covariance of the estimators is obtained using the <code>vcov</code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="co">##             (Intercept)          h</span></span>
<span><span class="co">## (Intercept)   0.0053673 -2.722e-04</span></span>
<span><span class="co">## h            -0.0002722  1.546e-05</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To get the standard deviations of the intercept and the slope estimators, we first extract the diagonal elements of this matrix and we next take the square roots of the values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt</span> <span class="op">%&gt;%</span> <span class="va">vcov</span> <span class="op">%&gt;%</span> <span class="va">diag</span> <span class="op">%&gt;%</span> <span class="va">sqrt</span></span>
<span><span class="co">## (Intercept)           h </span></span>
<span><span class="co">##    0.073262    0.003931</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>More simply, the <code><a href="https://rdrr.io/pkg/micsr/man/stder.html">micsr::stder</a></code> function can be used:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="sec-simple_ols_blue" class="level3"><h3 class="anchored" data-anchor-id="sec-simple_ols_blue">OLS estimator is BLUE</h3>
<p> We have seen previously that the OLS estimator is a linear estimator (i.e., it is a linear combination of the <span class="math inline">\(N\)</span> values of <span class="math inline">\(y\)</span> for the sample):</p>
<p><span class="math display">\[
\hat{\beta} = \sum_{n=1} ^ N \left(\frac{ (x_n - \bar{x})}
{\sum_{n=1} ^ N  (x_n - \bar{x}) ^ 2} \right) y_n =
\sum_{n=1} ^ N c_n y_n
\]</span></p>
<p>Moreover, we have seen that if <span class="math inline">\(\mbox{E}(\epsilon \mid x) = 0\)</span>, it is unbiased and, with the hypothesis of homoskedastic and uncorrelated errors, we have established that its variance is: <span class="math inline">\(\sigma_\epsilon ^ 2 / S_{xx}\)</span>. We’ll show in this subsection that among all the linear unbiased estimators, the OLS estimator is the one with the smallest variance. For these reasons, the OLS estimator is the <strong>best linear unbiased estimator</strong> (<strong>BLUE</strong>).</p>
<section id="comparing-ols-with-other-linear-unbiased-estimators" class="level4"><h4 class="anchored" data-anchor-id="comparing-ols-with-other-linear-unbiased-estimators">Comparing OLS with other linear unbiased estimators</h4>
<p>Consider another linear estimator, with weights <span class="math inline">\(a_n\)</span>:</p>
<p><span class="math display">\[
\tilde{\beta} =  \sum_{n=1} ^ N a_n y_n
\]</span></p>
<p>Replacing <span class="math inline">\(y_n\)</span> by <span class="math inline">\(\alpha + \beta x_n + \epsilon_n\)</span>, we have:</p>
<p><span class="math display">\[
\tilde{\beta} = \sum_{n=1} ^ N \ a_n (\alpha + \beta x_n + \epsilon_n)
= \alpha \sum_{n=1} ^ N  a_n +
\beta \sum_{n=1} ^ N  a_n x_n +
\sum_{n=1} ^ N  a_n \epsilon_n
\]</span></p>
<p>Therefore, for any unbiased estimator, one must have <span class="math inline">\(\sum_{n=1} ^ N a_n= 0\)</span> and <span class="math inline">\(\sum_{n=1} ^ N a_n x_n = 1\)</span>.</p>
<p>We then have: <span class="math inline">\(\tilde{\beta} - \beta = \sum_{n=1} ^ N a_n \epsilon_n\)</span> and the variance of <span class="math inline">\(\tilde{\beta}\)</span> is:</p>
<p><span class="math display">\[
\sigma_{\tilde{\beta}} ^ 2 =
\mbox{E} \left(\left[\sum_{n=1} ^ N a_n
\epsilon_n\right] ^ 2\right) =
\sigma_\epsilon ^ 2 \sum_{n=1} ^ N a_n ^ 2
\]</span></p>
<p>defining <span class="math inline">\(d_n = a_n - c_n\)</span>, we have:</p>
<p><span class="math display">\[
\sum_{n=1} ^ N  a_n ^ 2 =
\sum_{n=1} ^ N  (c_n + d_n) ^ 2 =
\sum_{n=1} ^ N c_n ^ 2 +
\sum_{n=1} ^ N d_n ^ 2 +
2 \sum_{n=1} ^ N d_n c_n
\]</span></p>
<p>But the last term is 0 because:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\sum_{n=1} ^ N d_n c_n
&amp;= &amp; \sum_{n=1} ^ N (a_n - c_n) c_n \\
&amp;=&amp; \frac{1}{S_{xx}}\sum_{n=1} ^ N a_n x_n -
\frac{1}{S_{xx}} \bar{x} \sum_{n=1} a_n - \sum_{n=1} ^ N c_n ^ 2 \\
&amp;=&amp;0
\end{array}
\]</span></p>
<p>so that <span class="math inline">\(\sum_{n=1} ^ N a_n ^ 2 = \sum_{n=1}^N c_n ^ 2 + \sum_{n=1}^N d_n ^ 2\)</span> and:</p>
<p><span class="math display">\[
\sigma_{\tilde{\beta}}^2 = \sigma_\epsilon ^ 2
\sum_{n=1} ^ N a_n ^ 2 =
\sigma_\epsilon ^ 2 \left( \sum_{n=1}^N c_n ^ 2 + \sum_{n=1}^N d_n ^ 2\right) =
\sigma_{\hat{\beta}}^2 + \sigma_\epsilon ^ 2 \sum_{n=1}^N d_n ^ 2
\]</span></p>
<p>Therefore, <span class="math inline">\(\sigma_{\tilde{\beta}}^2 &gt; \sigma_{\hat{\beta}}^2\)</span>, which means that the OLS estimator is <strong>BLUE</strong>, i.e., it is, among all the unbiased linear estimators, the one with the lower variance.</p>
</section><section id="sec-remove_intercept" class="level4"><h4 class="anchored" data-anchor-id="sec-remove_intercept">Practical example</h4>
<p>Consider as an example the price-time model. The model we have previously estimated is:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span>, <span class="va">prtime</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Consider now the same model without intercept (<span class="math inline">\(\alpha = 0\)</span>). As <span class="math inline">\(\alpha = - a / (b - a)\)</span>, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> being respectively the minimal and the maximum time value, <span class="math inline">\(\alpha = 0\)</span> implies that the minimal time value is 0. To fit the model that imposes this hypothesis, we need to fit the same model without intercept. In <strong>R</strong>, this is performed using either <code>- 1</code> or <code>+ 0</code> in the formula :</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span> <span class="op">-</span> <span class="fl">1</span>, <span class="va">prtime</span><span class="op">)</span></span>
<span><span class="va">pxt2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span> <span class="op">+</span> <span class="fl">0</span>, <span class="va">prtime</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The same model can also be estimated by updating the previous fitted model <code>pxt</code>, using the <code>update</code> function which takes as first argument the model we wish to update:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">pxt</span>, <span class="va">.</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">pxt2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">pxt</span>, <span class="va">.</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The formula is updated using <code>.</code>, which means the same thing as in the initial model. Therefore, <code>. ~ .</code> means the initial formula and we remove the intercept by either “adding” <code>0</code> or “subtracting” <code>1</code>.</p>
<p>The fitted model is presented in <a href="#fig-nointercept">Figure&nbsp;<span>2.6</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nointercept" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-nointercept-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.6: OLS estimator without intercept</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For this model without intercept, the formula for the slope is:</p>
<p><span class="math display">\[
\hat{\beta} = \frac{\sum_{n=1}^N x_n y_n}{\sum_{n=1}^N x_n ^ 2}
\]</span></p>
<p>Or, replacing <span class="math inline">\(y_n\)</span> by <span class="math inline">\(\beta x_n + \epsilon_n\)</span> :</p>
<p><span class="math display">\[
\hat{\beta} = \beta + \frac{\sum_{n=1}^N x_n \epsilon_n}{\sum_{n=1}^N x_n ^ 2}
\]</span></p>
<p>for which the variance is:</p>
<p><span class="math display">\[
\sigma_{\hat{\beta}} ^ 2 = \frac{\sigma_\epsilon ^ 2}{\sum_{n=1}^N x_n ^ 2} =
\frac{\sigma_\epsilon ^ 2}{N A_{x ^ 2}}
\]</span></p>
<p>where <span class="math inline">\(A_{x ^ 2}\)</span> is the arithmetic mean of the squares of <span class="math inline">\(x\)</span>.</p>
<p>An alternative estimation method consists of drawing lines from every point to the origin, as illustrated in <a href="#fig-indslopes">Figure&nbsp;<span>2.7</span></a>, and to estimate <span class="math inline">\(\beta\)</span> by the arithmetic mean of the <span class="math inline">\(N\)</span> slopes, which are <span class="math inline">\(y_n / x_n\)</span>. Formally, we have:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-indslopes" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-indslopes-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.7: Individual slopes</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="math display">\[
\tilde{\beta}=\frac{1}{N}\sum_{n = 1} ^ N \frac{y_n}{x_n}
\]</span></p>
<p>This is a linear estimator, with weights <span class="math inline">\(a_n = \frac{1}{N}\frac{1}{x_n}\)</span>. Replacing <span class="math inline">\(y_n\)</span> by <span class="math inline">\(\beta x_n + \epsilon_n\)</span>, we get :</p>
<p><span class="math display">\[
\tilde{\beta}= \beta + \frac{1}{N}\sum_{n = 1} ^ N \frac{\epsilon_n}{x_n}
\]</span></p>
<p>This linear estimator is therefore unbiased. Its variance is:</p>
<p><span class="math display">\[
\sigma_{\tilde{\beta}} = \frac{\sigma_\epsilon ^ 2}{N ^ 2}\sum_{n =
1}^N \frac{1}{x_n ^ 2} = \frac{\sigma_\epsilon ^ 2}{N H_{x ^ 2}}
\]</span></p>
<p>where <span class="math inline">\(H_{x ^ 2} = \frac{N}{\sum_{n=1}^N\frac{1}{x_n ^ 2}}\)</span> is the harmonic mean of <span class="math inline">\(x ^ 2\)</span>. As the harmonic mean is always lower than the arithmetic mean, <span class="math inline">\(\sigma_{\tilde{\beta}} &gt; \sigma_{\hat{\beta}}\)</span> and therefore <span class="math inline">\(\tilde{\beta}\)</span> is less precise than <span class="math inline">\(\hat{\beta}\)</span>. The value of this alternative estimator can be computed as follow:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slope</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">transmute</span><span class="op">(</span>slope <span class="op">=</span> <span class="va">sr</span> <span class="op">/</span> <span class="va">h</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">summarise</span><span class="op">(</span>slope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">slope</span><span class="op">)</span></span>
<span><span class="va">slope</span></span>
<span><span class="co">## [1] 0.02277</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once the estimator is computed, we can calculate <span class="math inline">\(\hat{\sigma}_\epsilon\)</span> and <span class="math inline">\(\hat{\sigma}_{\tilde{\beta}}\)</span> and, as intermediate results, the arithmetic and the harmonic means of <span class="math inline">\(x ^ 2\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">reg2</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>resid <span class="op">=</span> <span class="va">sr</span> <span class="op">-</span> <span class="va">slope</span> <span class="op">*</span> <span class="va">h</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span>seps <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">resid</span> <span class="op">^</span>  <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fl">8</span><span class="op">)</span>, </span>
<span>            H <span class="op">=</span> <span class="fl">9</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">h</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>, </span>
<span>            A <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">h</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fl">9</span>, </span>
<span>            sdtilde <span class="op">=</span> <span class="va">seps</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">reg2</span></span>
<span><span class="co">## # A tibble: 1 × 4</span></span>
<span><span class="co">##     seps     H     A sdtilde</span></span>
<span><span class="co">##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span><span class="co">## 1 0.0678  204.  347. 0.00158</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We check that the harmonic mean (204) is lower than the arithmetic mean (347). Comparing with the OLS results:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt2</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="co">##        h </span></span>
<span><span class="co">## 0.001205</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>we confirm that the OLS estimator has a lower standard error than the alternative estimator.</p>
<p></p>
</section></section></section><section id="sec-asymp_prop_ols" class="level2" data-number="2.2"><h2 data-number="2.2" class="anchored" data-anchor-id="sec-asymp_prop_ols">
<span class="header-section-number">2.2</span> Asymptotic properties of the estimator</h2>
<!-- !!! consist on mal dit -->
<p> </p>
<p>Asymptotic properties of an estimator deal with the behavior of this estimator when the sample size increases without bound. Compared to exact properties which are true and hold whatever the sample size is, asymptotic properties are approximations, the better the larger the sample size is. Two notions of convergence, that rely on two fundamental theorems are used:</p>
<ul>
<li>the convergence in probability, based on the <strong>law of large numbers</strong>,</li>
<li>the convergence in distribution, based on the <strong>central-limit</strong> theorem.</li>
</ul>
<p> </p>
<section id="convergence-in-probability" class="level3"><h3 class="anchored" data-anchor-id="convergence-in-probability">Convergence in probability</h3>
<p>We consider an estimator as a sequence of random numbers, indexed by the size of the sample on which it has been estimated: <span class="math inline">\(\left\{\hat{\beta}_N\right\}\)</span>. This sequence converges in probability to a constant <span class="math inline">\(\theta\)</span> if:</p>
<p><span class="math display">\[
\lim_{N\rightarrow \infty} \mbox{P}(\mid \hat{\beta}_N - \theta\mid &gt;
\nu) = 0 \;\forall \nu
\]</span></p>
<p>This is denoted by: <span class="math inline">\(\hat{\beta}_N \xrightarrow{p} \theta \;\mbox{ or } \;\mbox{plim}\,\hat{\beta} = \theta\)</span>. Convergence in probability implies convergence in mean square, which is defined by: </p>
<p><span class="math display">\[
\lim_{N\rightarrow + \infty} \mbox{E}\left( (\hat{\beta}_N - \theta) ^
2\right) = 0
\]</span></p>
<p>and means that:</p>
<p><span class="math display">\[
\left\{
\begin{array}{l}
\lim_{N\rightarrow  + \infty} \mbox{E}(\hat{\beta}_N) = \theta \\
\lim_{N\rightarrow  + \infty} V(\hat{\beta}_N) = 0 \\
\end{array}
\right.
\]</span></p>
<p>If an estimator converges in mean square to its true value <span class="math inline">\(\beta\)</span>, we’ll write <span class="math inline">\(\hat{\beta}_N \xrightarrow{\mbox{m.s.}} \beta\)</span> and we’ll also use <span class="math inline">\(\mbox{plim} \, \hat{\beta}_N = \beta\)</span>, as convergence in mean squares implies convergence in probability.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> We’ll also say in this case that the estimator is consistent. Note that, on the opposite, an estimator may be inconsistent for two reasons:</p>
<ul>
<li>the estimator doesn’t converge in probability to any value,</li>
<li>the estimator converges in probability to <span class="math inline">\(\theta \neq \beta\)</span>.</li>
</ul>
<p> The consistency of an estimator shouldn’t be confused with the property of unbiasedness, even if we often encounter estimators which are unbiased <em>and</em> consistent:</p>
<ul>
<li>unbiasedness is an exact property (true or false whatever the sample size), and it refers only to the expected value of the estimator and doesn’t say anything about its variance,</li>
<li>consistency is an asymptotic property, which implies a limit for the expected value (<span class="math inline">\(\beta\)</span>) and for the variance (0) of the estimator.</li>
</ul>
<p>Therefore, an unbiased estimator can be inconsistent and, conversely, a consistent estimator can be biased. Consider for example that we have a random sample of N observations of a variable <span class="math inline">\(x\)</span> which has a mean and a variance equal respectively to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. A natural estimator of <span class="math inline">\(\mu\)</span> is the arithmetic mean: <span class="math inline">\(\bar{x}_N = \frac{1}{N} \sum_{n=1}^N x_n\)</span>, with expected value and variance:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\mbox{E}(\bar{x}_N) &amp;=&amp; \mbox{E}\left(\frac{1}{N} \sum_{n=1}^N x_n\right) =
\frac{1}{N}\sum_{n=1}^N \mbox{E}(x_n)=\frac{1}{N}\sum_{n=1}^N \mu =
\mu \\
\mbox{V}(\bar{x}_N) &amp;=&amp; \mbox{V}\left(\frac{1}{N} \sum_{n=1}^N x_n\right) =
\frac{1}{N ^ 2}\sum_{n=1}^N \mbox{V}(x_n)=\frac{1}{N^2}\sum_{n=1}^N \sigma^2 =
\frac{\sigma ^ 2}{N}
\end{array}
\right.
\]</span></p>
<p>This estimator is unbiased and consistent (the variance tends to 0 and the expected value is equal to the population mean <span class="math inline">\(\mu\)</span>). Consider now two alternative estimators.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> The first one is:</p>
<p><span class="math display">\[
\dot{x}_N = \frac{1}{N - 1} \sum_{n=1}^N x_n
\]</span></p>
<p>Its first two moments can easily be obtained by writing <span class="math inline">\(\dot{x}_N\)</span> as a function of <span class="math inline">\(\bar{x}_N\)</span>: <span class="math inline">\(\dot{x}_N = \frac{N}{N-1} \bar{x}_N\)</span>, so that <span class="math inline">\(\mbox{E}(\dot{x}_N) = \frac{N}{N-1} \mu\)</span> and <span class="math inline">\(\mbox{V}(\dot{x}_N) = \left(\frac{N}{N-1}\right) ^ 2 \frac{\sigma^ 2}{N}\)</span>. The estimator is upward biased, by a multiplicative factor of <span class="math inline">\(\frac{N}{N-1}\)</span>. The bias is severe in small samples (for example 25% if <span class="math inline">\(N\)</span> is equal to 5), but becomes negligible as <span class="math inline">\(N\)</span> grows. As the variance tends to 0 and the expected value to <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\dot{x}_N\)</span> is consistent.</p>
<p>The second estimator is:</p>
<p><span class="math display">\[
\tilde{x}_N = \frac{1}{2} x_1 + \frac{1}{2} \frac{1}{N - 1} \sum_{n=2}^N x_n
\]</span></p>
<p>It consists of first taking the mean for the whole sample except the first observation and then taking the simple average between it and the first observation: <span class="math inline">\(\tilde{x}_N = \frac{1}{2} x_1 + \frac{1}{2} \bar{x}_{N-1}\)</span>. It is unbiased, as:</p>
<p><span class="math display">\[\mbox{E}(\tilde{x}_N) = \frac{1}{2}
\mbox{E}(x_1) + \frac{1}{2} \mbox{E}(\bar{x}_{N-1})=\frac{1}{2}\mu +
\frac{1}{2}\mu = \mu
\]</span></p>
<p>and the variance is: <span class="math display">\[\mbox{V}(\tilde{x}_N) = \frac{1}{4}
\mbox{V}(x_1) + \frac{1}{4} \mbox{V}(\bar{x}_{N-1})=
\frac{1}{4}\sigma^2 + \frac{1}{4}\frac{\sigma^2}{N - 1}
\]</span></p>
<p>which tends to <span class="math inline">\(\frac{1}{4}\sigma^2\)</span> as <span class="math inline">\(N \rightarrow +\infty\)</span>. Therefore, this unbiased estimator is not consistent. The problem is that the weight of the first observation is constant and, therefore, the value obtained for <span class="math inline">\(x_1\)</span> influences the estimator, whatever the size of the sample.</p>
<p>The OLS estimator writes:</p>
<p><span id="eq-beta_N"><span class="math display">\[
\hat{\beta}_N = \beta + \frac{\sum_{n=1} ^ N (x_n - \bar{x})
\epsilon_n}{N \hat{\sigma}_x ^ 2} = \beta +
\frac{\hat{\sigma}_{x\epsilon}}{\hat{\sigma}_x ^ 2}
\tag{2.10}\]</span></span></p>
<p>where <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\hat{\sigma}_x ^ 2\)</span> and <span class="math inline">\(\hat{\sigma}_{x\epsilon}\)</span> are the sample estimates of the population mean and variance of <span class="math inline">\(x\)</span> and of the covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(\epsilon\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> As the sample size increases, these three estimators converge to their population counterpart, namely <span class="math inline">\(\mu_x = \mbox{E}(x)\)</span>, <span class="math inline">\(\sigma_x^2 = \mbox{V}(x)\)</span> and <span class="math inline">\(\sigma_{x\epsilon}=\mbox{cov}(x, \epsilon)\)</span>. We therefore have:</p>
<p><span class="math display">\[
\mbox{plim}\,\hat{\beta}_N = \beta +
\frac{\sigma_{x\epsilon}}{\sigma_x ^2} = \theta
\]</span></p>
<p><span class="math inline">\(\theta\)</span> equals <span class="math inline">\(\beta\)</span>, in which case the estimator is consistent, if <span class="math inline">\(x\)</span> is uncorrelated in the population with <span class="math inline">\(\epsilon\)</span> (<span class="math inline">\(\sigma_{x\epsilon}=0\)</span>). </p>
</section><section id="sec-clt" class="level3"><h3 class="anchored" data-anchor-id="sec-clt">Convergence in distribution: central-limit theorem</h3>
<p> </p>
<p>When <span class="math inline">\(N\rightarrow +\infty\)</span>, <span class="math inline">\(\hat{\beta}_N\)</span> has a degenerate distribution, as it converges to a constant (which is <span class="math inline">\(\beta\)</span> if the estimator is consistent) as its variance tends to 0. In this subsection, we seek to analyze the shape of the distribution of <span class="math inline">\(\hat{\beta}\)</span> as the sample size grows. We therefore need to consider a transformation of <span class="math inline">\(\hat{\beta}_N\)</span> which has a constant variance, and we’ll see that it is <span class="math inline">\(\sqrt{N}(\hat{\beta} - \beta)\)</span>. Starting again with the equation that relates <span class="math inline">\(\hat{\beta}_N\)</span> to the errors and defining <span class="math inline">\(w_n = \frac{x_n - \bar{x}}{\sqrt{N}\hat{\sigma}_x}\)</span>, we have:</p>
<p><span class="math display">\[
\hat{\beta}_N = \beta + \sum_{n=1}^N c_n \epsilon_n = \beta + \frac{\sum_{n=1}^N w_n \epsilon_n}{\sqrt{N}\hat{\sigma}_x}
\]</span></p>
<p>Note that <span class="math inline">\(w_n\)</span> sums to 0 (as <span class="math inline">\(c_n\)</span>), but that <span class="math inline">\(\sum_{n=1}^ Nw_n ^ 2=1\)</span>. Subtracting <span class="math inline">\(\beta\)</span> and multiplying by <span class="math inline">\(\sqrt{N}\)</span>, we get:</p>
<p><span class="math display">\[
z = \sqrt{N}(\hat{\beta}_N - \beta) = \sum_{n=1}^N w_n \frac{\epsilon_n}{\hat{\sigma}_x}
\]</span></p>
<p>The distribution of <span class="math inline">\(\sqrt{N}(\hat{\beta}_N - \beta)\)</span> is the distribution of a linear combination of <span class="math inline">\(N\)</span> random deviates <span class="math inline">\(\epsilon_n / \hat{\sigma}_x\)</span>, with an unknown distribution, a 0 expected value and a standard deviation equal to <span class="math inline">\(\sigma_\epsilon / \hat{\sigma}_x\)</span>. The first two moments of <span class="math inline">\(z = \sqrt{N}(\hat{\beta}_N - \beta)\)</span> don’t depend on <span class="math inline">\(N\)</span> (<span class="math inline">\(\mbox{E}(z)=0\)</span> and <span class="math inline">\(\mbox{V}(z) = \sum_n \omega_n ^ 2 \sigma_\epsilon ^ 2/ \hat{\sigma}_x ^ 2 = \sigma_\epsilon ^ 2/ \hat{\sigma}_x ^ 2\)</span>). As <span class="math inline">\(N\)</span> tends to infinity, the distribution of <span class="math inline">\(\sqrt{N}(\hat{\beta}_N - \beta)\)</span> still has a 0 expected value and a standard deviation equals to <span class="math inline">\(\sigma_\epsilon/\hat{\sigma}_x\)</span>. The central-limit theorem states that the distribution of <span class="math inline">\(\sqrt{N}(\hat{\beta}_N - \beta)\)</span> <strong>converges in distribution</strong> to a normal distribution as <span class="math inline">\(N\)</span> tends to infinity, whatever the distribution of <span class="math inline">\(\epsilon\)</span>. This is denoted by:</p>
<p><span class="math display">\[
\sqrt{N}(\hat{\beta}_N - \beta) \xrightarrow{d} \mathcal{N}\left(0, \frac{\sigma_\epsilon}{\hat{\sigma}_x}\right)
\]</span> </p>
<p>Stated differently, the <strong>asymptotic distribution</strong> of <span class="math inline">\(\hat{\beta}\)</span> is a normal distribution with an expected value equal to <span class="math inline">\(\beta\)</span> and a standard deviation equal to <span class="math inline">\(\frac{\sigma_{\epsilon}}{\sqrt{N} \hat{\sigma}_x}\)</span>:</p>
<p><span class="math display">\[
\hat{\beta}_N \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}\right)
\]</span></p>
<p>To illustrate the strength of the central-limit theorem, consider the simple case of the arithmetic mean of <span class="math inline">\(N\)</span> independent random numbers with an expected value equal to 0 and a standard deviation equal to 1: <span class="math inline">\(\bar{x}_n = \frac{\sum_{n=1} x_n}{N}\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="math inline">\(\bar{x}_n\)</span> has a 0 expected value and a variance equal to <span class="math inline">\(1/N\)</span>. As we already now, <span class="math inline">\(\bar{x}_n\)</span> converges in probability to 0 and has therefore a degenerate distribution. Consider now:</p>
<p><span class="math display">\[
z_N = \sqrt{N} \bar{x}_n = \frac{\sum_{n=1} x_n}{\sqrt{N}}
\]</span></p>
<p>The expected value of <span class="math inline">\(z_N\)</span> is still 0, but its standard deviation is now 1. Its third moment is:</p>
<p><span class="math display">\[
E(z_N^3) = \frac{\mbox{E}\left((\sum_{n=1}^N x_n) ^ 3\right)}{N^{3/2}}
\]</span></p>
<p>Developing the sum for <span class="math inline">\(N=3\)</span>, we have:</p>
<p><span class="math display">\[
\left(\sum_{n=1}^3 x_n\right) ^ 3 =(x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 + 2 x_1 x_2 +
2x_1 x_3 + 2 x_2 x_3)(x_1 + x_2 + x_3)
\]</span></p>
<p>Taking the expected value of this sum, we get terms like:</p>
<ul>
<li>
<span class="math inline">\(\mbox{E}(x_n x_m^2) = \mbox{E}(x_n)\mbox{V}(x_m^2)=0 \times 1 = 0\)</span>,</li>
<li>
<span class="math inline">\(\mbox{E}(x_n, x_m, x_l) = \mbox{E}(x_n)\mbox{E}(x_m)\mbox{E}(x_l)= 0 \times 0 \times 0 = 0\)</span>,</li>
<li>
<span class="math inline">\(\mbox{E}(x_n^3) = \mu_3\)</span>.</li>
</ul>
<p>Therefore, only the last category of terms remains while taking the expected value of the sum. As we have <span class="math inline">\(N\)</span> of them, the third moment of <span class="math inline">\(z_N\)</span> is therefore:</p>
<p><span class="math display">\[
E(z_N^3) = \frac{N \mu_3}{N^{3/2}} = \frac{\mu_3}{\sqrt{N}}
\]</span></p>
<p>Therefore, as <span class="math inline">\(N\)</span> tends to infinity, <span class="math inline">\(E(z_N^3)\)</span> tends to 0, whatever the value of <span class="math inline">\(\mu_3\)</span>.</p>
<p>Consider now the fourth moment:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\left(\sum_{n=1}^3 x_n\right) ^ 4 &amp;=&amp;
(x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 + 2 x_1 x_2 + 2x_1 x_3 + 2 x_2 x_3) \\
&amp;\times&amp;
(x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 + 2 x_1 x_2 + 2x_1 x_3 + 2 x_2 x_3)
\end{array}
\]</span></p>
<ul>
<li>terms like <span class="math inline">\(x_n x_m ^ 3\)</span>, <span class="math inline">\(x_n x_m x_l ^ 2\)</span> and <span class="math inline">\(x_n x_m x_l x_p\)</span> have zero expected values,</li>
<li>terms like <span class="math inline">\(x_n ^ 2 x_m ^ 2\)</span> have an expected value of <span class="math inline">\(1 \times 1 = 1\)</span>. For <span class="math inline">\(N=3\)</span>, there are 18 of them and, more generally, for a given value of <span class="math inline">\(N\)</span>, there are <span class="math inline">\(3 N (N - 1)\)</span> of them,</li>
<li>terms like <span class="math inline">\(x_n ^ 4\)</span> have an expected value of <span class="math inline">\(\mu_4\)</span> and there are <span class="math inline">\(N\)</span> of them.</li>
</ul>
<p>Therefore:</p>
<p><span class="math display">\[
E(z_N^4) = \frac{3N(N-1) + N \mu_4}{N^2} = 3 \frac{N-1}{N} + \frac{\mu_4}{N}
\]</span></p>
<p>which tends to 3, as <span class="math inline">\(N\)</span> tends to infinity.</p>
<p>Therefore, for “large” <span class="math inline">\(N\)</span>, the distribution of <span class="math inline">\(z_N\)</span> doesn’t depend on the shape parameters of <span class="math inline">\(x_n\)</span> (<span class="math inline">\(\mu_3\)</span> and <span class="math inline">\(\mu_4\)</span>) and its third and fourth moments tend to 0 and 3, which are the corresponding values for a normal distribution. These reasonings can easily be extended to higher moments, the general conclusion being that, when <span class="math inline">\(N\)</span> tends to infinity, all the moments of <span class="math inline">\(z_N\)</span> tend to those of the normal distribution. The asymptotic distribution of <span class="math inline">\(z_N\)</span> is therefore normal and doesn’t depend on the characteristics of the distribution of <span class="math inline">\(x_N\)</span>.</p>
<p> </p>
</section><section id="simulations" class="level3"><h3 class="anchored" data-anchor-id="simulations">Simulations</h3>
<p> The law of large numbers and the central-limit theorem can be interestingly illustrated using simulations. Consider errors that follow a standardized chi-square distribution with one degree of freedom. Remember that a chi-squared with one degree of freedom is simply the square of a standard normal deviate: <span class="math inline">\(x = z ^ 2\)</span>. We thus have <span class="math inline">\(\mbox{E}(x) = \mbox{E}(z ^ 2) = \mbox{V}(z) = 1\)</span> and:</p>
<p><span class="math display">\[
\mbox{V}(x) = \mbox{E}\left((x - 1) ^ 2\right)=
\mbox{E}\left(z ^ 4\right) - 1 = 3 - 1 = 2
\]</span></p>
<p>Therefore, <span class="math inline">\(v = \frac{x-1}{\sqrt{2}}\)</span> has a zero expected value and a standard deviation equal to 1. One can show that its third and fourth centered moments are <span class="math inline">\(2\sqrt{2}\)</span> and <span class="math inline">\(15\)</span>. Therefore, the distribution is:</p>
<ul>
<li>highly asymmetric, i.e., it has a long tail on the right side of the distribution, and a negative median, which is lower than the mean (equal to zero),</li>
<li>highly leptokurtic, the fourth moment (15) is much larger than the value of 3 of the normal distribution; it has therefore a much higher mode and fatter tails than a normal distribution.</li>
</ul>
<p>Now, going back to the <code>prtime</code> data, we generate a sample using the following DGP:</p>
<p><span class="math display">\[
y_n = \alpha + \beta x_n + \epsilon_n
\]</span></p>
<p>with <span class="math inline">\(\alpha = -0.2\)</span>, <span class="math inline">\(\beta = 0.032\)</span> and: <span class="math inline">\(\epsilon_n = \sigma_\epsilon \frac{z_n ^ 2 - 1}{\sqrt{2}}\)</span>, where <span class="math inline">\(\sigma_\epsilon = 0.08\)</span> and <span class="math inline">\(z_n\)</span> is a random draw on a standard normal distribution.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">h</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">asmpl</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>h <span class="op">=</span> <span class="va">x</span>,</span>
<span>                eps <span class="op">=</span> <span class="va">seps</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>                sr <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">h</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span>
<span><span class="va">v</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span>, <span class="va">asmpl</span><span class="op">)</span></span>
<span><span class="va">v</span> <span class="op">%&gt;%</span> <span class="va">residuals</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">##     1     2     3     4     5     6     7     8     9 </span></span>
<span><span class="co">## -0.01 -0.03  0.01  0.10 -0.03  0.00 -0.02 -0.01 -0.02</span></span>
<span><span class="va">v</span> <span class="op">%&gt;%</span> <span class="va">residuals</span> <span class="op">%&gt;%</span> <span class="va">sum</span></span>
<span><span class="co">## [1] -1.041e-17</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The sum of the residuals is still equal to zero, but we can see in <a href="#fig-smpls4hbeta">Figure&nbsp;<span>2.8</span></a>, where a scatterplot is drawn for four random samples that the distribution of the errors (and therefore the distribution of the residuals) is highly asymmetric (we have only a couple of positive values, some of them being very large).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-smpls4hbeta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-smpls4hbeta-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.8: Four samples with <span class="math inline">\(\chi^2\)</span> errors</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We then generate a large number of samples and for each of them, we compute the estimator of the slope and we plot the empirical distribution of <span class="math inline">\(\hat{\beta}\)</span> using a histogram. We consider different sample sizes; we use the “repeated in fixed sample” hypothesis,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> i.e., we increase the size of the sample by duplicating the same values of <span class="math inline">\(x\)</span>. The histograms are presented in <a href="#fig-empdisthbeta">Figure&nbsp;<span>2.9</span></a>, along with the normal density curve. The distribution of <span class="math inline">\(\hat{\beta}\)</span> is centered on <span class="math inline">\(\beta\)</span>, whatever the sample size, which illustrates the fact that the estimator is unbiased. As the sample size is growing, we can see two changes in the shape of the histogram:</p>
<ul>
<li>it is more and more concentrated around the mean value of <span class="math inline">\(\hat{\beta}\)</span>, which is due to the fact that the standard deviation of <span class="math inline">\(\hat{\beta}\)</span> is inversely proportional to sample size,</li>
<li>the adjustment by the normal density curve is very bad in small samples; especially, the distribution of the estimator is highly leptokurtic, but the adjustment gets much better for larger samples.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-empdisthbeta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-empdisthbeta-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.9: Empirical distribution of <span class="math inline">\(\hat{\beta}\)</span> for different sample sizes and adjustment by a normal density</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Next, we plot in <a href="#fig-empdisttheta">Figure&nbsp;<span>2.10</span></a> the distribution of <span class="math inline">\(\sqrt{N}(\hat{\beta}-\beta)\)</span>, which has constant mean and standard deviation (respectively 0 and <span class="math inline">\(\sigma_\epsilon/\sigma_x)\)</span>. Therefore, only the shape of the distribution changes when the sample size increases. We can see more precisely on this figure the strength of the central-limit theorem, even for errors that follow a distribution very different from the normal.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-empdisttheta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-empdisttheta-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.10: Empirical distribution of <span class="math inline">\(\sqrt{N}(\hat{\beta}-\beta)\)</span> for different sample sizes and adjustment by a normal density</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p> </p>
<div style="page-break-after: always;"></div>
</section></section><section id="sec-confint_test_slm" class="level2" data-number="2.3"><h2 data-number="2.3" class="anchored" data-anchor-id="sec-confint_test_slm">
<span class="header-section-number">2.3</span> Confidence interval and tests</h2>
<p>With the set of hypotheses we have made concerning the errors of the model, the distribution of the estimator is completely defined by:</p>
<p><span class="math display">\[
\hat{\beta}_N \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_{\epsilon}}{\sqrt{N}\hat{\sigma}_x}\right)
\]</span></p>
<p>where <span class="math inline">\(\stackrel{a}{\sim}\)</span> means that the normal distribution is asymptotic and is actually a very good approximation if the sample size is large enough (which is the case in general in microeconometrics studies), whatever the distribution of the errors. Moreover, as <span class="math inline">\(\hat{\beta}_N = \beta + \sum_{n=1}^N c_n \epsilon_n\)</span> (the estimator is a linear combination of the errors), if the errors are normal, then the distribution of <span class="math inline">\(\hat{\beta}\)</span> is <strong>exactly normal</strong> (see <a href="#sec-exact_ols_distribution"><span>Section&nbsp;2.1.3.5</span></a>). Removing from <span class="math inline">\(\hat{\beta}\)</span> its expected value and dividing by its standard deviation, we get a standard normal variable:</p>
<p><span class="math display">\[
\frac{\hat{\beta}_N-\beta}{\sigma_{\hat{\beta}_N}}=
\frac{\sqrt{N}\hat{\sigma}_x}{\sigma_\epsilon}(\hat{\beta}_N-\beta)
\stackrel{a}{\sim}
\mathcal{N}(0, 1)
\]</span></p>
<p>This result enables to perform two tasks:</p>
<ul>
<li>testing hypothesis,</li>
<li>constructing a confidence interval, either for the coefficients or for the predictions of the model.</li>
</ul>
<section id="testing-hypothesis" class="level3"><h3 class="anchored" data-anchor-id="testing-hypothesis">Testing hypothesis</h3>
<p> </p>
<p>We want to test the hypothesis that <span class="math inline">\(\mbox{H}_0: \beta = \beta_0\)</span>, the alternative hypothesis being <span class="math inline">\(\mbox{H}_1: \beta \neq \beta_0\)</span>. Denote <span class="math inline">\(z_{\alpha/2}\)</span> the critical value of a standard normal distribution at the <span class="math inline">\(\alpha\)</span>% error level. It is defined by: <span class="math inline">\(\mbox{P}(\mid z \mid &gt; z_{\alpha/2}) = \alpha\)</span> or:</p>
<p><span id="eq-critvalue"><span class="math display">\[
\mbox{P}(\mid z \mid \leq z_{\alpha/2}) = 1 - \alpha
\tag{2.11}\]</span></span></p>
<p>Consider for example <span class="math inline">\(\alpha = 5\)</span>%. To obtain the critical value, the <code>qnorm</code> function can be used, which takes a probability <span class="math inline">\(p\)</span> as argument and returns a quantile <span class="math inline">\(q\)</span>. By default, it returns the value such that <span class="math inline">\(\mbox{P}(z &lt; q) = p\)</span>, but the value of <span class="math inline">\(\mbox{P}(z &gt; q) = p\)</span> is returned if the <code>lower.tail</code> argument is set to <code>FALSE</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span><span class="op">)</span></span>
<span><span class="co">## [1] -1.96</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.975</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.96</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.96</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this case, the critical value is <span class="math inline">\(1.96\)</span>, which means that, drawing in a standard normal distribution, one gets on average 95% of values lower, in absolute values, than 1.96. The preceding command indicates respectively that:</p>
<ul>
<li>2.5% of the values of a normal distribution are lower than <span class="math inline">\(-1.96\)</span>,</li>
<li>97.5% of the values of a normal distribution are lower than 1.96,</li>
<li>2.5% of the values of a normal distribution are greater than 1.96.</li>
</ul>
<p>The 5% critical value is presented in <a href="#fig-normal">Figure&nbsp;<span>2.11</span></a>. If <span class="math inline">\(\mbox{H}_0\)</span> is true, <span class="math inline">\((\hat{\beta}_N-\beta_0)/\sigma_{\hat{\beta}_N}\)</span> is a draw in a standard normal distribution and we therefore should have an absolute value lower than 1.96, 95% of the time. Obviously, <span class="math inline">\(\hat{\beta}\)</span> will almost never be exactly equal to <span class="math inline">\(\beta_0\)</span>, even if <span class="math inline">\(\mbox{H}_0\)</span> is true because of sampling error. We have therefore the following decision rule, say at the 95% confidence level:</p>
<ul>
<li>if the absolute value of the computed statistic <span class="math inline">\((\hat{\beta}_N-\beta_0)/\sigma_{\hat{\beta}_N}\)</span> is greater than the critical value, we’ll say that the difference between <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\beta_0\)</span> is too large to be caused by sampling error; we therefore reject the hypothesis,</li>
<li>if the absolute value of the computed statistic <span class="math inline">\((\hat{\beta}_N-\beta_0)/\sigma_{\hat{\beta}_N}\)</span> is lower than the critical value, we’ll say that the difference between <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\beta_0\)</span> is small enough to be caused by sampling error; we therefore don’t reject the hypothesis.</li>
</ul>
<p></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-normal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-normal-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.11: Normal distribution and 5% critical value</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Consider as an example <span class="math inline">\(\hat{\beta} = 3.46\)</span>, <span class="math inline">\(\beta_0 = 4\)</span> and <span class="math inline">\(\sigma_\epsilon = 0.3\)</span>. The computed statistic is <span class="math inline">\(\frac{3.46 - 4}{0.3} = - 1.8\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hbeta</span> <span class="op">&lt;-</span> <span class="fl">3.46</span> ; <span class="va">betao</span> <span class="op">&lt;-</span> <span class="fl">4</span> ; <span class="va">shbeta</span> <span class="op">&lt;-</span> <span class="fl">0.3</span></span>
<span><span class="va">stat</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">hbeta</span> <span class="op">-</span> <span class="va">betao</span><span class="op">)</span> <span class="op">/</span> <span class="va">shbeta</span></span>
<span><span class="va">stat</span></span>
<span><span class="co">## [1] -1.8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It is lower, in absolute value, than <span class="math inline">\(1.96\)</span>; we therefore don’t reject the null hypothesis at the 5% error level.</p>
<p> A more general tool is the <strong>probability value</strong>. It is the probability of drawing a value at least as large as the one we obtained (in absolute value) if the hypothesis is true. It is given by:</p>
<p><span class="math display">\[
p = 2 \left[1 -\Phi\left(\left| \frac{\hat{\beta}-\beta_0}{\sigma_{\hat{\beta}}} \right|\right)\right]
\]</span></p>
<p>Probability values are computed using the <code>pnorm</code> function, which takes as argument a value of the variable (<span class="math inline">\(q\)</span>) and computes the probability for a given value of its argument. The default behavior of <code>pnorm</code> is to return <span class="math inline">\(p = \mbox{P}(z &lt; q)\)</span>, but the upper tail, given by <span class="math inline">\(\mbox{P}(z &gt; x)\)</span> is returned by setting the <code>lower.tail</code> argument to <code>FALSE</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.03593</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.9641</span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.03593</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.03593</span></span>
<span><span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.07186</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The computed statistics can be of both signs, so the last formula is the most robust: first take the absolute value of the statistic, then compute the upper tail for a normal distribution and finally multiply it by 2. The p-value is greater than <span class="math inline">\(5\)</span>%; therefore, the hypothesis is not rejected at the 5%. The interest of the p-value is that, once it is computed, it is very easy to get the decision, whatever the error level (and even whatever the distribution). The 5-10% critical values and the p-value are represented in <a href="#fig-pvalue">Figure&nbsp;<span>2.12</span></a>. The absolute value of the statistic is <span class="math inline">\(1.80\)</span>, the critical values at the 5 and 10% are <span class="math inline">\(1.96\)</span> and <span class="math inline">\(1.64\)</span>. Then:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pvalue" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-pvalue-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.12: Critical value and p-value</figcaption><p></p>
</figure>
</div>
</div>
</div>
<ul>
<li>the absolute value of the statistic being lower than the 5% critical value; the hypothesis is not rejected at the 5% level,</li>
<li>the absolute value of the statistic being greater than the 10% critical value; the hypothesis is rejected at the 10% level.</li>
</ul>
<!-- Next page --><div style="page-break-after: always;"></div>
<p>The p-value is equal to 7.2%:</p>
<ul>
<li>the p-value is greater than 5%; the hypothesis is not rejected at the 5% level,</li>
<li>the p-value is lower than 10%; the hypothesis is rejected at the 10% level.</li>
</ul>
<p></p>
<p></p>
</section><section id="sec-confint_simple_ols" class="level3"><h3 class="anchored" data-anchor-id="sec-confint_simple_ols">Confidence interval</h3>
<p></p>
<p>Knowing the distribution of the estimator enables one to go beyond the point estimation of the unknown parameter and to introduce the uncertainty by giving an interval of values which contains the real value of the unknown parameter with a given confidence. This is called a <strong>confidence interval</strong>. To obtain it, we start with <a href="#eq-critvalue">Equation&nbsp;<span>2.11</span></a>:</p>
<p><span class="math display">\[
\mbox{P}\left(\left|\frac{\hat{\beta} -
\beta}{\sigma_{\hat{\beta}}}\right|&lt;z_{\alpha/2}\right) = 1 - \alpha
\]</span></p>
<p>Developing this expression, we get:</p>
<p><span class="math display">\[
\mbox{P}\left(\hat{\beta} - \sigma_{\hat{\beta}} z_{\alpha/2} &lt; \beta &lt; \hat{\beta} + \sigma_{\hat{\beta}} z_{\alpha/2}\right) = 1 - \alpha
\]</span></p>
<p>which gives, in our example:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">hbeta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">shbeta</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">ic</span></span>
<span><span class="co">## [1] 2.872 4.048</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This confidence interval indicates that there is a probability of 95% that the true value of <span class="math inline">\(\beta\)</span> is between 2.872 and 4.048. </p>
</section><section id="exact-distribution-the-student-distribution" class="level3"><h3 class="anchored" data-anchor-id="exact-distribution-the-student-distribution">Exact distribution, the Student distribution</h3>
<p> In real settings, <span class="math inline">\(\sigma_{\hat{\beta}}=\frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}\)</span> is unknown because <span class="math inline">\(\sigma_\epsilon\)</span> is an unknown parameter. Replacing <span class="math inline">\(\sigma_\epsilon\)</span> by the unbiased estimator of <span class="math inline">\(\dot{\sigma}_\epsilon\)</span> we get <span class="math inline">\(\dot{\sigma}_{\hat{\beta}}\)</span>, the standard error of the estimation of the slope:</p>
<p><span class="math display">\[
\dot{\sigma}_{\hat{\beta}} = \frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_x}
\]</span></p>
<p>As <span class="math inline">\(\sigma_\epsilon\)</span> is estimated, some more noise is added, so that the distribution of <span class="math inline">\(\hat{\beta}\)</span> is no longer a normal, but a Student t with <span class="math inline">\(N-2\)</span> degrees of freedom (see <a href="#sec-exact_ols_distribution"><span>Section&nbsp;2.1.3.5</span></a>):</p>
<p><span class="math display">\[
\frac{\hat{\beta}_N-\beta}{\dot{\sigma}_{\hat{\beta}_N}}=
\frac{\sqrt{N}\hat{\sigma}_x}{\dot{\sigma}_\epsilon}(\hat{\beta}_N-\beta)
\sim t_{N-2}
\]</span></p>
<p>The Student distribution has a 0 expected value and a variance equal to <span class="math inline">\(\frac{N - 2}{N -4}\)</span>, which tends to 1 for large <span class="math inline">\(N\)</span>. Moreover, the Student distribution converges in distribution to a normal distribution. Therefore, for large <span class="math inline">\(N\)</span>, the same inference as the one presented for known <span class="math inline">\(\sigma_\epsilon\)</span> can be applied, using the normal distribution as a good approximation. For small samples, however, critical values of the Student distribution should be used. The relevant 95% critical values are computed below for numbers of degrees of freedom equal to 5, 10, 50, 100 and 1000:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.025</span>, df <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">50</span>, <span class="fl">100</span>, <span class="fl">1000</span><span class="op">)</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.571 2.228 2.009 1.984 1.962</code></pre>
</div>
</div>
<p>Therefore, the normal distribution can be safely used if the sample has at least a few hundreds of observations. </p>
</section><section id="inference-with-r" class="level3"><h3 class="anchored" data-anchor-id="inference-with-r">Inference with R</h3>
<p><strong>R</strong> has different functions that extract information about the statistical properties of the fitted model. To illustrate their use, we use once again the price-time model:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span>, <span class="va">prtime</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Detailed results of the model are computed using the <code>summary</code> method for <code>lm</code> objects:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">spxt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which returns an object of class <code>summary.lm</code>. Moreover, <code>summary.lm</code> prints nicely. It is therefore customary to use <code>summary</code> without storing the result in an object but only to visualize the detailed results of the fitted model:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = sr ~ h, data = prtime)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.07938 -0.03535 -0.00034  0.01430  0.14291 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.01548    0.07326   -0.21   0.8386    
h            0.02393    0.00393    6.09   0.0005 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.0718 on 7 degrees of freedom
Multiple R-squared:  0.841, Adjusted R-squared:  0.818 
F-statistic: 37.1 on 1 and 7 DF,  p-value: 0.000497</code></pre>
</div>
</div>
<p>The output first indicates the “call”, i.e., the function that has been used to estimate the model. Then, the distribution of the residuals is summarized using the <strong>five numbers</strong> (the range <code>min</code> and <code>max</code>, the two quartiles and the median).<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Note that the mean is not indicated, as it is necessarily 0 for a model fitted by OLS. Next, the table of coefficients is printed, containing:</p>
<ul>
<li>the names of the effects,</li>
<li>the value of the estimates (<span class="math inline">\(\hat{\beta}\)</span>),</li>
<li>their standard errors (<span class="math inline">\(\dot{\sigma}_{\hat{\beta}}\)</span>),</li>
<li>the Student statistic which is the ratio of the previous two columns and is a special case of the test statistic <span class="math inline">\((\hat{\beta}-\beta_0) /\dot{\sigma}_{\hat{\beta}}\)</span> where <span class="math inline">\(\beta_0=0\)</span>,</li>
<li>the probability value of this statistic.</li>
</ul>
<p>Thes kinds of tests are often considered as tests of significance of the corresponding covariate. If the hypothesis that <span class="math inline">\(\beta_0=0\)</span> is rejected, we would say that the coefficient is “significant”, which means more precisely that it is significantly different from 0. As we have a very small sample, it is worth considering the critical value of a Student instead of a normal distribution. We get here: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.025</span>, df <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">cv</span></span>
<span><span class="co">## [1] 2.365</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>for the intercept, the t statistic is much lower than the critical value and the probability value is far greater than 5%; therefore, the hypothesis that <span class="math inline">\(\alpha = 0\)</span> is not rejected,</li>
<li>for the slope, the t statistic is much higher than the critical value and the probability value is far lower than 5%; therefore, the hypothesis that <span class="math inline">\(\beta = 0\)</span> is rejected.</li>
</ul>
<div style="page-break-after: always;"></div>
<!-- new page -->
<p>This table of coefficients is a matrix that is stored in the <code>summary.lm</code> object with the <code>coefficients</code> name. As such, it can be extracted using <code>spxt$coefficients</code> or using the <code>coef</code> method of <code>summary.lm</code>: <code>coef(spxt)</code>. Finally, the printed output ends with some general indicators (often <strong>GOF</strong> for goodness-of-fit indicators) as the residual standard error (<span class="math inline">\(\dot{\sigma}_\epsilon\)</span>, which can be extracted using the <code>sigma</code> function), two measures of the coefficient of determination, and the <span class="math inline">\(F\)</span> statistic that is relevant for the multiple regression model. </p>
<p>The <code>confint</code> function computes the confidence interval for the coefficients:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">pxt</span>, level <span class="op">=</span> <span class="fl">0.9</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 5 %    95 %
(Intercept) -0.15428 0.12332
h            0.01648 0.03138</code></pre>
</div>
</div>
<p>we set here the <code>level</code> argument to <code>0.9</code> (the default value being <code>0.95</code>) and the results indicate that there is a 90% probability that the true value of the slope is between 0.016 and 0.031.</p>
</section><section id="delta-method" class="level3"><h3 class="anchored" data-anchor-id="delta-method">Delta method</h3>
<p></p>
<p>It’s often the case that the parameters of interest are not the fitted parameters, but some functions of them. In the price-time model, the fitted parameters are <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, but the structural parameters (the lower and higher values of the travel time) are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span id="eq-structpar"><span class="math display">\[
\left\{
\begin{array}{rcl}
a &amp;=&amp; F^a(\alpha, \beta) = -\frac{\alpha}{\beta} \\
b &amp;=&amp; F^b(\alpha, \beta) = \frac{1 - \alpha}{\beta}
\end{array}
\right.
\tag{2.12}\]</span></span></p>
<p>The structural parameters are easily retrieved using <a href="#eq-structpar">Equation&nbsp;<span>2.12</span></a>. The so-called delta method can be used to compute their standard deviations. Denoting <span class="math inline">\(f\)</span> the first derivatives of <span class="math inline">\(F\)</span>, we write a first-order Taylor expansion for <span class="math inline">\(F^a\)</span> and <span class="math inline">\(F^b\)</span>:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
a &amp;=&amp; F^a(\alpha_0, \beta_0) +
(\alpha - \alpha_0) f^a_\alpha(\alpha_0, \beta_0) +
(\beta - \beta_0) f^a_\beta(\alpha_0, \beta_0)\\
b &amp;=&amp; F^b(\alpha_0, \beta_0) +
(\alpha - \alpha_0) f^b_\alpha(\alpha_0, \beta_0) +
(\beta - \beta_0) f^b_\beta(\alpha_0, \beta_0)\\
\end{array}
\right.
\]</span></p>
<p>So that the variances of the fitted structural parameters are approximately:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
  \hat{\sigma}_{\hat{a}} ^ 2 &amp;=&amp;
  f^a_\alpha(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\alpha}} ^ 2 +
  f^a_\beta(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\beta}} ^ 2 +
  2 f^a_\alpha(\alpha_0, \beta_0)f^a_\beta(\alpha_0, \beta_0)
  \hat{\sigma}_{\hat{\alpha}\hat{\beta}} \\
  \hat{\sigma}_{\hat{b}} ^ 2 &amp;= &amp;
  f^b_\alpha(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\alpha}} ^ 2 +
  f^b_\beta(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\beta}} ^ 2 +
  2 f^b_\alpha(\alpha_0, \beta_0)f^b_\beta(\alpha_0, \beta_0)
  \hat{\sigma}_{\hat{\alpha}\hat{\beta}} \\
\end{array}
\right.
\]</span></p>
<p>Replacing (<span class="math inline">\(\alpha_0, \beta_0\)</span>) by (<span class="math inline">\(\hat{\alpha}, \hat{\beta}\)</span>) and using the formulas for the variances and covariance of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> given in <a href="#eq-covariance_gamma">Equation&nbsp;<span>2.9</span></a>, we get:</p>
<p><span class="math display">\[
\left\{
  \begin{array}{rcl}
\hat{\sigma}_{\hat{a}} &amp;=&amp;
\frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_x}\frac{1}{\hat{\beta}}
  \sqrt{\hat{\sigma}_x ^ 2 + \left(\bar{x} +
                           \frac{\hat{\alpha}}{\hat{\beta}}\right) ^ 2} \\
\hat{\sigma}_{\hat{b}} &amp;=&amp;
\frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_x}\frac{1}{\hat{\beta}}
  \sqrt{\hat{\sigma}_x ^ 2 + \left(\bar{x} -
                           \frac{1 - \hat{\alpha}}{\hat{\beta}}\right) ^ 2}
  \end{array}
\right.
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">sx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">bx</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">halpha</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span><span class="va">hbeta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span><span class="va">hseps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="va">ab</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span> <span class="va">halpha</span> <span class="op">/</span> <span class="va">hbeta</span>, <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">halpha</span><span class="op">)</span> <span class="op">/</span> <span class="va">hbeta</span><span class="op">)</span></span>
<span><span class="va">sab</span> <span class="op">&lt;-</span> <span class="va">hseps</span> <span class="op">/</span> <span class="va">sx</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">hbeta</span> <span class="op">*</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">sx</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="va">bx</span> <span class="op">+</span> <span class="va">halpha</span> <span class="op">/</span> <span class="va">hbeta</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>,</span>
<span>           <span class="va">sx</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="va">bx</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">halpha</span><span class="op">)</span> <span class="op">/</span> <span class="va">hbeta</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span>         <span class="op">)</span></span>
<span><span class="va">ab</span></span>
<span><span class="co">## [1]  0.647 42.431</span></span>
<span><span class="va">sab</span></span>
<span><span class="co">## [1] 2.961 4.197</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which finally leads to the 95% confidence interval: </p>
<div class="cell" data-layout-align="center">

</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">ab</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span> <span class="va">sab</span>, <span class="va">sab</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="va">cv</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]   [,2]
[1,] -6.354  7.648
[2,] 32.505 52.356</code></pre>
</div>
</div>
<p>There is therefore a 95% probability that the maximum value of time is between 32.5 and 52.4 euros and the hypotheses that the minimum value of time is 0 is not rejected.</p>
</section><section id="confidence-interval-for-the-prediction" class="level3"><h3 class="anchored" data-anchor-id="confidence-interval-for-the-prediction">Confidence interval for the prediction</h3>
<p></p>
<p>Once the model is estimated, a prediction for every observation can be computed using the formula of the conditional expectation of <span class="math inline">\(y\)</span> for <span class="math inline">\(x = x_n\)</span>, which is:</p>
<p><span class="math display">\[
\mbox{E}(y \mid x = x_n) = \alpha + \beta x_n + \mbox{E}(\epsilon \mid
x = x_n) = \alpha + \beta x_n
\]</span></p>
<p>As <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are unbiased estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\hat{y}_n = \hat{\alpha} + \hat{\beta} x_n\)</span> is an unbiased estimator of <span class="math inline">\(\mbox{E}(y \mid x = x_n)\)</span>. Applying the formula for the variance of a sum, we have: <span class="math inline">\(\mbox{V}(\hat{y}) = \mbox{V}(\hat{\alpha}) + x_n ^ 2 \mbox{V}(\hat{\beta}) + 2 x_n \mbox{cov}(\hat{\alpha}, \hat{\beta})\)</span>. Using <a href="#eq-covariance_gamma">Equation&nbsp;<span>2.9</span></a>, we get:</p>
<p><span class="math display">\[
\sigma_{\hat{y}_n} ^ 2 =\frac{\sigma_\epsilon ^ 2}{N \sigma_x ^ 2}
\left( \sigma_x ^ 2 + (x_n - \bar{x}) ^ 2\right)
\]</span></p>
<p>which finally leads to the formula of the standard deviation of the predictions:</p>
<p><span class="math display">\[
\sigma_{\hat{y}_n} = \frac{\sigma_\epsilon}{\sqrt{N}}
\sqrt{1 + \frac{(x_n - \bar{x}) ^ 2}{\hat{\sigma}_x ^ 2}}
\]</span> <span class="math inline">\(\sigma_{\hat{y}_n}\)</span> increases with the deviation of <span class="math inline">\(x_n\)</span> from the sample mean. Moreover, <span class="math inline">\(\sigma_{\hat{y}}\)</span> tends to 0 when <span class="math inline">\(N\)</span> tends to infinity, which means that <span class="math inline">\(\hat{y}_n\)</span> is a consistent estimator of <span class="math inline">\(\mbox{E}(y \mid x = x_n)\)</span>.</p>
<p>Consider now the standard deviation of <span class="math inline">\(y_n = \mbox{E}(y \mid x = x_n) + \epsilon_n\)</span>. To the variation due to the estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we have to add the one associated with <span class="math inline">\(\epsilon_n\)</span>. Therefore, the variance of <span class="math inline">\(y_n\)</span> is the sum of <span class="math inline">\(\sigma_{\hat{y}_n} ^ 2\)</span> and <span class="math inline">\(\sigma_\epsilon^2\)</span> and therefore its standard deviation is:</p>
<p><span class="math display">\[
\sigma_{y_n} = \sqrt{\sigma_{\hat{y}_n} ^ 2 + \sigma_{\epsilon} ^ 2}=
\frac{\sigma_{\epsilon}}{\sqrt{N}}
\sqrt{1 + \frac{(x_n - \bar{x}) ^ 2}{\hat{\sigma}_x ^ 2} + N}
\]</span> Note that when <span class="math inline">\(N \rightarrow \infty\)</span>, <span class="math inline">\(\sigma_{y_n}\)</span>, contrary Note that <span class="math inline">\(\sigma_{\hat{y}_n}\)</span> tends to <span class="math inline">\(\sigma_\epsilon\)</span> and not to 0.</p>
<p> A <strong>confidence interval</strong> for <span class="math inline">\(\hat{y}_n\)</span> is obtained by adding and subtracting to the point estimator the estimated standard deviation <span class="math inline">\(\sigma_{\hat{y}_n}\)</span> times the critical value (here a Student <span class="math inline">\(t\)</span> with <span class="math inline">\(N-2=7\)</span> degrees of freedom). A <strong>prediction interval</strong> for <span class="math inline">\(y_n\)</span> is obtained the same way, but using <span class="math inline">\(\sigma_{y_n}\)</span> instead of <span class="math inline">\(\sigma_{\hat{y}_n}\)</span>. The following code computes the two standard deviations and the relevant limits of the confidence / prediction intervals:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mux</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">prtime</span><span class="op">$</span><span class="va">h</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="va">sx2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">prtime</span><span class="op">$</span><span class="va">h</span> <span class="op">-</span> <span class="va">mux</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">N</span></span>
<span><span class="va">tcv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.975</span>, df <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>fitted <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span>,</span>
<span>           sehy <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">*</span></span>
<span>             <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="va">prtime</span><span class="op">$</span><span class="va">h</span> <span class="op">-</span> <span class="va">mux</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="va">sx2</span><span class="op">)</span>,</span>
<span>           sey <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">*</span></span>
<span>             <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="va">prtime</span><span class="op">$</span><span class="va">h</span> <span class="op">-</span> <span class="va">mux</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="va">sx2</span> <span class="op">+</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>           lowhy <span class="op">=</span> <span class="va">fitted</span> <span class="op">-</span> <span class="va">tcv</span> <span class="op">*</span> <span class="va">sehy</span>, uphy  <span class="op">=</span> <span class="va">fitted</span> <span class="op">+</span> <span class="va">tcv</span> <span class="op">*</span> <span class="va">sehy</span>,</span>
<span>           lowy  <span class="op">=</span> <span class="va">fitted</span> <span class="op">-</span> <span class="va">tcv</span> <span class="op">*</span><span class="va">sey</span>, upy   <span class="op">=</span> <span class="va">fitted</span> <span class="op">+</span> <span class="va">tcv</span> <span class="op">*</span> <span class="va">sey</span><span class="op">)</span></span>
<span><span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">select</span><span class="op">(</span><span class="va">fitted</span>, <span class="va">lowhy</span>, <span class="va">uphy</span>, <span class="va">lowy</span>, <span class="va">upy</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 5
  fitted lowhy  uphy  lowy   upy
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1  0.624 0.522 0.726 0.426 0.822
2  0.474 0.412 0.536 0.293 0.655
3  0.631 0.527 0.735 0.432 0.830</code></pre>
</div>
</div>
<p>These values can also be obtained with the <code>predict</code> function, with the <code>interval</code> argument set to <code>"confidence"</code> or <code>"prediction"</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">pxt</span>, interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">pxt</span>, interval <span class="op">=</span> <span class="st">"prediction"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <a href="#fig-confpredit">Figure&nbsp;<span>2.13</span></a>, the two intervals are represented:</p>
<ul>
<li>for the confidence interval, we use <code>geom_smooth</code> with <code>method = "lm"</code> and the default <code>TRUE</code> value for the <code>se</code> argument; in this case, we have a gray zone which figures the confidence interval (by default at the 95% level, but another level can be used by setting the <code>level</code> argument to the desired level),</li>
<li>for the confidence interval, we use <code>geom_errorbar</code> that draws vertical segments, which represent here the limits of the confidence interval we have computed.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">h</span>, <span class="va">sr</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_errorbar</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">lowy</span>, ymax <span class="op">=</span> <span class="va">upy</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-confpredit" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-confpredit-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.13: Confidence and prediction intervals</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As an example, consider trips from Bordeaux to Paris. Reported transport time is 242 minutes, which is approximately 4 hours. The high speed track, opened in 2018 reduces this transport time to a minimum of 2 hours and 6 minutes. We consider 3 hours as the mean transport time, and we consider the average price to be 75 euros. Assuming that the conditions on the air transport market are unchanged, what prediction can we make about the change of the modal share of rail? We first construct a tibble called <code>bordeaux</code> with two lines: the first contains the actual features of the Paris-Bordeaux trip, and the second the new features.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bordeaux</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">town</span> <span class="op">==</span> <span class="st">"Bordeaux"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">town</span>, <span class="va">pr</span>, <span class="va">pa</span>, <span class="va">tr</span>, <span class="va">ta</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_row</span><span class="op">(</span>town <span class="op">=</span> <span class="st">"BordeauxSim"</span>, pr <span class="op">=</span> <span class="fl">75</span>, </span>
<span>          pa <span class="op">=</span> <span class="fl">82.6</span>, tr <span class="op">=</span> <span class="fl">180</span>, ta <span class="op">=</span> <span class="fl">165</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>h <span class="op">=</span> <span class="op">(</span><span class="va">pa</span> <span class="op">-</span> <span class="va">pr</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="va">tr</span> <span class="op">-</span> <span class="va">ta</span><span class="op">)</span> <span class="op">/</span> <span class="fl">60</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The prediction of train’s modal share is obtained using the <code>predict</code> function with the <code>new</code> argument which is a data frame containing the values of the covariates for which we want to compute predictions (this is the <code>bordeaux</code> table in our example):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">pxt</span>, new <span class="op">=</span> <span class="va">bordeaux</span>, interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">as_tibble</span></span>
<span><span class="va">prd</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 3
    fit   lwr   upr
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 0.624 0.522 0.726
2 0.712 0.580 0.844</code></pre>
</div>
</div>
<div style="page-break-after: always;"></div>
<!-- new page -->
<p>The model predicts that the train’s share increases from 0.624 to 0.712, but the confidence intervals are quite large and overlap. Present and predicted market shares are represented by a triangle and by a circle in <a href="#fig-bordeaux">Figure&nbsp;<span>2.14</span></a>, along with the confidence intervals.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bordeaux" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_properties_files/figure-html/fig-bordeaux-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.14: Predictions for train’s model share</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-AMEM:85" class="csl-entry" role="doc-biblioentry">
Amemiya, Takeshi. 1985. <em>Advanced Econometrics</em>. Harvard University Press.
</div>
<div id="ref-DAVI:MACK:93" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, and James G. MacKinnon. 1993. <em>Estimation and Inference in Econometrics</em>. New-York: Oxford University Press.
</div>
<div id="ref-DAVI:MACK:04" class="csl-entry" role="doc-biblioentry">
———. 2004. <em>Econometric Theory and Methods</em>. Oxford University Press.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>We don’t explicitly indicate that the expected values are conditional on <span class="math inline">\(x\)</span> to save space.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Note that the mean of the residuals is 0.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See for example <span class="citation" data-cites="AMEM:85">Amemiya (<a href="#ref-AMEM:85" role="doc-biblioref">1985</a>)</span>, pages 89-90.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>These two estimators are inspired by <span class="citation" data-cites="DAVI:MACK:04">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:04" role="doc-biblioref">2004</a>)</span>, page 97, and <span class="citation" data-cites="DAVI:MACK:93">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:93" role="doc-biblioref">1993</a>)</span>, pages 123-124.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Note that the numerator of <a href="#eq-beta_N">Equation&nbsp;<span>2.10</span></a> is not exactly the sample covariance, which is <span class="math inline">\(\sum_n (x_n - \bar{x})(\epsilon_n - \bar{\epsilon})/N\)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Inspired by <span class="citation" data-cites="DAVI:MACK:93">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:93" role="doc-biblioref">1993</a>)</span>, pages 126-127.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>See <span class="citation" data-cites="DAVI:MACK:93">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:93" role="doc-biblioref">1993</a>)</span>, pp.&nbsp;116-117 for details.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Any series can be summarized this way using the <code>fivenum</code> function.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../chapters/simple_regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/multiple_regression.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb54" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Statistical properties of the simple linear estimator {#sec-stat_prop_slm}</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setup_simple_regression_properties</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../_commonR.R"</span>)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> price_time <span class="sc">%&gt;%</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_names</span>(<span class="fu">c</span>(<span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span>))</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> <span class="fu">mutate</span>(prtime, <span class="at">sr =</span> qr <span class="sc">/</span> (qr <span class="sc">+</span> qa))</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> <span class="fu">mutate</span>(prtime,  <span class="at">h =</span> (pa <span class="sc">-</span> pr) <span class="sc">/</span> ( (tr <span class="sc">-</span> ta) <span class="sc">/</span> <span class="dv">60</span>) )</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> <span class="fu">filter</span>(prtime, sr <span class="sc">&lt;</span> <span class="fl">0.75</span>)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>pxt <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h, prtime)</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- see the sentence of Davidson McKinnon for inconsistency --&gt;</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- The second term is the ratio of covariance and variance, not exactly --&gt;</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>To analyze the statistical properties of the OLS estimator, we use @eq-ols_lin_comb_errors that indicates that the difference between the estimated slope and the true value is a linear</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>combination of the errors:</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>\hat{\beta}=\beta + \sum_{n=1}^N c_n \epsilon_n, \mbox{ with } c_n = (x_n - \bar{x}) / S_{xx}</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>$$ {#eq-linest}</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>The properties of $\hat{\beta}$ are therefore directly deduced from</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>those of $\epsilon$. We'll consider two sets of properties:</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**exact** properties that apply whatever the size of the sample is (@sec-exact_prop_ols),</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**asymptotic** properties that indicate approximate results, the approximation being better and better as the sample size grows (@sec-asymp_prop_ols).</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>@sec-confint_test_slm explains how these properties can be used to construct confidence intervals and tests.</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exact properties of the OLS estimator {#sec-exact_prop_ols}</span></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>The OLS estimator is a random variable, for which we observe one value,</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>obtained with a given sample. The exact properties of the OLS estimator</span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>concern:</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>its expected value: if the true value is $\beta_o$, what is the</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>    expected value of $\hat{\beta}$, $\beta_o$ or another value?</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>its variance (or standard deviation): is the variance small (the</span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>    estimation is precise) or large?</span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>The computation of the expected value indicates the presence or</span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a>absence of a **bias**. Therefore, we check here whether there is a</span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>systematic error (called the bias) while performing the estimation. The</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>variance indicates the **efficiency** (or the precision) of the</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a>estimator. It measures the amount of the **sampling error**, i.e.,  the</span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>average distance between the value of the estimator and its expected</span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>value.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{efficiency}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sampling error}</span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a>To analyze the properties of the OLS estimator, we'll make different</span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>hypotheses and we'll see that if these hypotheses are satisfied, the OLS</span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>is the best (most efficient) linear unbiased estimator. To illustrate</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>the results of this chapter, we'll use the price-time model, with the</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>same GDP as previously: $\alpha = -0.2$, $\beta = 0.032$ and</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon = 0.08$ and we'll consider different departures from</span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>this reference case.</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a><span class="fu">### Errors have 0 expected value</span></span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>The reference model being $y_n = \alpha + \beta x_n + \epsilon_n$ with</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon_n) = 0$, consider the alternative model:</span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a>$y_n = \gamma + \beta x_n + \eta_n$, for which the slope is the same and</span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>the error term is $\eta_n$, with $\mbox{E}(\eta) = \mu_\eta \neq 0$. We</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>have therefore: $y_n = \alpha + \beta x_n + (\eta_n + \gamma - \alpha)$</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>or: $\eta_n = \epsilon_n + \alpha - \gamma$ and finally:</span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\eta) = \mu_\eta = \mbox{E}(\epsilon) + \alpha - \gamma = \alpha - \gamma$.</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>Therefore, the alternative model is:</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a>$y_n = \gamma + \mu_\gamma + \beta x_n + \epsilon$, which is the same</span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a>model as the initial model with $\alpha$ replaced by</span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>$\gamma + \mu_\gamma$. Therefore, it is impossible to discriminate</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a>between the initial and the alternative model, as what can be estimated</span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a>is the sum of the intercept and the expected value of the errors</span>
<span id="cb54-78"><a href="#cb54-78" aria-hidden="true" tabindex="-1"></a>($\gamma + \mu_\gamma$) and the two elements of this sum can't be</span>
<span id="cb54-79"><a href="#cb54-79" aria-hidden="true" tabindex="-1"></a>estimated separately. This is an illustration of a very important</span>
<span id="cb54-80"><a href="#cb54-80" aria-hidden="true" tabindex="-1"></a>problem in econometrics called **identification**\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{identification} that we'll encounter</span>
<span id="cb54-81"><a href="#cb54-81" aria-hidden="true" tabindex="-1"></a>in subsequent chapters. We can say here that $\gamma$ and $\mu_\gamma$</span>
<span id="cb54-82"><a href="#cb54-82" aria-hidden="true" tabindex="-1"></a>are not identified, but that their sum is. Therefore, we can set one of</span>
<span id="cb54-83"><a href="#cb54-83" aria-hidden="true" tabindex="-1"></a>the two parameters to any value. For example, we can simply set</span>
<span id="cb54-84"><a href="#cb54-84" aria-hidden="true" tabindex="-1"></a>$\mu_\gamma=0$, i.e., suppose that the expected value of the errors is 0</span>
<span id="cb54-85"><a href="#cb54-85" aria-hidden="true" tabindex="-1"></a>and the other parameter $\gamma$ became identified, i.e., it can be</span>
<span id="cb54-86"><a href="#cb54-86" aria-hidden="true" tabindex="-1"></a>estimated using the data.</span>
<span id="cb54-87"><a href="#cb54-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-88"><a href="#cb54-88" aria-hidden="true" tabindex="-1"></a>@fig-everrors illustrates the "reference" model (plain line and</span>
<span id="cb54-89"><a href="#cb54-89" aria-hidden="true" tabindex="-1"></a>$\epsilon_n$ represented by plain vectors) and the alternative model</span>
<span id="cb54-90"><a href="#cb54-90" aria-hidden="true" tabindex="-1"></a>(dashed line and $\eta_n$ represented by dashed vectors).</span>
<span id="cb54-91"><a href="#cb54-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-92"><a href="#cb54-92" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-93"><a href="#cb54-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-everrors</span></span>
<span id="cb54-94"><a href="#cb54-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Intercept and the expected value of the error"</span></span>
<span id="cb54-95"><a href="#cb54-95" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-96"><a href="#cb54-96" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"latex2exp"</span>)</span>
<span id="cb54-97"><a href="#cb54-97" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.2</span></span>
<span id="cb54-98"><a href="#cb54-98" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.032</span></span>
<span id="cb54-99"><a href="#cb54-99" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb54-100"><a href="#cb54-100" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span></span>
<span id="cb54-101"><a href="#cb54-101" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">Eyx =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> h,</span>
<span id="cb54-102"><a href="#cb54-102" aria-hidden="true" tabindex="-1"></a>           <span class="at">Eyx2 =</span> gamma <span class="sc">+</span> beta <span class="sc">*</span> h)</span>
<span id="cb54-103"><a href="#cb54-103" aria-hidden="true" tabindex="-1"></a>dx <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb54-104"><a href="#cb54-104" aria-hidden="true" tabindex="-1"></a>ticks <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb54-105"><a href="#cb54-105" aria-hidden="true" tabindex="-1"></a>myarrow <span class="ot">&lt;-</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.015</span>, <span class="st">"npc"</span>), <span class="at">angle =</span> <span class="dv">15</span>, <span class="at">type =</span> <span class="st">"open"</span>)</span>
<span id="cb54-106"><a href="#cb54-106" aria-hidden="true" tabindex="-1"></a>myarrow2 <span class="ot">&lt;-</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.015</span>, <span class="st">"npc"</span>), <span class="at">angle =</span> <span class="dv">15</span>, <span class="at">type =</span> <span class="st">"closed"</span>)</span>
<span id="cb54-107"><a href="#cb54-107" aria-hidden="true" tabindex="-1"></a>subdata <span class="ot">&lt;-</span> <span class="fu">filter</span>(prtime, town <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"Toulouse"</span>, <span class="st">"Strasbourg"</span>, <span class="st">"Brest"</span>))</span>
<span id="cb54-108"><a href="#cb54-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-109"><a href="#cb54-109" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span></span>
<span id="cb54-110"><a href="#cb54-110" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-111"><a href="#cb54-111" aria-hidden="true" tabindex="-1"></a><span class="co">#    geom_label(aes(label = town)) + </span></span>
<span id="cb54-112"><a href="#cb54-112" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="fl">0.032</span>, <span class="at">intercept =</span> alpha) <span class="sc">+</span></span>
<span id="cb54-113"><a href="#cb54-113" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="fl">0.032</span>, <span class="at">intercept =</span> gamma, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb54-114"><a href="#cb54-114" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="at">data =</span> subdata, <span class="fu">aes</span>(<span class="at">x =</span> h <span class="sc">+</span> dx, <span class="at">xend =</span> h <span class="sc">+</span> dx, <span class="at">yend =</span> sr, <span class="at">y =</span> Eyx), <span class="at">arrow =</span> myarrow) <span class="sc">+</span></span>
<span id="cb54-115"><a href="#cb54-115" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="at">data =</span> subdata, <span class="fu">aes</span>(<span class="at">x =</span> h <span class="sc">-</span> dx, <span class="at">xend =</span> h <span class="sc">-</span> dx, <span class="at">yend =</span> sr, <span class="at">y =</span> Eyx2), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">arrow =</span> myarrow2) <span class="sc">+</span></span>
<span id="cb54-116"><a href="#cb54-116" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb54-117"><a href="#cb54-117" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span> <span class="dv">2</span>, <span class="at">y =</span> alpha, <span class="at">label =</span> <span class="fu">TeX</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">alpha = </span><span class="sc">\\</span><span class="st">gamma + </span><span class="sc">\\</span><span class="st">mu_</span><span class="sc">\\</span><span class="st">gamma$"</span>, <span class="at">output =</span> <span class="st">"character"</span>)), <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb54-118"><a href="#cb54-118" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span> <span class="dv">2</span>, <span class="at">y =</span> gamma, <span class="at">label =</span> <span class="fu">TeX</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">gamma$"</span>, <span class="at">output =</span> <span class="st">"character"</span>)), <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb54-119"><a href="#cb54-119" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">c</span>(alpha, gamma)), <span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span> ticks, <span class="at">xend =</span> <span class="sc">+</span> ticks, <span class="at">y =</span> y, <span class="at">yend =</span> y)) <span class="sc">+</span> </span>
<span id="cb54-120"><a href="#cb54-120" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span> <span class="dv">3</span>, <span class="dv">30</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span> <span class="fl">0.3</span>, <span class="fl">0.7</span>))</span>
<span id="cb54-121"><a href="#cb54-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-122"><a href="#cb54-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-123"><a href="#cb54-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditional expectation of the errors is 0</span></span>
<span id="cb54-124"><a href="#cb54-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-125"><a href="#cb54-125" aria-hidden="true" tabindex="-1"></a>As we have seen, the hypothesis that $\mbox{E}(\epsilon)=0$ can always</span>
<span id="cb54-126"><a href="#cb54-126" aria-hidden="true" tabindex="-1"></a>be stated if the model contains an intercept. On the contrary, the</span>
<span id="cb54-127"><a href="#cb54-127" aria-hidden="true" tabindex="-1"></a>hypothesis that the expected value of $\epsilon$ conditional on $x$ is 0</span>
<span id="cb54-128"><a href="#cb54-128" aria-hidden="true" tabindex="-1"></a>($\mbox{E}(\epsilon |x)=0$) is much more problematic and the violation</span>
<span id="cb54-129"><a href="#cb54-129" aria-hidden="true" tabindex="-1"></a>of this hypothesis has dramatic consequences for the OLS estimator. It</span>
<span id="cb54-130"><a href="#cb54-130" aria-hidden="true" tabindex="-1"></a>is important to understand that this condition actually implies that</span>
<span id="cb54-131"><a href="#cb54-131" aria-hidden="true" tabindex="-1"></a>there is no correlation between the error and the covariate. Starting</span>
<span id="cb54-132"><a href="#cb54-132" aria-hidden="true" tabindex="-1"></a>with the expression of the covariance between the error and the</span>
<span id="cb54-133"><a href="#cb54-133" aria-hidden="true" tabindex="-1"></a>covariate:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{conditional expectation}</span>
<span id="cb54-134"><a href="#cb54-134" aria-hidden="true" tabindex="-1"></a>$\mbox{cov}(x, \epsilon) = \mbox{E}\left((x - \mu_x)(\epsilon-\mu_\epsilon)\right)$,</span>
<span id="cb54-135"><a href="#cb54-135" aria-hidden="true" tabindex="-1"></a>with $\mu_x$ and $\mu_\epsilon$ the expected values of $x$ and</span>
<span id="cb54-136"><a href="#cb54-136" aria-hidden="true" tabindex="-1"></a>$\epsilon$, we can rewrite this covariance using conditional</span>
<span id="cb54-137"><a href="#cb54-137" aria-hidden="true" tabindex="-1"></a>expectation, using the rule of repeated expectations:</span>
<span id="cb54-138"><a href="#cb54-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-139"><a href="#cb54-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-140"><a href="#cb54-140" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-141"><a href="#cb54-141" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(x, \epsilon) &amp;=&amp; \mbox{E}_x\left[\mbox{E}\left((x - \mu_x)(\epsilon-\mu_\epsilon)| x\right)\right]<span class="sc">\\</span></span>
<span id="cb54-142"><a href="#cb54-142" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;\mbox{E}_x\left[(x - \mu_x)\left(\mbox{E}(\epsilon-\mu_\epsilon)| x\right)\right]<span class="sc">\\</span></span>
<span id="cb54-143"><a href="#cb54-143" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;\mbox{E}_x\left<span class="co">[</span><span class="ot">(x - \mu_x)\mbox{E}(\epsilon| x)\right</span><span class="co">]</span></span>
<span id="cb54-144"><a href="#cb54-144" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-145"><a href="#cb54-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-146"><a href="#cb54-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-147"><a href="#cb54-147" aria-hidden="true" tabindex="-1"></a>The covariance between $x$ and $\epsilon$ is therefore equal to the</span>
<span id="cb54-148"><a href="#cb54-148" aria-hidden="true" tabindex="-1"></a>covariance between $x$ and the conditional expectation of $\epsilon$. If</span>
<span id="cb54-149"><a href="#cb54-149" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon|x)$ is a constant (equal to $\mu_\epsilon$, the</span>
<span id="cb54-150"><a href="#cb54-150" aria-hidden="true" tabindex="-1"></a>unconditional expectation), the covariance is:</span>
<span id="cb54-151"><a href="#cb54-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-152"><a href="#cb54-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-153"><a href="#cb54-153" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(x, \epsilon)=\mbox{E}\left<span class="co">[</span><span class="ot">(x - \mu_x)\mu_\epsilon\right</span><span class="co">]</span> = \mu_\epsilon\mbox{E}\left<span class="co">[</span><span class="ot">x - \mu_x\right</span><span class="co">]</span>=0</span>
<span id="cb54-154"><a href="#cb54-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-155"><a href="#cb54-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-156"><a href="#cb54-156" aria-hidden="true" tabindex="-1"></a>Therefore, a constant conditional expectation of $\epsilon$ (not</span>
<span id="cb54-157"><a href="#cb54-157" aria-hidden="true" tabindex="-1"></a>necessarily 0 but we have seen previously than we can safely suppose that</span>
<span id="cb54-158"><a href="#cb54-158" aria-hidden="true" tabindex="-1"></a>it is 0) implies that the covariance between the errors and the</span>
<span id="cb54-159"><a href="#cb54-159" aria-hidden="true" tabindex="-1"></a>covariate is 0 or, stated differently, that the errors are uncorrelated</span>
<span id="cb54-160"><a href="#cb54-160" aria-hidden="true" tabindex="-1"></a>with the covariate. From @eq-linest, the conditional expectation of the</span>
<span id="cb54-161"><a href="#cb54-161" aria-hidden="true" tabindex="-1"></a>estimator is:</span>
<span id="cb54-162"><a href="#cb54-162" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{unbiasedness!simple linear regression model}</span>
<span id="cb54-163"><a href="#cb54-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-164"><a href="#cb54-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-165"><a href="#cb54-165" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\beta}\mid x) = \beta + \sum_{n = 1} ^ N</span>
<span id="cb54-166"><a href="#cb54-166" aria-hidden="true" tabindex="-1"></a>\mbox{E}(c_{n}\epsilon_{n} \mid x_n)=</span>
<span id="cb54-167"><a href="#cb54-167" aria-hidden="true" tabindex="-1"></a>\beta + \sum_{n = 1} ^ N</span>
<span id="cb54-168"><a href="#cb54-168" aria-hidden="true" tabindex="-1"></a>c_{n}\mbox{E}(\epsilon_{n} \mid x_n)</span>
<span id="cb54-169"><a href="#cb54-169" aria-hidden="true" tabindex="-1"></a>$$ {#eq-condexp}</span>
<span id="cb54-170"><a href="#cb54-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-171"><a href="#cb54-171" aria-hidden="true" tabindex="-1"></a>If the conditional expectation of the errors is constant</span>
<span id="cb54-172"><a href="#cb54-172" aria-hidden="true" tabindex="-1"></a>($\mbox{E}(\epsilon_n \mid x_n) = \mu_\epsilon$),</span>
<span id="cb54-173"><a href="#cb54-173" aria-hidden="true" tabindex="-1"></a>$\sum_{n=1}^N c_n\mbox{E}(\epsilon_n | x) = \mu_\epsilon\sum_{n=1}^N c_n = 0$</span>
<span id="cb54-174"><a href="#cb54-174" aria-hidden="true" tabindex="-1"></a>as $\sum_n c_n = 0$, so that $\mbox{E}(\hat{\beta} | x) = \beta$, which</span>
<span id="cb54-175"><a href="#cb54-175" aria-hidden="true" tabindex="-1"></a>means that the expected value of the estimator is the true value. In</span>
<span id="cb54-176"><a href="#cb54-176" aria-hidden="true" tabindex="-1"></a>this case, the estimator is **unbiased**. Therefore, the hypothesis of</span>
<span id="cb54-177"><a href="#cb54-177" aria-hidden="true" tabindex="-1"></a>constant conditional expectation of the errors is crucial.</span>
<span id="cb54-178"><a href="#cb54-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-179"><a href="#cb54-179" aria-hidden="true" tabindex="-1"></a>It is very important to understand why, in practice, this hypothesis may</span>
<span id="cb54-180"><a href="#cb54-180" aria-hidden="true" tabindex="-1"></a>be violated. As an illustration, consider the wage / education model. It</span>
<span id="cb54-181"><a href="#cb54-181" aria-hidden="true" tabindex="-1"></a>is well documented in a lot of countries that, for a given value of</span>
<span id="cb54-182"><a href="#cb54-182" aria-hidden="true" tabindex="-1"></a>education, women earn less than men on average. This means that the</span>
<span id="cb54-183"><a href="#cb54-183" aria-hidden="true" tabindex="-1"></a>conditional expectation of wage is lower for women or, graphically, that</span>
<span id="cb54-184"><a href="#cb54-184" aria-hidden="true" tabindex="-1"></a>in a scatterplot, points for women will be in general below the line</span>
<span id="cb54-185"><a href="#cb54-185" aria-hidden="true" tabindex="-1"></a>which indicates the conditional expectation of wages and that points for men will be above this line. To see whether this can induce a bias in the</span>
<span id="cb54-186"><a href="#cb54-186" aria-hidden="true" tabindex="-1"></a>OLS estimator, rewrite @eq-condexp as:</span>
<span id="cb54-187"><a href="#cb54-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-188"><a href="#cb54-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-189"><a href="#cb54-189" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\beta}\mid x) = \beta  + \frac{\sum_{n=1} ^ N (x_n - \bar{x})\mbox{E}(\epsilon_n | x_n)}</span>
<span id="cb54-190"><a href="#cb54-190" aria-hidden="true" tabindex="-1"></a>{\sum_{n=1} ^ N (x_n - \bar{x}) ^ 2}</span>
<span id="cb54-191"><a href="#cb54-191" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb54-192"><a href="#cb54-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-193"><a href="#cb54-193" aria-hidden="true" tabindex="-1"></a>The second term is the ratio of the covariance between $\epsilon$ and</span>
<span id="cb54-194"><a href="#cb54-194" aria-hidden="true" tabindex="-1"></a>$x$ and the variance of $x$. If not zero, this ratio is the bias of the</span>
<span id="cb54-195"><a href="#cb54-195" aria-hidden="true" tabindex="-1"></a>OLS estimator. In our example, the key question is whether being a male</span>
<span id="cb54-196"><a href="#cb54-196" aria-hidden="true" tabindex="-1"></a>is correlated with education:</span>
<span id="cb54-197"><a href="#cb54-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-198"><a href="#cb54-198" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>with no correlation, the OLS estimator is unbiased,</span>
<span id="cb54-199"><a href="#cb54-199" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>with a negative correlation, the OLS estimator is downward biased,</span>
<span id="cb54-200"><a href="#cb54-200" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>with a positive correlation, the OLS estimator is upward biased.</span>
<span id="cb54-201"><a href="#cb54-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-202"><a href="#cb54-202" aria-hidden="true" tabindex="-1"></a>These three situations are depicted in @fig-sex_educ. The common feature of the three rows of the figure is that women (depicted by circles) are in general below the expected value line (plain line) and men (depicted</span>
<span id="cb54-203"><a href="#cb54-203" aria-hidden="true" tabindex="-1"></a>by triangles) are above. For row 1 of the figure, considering the</span>
<span id="cb54-204"><a href="#cb54-204" aria-hidden="true" tabindex="-1"></a>horizontal position of the points, women and men are approximately</span>
<span id="cb54-205"><a href="#cb54-205" aria-hidden="true" tabindex="-1"></a>uniformly disposed, which indicates the absence of correlation between</span>
<span id="cb54-206"><a href="#cb54-206" aria-hidden="true" tabindex="-1"></a>education and being a male. The regression (dashed) line is then very close to</span>
<span id="cb54-207"><a href="#cb54-207" aria-hidden="true" tabindex="-1"></a>the expected value line, the OLS estimator is unbiased. For row 2 of the</span>
<span id="cb54-208"><a href="#cb54-208" aria-hidden="true" tabindex="-1"></a>figure, males have in general a lower level of education than females.</span>
<span id="cb54-209"><a href="#cb54-209" aria-hidden="true" tabindex="-1"></a>There is therefore a negative correlation between education and being a</span>
<span id="cb54-210"><a href="#cb54-210" aria-hidden="true" tabindex="-1"></a>man and the consequence is that the OLS estimator is downward biased.</span>
<span id="cb54-211"><a href="#cb54-211" aria-hidden="true" tabindex="-1"></a>Finally, for row 3 of the figure, the correlation is positive and the OLS</span>
<span id="cb54-212"><a href="#cb54-212" aria-hidden="true" tabindex="-1"></a>estimator is upward biased.</span>
<span id="cb54-213"><a href="#cb54-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-214"><a href="#cb54-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-217"><a href="#cb54-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-218"><a href="#cb54-218" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-sex_educ</span></span>
<span id="cb54-219"><a href="#cb54-219" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Education, sex and wage"</span></span>
<span id="cb54-220"><a href="#cb54-220" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-221"><a href="#cb54-221" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1.5</span></span>
<span id="cb54-222"><a href="#cb54-222" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 45%</span></span>
<span id="cb54-223"><a href="#cb54-223" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb54-224"><a href="#cb54-224" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(1L)</span>
<span id="cb54-225"><a href="#cb54-225" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb54-226"><a href="#cb54-226" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb54-227"><a href="#cb54-227" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb54-228"><a href="#cb54-228" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">1.2</span></span>
<span id="cb54-229"><a href="#cb54-229" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">8</span>, N, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb54-230"><a href="#cb54-230" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N <span class="sc">*</span> R)</span>
<span id="cb54-231"><a href="#cb54-231" aria-hidden="true" tabindex="-1"></a>sex <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="fl">0.5</span>),</span>
<span id="cb54-232"><a href="#cb54-232" aria-hidden="true" tabindex="-1"></a>         <span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="dv">1</span> <span class="sc">-</span> x <span class="sc">/</span> <span class="dv">8</span>),</span>
<span id="cb54-233"><a href="#cb54-233" aria-hidden="true" tabindex="-1"></a>         <span class="fu">rbinom</span>(N, <span class="dv">1</span>, x <span class="sc">/</span> <span class="dv">8</span>))</span>
<span id="cb54-234"><a href="#cb54-234" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rep</span>(x, <span class="dv">3</span>)</span>
<span id="cb54-235"><a href="#cb54-235" aria-hidden="true" tabindex="-1"></a>delta <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb54-236"><a href="#cb54-236" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">tibble</span>(x, <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> delta <span class="sc">*</span> (<span class="dv">2</span> <span class="sc">*</span> sex <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">+</span> eps, sex,</span>
<span id="cb54-237"><a href="#cb54-237" aria-hidden="true" tabindex="-1"></a>            <span class="at">smpl =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N))</span>
<span id="cb54-238"><a href="#cb54-238" aria-hidden="true" tabindex="-1"></a>z <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="fu">factor</span>(sex)) <span class="sc">+</span></span>
<span id="cb54-239"><a href="#cb54-239" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">10</span>, <span class="at">slope =</span> <span class="fl">1.2</span>) <span class="sc">+</span> </span>
<span id="cb54-240"><a href="#cb54-240" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">linetype =</span>  <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb54-241"><a href="#cb54-241" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> smpl, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb54-242"><a href="#cb54-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-243"><a href="#cb54-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-244"><a href="#cb54-244" aria-hidden="true" tabindex="-1"></a>Consider the latter case in details. Women have a lower wage than men</span>
<span id="cb54-245"><a href="#cb54-245" aria-hidden="true" tabindex="-1"></a>for two reasons: they are less educated and, for a given value of</span>
<span id="cb54-246"><a href="#cb54-246" aria-hidden="true" tabindex="-1"></a>education, they receive a lower wage than males. Increasing the</span>
<span id="cb54-247"><a href="#cb54-247" aria-hidden="true" tabindex="-1"></a>education level from, say, 4 to 5 years will have two effects on the</span>
<span id="cb54-248"><a href="#cb54-248" aria-hidden="true" tabindex="-1"></a>expected wage:</span>
<span id="cb54-249"><a href="#cb54-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-250"><a href="#cb54-250" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>direct positive effect of education on wage,</span>
<span id="cb54-251"><a href="#cb54-251" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>indirect positive effect: as being a man is positively correlated with education, considering a higher level of education, we'll get a subpopulation with a higher</span>
<span id="cb54-252"><a href="#cb54-252" aria-hidden="true" tabindex="-1"></a>    share of males, and therefore a higher wage.</span>
<span id="cb54-253"><a href="#cb54-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-254"><a href="#cb54-254" aria-hidden="true" tabindex="-1"></a>The OLS estimator estimates the sum of these two effects and is therefore in this case upward biased.</span>
<span id="cb54-255"><a href="#cb54-255" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias}</span>
<span id="cb54-256"><a href="#cb54-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-257"><a href="#cb54-257" aria-hidden="true" tabindex="-1"></a><span class="fu">### Estimator for the variance of the OLS estimator</span></span>
<span id="cb54-258"><a href="#cb54-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-259"><a href="#cb54-259" aria-hidden="true" tabindex="-1"></a>Consider now the conditional variance of the OLS estimator:</span>
<span id="cb54-260"><a href="#cb54-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-261"><a href="#cb54-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-262"><a href="#cb54-262" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-263"><a href="#cb54-263" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}\mid x )&amp; =</span>
<span id="cb54-264"><a href="#cb54-264" aria-hidden="true" tabindex="-1"></a>&amp;\mbox{E}\left(\left(\hat{\beta}-\beta\right)^ 2 \mid x\right)<span class="sc">\\</span></span>
<span id="cb54-265"><a href="#cb54-265" aria-hidden="true" tabindex="-1"></a>&amp; = &amp;\mbox{E}\left(\left(\sum_{n = 1} ^ N c_{n}\epsilon_{n}\right)^</span>
<span id="cb54-266"><a href="#cb54-266" aria-hidden="true" tabindex="-1"></a>2 \mid x \right)<span class="sc">\\</span></span>
<span id="cb54-267"><a href="#cb54-267" aria-hidden="true" tabindex="-1"></a>&amp; = &amp; \frac{1}{S_{xx}^2}\mbox{E}\left(\left(\sum_{n = 1} ^ N (x_n - \bar{x})\epsilon_{n}\right)^</span>
<span id="cb54-268"><a href="#cb54-268" aria-hidden="true" tabindex="-1"></a>2  \mid x\right)</span>
<span id="cb54-269"><a href="#cb54-269" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-270"><a href="#cb54-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-271"><a href="#cb54-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-272"><a href="#cb54-272" aria-hidden="true" tabindex="-1"></a>To compute the variance, we therefore have to take the expected value of</span>
<span id="cb54-273"><a href="#cb54-273" aria-hidden="true" tabindex="-1"></a>$N ^ 2$ terms, $N$ of them being of the form:</span>
<span id="cb54-274"><a href="#cb54-274" aria-hidden="true" tabindex="-1"></a>$(x_n - \bar{x}) ^ 2 \epsilon_n ^ 2$ and the $N ^ 2 - N$ other of the</span>
<span id="cb54-275"><a href="#cb54-275" aria-hidden="true" tabindex="-1"></a>form: $(x_n - \bar{x})(x_m - \bar{x})\epsilon_n\epsilon_m$. This is best</span>
<span id="cb54-276"><a href="#cb54-276" aria-hidden="true" tabindex="-1"></a>understood by arranging the $N^2$ terms in a square matrix of dimension $N$. With $N=4$, we have<span class="ot">[^simple_regression_properties-1]</span>:</span>
<span id="cb54-277"><a href="#cb54-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-278"><a href="#cb54-278" aria-hidden="true" tabindex="-1"></a><span class="ot">[^simple_regression_properties-1]: </span>We don't explicitly indicate that</span>
<span id="cb54-279"><a href="#cb54-279" aria-hidden="true" tabindex="-1"></a>    the expected values are conditional on $x$ to save space.</span>
<span id="cb54-280"><a href="#cb54-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-281"><a href="#cb54-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-282"><a href="#cb54-282" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-283"><a href="#cb54-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: asis</span></span>
<span id="cb54-284"><a href="#cb54-284" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: general_variance_terms</span></span>
<span id="cb54-285"><a href="#cb54-285" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"$$</span><span class="sc">\n\\</span><span class="st">scriptsize{</span><span class="sc">\\</span><span class="st">left(</span><span class="sc">\\</span><span class="st">begin{array}{cccc}</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb54-286"><a href="#cb54-286" aria-hidden="true" tabindex="-1"></a>strgr <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb54-287"><a href="#cb54-287" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb54-288"><a href="#cb54-288" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb54-289"><a href="#cb54-289" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="sc">==</span> j){</span>
<span id="cb54-290"><a href="#cb54-290" aria-hidden="true" tabindex="-1"></a>            strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">paste</span>(<span class="st">"(x_"</span>, i, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x})^2 </span><span class="sc">\\</span><span class="st">mbox{E}(</span><span class="sc">\\</span><span class="st">epsilon_"</span>, i, <span class="st">"^2)"</span>, <span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb54-291"><a href="#cb54-291" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb54-292"><a href="#cb54-292" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb54-293"><a href="#cb54-293" aria-hidden="true" tabindex="-1"></a>            strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">paste</span>(<span class="st">"(x_"</span>, i, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x}) (x_"</span>, j, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x}) </span><span class="sc">\\</span><span class="st">mbox{E}(</span><span class="sc">\\</span><span class="st">epsilon_"</span>, i, <span class="st">"</span><span class="sc">\\</span><span class="st">epsilon_"</span>, j, <span class="st">")"</span>,<span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb54-294"><a href="#cb54-294" aria-hidden="true" tabindex="-1"></a>        strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">ifelse</span>(j <span class="sc">==</span> <span class="dv">4</span>, <span class="st">" </span><span class="sc">\\\\</span><span class="st"> </span><span class="sc">\n</span><span class="st">"</span>, <span class="st">" &amp; </span><span class="sc">\n</span><span class="st">"</span>))</span>
<span id="cb54-295"><a href="#cb54-295" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb54-296"><a href="#cb54-296" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb54-297"><a href="#cb54-297" aria-hidden="true" tabindex="-1"></a>strgr <span class="ot">&lt;-</span> <span class="fu">paste</span>(strgr, <span class="at">collapse =</span> <span class="st">""</span>)</span>
<span id="cb54-298"><a href="#cb54-298" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(strgr)</span>
<span id="cb54-299"><a href="#cb54-299" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">end{array}</span><span class="sc">\n\\</span><span class="st">right)</span><span class="sc">\n</span><span class="st">}</span><span class="sc">\n</span><span class="st">$$"</span>)</span>
<span id="cb54-300"><a href="#cb54-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-301"><a href="#cb54-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-302"><a href="#cb54-302" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Uncorrelation and homoskedasticity</span></span>
<span id="cb54-303"><a href="#cb54-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-304"><a href="#cb54-304" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{uncorrelation|(}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{homoskedasticity|(}</span>
<span id="cb54-305"><a href="#cb54-305" aria-hidden="true" tabindex="-1"></a>$\mbox{V}(\hat{\beta}\mid x)$ is obtained by taking the sum of these</span>
<span id="cb54-306"><a href="#cb54-306" aria-hidden="true" tabindex="-1"></a>$N ^ 2$ terms, $N$ terms depending on conditional variances</span>
<span id="cb54-307"><a href="#cb54-307" aria-hidden="true" tabindex="-1"></a>($\mbox{E}(\epsilon_n ^ 2 \mid x_n) = \mbox{V}(\epsilon_n \mid x_n)$)</span>
<span id="cb54-308"><a href="#cb54-308" aria-hidden="true" tabindex="-1"></a>and $N\times(N -1)$ on conditional covariances</span>
<span id="cb54-309"><a href="#cb54-309" aria-hidden="true" tabindex="-1"></a>($\mbox{E}(\epsilon_n\epsilon_m\mid x_n, x_m) = \mbox{cov}(\epsilon_n, \epsilon_m\mid x_n, x_m)$).</span>
<span id="cb54-310"><a href="#cb54-310" aria-hidden="true" tabindex="-1"></a>The resulting estimator has a very simple form if two hypothesis are</span>
<span id="cb54-311"><a href="#cb54-311" aria-hidden="true" tabindex="-1"></a>made:</span>
<span id="cb54-312"><a href="#cb54-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-313"><a href="#cb54-313" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the errors are **homoskedastic**, which means that their variances</span>
<span id="cb54-314"><a href="#cb54-314" aria-hidden="true" tabindex="-1"></a>    don't depend on $x$, the $N$ terms that contain the conditional</span>
<span id="cb54-315"><a href="#cb54-315" aria-hidden="true" tabindex="-1"></a>    variances are then equal to $(x_n - \bar{x})^2\sigma_\epsilon ^ 2$,</span>
<span id="cb54-316"><a href="#cb54-316" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the errors are **uncorrelated**, the $N\times(N-1)$ terms that</span>
<span id="cb54-317"><a href="#cb54-317" aria-hidden="true" tabindex="-1"></a>    involve the covariance are then equal to 0.</span>
<span id="cb54-318"><a href="#cb54-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-319"><a href="#cb54-319" aria-hidden="true" tabindex="-1"></a>With these two hypotheses in hand, only the diagonal terms are not zero</span>
<span id="cb54-320"><a href="#cb54-320" aria-hidden="true" tabindex="-1"></a>and their sum is</span>
<span id="cb54-321"><a href="#cb54-321" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon ^ 2 \sum_{n=1}^N (x_n - \bar{x}) ^ 2 = \sigma_\epsilon ^ 2 S_{xx}$,</span>
<span id="cb54-322"><a href="#cb54-322" aria-hidden="true" tabindex="-1"></a>which finally leads to the simplified formula of the variance of</span>
<span id="cb54-323"><a href="#cb54-323" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}$:</span>
<span id="cb54-324"><a href="#cb54-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-325"><a href="#cb54-325" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-326"><a href="#cb54-326" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}\mid x) = \sigma_{\hat{\beta}} ^ 2 =  \frac{\sigma_\epsilon ^ 2S_{xx}}{S_{xx} ^ 2}=</span>
<span id="cb54-327"><a href="#cb54-327" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon ^ 2}{S_{xx}}</span>
<span id="cb54-328"><a href="#cb54-328" aria-hidden="true" tabindex="-1"></a>= \frac{\sigma_\epsilon ^ 2}{N\hat{\sigma}_x^2}</span>
<span id="cb54-329"><a href="#cb54-329" aria-hidden="true" tabindex="-1"></a>$$ {#eq-var_hbeta}</span>
<span id="cb54-330"><a href="#cb54-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-331"><a href="#cb54-331" aria-hidden="true" tabindex="-1"></a>Note that this is the "true" variance of $\hat{\beta}$ if the two</span>
<span id="cb54-332"><a href="#cb54-332" aria-hidden="true" tabindex="-1"></a>hypotheses are satisfied, and that it can't be computed as it depends on</span>
<span id="cb54-333"><a href="#cb54-333" aria-hidden="true" tabindex="-1"></a>the unknown parameter $\sigma_\epsilon$. The square root of @eq-var_hbeta</span>
<span id="cb54-334"><a href="#cb54-334" aria-hidden="true" tabindex="-1"></a>is the standard deviation of $\beta$, is measured in the same unit as</span>
<span id="cb54-335"><a href="#cb54-335" aria-hidden="true" tabindex="-1"></a>$\beta$ and is commonly called the **standard error** of $\hat{\beta}$. It is therefore a convenient indicator of the precision of the estimator:</span>
<span id="cb54-336"><a href="#cb54-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-337"><a href="#cb54-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-338"><a href="#cb54-338" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\beta}} = \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}</span>
<span id="cb54-339"><a href="#cb54-339" aria-hidden="true" tabindex="-1"></a>$$ {#eq-stder_hat_beta}</span>
<span id="cb54-340"><a href="#cb54-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-341"><a href="#cb54-341" aria-hidden="true" tabindex="-1"></a>It is clear from @eq-stder_hat_beta that the precision of the estimator depends on three components</span>
<span id="cb54-342"><a href="#cb54-342" aria-hidden="true" tabindex="-1"></a>which will be described in details in the next subsection.</span>
<span id="cb54-343"><a href="#cb54-343" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{uncorrelation|)}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{homoskedasticity|)}</span>
<span id="cb54-344"><a href="#cb54-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-345"><a href="#cb54-345" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Determinants of the precision of the OLS estimator</span></span>
<span id="cb54-346"><a href="#cb54-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-347"><a href="#cb54-347" aria-hidden="true" tabindex="-1"></a>First consider the "size" of the error, measured by its standard</span>
<span id="cb54-348"><a href="#cb54-348" aria-hidden="true" tabindex="-1"></a>deviation $\sigma_\epsilon$. @fig-sigmaeps presents a scatterplot for six</span>
<span id="cb54-349"><a href="#cb54-349" aria-hidden="true" tabindex="-1"></a>samples which use the same DGP, except that samples on the second line</span>
<span id="cb54-350"><a href="#cb54-350" aria-hidden="true" tabindex="-1"></a>are generated with a smaller value of $\sigma_\epsilon$. The "true</span>
<span id="cb54-351"><a href="#cb54-351" aria-hidden="true" tabindex="-1"></a>model" ($\alpha + \beta x$) is represented by a plain line and the regression line is dashed. Obviously, the estimation is much more precise on the second line of @fig-sigmaeps, because of small-sized errors.</span>
<span id="cb54-352"><a href="#cb54-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-353"><a href="#cb54-353" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-354"><a href="#cb54-354" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-sigmaeps</span></span>
<span id="cb54-355"><a href="#cb54-355" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Size of the error and the precision of the slope estimator"</span></span>
<span id="cb54-356"><a href="#cb54-356" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb54-357"><a href="#cb54-357" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb54-358"><a href="#cb54-358" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-359"><a href="#cb54-359" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-360"><a href="#cb54-360" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb54-361"><a href="#cb54-361" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb54-362"><a href="#cb54-362" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb54-363"><a href="#cb54-363" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb54-364"><a href="#cb54-364" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N <span class="sc">*</span> R, <span class="at">sd =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="fu">c</span>(s1, s2), <span class="at">each =</span> N <span class="sc">*</span> (R <span class="sc">/</span> <span class="dv">2</span>))))</span>
<span id="cb54-365"><a href="#cb54-365" aria-hidden="true" tabindex="-1"></a>.xref <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb54-366"><a href="#cb54-366" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rep</span>(.xref, R) , <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> <span class="fu">rep</span>(.xref, R) <span class="sc">+</span> eps,</span>
<span id="cb54-367"><a href="#cb54-367" aria-hidden="true" tabindex="-1"></a>       <span class="at">sd =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"high"</span>, <span class="st">"low"</span>), <span class="at">each =</span> N <span class="sc">*</span> (R <span class="sc">/</span> <span class="dv">2</span>)),</span>
<span id="cb54-368"><a href="#cb54-368" aria-hidden="true" tabindex="-1"></a>       <span class="at">smpl =</span> <span class="fu">rep</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> N), <span class="dv">2</span>))<span class="sc">%&gt;%</span></span>
<span id="cb54-369"><a href="#cb54-369" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb54-370"><a href="#cb54-370" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb54-371"><a href="#cb54-371" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> alpha, <span class="at">slope =</span> beta) <span class="sc">+</span> </span>
<span id="cb54-372"><a href="#cb54-372" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(sd <span class="sc">~</span> smpl)</span>
<span id="cb54-373"><a href="#cb54-373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-374"><a href="#cb54-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-375"><a href="#cb54-375" aria-hidden="true" tabindex="-1"></a>Next consider the sample size. In @fig-smplsize, we take the same value</span>
<span id="cb54-376"><a href="#cb54-376" aria-hidden="true" tabindex="-1"></a>of $\sigma_\epsilon$ as in row 1 of @fig-sigmaeps, but we</span>
<span id="cb54-377"><a href="#cb54-377" aria-hidden="true" tabindex="-1"></a>increase the sample size to 40 for the samples of the second line.</span>
<span id="cb54-378"><a href="#cb54-378" aria-hidden="true" tabindex="-1"></a>In large samples (second line in @fig-smplsize) the slope is very</span>
<span id="cb54-379"><a href="#cb54-379" aria-hidden="true" tabindex="-1"></a>precisely estimated, which means that the value of $\hat{\beta}$ is</span>
<span id="cb54-380"><a href="#cb54-380" aria-hidden="true" tabindex="-1"></a>almost the same from a sample to another. On the contrary, with a small</span>
<span id="cb54-381"><a href="#cb54-381" aria-hidden="true" tabindex="-1"></a>sample size (first line in @fig-smplsize), the slopes of the regression</span>
<span id="cb54-382"><a href="#cb54-382" aria-hidden="true" tabindex="-1"></a>lines are very different for the three samples, which indicates that the</span>
<span id="cb54-383"><a href="#cb54-383" aria-hidden="true" tabindex="-1"></a>standard error of $\hat{\beta}$ is high (or that the estimator is</span>
<span id="cb54-384"><a href="#cb54-384" aria-hidden="true" tabindex="-1"></a>imprecise).</span>
<span id="cb54-385"><a href="#cb54-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-386"><a href="#cb54-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-387"><a href="#cb54-387" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-smplsize</span></span>
<span id="cb54-388"><a href="#cb54-388" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Sample size and precision of the estimator"</span></span>
<span id="cb54-389"><a href="#cb54-389" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb54-390"><a href="#cb54-390" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb54-391"><a href="#cb54-391" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-392"><a href="#cb54-392" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-393"><a href="#cb54-393" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb54-394"><a href="#cb54-394" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb54-395"><a href="#cb54-395" aria-hidden="true" tabindex="-1"></a>N2 <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb54-396"><a href="#cb54-396" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb54-397"><a href="#cb54-397" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>((N <span class="sc">+</span> N2) <span class="sc">*</span> R <span class="sc">/</span> <span class="dv">2</span> , <span class="at">sd =</span> s1)</span>
<span id="cb54-398"><a href="#cb54-398" aria-hidden="true" tabindex="-1"></a>.x2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(N2, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb54-399"><a href="#cb54-399" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(.xref, R<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(.x2, R<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb54-400"><a href="#cb54-400" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> x , <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps,</span>
<span id="cb54-401"><a href="#cb54-401" aria-hidden="true" tabindex="-1"></a>       <span class="at">sd =</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"small"</span>, N <span class="sc">*</span> R <span class="sc">/</span> <span class="dv">2</span>), <span class="fu">rep</span>(<span class="st">"large"</span>, N2 <span class="sc">*</span> R <span class="sc">/</span> <span class="dv">2</span>)),</span>
<span id="cb54-402"><a href="#cb54-402" aria-hidden="true" tabindex="-1"></a>                   <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"small"</span>, <span class="st">"large"</span>)),</span>
<span id="cb54-403"><a href="#cb54-403" aria-hidden="true" tabindex="-1"></a>       <span class="at">smpl =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> N), <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> N2))) <span class="sc">%&gt;%</span> </span>
<span id="cb54-404"><a href="#cb54-404" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb54-405"><a href="#cb54-405" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> alpha, <span class="at">slope =</span> beta) <span class="sc">+</span> </span>
<span id="cb54-406"><a href="#cb54-406" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb54-407"><a href="#cb54-407" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(sd <span class="sc">~</span> smpl)</span>
<span id="cb54-408"><a href="#cb54-408" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-409"><a href="#cb54-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-410"><a href="#cb54-410" aria-hidden="true" tabindex="-1"></a>Finally, in @fig-varx, we consider a variation of the variance of $x$. For</span>
<span id="cb54-411"><a href="#cb54-411" aria-hidden="true" tabindex="-1"></a>samples on the second line, the variance of $x$ is much smaller than for</span>
<span id="cb54-412"><a href="#cb54-412" aria-hidden="true" tabindex="-1"></a>samples on the first line. The larger the variance of $x$ is, the more</span>
<span id="cb54-413"><a href="#cb54-413" aria-hidden="true" tabindex="-1"></a>precise is the estimator of the slope. Obviously, it is difficult to</span>
<span id="cb54-414"><a href="#cb54-414" aria-hidden="true" tabindex="-1"></a>estimate the effect of education on wage if all the individuals in the</span>
<span id="cb54-415"><a href="#cb54-415" aria-hidden="true" tabindex="-1"></a>sample have almost the same level of education. Consider the extreme</span>
<span id="cb54-416"><a href="#cb54-416" aria-hidden="true" tabindex="-1"></a>case of no variation of $x$ in a sample; in this case it is impossible to estimate the effect of $x$, as all the observations are characterized by the same</span>
<span id="cb54-417"><a href="#cb54-417" aria-hidden="true" tabindex="-1"></a>value of $x$.</span>
<span id="cb54-418"><a href="#cb54-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-419"><a href="#cb54-419" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-420"><a href="#cb54-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-varx</span></span>
<span id="cb54-421"><a href="#cb54-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Variance of $x$ and precision of the estimator"</span></span>
<span id="cb54-422"><a href="#cb54-422" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-423"><a href="#cb54-423" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb54-424"><a href="#cb54-424" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-425"><a href="#cb54-425" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-426"><a href="#cb54-426" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb54-427"><a href="#cb54-427" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb54-428"><a href="#cb54-428" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb54-429"><a href="#cb54-429" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N <span class="sc">*</span> R, <span class="at">sd =</span> s1)</span>
<span id="cb54-430"><a href="#cb54-430" aria-hidden="true" tabindex="-1"></a>.x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb54-431"><a href="#cb54-431" aria-hidden="true" tabindex="-1"></a>.x2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="fl">0.7</span>, <span class="fl">1.3</span>)</span>
<span id="cb54-432"><a href="#cb54-432" aria-hidden="true" tabindex="-1"></a>.x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(.xref, R <span class="sc">/</span> <span class="dv">2</span>), <span class="fu">rep</span>(.x2, R <span class="sc">/</span> <span class="dv">2</span>))</span>
<span id="cb54-433"><a href="#cb54-433" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> .x , <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> .x <span class="sc">+</span> eps,</span>
<span id="cb54-434"><a href="#cb54-434" aria-hidden="true" tabindex="-1"></a>       <span class="at">sd =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"large"</span>, <span class="st">"small"</span>), <span class="at">each =</span> N <span class="sc">*</span> (R <span class="sc">/</span> <span class="dv">2</span>)), </span>
<span id="cb54-435"><a href="#cb54-435" aria-hidden="true" tabindex="-1"></a>                   <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"large"</span>, <span class="st">"small"</span>)),</span>
<span id="cb54-436"><a href="#cb54-436" aria-hidden="true" tabindex="-1"></a>       <span class="at">smpl =</span> <span class="fu">rep</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> N), <span class="dv">2</span>))<span class="sc">%&gt;%</span></span>
<span id="cb54-437"><a href="#cb54-437" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-438"><a href="#cb54-438" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> alpha, <span class="at">slope =</span> beta) <span class="sc">+</span> </span>
<span id="cb54-439"><a href="#cb54-439" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb54-440"><a href="#cb54-440" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(sd <span class="sc">~</span> smpl)</span>
<span id="cb54-441"><a href="#cb54-441" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-442"><a href="#cb54-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-445"><a href="#cb54-445" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-446"><a href="#cb54-446" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-447"><a href="#cb54-447" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: sample_sd_x</span></span>
<span id="cb54-448"><a href="#cb54-448" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">pull</span>(prtime, h)</span>
<span id="cb54-449"><a href="#cb54-449" aria-hidden="true" tabindex="-1"></a>hsx <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb54-450"><a href="#cb54-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-451"><a href="#cb54-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-452"><a href="#cb54-452" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!variance of the OLS estimator|(}</span>
<span id="cb54-453"><a href="#cb54-453" aria-hidden="true" tabindex="-1"></a>All these results can be illustrated by simulations, using the <span class="in">`price_time`</span> data set. For convenience, we replicate in the following code the operations we performed on this data set in @sec-simple_ols:</span>
<span id="cb54-454"><a href="#cb54-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-457"><a href="#cb54-457" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-458"><a href="#cb54-458" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> price_time <span class="sc">%&gt;%</span></span>
<span id="cb54-459"><a href="#cb54-459" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="fu">c</span>(<span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-460"><a href="#cb54-460" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">sr =</span> qr <span class="sc">/</span> (qr <span class="sc">+</span> qa),</span>
<span id="cb54-461"><a href="#cb54-461" aria-hidden="true" tabindex="-1"></a>         <span class="at">h =</span> (pa <span class="sc">-</span> pr) <span class="sc">/</span> ( (tr <span class="sc">-</span> ta) <span class="sc">/</span> <span class="dv">60</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-462"><a href="#cb54-462" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(sr <span class="sc">&lt;</span> <span class="fl">0.75</span>)</span>
<span id="cb54-463"><a href="#cb54-463" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-464"><a href="#cb54-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-465"><a href="#cb54-465" aria-hidden="true" tabindex="-1"></a>We start with our reference case ($\sigma_\epsilon = 0.08$, $N = 9$ and $x$ is the vector of the threshold value of time for the nine selected cities of the</span>
<span id="cb54-466"><a href="#cb54-466" aria-hidden="true" tabindex="-1"></a><span class="in">`prtime`</span> data set, with a sample standard deviation</span>
<span id="cb54-467"><a href="#cb54-467" aria-hidden="true" tabindex="-1"></a>$\hat{\sigma}_x = <span class="in">`r round(hsx, 3)`</span>$).</span>
<span id="cb54-468"><a href="#cb54-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-469"><a href="#cb54-469" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-470"><a href="#cb54-470" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.2</span> ; beta <span class="ot">&lt;-</span> <span class="fl">0.032</span></span>
<span id="cb54-471"><a href="#cb54-471" aria-hidden="true" tabindex="-1"></a>seps <span class="ot">&lt;-</span> <span class="fl">0.08</span></span>
<span id="cb54-472"><a href="#cb54-472" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">pull</span>(prtime, h)</span>
<span id="cb54-473"><a href="#cb54-473" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb54-474"><a href="#cb54-474" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-475"><a href="#cb54-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-476"><a href="#cb54-476" aria-hidden="true" tabindex="-1"></a>We generate $R = 100$ samples using each time the same vector of covariate ($x$) and drawing the errors in a normal</span>
<span id="cb54-477"><a href="#cb54-477" aria-hidden="true" tabindex="-1"></a>distribution:</span>
<span id="cb54-478"><a href="#cb54-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-479"><a href="#cb54-479" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-480"><a href="#cb54-480" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data_reference</span></span>
<span id="cb54-481"><a href="#cb54-481" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb54-482"><a href="#cb54-482" aria-hidden="true" tabindex="-1"></a>dataref <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">smpls =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb54-483"><a href="#cb54-483" aria-hidden="true" tabindex="-1"></a>                  <span class="at">x     =</span> <span class="fu">rep</span>(x, R),</span>
<span id="cb54-484"><a href="#cb54-484" aria-hidden="true" tabindex="-1"></a>                  <span class="at">eps   =</span> <span class="fu">rnorm</span>(R <span class="sc">*</span> N, <span class="at">sd =</span> seps),</span>
<span id="cb54-485"><a href="#cb54-485" aria-hidden="true" tabindex="-1"></a>                  <span class="at">y     =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps)</span>
<span id="cb54-486"><a href="#cb54-486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-487"><a href="#cb54-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-488"><a href="#cb54-488" aria-hidden="true" tabindex="-1"></a>To illustrate the influence of $\sigma_\epsilon$ on the precision of the</span>
<span id="cb54-489"><a href="#cb54-489" aria-hidden="true" tabindex="-1"></a>estimator, we take a value of $\sigma_\epsilon =0.04$, i.e., we divide</span>
<span id="cb54-490"><a href="#cb54-490" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon$ by 2 compared to the reference case:</span>
<span id="cb54-491"><a href="#cb54-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-492"><a href="#cb54-492" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-493"><a href="#cb54-493" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data_low_sd_epsilon</span></span>
<span id="cb54-494"><a href="#cb54-494" aria-hidden="true" tabindex="-1"></a>dataseps <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">smpls =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb54-495"><a href="#cb54-495" aria-hidden="true" tabindex="-1"></a>                   <span class="at">x     =</span> <span class="fu">rep</span>(x, R),</span>
<span id="cb54-496"><a href="#cb54-496" aria-hidden="true" tabindex="-1"></a>                   <span class="at">eps   =</span> <span class="fu">rnorm</span>(R <span class="sc">*</span> N, <span class="at">sd =</span> seps <span class="sc">/</span> <span class="dv">2</span>),</span>
<span id="cb54-497"><a href="#cb54-497" aria-hidden="true" tabindex="-1"></a>                   <span class="at">y     =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps)</span>
<span id="cb54-498"><a href="#cb54-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-499"><a href="#cb54-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-500"><a href="#cb54-500" aria-hidden="true" tabindex="-1"></a>Next, we increase the sample size to $N = 36$, i.e., we multiply the sample size by 4. More specifically, for every sample each value of $x$ is</span>
<span id="cb54-501"><a href="#cb54-501" aria-hidden="true" tabindex="-1"></a>repeated four times:</span>
<span id="cb54-502"><a href="#cb54-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-503"><a href="#cb54-503" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-504"><a href="#cb54-504" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data_sample_size</span></span>
<span id="cb54-505"><a href="#cb54-505" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x) <span class="sc">*</span> <span class="dv">4</span></span>
<span id="cb54-506"><a href="#cb54-506" aria-hidden="true" tabindex="-1"></a>xN <span class="ot">&lt;-</span> <span class="fu">rep</span>(x, <span class="dv">4</span>)</span>
<span id="cb54-507"><a href="#cb54-507" aria-hidden="true" tabindex="-1"></a>datasN <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">smpls =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb54-508"><a href="#cb54-508" aria-hidden="true" tabindex="-1"></a>                   <span class="at">x     =</span> <span class="fu">rep</span>(xN, R),</span>
<span id="cb54-509"><a href="#cb54-509" aria-hidden="true" tabindex="-1"></a>                   <span class="at">eps   =</span> <span class="fu">rnorm</span>(R <span class="sc">*</span> N, <span class="at">sd =</span> seps),</span>
<span id="cb54-510"><a href="#cb54-510" aria-hidden="true" tabindex="-1"></a>                   <span class="at">y     =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps)</span>
<span id="cb54-511"><a href="#cb54-511" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-512"><a href="#cb54-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-513"><a href="#cb54-513" aria-hidden="true" tabindex="-1"></a>Finally, we increase the variation of $x$, simply by multiplying all the</span>
<span id="cb54-514"><a href="#cb54-514" aria-hidden="true" tabindex="-1"></a>values by 2. In this case, the standard deviation of $x$ is also</span>
<span id="cb54-515"><a href="#cb54-515" aria-hidden="true" tabindex="-1"></a>multiplied by 2.</span>
<span id="cb54-516"><a href="#cb54-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-517"><a href="#cb54-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-518"><a href="#cb54-518" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data_increased_sd_x</span></span>
<span id="cb54-519"><a href="#cb54-519" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb54-520"><a href="#cb54-520" aria-hidden="true" tabindex="-1"></a>xv <span class="ot">&lt;-</span> x <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb54-521"><a href="#cb54-521" aria-hidden="true" tabindex="-1"></a>datasvx <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">smpls =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb54-522"><a href="#cb54-522" aria-hidden="true" tabindex="-1"></a>                   <span class="at">x     =</span> <span class="fu">rep</span>(xv, R),</span>
<span id="cb54-523"><a href="#cb54-523" aria-hidden="true" tabindex="-1"></a>                   <span class="at">eps   =</span> <span class="fu">rnorm</span>(R <span class="sc">*</span> N, <span class="at">sd=</span> seps),</span>
<span id="cb54-524"><a href="#cb54-524" aria-hidden="true" tabindex="-1"></a>                   <span class="at">y     =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps)</span>
<span id="cb54-525"><a href="#cb54-525" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-526"><a href="#cb54-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-527"><a href="#cb54-527" aria-hidden="true" tabindex="-1"></a>The standard deviation for the reference case is:</span>
<span id="cb54-528"><a href="#cb54-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-529"><a href="#cb54-529" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-530"><a href="#cb54-530" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\beta}} = \frac{\sigma_\epsilon}{\sqrt{N} \hat{\sigma}_x}=</span>
<span id="cb54-531"><a href="#cb54-531" aria-hidden="true" tabindex="-1"></a>\frac{<span class="in">`r round(seps, 3)`</span>}{\sqrt{<span class="in">`r round(N, 0)`</span>} \times</span>
<span id="cb54-532"><a href="#cb54-532" aria-hidden="true" tabindex="-1"></a>     <span class="in">`r round(hsx, 3)`</span>}</span>
<span id="cb54-533"><a href="#cb54-533" aria-hidden="true" tabindex="-1"></a>= <span class="in">`r round(seps / sqrt(N) / hsx, 5)`</span></span>
<span id="cb54-534"><a href="#cb54-534" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb54-535"><a href="#cb54-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-536"><a href="#cb54-536" aria-hidden="true" tabindex="-1"></a>which is very close to the standard deviation of $\hat{\beta}$</span>
<span id="cb54-537"><a href="#cb54-537" aria-hidden="true" tabindex="-1"></a>for our $R = 100$ samples:</span>
<span id="cb54-538"><a href="#cb54-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-539"><a href="#cb54-539" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-540"><a href="#cb54-540" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: results_reference</span></span>
<span id="cb54-541"><a href="#cb54-541" aria-hidden="true" tabindex="-1"></a>dataref <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(smpls) <span class="sc">%&gt;%</span></span>
<span id="cb54-542"><a href="#cb54-542" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">slope =</span> <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y)) ) <span class="sc">/</span> </span>
<span id="cb54-543"><a href="#cb54-543" aria-hidden="true" tabindex="-1"></a>              <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-544"><a href="#cb54-544" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(slope), <span class="at">sd =</span> <span class="fu">sd</span>(slope))</span>
<span id="cb54-545"><a href="#cb54-545" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-546"><a href="#cb54-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-547"><a href="#cb54-547" aria-hidden="true" tabindex="-1"></a>Note that we used twice the <span class="in">`summarise`</span>\idxfun{summarise}{dplyr} function. The first time, it is used with <span class="in">`group_by`</span>\idxfun{group<span class="sc">\_</span>by}{dplyr} so that a tibble with <span class="in">`R`</span> lines is returned, containing the values of the estimator <span class="in">`slope`</span>. The second time, a one-line tibble is returned containing the mean and the standard</span>
<span id="cb54-548"><a href="#cb54-548" aria-hidden="true" tabindex="-1"></a>deviation of the <span class="in">`R`</span> values of the estimator.</span>
<span id="cb54-549"><a href="#cb54-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-550"><a href="#cb54-550" aria-hidden="true" tabindex="-1"></a>When $\sigma_\epsilon$ is divided by 2 (from 0.08 to 0.04), the standard</span>
<span id="cb54-551"><a href="#cb54-551" aria-hidden="true" tabindex="-1"></a>deviation of $\hat{\beta}$ should also be divided by 2 (from $0.00438$</span>
<span id="cb54-552"><a href="#cb54-552" aria-hidden="true" tabindex="-1"></a>to $0.00219$), which is approximately the value of the standard</span>
<span id="cb54-553"><a href="#cb54-553" aria-hidden="true" tabindex="-1"></a>deviation of the values of $\hat{\beta}$ for our $R = 100$ samples:</span>
<span id="cb54-554"><a href="#cb54-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-555"><a href="#cb54-555" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-556"><a href="#cb54-556" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: results_small_sd_epsilon</span></span>
<span id="cb54-557"><a href="#cb54-557" aria-hidden="true" tabindex="-1"></a>dataseps <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(smpls) <span class="sc">%&gt;%</span> </span>
<span id="cb54-558"><a href="#cb54-558" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">hbeta =</span> <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y)) ) <span class="sc">/</span> </span>
<span id="cb54-559"><a href="#cb54-559" aria-hidden="true" tabindex="-1"></a>              <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-560"><a href="#cb54-560" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(hbeta), <span class="at">sd =</span> <span class="fu">sd</span>(hbeta))</span>
<span id="cb54-561"><a href="#cb54-561" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-562"><a href="#cb54-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-563"><a href="#cb54-563" aria-hidden="true" tabindex="-1"></a>When the sample size is multiplied by 4, $\hat{\sigma}_\beta$ should </span>
<span id="cb54-564"><a href="#cb54-564" aria-hidden="true" tabindex="-1"></a>also be divided by 2:</span>
<span id="cb54-565"><a href="#cb54-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-566"><a href="#cb54-566" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-567"><a href="#cb54-567" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: results_sample_size</span></span>
<span id="cb54-568"><a href="#cb54-568" aria-hidden="true" tabindex="-1"></a>datasN <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(smpls) <span class="sc">%&gt;%</span> </span>
<span id="cb54-569"><a href="#cb54-569" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">hbeta =</span> <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y)) ) <span class="sc">/</span> </span>
<span id="cb54-570"><a href="#cb54-570" aria-hidden="true" tabindex="-1"></a>              <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-571"><a href="#cb54-571" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(hbeta), <span class="at">sd =</span> <span class="fu">sd</span>(hbeta))</span>
<span id="cb54-572"><a href="#cb54-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-573"><a href="#cb54-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-574"><a href="#cb54-574" aria-hidden="true" tabindex="-1"></a>Finally, when every value of $x$ is multiplied by 2, $x$'s standard</span>
<span id="cb54-575"><a href="#cb54-575" aria-hidden="true" tabindex="-1"></a>deviation is also multiplied by 2 and $\hat{\sigma}_\beta$ should be</span>
<span id="cb54-576"><a href="#cb54-576" aria-hidden="true" tabindex="-1"></a>divided by 2:</span>
<span id="cb54-577"><a href="#cb54-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-578"><a href="#cb54-578" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-579"><a href="#cb54-579" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: results_increased_sd_x</span></span>
<span id="cb54-580"><a href="#cb54-580" aria-hidden="true" tabindex="-1"></a>datasvx <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(smpls) <span class="sc">%&gt;%</span> </span>
<span id="cb54-581"><a href="#cb54-581" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">hbeta =</span> <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y)) ) <span class="sc">/</span> </span>
<span id="cb54-582"><a href="#cb54-582" aria-hidden="true" tabindex="-1"></a>              <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-583"><a href="#cb54-583" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(hbeta), <span class="at">sd =</span> <span class="fu">sd</span>(hbeta))</span>
<span id="cb54-584"><a href="#cb54-584" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-585"><a href="#cb54-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-586"><a href="#cb54-586" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!variance of the OLS estimator|)}</span>
<span id="cb54-587"><a href="#cb54-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-588"><a href="#cb54-588" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Variance of $\hat{\alpha}$ and covariance between $\hat{\alpha}$ and $\hat{\beta}$</span></span>
<span id="cb54-589"><a href="#cb54-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-590"><a href="#cb54-590" aria-hidden="true" tabindex="-1"></a>To get the variance of the estimator of the intercept, consider the</span>
<span id="cb54-591"><a href="#cb54-591" aria-hidden="true" tabindex="-1"></a>"true" and the fitted model for one observation:</span>
<span id="cb54-592"><a href="#cb54-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-593"><a href="#cb54-593" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-594"><a href="#cb54-594" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-595"><a href="#cb54-595" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-596"><a href="#cb54-596" aria-hidden="true" tabindex="-1"></a>y_n &amp;=&amp; \alpha +\beta x_n + \epsilon_n<span class="sc">\\</span></span>
<span id="cb54-597"><a href="#cb54-597" aria-hidden="true" tabindex="-1"></a>y_n &amp;=&amp; \hat{\alpha} + \hat{\beta} x_n+ \hat{\epsilon}_n<span class="sc">\\</span></span>
<span id="cb54-598"><a href="#cb54-598" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-599"><a href="#cb54-599" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-600"><a href="#cb54-600" aria-hidden="true" tabindex="-1"></a>$$ {#eq-true_estimated_model}</span>
<span id="cb54-601"><a href="#cb54-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-602"><a href="#cb54-602" aria-hidden="true" tabindex="-1"></a>Equating the two expressions in @eq-true_estimated_model, we get:</span>
<span id="cb54-603"><a href="#cb54-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-604"><a href="#cb54-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-605"><a href="#cb54-605" aria-hidden="true" tabindex="-1"></a>(\hat{\alpha} - \alpha) + (\hat{\beta} - \beta) x_n + (\hat{\epsilon}_n - \epsilon_n) = 0</span>
<span id="cb54-606"><a href="#cb54-606" aria-hidden="true" tabindex="-1"></a>$$ {#eq-diff_true_estimated_model}</span>
<span id="cb54-607"><a href="#cb54-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-608"><a href="#cb54-608" aria-hidden="true" tabindex="-1"></a>Summing @eq-diff_true_estimated_model for the whole sample and dividing</span>
<span id="cb54-609"><a href="#cb54-609" aria-hidden="true" tabindex="-1"></a>by $N$:^<span class="co">[</span><span class="ot">Note that the mean of the residuals is 0.</span><span class="co">]</span></span>
<span id="cb54-610"><a href="#cb54-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-611"><a href="#cb54-611" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-612"><a href="#cb54-612" aria-hidden="true" tabindex="-1"></a>(\hat{\alpha} - \alpha) + (\hat{\beta} - \beta) \bar{x} - \bar{\epsilon} = 0</span>
<span id="cb54-613"><a href="#cb54-613" aria-hidden="true" tabindex="-1"></a>$$ {#eq-diff_true_estimated_mean}</span>
<span id="cb54-614"><a href="#cb54-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-615"><a href="#cb54-615" aria-hidden="true" tabindex="-1"></a>Finally, subtracting @eq-diff_true_estimated_mean from @eq-diff_true_estimated_model:</span>
<span id="cb54-616"><a href="#cb54-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-617"><a href="#cb54-617" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-618"><a href="#cb54-618" aria-hidden="true" tabindex="-1"></a>(\hat{\beta} - \beta)(x_n - \bar{x}) + \hat{\epsilon}_n - (\epsilon_n - \bar{\epsilon}) = 0</span>
<span id="cb54-619"><a href="#cb54-619" aria-hidden="true" tabindex="-1"></a>$$ {#eq-diff_true_estimated_dev}</span>
<span id="cb54-620"><a href="#cb54-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-621"><a href="#cb54-621" aria-hidden="true" tabindex="-1"></a>From @eq-diff_true_estimated_mean, the variance of $\hat{\alpha}$ is the expected value of the square of:</span>
<span id="cb54-622"><a href="#cb54-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-623"><a href="#cb54-623" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-624"><a href="#cb54-624" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-625"><a href="#cb54-625" aria-hidden="true" tabindex="-1"></a>(\hat{\alpha} - \alpha) &amp;=&amp;  - (\hat{\beta} - \beta) \bar{x} +</span>
<span id="cb54-626"><a href="#cb54-626" aria-hidden="true" tabindex="-1"></a>\bar{\epsilon}<span class="sc">\\</span></span>
<span id="cb54-627"><a href="#cb54-627" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; -\sum_n c_n \epsilon_n \bar{x} + \bar{\epsilon} <span class="sc">\\</span></span>
<span id="cb54-628"><a href="#cb54-628" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; -\sum_n \left(\bar{x} c_n - \frac{1}{N}\right)\epsilon_n</span>
<span id="cb54-629"><a href="#cb54-629" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-630"><a href="#cb54-630" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-631"><a href="#cb54-631" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{homoskedasticity}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{uncorrelation}</span>
<span id="cb54-632"><a href="#cb54-632" aria-hidden="true" tabindex="-1"></a>With the hypothesis of homoskedastic and uncorrelated errors, the variance simplifies to:</span>
<span id="cb54-633"><a href="#cb54-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-634"><a href="#cb54-634" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-635"><a href="#cb54-635" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\alpha}} ^ 2 = \sigma_\epsilon^2 </span>
<span id="cb54-636"><a href="#cb54-636" aria-hidden="true" tabindex="-1"></a>\sum_n \left(\bar{x} ^ 2 c_n ^ 2 + \frac{1}{N ^ 2} -</span>
<span id="cb54-637"><a href="#cb54-637" aria-hidden="true" tabindex="-1"></a>\frac{2\bar{x}}{N} c_n\right)</span>
<span id="cb54-638"><a href="#cb54-638" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-639"><a href="#cb54-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-640"><a href="#cb54-640" aria-hidden="true" tabindex="-1"></a>As $\sum_n c_n = 0$ and $\sum_n c_n ^ 2 = \frac{1}{N \hat{\sigma}_x ^ 2}$, we finally get:</span>
<span id="cb54-641"><a href="#cb54-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-642"><a href="#cb54-642" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-643"><a href="#cb54-643" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\alpha}} ^ 2 = \frac{\sigma_\epsilon ^ 2}{N \hat{\sigma}_x ^</span>
<span id="cb54-644"><a href="#cb54-644" aria-hidden="true" tabindex="-1"></a>2}(\hat{\sigma}_x ^ 2 + \bar{x} ^ 2)</span>
<span id="cb54-645"><a href="#cb54-645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-646"><a href="#cb54-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-647"><a href="#cb54-647" aria-hidden="true" tabindex="-1"></a>Finally, to get the covariance between the slope and the intercept, we</span>
<span id="cb54-648"><a href="#cb54-648" aria-hidden="true" tabindex="-1"></a>take the product of the two estimators in deviation from their expected</span>
<span id="cb54-649"><a href="#cb54-649" aria-hidden="true" tabindex="-1"></a>values.</span>
<span id="cb54-650"><a href="#cb54-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-651"><a href="#cb54-651" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-652"><a href="#cb54-652" aria-hidden="true" tabindex="-1"></a>(\hat{\alpha} - \alpha)(\hat{\beta} - \beta) = </span>
<span id="cb54-653"><a href="#cb54-653" aria-hidden="true" tabindex="-1"></a>-\left<span class="co">[</span><span class="ot">\sum_n \left(\bar{x} c_n - \frac{1}{N}\right)\epsilon_n\right</span><span class="co">]</span></span>
<span id="cb54-654"><a href="#cb54-654" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot">\sum_n c_n \epsilon_n\right</span><span class="co">]</span></span>
<span id="cb54-655"><a href="#cb54-655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-656"><a href="#cb54-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-657"><a href="#cb54-657" aria-hidden="true" tabindex="-1"></a>Taking the expected value, we get:</span>
<span id="cb54-658"><a href="#cb54-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-659"><a href="#cb54-659" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-660"><a href="#cb54-660" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\alpha}\hat{\beta}} = - \sigma_\epsilon ^ 2 \sum_n</span>
<span id="cb54-661"><a href="#cb54-661" aria-hidden="true" tabindex="-1"></a>\left(\bar{x} c_n ^ 2  - \frac{1}{N} c_n\right) = -</span>
<span id="cb54-662"><a href="#cb54-662" aria-hidden="true" tabindex="-1"></a>\bar{x}\frac{\sigma_\epsilon ^ 2}{N \hat{\sigma}_x ^ 2}</span>
<span id="cb54-663"><a href="#cb54-663" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-664"><a href="#cb54-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-665"><a href="#cb54-665" aria-hidden="true" tabindex="-1"></a>We can then compactly write the variances and the covariance of the OLS</span>
<span id="cb54-666"><a href="#cb54-666" aria-hidden="true" tabindex="-1"></a>estimator in matrix form:</span>
<span id="cb54-667"><a href="#cb54-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-668"><a href="#cb54-668" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-669"><a href="#cb54-669" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb54-670"><a href="#cb54-670" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb54-671"><a href="#cb54-671" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\alpha}} ^ 2 &amp; \sigma_{\hat{\alpha}\hat{\beta}} <span class="sc">\\</span></span>
<span id="cb54-672"><a href="#cb54-672" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\alpha}\hat{\beta}} &amp; \sigma_{\hat{\beta}} ^ 2</span>
<span id="cb54-673"><a href="#cb54-673" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-674"><a href="#cb54-674" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb54-675"><a href="#cb54-675" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb54-676"><a href="#cb54-676" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon ^ 2}{N\hat{\sigma}_x ^ 2}</span>
<span id="cb54-677"><a href="#cb54-677" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb54-678"><a href="#cb54-678" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb54-679"><a href="#cb54-679" aria-hidden="true" tabindex="-1"></a>\bar{x} ^ 2 + \hat{\sigma}_x ^ 2 &amp; - \bar{x} <span class="sc">\\</span></span>
<span id="cb54-680"><a href="#cb54-680" aria-hidden="true" tabindex="-1"></a>-\bar{x} &amp; 1</span>
<span id="cb54-681"><a href="#cb54-681" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-682"><a href="#cb54-682" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb54-683"><a href="#cb54-683" aria-hidden="true" tabindex="-1"></a>$$ {#eq-covariance_gamma}</span>
<span id="cb54-684"><a href="#cb54-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-685"><a href="#cb54-685" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimation of the variance of the errors</span></span>
<span id="cb54-686"><a href="#cb54-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-687"><a href="#cb54-687" aria-hidden="true" tabindex="-1"></a>The standard deviation of the OLS estimator can't be computed because it</span>
<span id="cb54-688"><a href="#cb54-688" aria-hidden="true" tabindex="-1"></a>depends on an unknown parameter $\sigma_\epsilon$. To get an estimation</span>
<span id="cb54-689"><a href="#cb54-689" aria-hidden="true" tabindex="-1"></a>of $\sigma_{\hat{\beta}}$, we therefore need to estimate first</span>
<span id="cb54-690"><a href="#cb54-690" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon$. If the error were observed, a natural estimator would</span>
<span id="cb54-691"><a href="#cb54-691" aria-hidden="true" tabindex="-1"></a>be obtained by computing the empirical variance of the errors in the</span>
<span id="cb54-692"><a href="#cb54-692" aria-hidden="true" tabindex="-1"></a>sample: $\frac{1}{N}\sum_{n=1} ^ N (\epsilon_n -\bar{\epsilon}) ^ 2$. As</span>
<span id="cb54-693"><a href="#cb54-693" aria-hidden="true" tabindex="-1"></a>the errors are not observed, this estimator cannot be computed, but a</span>
<span id="cb54-694"><a href="#cb54-694" aria-hidden="true" tabindex="-1"></a>feasible estimator is obtained by replacing the unobserved errors by the</span>
<span id="cb54-695"><a href="#cb54-695" aria-hidden="true" tabindex="-1"></a>residuals:</span>
<span id="cb54-696"><a href="#cb54-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-697"><a href="#cb54-697" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-698"><a href="#cb54-698" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_\epsilon ^ 2 = \frac{\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2}{N}</span>
<span id="cb54-699"><a href="#cb54-699" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-700"><a href="#cb54-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-701"><a href="#cb54-701" aria-hidden="true" tabindex="-1"></a>To analyze the properties of this estimator, we first compute the</span>
<span id="cb54-702"><a href="#cb54-702" aria-hidden="true" tabindex="-1"></a>variance of one residual. From @eq-diff_true_estimated_dev, a residual can be written as:</span>
<span id="cb54-703"><a href="#cb54-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-704"><a href="#cb54-704" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-705"><a href="#cb54-705" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_n = \epsilon_n - \bar{\epsilon}- (\hat{\beta} - \beta)(x_n - \bar{x})=</span>
<span id="cb54-706"><a href="#cb54-706" aria-hidden="true" tabindex="-1"></a>\epsilon_n - \frac{1}{N}\sum_n \epsilon_n - (x_n - \bar{x})\sum_n c_n\epsilon_n</span>
<span id="cb54-707"><a href="#cb54-707" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-708"><a href="#cb54-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-709"><a href="#cb54-709" aria-hidden="true" tabindex="-1"></a>Taking the expected value of the square of this expression and noting</span>
<span id="cb54-710"><a href="#cb54-710" aria-hidden="true" tabindex="-1"></a>that</span>
<span id="cb54-711"><a href="#cb54-711" aria-hidden="true" tabindex="-1"></a>$\mbox{E}\left(\bar{\epsilon} (\hat{\beta} - \beta) (x_n - \bar{x})\right) = \frac{1}{N}(x_n - \bar{x})\sigma_\epsilon ^ 2\sum_n c_n = 0$,</span>
<span id="cb54-712"><a href="#cb54-712" aria-hidden="true" tabindex="-1"></a>we get the following variance:</span>
<span id="cb54-713"><a href="#cb54-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-714"><a href="#cb54-714" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-715"><a href="#cb54-715" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\epsilon}_n) = \sigma_\epsilon ^ 2 +</span>
<span id="cb54-716"><a href="#cb54-716" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sigma_\epsilon ^ 2 + \frac{(x_n - \bar{x}) ^ 2}{S_{xx}}</span>
<span id="cb54-717"><a href="#cb54-717" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon ^ 2 -</span>
<span id="cb54-718"><a href="#cb54-718" aria-hidden="true" tabindex="-1"></a>2  \frac{1}{N}\sigma_\epsilon ^ 2 - 2  \frac{(x_n - \bar{x}) ^</span>
<span id="cb54-719"><a href="#cb54-719" aria-hidden="true" tabindex="-1"></a>2}{S_{xx}}\sigma_\epsilon ^ 2</span>
<span id="cb54-720"><a href="#cb54-720" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-721"><a href="#cb54-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-722"><a href="#cb54-722" aria-hidden="true" tabindex="-1"></a>Re-arranging terms:</span>
<span id="cb54-723"><a href="#cb54-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-724"><a href="#cb54-724" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-725"><a href="#cb54-725" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\epsilon}_n}^2= \sigma_\epsilon ^ 2 \left(1 -</span>
<span id="cb54-726"><a href="#cb54-726" aria-hidden="true" tabindex="-1"></a>\frac{1}{N} - </span>
<span id="cb54-727"><a href="#cb54-727" aria-hidden="true" tabindex="-1"></a>\frac{(x_n- \bar{x}) ^</span>
<span id="cb54-728"><a href="#cb54-728" aria-hidden="true" tabindex="-1"></a>2}{S_{xx}}\right)</span>
<span id="cb54-729"><a href="#cb54-729" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-730"><a href="#cb54-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-731"><a href="#cb54-731" aria-hidden="true" tabindex="-1"></a>Note that $\sigma_{\hat{\epsilon}_n} &lt; \sigma_\epsilon$, which means</span>
<span id="cb54-732"><a href="#cb54-732" aria-hidden="true" tabindex="-1"></a>that residuals are on average "smaller" than errors; this is a direct</span>
<span id="cb54-733"><a href="#cb54-733" aria-hidden="true" tabindex="-1"></a>consequence of the fact that we minimize the sum of the squares of the</span>
<span id="cb54-734"><a href="#cb54-734" aria-hidden="true" tabindex="-1"></a>residuals (see @sec-geometry_ols). Summing for all the observations, we</span>
<span id="cb54-735"><a href="#cb54-735" aria-hidden="true" tabindex="-1"></a>get the expected value of the sum of squares residuals:</span>
<span id="cb54-736"><a href="#cb54-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-737"><a href="#cb54-737" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-738"><a href="#cb54-738" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left(\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2\right) =</span>
<span id="cb54-739"><a href="#cb54-739" aria-hidden="true" tabindex="-1"></a>\sigma_{\epsilon} ^ 2 (N - 2)</span>
<span id="cb54-740"><a href="#cb54-740" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-741"><a href="#cb54-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-742"><a href="#cb54-742" aria-hidden="true" tabindex="-1"></a>Therefore, the previously computed estimator of the variance of the</span>
<span id="cb54-743"><a href="#cb54-743" aria-hidden="true" tabindex="-1"></a>errors $\hat{\sigma}_\epsilon ^ 2$ is biased:</span>
<span id="cb54-744"><a href="#cb54-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-745"><a href="#cb54-745" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-746"><a href="#cb54-746" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\sigma}_\epsilon ^ 2) =</span>
<span id="cb54-747"><a href="#cb54-747" aria-hidden="true" tabindex="-1"></a>\frac{\mbox{E}\left(\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2\right)}{N} = \sigma_\epsilon ^ 2 \frac{N-2}{N}</span>
<span id="cb54-748"><a href="#cb54-748" aria-hidden="true" tabindex="-1"></a>$$ More precisely, $\hat{\sigma}_\epsilon$ is downward biased, by a</span>
<span id="cb54-749"><a href="#cb54-749" aria-hidden="true" tabindex="-1"></a>factor of $\sqrt{\frac{N-2}{N}}$. For example, for $N=10, 20, 100$, we</span>
<span id="cb54-750"><a href="#cb54-750" aria-hidden="true" tabindex="-1"></a>get</span>
<span id="cb54-751"><a href="#cb54-751" aria-hidden="true" tabindex="-1"></a>$\sqrt{\frac{N-2}{N}} = <span class="in">`r round(sqrt(8/10), 2)`</span>, <span class="in">`r round(sqrt(18/20), 2)`</span>, <span class="in">`r round(sqrt(98/100), 2)`</span>$,</span>
<span id="cb54-752"><a href="#cb54-752" aria-hidden="true" tabindex="-1"></a>which means a <span class="in">`r round(100 * (1 - sqrt(8/10)), 0)`</span>,</span>
<span id="cb54-753"><a href="#cb54-753" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(100 * (1 - sqrt(18/20)), 0)`</span>,</span>
<span id="cb54-754"><a href="#cb54-754" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(100 * (1 - sqrt(98/100)), 0)`</span>% downward bias for the estimated</span>
<span id="cb54-755"><a href="#cb54-755" aria-hidden="true" tabindex="-1"></a>standard deviation. As the factor $\frac{N-2}{N}$ tends to 1 for</span>
<span id="cb54-756"><a href="#cb54-756" aria-hidden="true" tabindex="-1"></a>$N\rightarrow +\infty$, the bias will be negligible for large samples,</span>
<span id="cb54-757"><a href="#cb54-757" aria-hidden="true" tabindex="-1"></a>but can be severe in small samples. We'll from now denote</span>
<span id="cb54-758"><a href="#cb54-758" aria-hidden="true" tabindex="-1"></a>$\dot{\sigma}_\epsilon$ the unbiased estimator:</span>
<span id="cb54-759"><a href="#cb54-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-760"><a href="#cb54-760" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-761"><a href="#cb54-761" aria-hidden="true" tabindex="-1"></a>\dot{\sigma}_\epsilon = \sqrt{\frac{N}{N-2}}\hat{\sigma}_\epsilon</span>
<span id="cb54-762"><a href="#cb54-762" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-763"><a href="#cb54-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-764"><a href="#cb54-764" aria-hidden="true" tabindex="-1"></a>$\dot{\sigma}_\epsilon$ is often called the **residual standard error**.</span>
<span id="cb54-765"><a href="#cb54-765" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{residual standard error}</span>
<span id="cb54-766"><a href="#cb54-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-767"><a href="#cb54-767" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exact distribution of the OLS estimator with normal errors {#sec-exact_ols_distribution}</span></span>
<span id="cb54-768"><a href="#cb54-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-769"><a href="#cb54-769" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{normal distribution|(}</span>
<span id="cb54-770"><a href="#cb54-770" aria-hidden="true" tabindex="-1"></a>If the distribution of the errors is normal, as the OLS estimator is a</span>
<span id="cb54-771"><a href="#cb54-771" aria-hidden="true" tabindex="-1"></a>linear combination of the errors, its exact distribution is also normal.</span>
<span id="cb54-772"><a href="#cb54-772" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb54-773"><a href="#cb54-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-774"><a href="#cb54-774" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-775"><a href="#cb54-775" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_N \sim \mathcal{N}\left(\beta, \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}\right)</span>
<span id="cb54-776"><a href="#cb54-776" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-777"><a href="#cb54-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-778"><a href="#cb54-778" aria-hidden="true" tabindex="-1"></a>Subtracting the mean and dividing by the standard deviation, we get a</span>
<span id="cb54-779"><a href="#cb54-779" aria-hidden="true" tabindex="-1"></a>standard normal deviate:</span>
<span id="cb54-780"><a href="#cb54-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-781"><a href="#cb54-781" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-782"><a href="#cb54-782" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\beta}-\beta}{\sigma_\epsilon / (\sqrt{N}\hat{\sigma}_x)} \sim \mathcal{N}(0, 1)</span>
<span id="cb54-783"><a href="#cb54-783" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-784"><a href="#cb54-784" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{normal distribution|)}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Student t distribution|(}</span>
<span id="cb54-785"><a href="#cb54-785" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon$ is unknown; replacing it by its unbiased</span>
<span id="cb54-786"><a href="#cb54-786" aria-hidden="true" tabindex="-1"></a>estimator $\dot{\sigma}_\epsilon$ induces an increase of the uncertainty</span>
<span id="cb54-787"><a href="#cb54-787" aria-hidden="true" tabindex="-1"></a>and the distribution changes from a normal to a Student t distribution:</span>
<span id="cb54-788"><a href="#cb54-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-789"><a href="#cb54-789" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-790"><a href="#cb54-790" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\beta}-\beta}{\dot{\sigma}_\epsilon / (\sqrt{N}\hat{\sigma}_x)} \sim t_{N-2}</span>
<span id="cb54-791"><a href="#cb54-791" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb54-792"><a href="#cb54-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-793"><a href="#cb54-793" aria-hidden="true" tabindex="-1"></a>The Student distribution is symmetric, has fatter tails than the normal distribution</span>
<span id="cb54-794"><a href="#cb54-794" aria-hidden="true" tabindex="-1"></a>and converges in distribution to the normal distribution. Actually, it</span>
<span id="cb54-795"><a href="#cb54-795" aria-hidden="true" tabindex="-1"></a>is worth considering the Student and not the normal distribution as an</span>
<span id="cb54-796"><a href="#cb54-796" aria-hidden="true" tabindex="-1"></a>approximation only for small-sized samples. For example, for sample</span>
<span id="cb54-797"><a href="#cb54-797" aria-hidden="true" tabindex="-1"></a>sizes of 10, 20, 50 and 100, 5% critical values for a Student are</span>
<span id="cb54-798"><a href="#cb54-798" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(qt(0.975, 10 - 2), 2)`</span>, <span class="in">`r round(qt(0.975, 20 - 2), 2)`</span>,</span>
<span id="cb54-799"><a href="#cb54-799" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(qt(0.975, 50 - 2), 2)`</span> and <span class="in">`r round(qt(0.975, 100 - 2), 2)`</span>, as</span>
<span id="cb54-800"><a href="#cb54-800" aria-hidden="true" tabindex="-1"></a>the critical value is <span class="in">`r round(qnorm(0.975), 2)`</span> for the normal</span>
<span id="cb54-801"><a href="#cb54-801" aria-hidden="true" tabindex="-1"></a>distribution.</span>
<span id="cb54-802"><a href="#cb54-802" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Student t distribution|)}</span>
<span id="cb54-803"><a href="#cb54-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-804"><a href="#cb54-804" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Computation of the variance of the OLS estimator with R</span></span>
<span id="cb54-805"><a href="#cb54-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-806"><a href="#cb54-806" aria-hidden="true" tabindex="-1"></a>We go back to the price-time model estimated in the previous chapter. Remember that the fitted model (called <span class="in">`pxt`</span>) was obtained with the following code:</span>
<span id="cb54-807"><a href="#cb54-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-810"><a href="#cb54-810" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-811"><a href="#cb54-811" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: remind_pxt</span></span>
<span id="cb54-812"><a href="#cb54-812" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> price_time <span class="sc">%&gt;%</span></span>
<span id="cb54-813"><a href="#cb54-813" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="fu">c</span>(<span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-814"><a href="#cb54-814" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">sr =</span> qr <span class="sc">/</span> (qr <span class="sc">+</span> qa),</span>
<span id="cb54-815"><a href="#cb54-815" aria-hidden="true" tabindex="-1"></a>         <span class="at">h =</span> (pa <span class="sc">-</span> pr) <span class="sc">/</span> ( (tr <span class="sc">-</span> ta) <span class="sc">/</span> <span class="dv">60</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb54-816"><a href="#cb54-816" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(sr <span class="sc">&lt;</span> <span class="fl">0.75</span>)</span>
<span id="cb54-817"><a href="#cb54-817" aria-hidden="true" tabindex="-1"></a>pxt <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h, prtime)</span>
<span id="cb54-818"><a href="#cb54-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-819"><a href="#cb54-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-820"><a href="#cb54-820" aria-hidden="true" tabindex="-1"></a>We first compute "by hand" the standard error of the OLS estimator</span>
<span id="cb54-821"><a href="#cb54-821" aria-hidden="true" tabindex="-1"></a>and we then use the relevant methods for <span class="in">`lm`</span>\idxfun{lm}{stats} objects to do so. We first</span>
<span id="cb54-822"><a href="#cb54-822" aria-hidden="true" tabindex="-1"></a>extract the $x$ vector, its length $N$, its mean $\bar{x}$ and its</span>
<span id="cb54-823"><a href="#cb54-823" aria-hidden="true" tabindex="-1"></a>sample standard deviation $\hat{\sigma}_x$:\idxfun{pull}{dplyr}\idxfun{length}{base}\idxfun{sd}{stats}</span>
<span id="cb54-824"><a href="#cb54-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-825"><a href="#cb54-825" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-826"><a href="#cb54-826" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: sample_sd_x2</span></span>
<span id="cb54-827"><a href="#cb54-827" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">pull</span>(prtime, h)</span>
<span id="cb54-828"><a href="#cb54-828" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb54-829"><a href="#cb54-829" aria-hidden="true" tabindex="-1"></a>bx <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb54-830"><a href="#cb54-830" aria-hidden="true" tabindex="-1"></a>sx <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((x <span class="sc">-</span> bx) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb54-831"><a href="#cb54-831" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-832"><a href="#cb54-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-833"><a href="#cb54-833" aria-hidden="true" tabindex="-1"></a>We then get the sum of square residuals, and the residual standard</span>
<span id="cb54-834"><a href="#cb54-834" aria-hidden="true" tabindex="-1"></a>error:\idxfun{resid}{stats}</span>
<span id="cb54-835"><a href="#cb54-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-836"><a href="#cb54-836" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-837"><a href="#cb54-837" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-838"><a href="#cb54-838" aria-hidden="true" tabindex="-1"></a>heps <span class="ot">&lt;-</span> <span class="fu">resid</span>(pxt)</span>
<span id="cb54-839"><a href="#cb54-839" aria-hidden="true" tabindex="-1"></a>SSR <span class="ot">&lt;-</span> <span class="fu">sum</span>(heps <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb54-840"><a href="#cb54-840" aria-hidden="true" tabindex="-1"></a>seps <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(SSR <span class="sc">/</span> (N <span class="sc">-</span> <span class="dv">2</span>))</span>
<span id="cb54-841"><a href="#cb54-841" aria-hidden="true" tabindex="-1"></a>seps</span>
<span id="cb54-842"><a href="#cb54-842" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-843"><a href="#cb54-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-844"><a href="#cb54-844" aria-hidden="true" tabindex="-1"></a>which finally leads to the estimators of the standard deviation of the</span>
<span id="cb54-845"><a href="#cb54-845" aria-hidden="true" tabindex="-1"></a>OLS coefficients:</span>
<span id="cb54-846"><a href="#cb54-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-847"><a href="#cb54-847" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-848"><a href="#cb54-848" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: sd_coefficients_by_hand</span></span>
<span id="cb54-849"><a href="#cb54-849" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-850"><a href="#cb54-850" aria-hidden="true" tabindex="-1"></a>sbeta <span class="ot">&lt;-</span> seps <span class="sc">/</span> (<span class="fu">sqrt</span>(N) <span class="sc">*</span> sx)</span>
<span id="cb54-851"><a href="#cb54-851" aria-hidden="true" tabindex="-1"></a>salpha <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(bx <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> sx <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">*</span> seps <span class="sc">/</span> (<span class="fu">sqrt</span>(N) <span class="sc">*</span> sx)</span>
<span id="cb54-852"><a href="#cb54-852" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(salpha, sbeta)</span>
<span id="cb54-853"><a href="#cb54-853" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-854"><a href="#cb54-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-855"><a href="#cb54-855" aria-hidden="true" tabindex="-1"></a>All this information can be retrieved easily with **R** using specific</span>
<span id="cb54-856"><a href="#cb54-856" aria-hidden="true" tabindex="-1"></a>functions. To get the sample size and the number of degrees of freedom</span>
<span id="cb54-857"><a href="#cb54-857" aria-hidden="true" tabindex="-1"></a>(which is, in the simple linear model, $N-2$), we have:\idxfun{nobs}{stats}\idxfun{df.residual}{stats}</span>
<span id="cb54-858"><a href="#cb54-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-859"><a href="#cb54-859" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-860"><a href="#cb54-860" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-861"><a href="#cb54-861" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lm_methods</span></span>
<span id="cb54-862"><a href="#cb54-862" aria-hidden="true" tabindex="-1"></a><span class="fu">nobs</span>(pxt)</span>
<span id="cb54-863"><a href="#cb54-863" aria-hidden="true" tabindex="-1"></a><span class="fu">df.residual</span>(pxt)</span>
<span id="cb54-864"><a href="#cb54-864" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-865"><a href="#cb54-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-866"><a href="#cb54-866" aria-hidden="true" tabindex="-1"></a>$\dot{\sigma}_\epsilon$ is computed using:\idxfun{sigma}{stats}</span>
<span id="cb54-867"><a href="#cb54-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-868"><a href="#cb54-868" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-869"><a href="#cb54-869" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: sigma_lm</span></span>
<span id="cb54-870"><a href="#cb54-870" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-871"><a href="#cb54-871" aria-hidden="true" tabindex="-1"></a><span class="fu">sigma</span>(pxt)</span>
<span id="cb54-872"><a href="#cb54-872" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-873"><a href="#cb54-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-874"><a href="#cb54-874" aria-hidden="true" tabindex="-1"></a>The matrix of variance-covariance of the estimators is obtained using</span>
<span id="cb54-875"><a href="#cb54-875" aria-hidden="true" tabindex="-1"></a>the <span class="in">`vcov`</span> function:\idxfun{vcov}{stats}</span>
<span id="cb54-876"><a href="#cb54-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-877"><a href="#cb54-877" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-878"><a href="#cb54-878" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vcov_lm</span></span>
<span id="cb54-879"><a href="#cb54-879" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-880"><a href="#cb54-880" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(pxt)</span>
<span id="cb54-881"><a href="#cb54-881" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-882"><a href="#cb54-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-883"><a href="#cb54-883" aria-hidden="true" tabindex="-1"></a>To get the standard deviations of the intercept and the slope</span>
<span id="cb54-884"><a href="#cb54-884" aria-hidden="true" tabindex="-1"></a>estimators, we first extract the diagonal elements of this matrix and we</span>
<span id="cb54-885"><a href="#cb54-885" aria-hidden="true" tabindex="-1"></a>next take the square roots of the values:\idxfun{diag}{base}\idxfun{sqrt}{base}</span>
<span id="cb54-886"><a href="#cb54-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-887"><a href="#cb54-887" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-888"><a href="#cb54-888" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-889"><a href="#cb54-889" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vcov_lm_diag_sqrt</span></span>
<span id="cb54-890"><a href="#cb54-890" aria-hidden="true" tabindex="-1"></a>pxt <span class="sc">%&gt;%</span> vcov <span class="sc">%&gt;%</span> diag <span class="sc">%&gt;%</span> sqrt</span>
<span id="cb54-891"><a href="#cb54-891" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-892"><a href="#cb54-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-893"><a href="#cb54-893" aria-hidden="true" tabindex="-1"></a>More simply, the <span class="in">`micsr::stder`</span>\idxfun{stder}{micsr} function can be used:</span>
<span id="cb54-894"><a href="#cb54-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-895"><a href="#cb54-895" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-896"><a href="#cb54-896" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb54-897"><a href="#cb54-897" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: micsr_stder</span></span>
<span id="cb54-898"><a href="#cb54-898" aria-hidden="true" tabindex="-1"></a>pxt <span class="sc">%&gt;%</span> stder</span>
<span id="cb54-899"><a href="#cb54-899" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-900"><a href="#cb54-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-901"><a href="#cb54-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-902"><a href="#cb54-902" aria-hidden="true" tabindex="-1"></a><span class="fu">### OLS estimator is BLUE {#sec-simple_ols_blue}</span></span>
<span id="cb54-903"><a href="#cb54-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-904"><a href="#cb54-904" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{best linear unbiased estimator!simple linear regression model|(}</span>
<span id="cb54-905"><a href="#cb54-905" aria-hidden="true" tabindex="-1"></a>We have seen previously that the OLS estimator is a linear estimator (i.e., it is a linear combination of the $N$ values of $y$ for the sample):</span>
<span id="cb54-906"><a href="#cb54-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-907"><a href="#cb54-907" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-908"><a href="#cb54-908" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \sum_{n=1} ^ N \left(\frac{ (x_n - \bar{x})}</span>
<span id="cb54-909"><a href="#cb54-909" aria-hidden="true" tabindex="-1"></a>{\sum_{n=1} ^ N  (x_n - \bar{x}) ^ 2} \right) y_n =</span>
<span id="cb54-910"><a href="#cb54-910" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N c_n y_n</span>
<span id="cb54-911"><a href="#cb54-911" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb54-912"><a href="#cb54-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-913"><a href="#cb54-913" aria-hidden="true" tabindex="-1"></a>Moreover, we have seen that if $\mbox{E}(\epsilon \mid x) = 0$, it is</span>
<span id="cb54-914"><a href="#cb54-914" aria-hidden="true" tabindex="-1"></a>unbiased and, with the hypothesis of homoskedastic and uncorrelated</span>
<span id="cb54-915"><a href="#cb54-915" aria-hidden="true" tabindex="-1"></a>errors, we have established that its variance is:</span>
<span id="cb54-916"><a href="#cb54-916" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon ^ 2 / S_{xx}$. We'll show in this subsection that among</span>
<span id="cb54-917"><a href="#cb54-917" aria-hidden="true" tabindex="-1"></a>all the linear unbiased estimators, the OLS estimator is the one with</span>
<span id="cb54-918"><a href="#cb54-918" aria-hidden="true" tabindex="-1"></a>the smallest variance. For these reasons, the OLS estimator is the</span>
<span id="cb54-919"><a href="#cb54-919" aria-hidden="true" tabindex="-1"></a>**best linear unbiased estimator** (**BLUE**).</span>
<span id="cb54-920"><a href="#cb54-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-921"><a href="#cb54-921" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Comparing OLS with other linear unbiased estimators</span></span>
<span id="cb54-922"><a href="#cb54-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-923"><a href="#cb54-923" aria-hidden="true" tabindex="-1"></a>Consider another linear estimator, with weights $a_n$:</span>
<span id="cb54-924"><a href="#cb54-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-925"><a href="#cb54-925" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-926"><a href="#cb54-926" aria-hidden="true" tabindex="-1"></a>\tilde{\beta} =  \sum_{n=1} ^ N a_n y_n</span>
<span id="cb54-927"><a href="#cb54-927" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-928"><a href="#cb54-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-929"><a href="#cb54-929" aria-hidden="true" tabindex="-1"></a>Replacing $y_n$ by $\alpha + \beta x_n + \epsilon_n$, we have:</span>
<span id="cb54-930"><a href="#cb54-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-931"><a href="#cb54-931" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-932"><a href="#cb54-932" aria-hidden="true" tabindex="-1"></a>\tilde{\beta} = \sum_{n=1} ^ N \ a_n (\alpha + \beta x_n + \epsilon_n)</span>
<span id="cb54-933"><a href="#cb54-933" aria-hidden="true" tabindex="-1"></a>= \alpha \sum_{n=1} ^ N  a_n +</span>
<span id="cb54-934"><a href="#cb54-934" aria-hidden="true" tabindex="-1"></a>\beta \sum_{n=1} ^ N  a_n x_n +</span>
<span id="cb54-935"><a href="#cb54-935" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N  a_n \epsilon_n</span>
<span id="cb54-936"><a href="#cb54-936" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-937"><a href="#cb54-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-938"><a href="#cb54-938" aria-hidden="true" tabindex="-1"></a>Therefore, for any unbiased estimator, one must have</span>
<span id="cb54-939"><a href="#cb54-939" aria-hidden="true" tabindex="-1"></a>$\sum_{n=1} ^ N a_n= 0$ and $\sum_{n=1} ^ N a_n x_n = 1$.</span>
<span id="cb54-940"><a href="#cb54-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-941"><a href="#cb54-941" aria-hidden="true" tabindex="-1"></a>We then have: $\tilde{\beta} - \beta = \sum_{n=1} ^ N a_n \epsilon_n$</span>
<span id="cb54-942"><a href="#cb54-942" aria-hidden="true" tabindex="-1"></a>and the variance of $\tilde{\beta}$ is:</span>
<span id="cb54-943"><a href="#cb54-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-944"><a href="#cb54-944" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-945"><a href="#cb54-945" aria-hidden="true" tabindex="-1"></a>\sigma_{\tilde{\beta}} ^ 2 = </span>
<span id="cb54-946"><a href="#cb54-946" aria-hidden="true" tabindex="-1"></a>\mbox{E} \left(\left[\sum_{n=1} ^ N a_n</span>
<span id="cb54-947"><a href="#cb54-947" aria-hidden="true" tabindex="-1"></a>\epsilon_n\right] ^ 2\right) = </span>
<span id="cb54-948"><a href="#cb54-948" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon ^ 2 \sum_{n=1} ^ N a_n ^ 2</span>
<span id="cb54-949"><a href="#cb54-949" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-950"><a href="#cb54-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-951"><a href="#cb54-951" aria-hidden="true" tabindex="-1"></a>defining $d_n = a_n - c_n$, we have:</span>
<span id="cb54-952"><a href="#cb54-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-953"><a href="#cb54-953" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-954"><a href="#cb54-954" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N  a_n ^ 2 = </span>
<span id="cb54-955"><a href="#cb54-955" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N  (c_n + d_n) ^ 2 =</span>
<span id="cb54-956"><a href="#cb54-956" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N c_n ^ 2 + </span>
<span id="cb54-957"><a href="#cb54-957" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N d_n ^ 2 + </span>
<span id="cb54-958"><a href="#cb54-958" aria-hidden="true" tabindex="-1"></a>2 \sum_{n=1} ^ N d_n c_n</span>
<span id="cb54-959"><a href="#cb54-959" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb54-960"><a href="#cb54-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-961"><a href="#cb54-961" aria-hidden="true" tabindex="-1"></a>But the last term is 0 because:</span>
<span id="cb54-962"><a href="#cb54-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-963"><a href="#cb54-963" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-964"><a href="#cb54-964" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-965"><a href="#cb54-965" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N d_n c_n </span>
<span id="cb54-966"><a href="#cb54-966" aria-hidden="true" tabindex="-1"></a>&amp;= &amp; \sum_{n=1} ^ N (a_n - c_n) c_n <span class="sc">\\</span></span>
<span id="cb54-967"><a href="#cb54-967" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \frac{1}{S_{xx}}\sum_{n=1} ^ N a_n x_n - </span>
<span id="cb54-968"><a href="#cb54-968" aria-hidden="true" tabindex="-1"></a>\frac{1}{S_{xx}} \bar{x} \sum_{n=1} a_n - \sum_{n=1} ^ N c_n ^ 2 <span class="sc">\\</span></span>
<span id="cb54-969"><a href="#cb54-969" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;0</span>
<span id="cb54-970"><a href="#cb54-970" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-971"><a href="#cb54-971" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-972"><a href="#cb54-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-973"><a href="#cb54-973" aria-hidden="true" tabindex="-1"></a>so that</span>
<span id="cb54-974"><a href="#cb54-974" aria-hidden="true" tabindex="-1"></a>$\sum_{n=1} ^ N a_n ^ 2 = \sum_{n=1}^N c_n ^ 2 + \sum_{n=1}^N d_n ^ 2$</span>
<span id="cb54-975"><a href="#cb54-975" aria-hidden="true" tabindex="-1"></a>and:</span>
<span id="cb54-976"><a href="#cb54-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-977"><a href="#cb54-977" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-978"><a href="#cb54-978" aria-hidden="true" tabindex="-1"></a>\sigma_{\tilde{\beta}}^2 = \sigma_\epsilon ^ 2</span>
<span id="cb54-979"><a href="#cb54-979" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N a_n ^ 2 = </span>
<span id="cb54-980"><a href="#cb54-980" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon ^ 2 \left( \sum_{n=1}^N c_n ^ 2 + \sum_{n=1}^N d_n ^ 2\right) = </span>
<span id="cb54-981"><a href="#cb54-981" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\beta}}^2 + \sigma_\epsilon ^ 2 \sum_{n=1}^N d_n ^ 2 </span>
<span id="cb54-982"><a href="#cb54-982" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-983"><a href="#cb54-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-984"><a href="#cb54-984" aria-hidden="true" tabindex="-1"></a>Therefore, $\sigma_{\tilde{\beta}}^2 &gt; \sigma_{\hat{\beta}}^2$, which</span>
<span id="cb54-985"><a href="#cb54-985" aria-hidden="true" tabindex="-1"></a>means that the OLS estimator is **BLUE**, i.e., it is, among all the</span>
<span id="cb54-986"><a href="#cb54-986" aria-hidden="true" tabindex="-1"></a>unbiased linear estimators, the one with the lower variance.</span>
<span id="cb54-987"><a href="#cb54-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-988"><a href="#cb54-988" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Practical example {#sec-remove_intercept}</span></span>
<span id="cb54-989"><a href="#cb54-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-990"><a href="#cb54-990" aria-hidden="true" tabindex="-1"></a>Consider as an example the price-time model. The model we have</span>
<span id="cb54-991"><a href="#cb54-991" aria-hidden="true" tabindex="-1"></a>previously estimated is:\idxfun{lm}{stats}</span>
<span id="cb54-992"><a href="#cb54-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-995"><a href="#cb54-995" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-996"><a href="#cb54-996" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: reminding_lm_model_price_time</span></span>
<span id="cb54-997"><a href="#cb54-997" aria-hidden="true" tabindex="-1"></a>pxt <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h, prtime)</span>
<span id="cb54-998"><a href="#cb54-998" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-999"><a href="#cb54-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1000"><a href="#cb54-1000" aria-hidden="true" tabindex="-1"></a>Consider now the same model without intercept ($\alpha = 0$). As</span>
<span id="cb54-1001"><a href="#cb54-1001" aria-hidden="true" tabindex="-1"></a>$\alpha = - a / (b - a)$, $a$ and $b$ being respectively the minimal and</span>
<span id="cb54-1002"><a href="#cb54-1002" aria-hidden="true" tabindex="-1"></a>the maximum time value, $\alpha = 0$ implies that the minimal time value</span>
<span id="cb54-1003"><a href="#cb54-1003" aria-hidden="true" tabindex="-1"></a>is 0. To fit the model that imposes this hypothesis, we need to fit the</span>
<span id="cb54-1004"><a href="#cb54-1004" aria-hidden="true" tabindex="-1"></a>same model without intercept. In **R**, this is performed using either</span>
<span id="cb54-1005"><a href="#cb54-1005" aria-hidden="true" tabindex="-1"></a><span class="in">`- 1`</span> or <span class="in">`+ 0`</span> in the formula :\idxfun{lm}{stats}</span>
<span id="cb54-1006"><a href="#cb54-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1007"><a href="#cb54-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1008"><a href="#cb54-1008" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_without_intercept</span></span>
<span id="cb54-1009"><a href="#cb54-1009" aria-hidden="true" tabindex="-1"></a>pxt2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h <span class="sc">-</span> <span class="dv">1</span>, prtime)</span>
<span id="cb54-1010"><a href="#cb54-1010" aria-hidden="true" tabindex="-1"></a>pxt2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h <span class="sc">+</span> <span class="dv">0</span>, prtime)</span>
<span id="cb54-1011"><a href="#cb54-1011" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1012"><a href="#cb54-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1013"><a href="#cb54-1013" aria-hidden="true" tabindex="-1"></a>The same model can also be estimated by updating the previous fitted</span>
<span id="cb54-1014"><a href="#cb54-1014" aria-hidden="true" tabindex="-1"></a>model <span class="in">`pxt`</span>, using the <span class="in">`update`</span> function which takes as first argument</span>
<span id="cb54-1015"><a href="#cb54-1015" aria-hidden="true" tabindex="-1"></a>the model we wish to update:\idxfun{update}{stats}</span>
<span id="cb54-1016"><a href="#cb54-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1017"><a href="#cb54-1017" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1018"><a href="#cb54-1018" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: update_lm_model</span></span>
<span id="cb54-1019"><a href="#cb54-1019" aria-hidden="true" tabindex="-1"></a>pxt2 <span class="ot">&lt;-</span> <span class="fu">update</span>(pxt, . <span class="sc">~</span> . <span class="sc">+</span> <span class="dv">0</span>)</span>
<span id="cb54-1020"><a href="#cb54-1020" aria-hidden="true" tabindex="-1"></a>pxt2 <span class="ot">&lt;-</span> <span class="fu">update</span>(pxt, . <span class="sc">~</span> . <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb54-1021"><a href="#cb54-1021" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1022"><a href="#cb54-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1023"><a href="#cb54-1023" aria-hidden="true" tabindex="-1"></a>The formula is updated using <span class="in">`.`</span>, which means the same thing as in the</span>
<span id="cb54-1024"><a href="#cb54-1024" aria-hidden="true" tabindex="-1"></a>initial model. Therefore, <span class="in">`. ~ .`</span> means the initial formula and we</span>
<span id="cb54-1025"><a href="#cb54-1025" aria-hidden="true" tabindex="-1"></a>remove the intercept by either "adding" <span class="in">`0`</span> or "subtracting" <span class="in">`1`</span>.</span>
<span id="cb54-1026"><a href="#cb54-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1027"><a href="#cb54-1027" aria-hidden="true" tabindex="-1"></a>The fitted model is presented in @fig-nointercept.</span>
<span id="cb54-1028"><a href="#cb54-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1029"><a href="#cb54-1029" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1030"><a href="#cb54-1030" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-nointercept</span></span>
<span id="cb54-1031"><a href="#cb54-1031" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1032"><a href="#cb54-1032" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "OLS estimator without intercept"</span></span>
<span id="cb54-1033"><a href="#cb54-1033" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-1034"><a href="#cb54-1034" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">28</span>)) <span class="sc">+</span> <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.70</span>)) <span class="sc">+</span></span>
<span id="cb54-1035"><a href="#cb54-1035" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x <span class="sc">-</span> <span class="dv">1</span>, <span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>, <span class="at">color =</span> <span class="st">"black"</span>)</span>
<span id="cb54-1036"><a href="#cb54-1036" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1037"><a href="#cb54-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1038"><a href="#cb54-1038" aria-hidden="true" tabindex="-1"></a>For this model without intercept, the formula for the slope is:</span>
<span id="cb54-1039"><a href="#cb54-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1040"><a href="#cb54-1040" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1041"><a href="#cb54-1041" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_{n=1}^N x_n y_n}{\sum_{n=1}^N x_n ^ 2}</span>
<span id="cb54-1042"><a href="#cb54-1042" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1043"><a href="#cb54-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1044"><a href="#cb54-1044" aria-hidden="true" tabindex="-1"></a>Or, replacing $y_n$ by $\beta x_n + \epsilon_n$ :</span>
<span id="cb54-1045"><a href="#cb54-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1046"><a href="#cb54-1046" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1047"><a href="#cb54-1047" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \beta + \frac{\sum_{n=1}^N x_n \epsilon_n}{\sum_{n=1}^N x_n ^ 2}</span>
<span id="cb54-1048"><a href="#cb54-1048" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1049"><a href="#cb54-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1050"><a href="#cb54-1050" aria-hidden="true" tabindex="-1"></a>for which the variance is:</span>
<span id="cb54-1051"><a href="#cb54-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1052"><a href="#cb54-1052" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1053"><a href="#cb54-1053" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\beta}} ^ 2 = \frac{\sigma_\epsilon ^ 2}{\sum_{n=1}^N x_n ^ 2} =</span>
<span id="cb54-1054"><a href="#cb54-1054" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon ^ 2}{N A_{x ^ 2}}</span>
<span id="cb54-1055"><a href="#cb54-1055" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1056"><a href="#cb54-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1057"><a href="#cb54-1057" aria-hidden="true" tabindex="-1"></a>where $A_{x ^ 2}$ is the arithmetic mean of the squares of $x$.</span>
<span id="cb54-1058"><a href="#cb54-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1059"><a href="#cb54-1059" aria-hidden="true" tabindex="-1"></a>An alternative estimation method consists of drawing lines from every</span>
<span id="cb54-1060"><a href="#cb54-1060" aria-hidden="true" tabindex="-1"></a>point to the origin, as illustrated in @fig-indslopes, and to estimate</span>
<span id="cb54-1061"><a href="#cb54-1061" aria-hidden="true" tabindex="-1"></a>$\beta$ by the arithmetic mean of the $N$ slopes, which are $y_n / x_n$. Formally,</span>
<span id="cb54-1062"><a href="#cb54-1062" aria-hidden="true" tabindex="-1"></a>we have:</span>
<span id="cb54-1063"><a href="#cb54-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1064"><a href="#cb54-1064" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1065"><a href="#cb54-1065" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-indslopes</span></span>
<span id="cb54-1066"><a href="#cb54-1066" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1067"><a href="#cb54-1067" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Individual slopes"</span></span>
<span id="cb54-1068"><a href="#cb54-1068" aria-hidden="true" tabindex="-1"></a>slope <span class="ot">&lt;-</span> <span class="fu">summarise</span>(prtime, <span class="at">slope =</span> <span class="fu">mean</span>(sr <span class="sc">/</span> h)) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(slope)</span>
<span id="cb54-1069"><a href="#cb54-1069" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-1070"><a href="#cb54-1070" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> h, <span class="at">yend =</span> sr)) <span class="sc">+</span></span>
<span id="cb54-1071"><a href="#cb54-1071" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> slope,</span>
<span id="cb54-1072"><a href="#cb54-1072" aria-hidden="true" tabindex="-1"></a>                <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>)</span>
<span id="cb54-1073"><a href="#cb54-1073" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1074"><a href="#cb54-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1075"><a href="#cb54-1075" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1076"><a href="#cb54-1076" aria-hidden="true" tabindex="-1"></a>\tilde{\beta}=\frac{1}{N}\sum_{n = 1} ^ N \frac{y_n}{x_n}</span>
<span id="cb54-1077"><a href="#cb54-1077" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1078"><a href="#cb54-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1079"><a href="#cb54-1079" aria-hidden="true" tabindex="-1"></a>This is a linear estimator, with weights</span>
<span id="cb54-1080"><a href="#cb54-1080" aria-hidden="true" tabindex="-1"></a>$a_n = \frac{1}{N}\frac{1}{x_n}$. Replacing $y_n$ by</span>
<span id="cb54-1081"><a href="#cb54-1081" aria-hidden="true" tabindex="-1"></a>$\beta x_n + \epsilon_n$, we get :</span>
<span id="cb54-1082"><a href="#cb54-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1083"><a href="#cb54-1083" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1084"><a href="#cb54-1084" aria-hidden="true" tabindex="-1"></a>\tilde{\beta}= \beta + \frac{1}{N}\sum_{n = 1} ^ N \frac{\epsilon_n}{x_n}</span>
<span id="cb54-1085"><a href="#cb54-1085" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1086"><a href="#cb54-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1087"><a href="#cb54-1087" aria-hidden="true" tabindex="-1"></a>This linear estimator is therefore unbiased. Its variance is:</span>
<span id="cb54-1088"><a href="#cb54-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1089"><a href="#cb54-1089" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1090"><a href="#cb54-1090" aria-hidden="true" tabindex="-1"></a>\sigma_{\tilde{\beta}} = \frac{\sigma_\epsilon ^ 2}{N ^ 2}\sum_{n =</span>
<span id="cb54-1091"><a href="#cb54-1091" aria-hidden="true" tabindex="-1"></a>1}^N \frac{1}{x_n ^ 2} = \frac{\sigma_\epsilon ^ 2}{N H_{x ^ 2}}</span>
<span id="cb54-1092"><a href="#cb54-1092" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1093"><a href="#cb54-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1094"><a href="#cb54-1094" aria-hidden="true" tabindex="-1"></a>where $H_{x ^ 2} = \frac{N}{\sum_{n=1}^N\frac{1}{x_n ^ 2}}$ is the</span>
<span id="cb54-1095"><a href="#cb54-1095" aria-hidden="true" tabindex="-1"></a>harmonic mean of $x ^ 2$. As the harmonic mean is always lower than the</span>
<span id="cb54-1096"><a href="#cb54-1096" aria-hidden="true" tabindex="-1"></a>arithmetic mean, $\sigma_{\tilde{\beta}} &gt; \sigma_{\hat{\beta}}$ and</span>
<span id="cb54-1097"><a href="#cb54-1097" aria-hidden="true" tabindex="-1"></a>therefore $\tilde{\beta}$ is less precise than $\hat{\beta}$. The value</span>
<span id="cb54-1098"><a href="#cb54-1098" aria-hidden="true" tabindex="-1"></a>of this alternative estimator can be computed as follow:\idxfun{transmute}{dplyr}\idxfun{pull}{dplyr}\idxfun{summarise}{dplyr}</span>
<span id="cb54-1099"><a href="#cb54-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1100"><a href="#cb54-1100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1101"><a href="#cb54-1101" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: individual_slopes</span></span>
<span id="cb54-1102"><a href="#cb54-1102" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1103"><a href="#cb54-1103" aria-hidden="true" tabindex="-1"></a>slope <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">transmute</span>(<span class="at">slope =</span> sr <span class="sc">/</span> h) <span class="sc">%&gt;%</span></span>
<span id="cb54-1104"><a href="#cb54-1104" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">slope =</span> <span class="fu">mean</span>(slope)) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(slope)</span>
<span id="cb54-1105"><a href="#cb54-1105" aria-hidden="true" tabindex="-1"></a>slope</span>
<span id="cb54-1106"><a href="#cb54-1106" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1107"><a href="#cb54-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1108"><a href="#cb54-1108" aria-hidden="true" tabindex="-1"></a>Once the estimator is computed, we can calculate $\hat{\sigma}_\epsilon$</span>
<span id="cb54-1109"><a href="#cb54-1109" aria-hidden="true" tabindex="-1"></a>and $\hat{\sigma}_{\tilde{\beta}}$ and, as intermediate results, the</span>
<span id="cb54-1110"><a href="#cb54-1110" aria-hidden="true" tabindex="-1"></a>arithmetic and the harmonic means of $x ^ 2$:\idxfun{mutate}{dplyr}\idxfun{summarise}{dplyr}\idxfun{sqrt}{base}</span>
<span id="cb54-1111"><a href="#cb54-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1114"><a href="#cb54-1114" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-1115"><a href="#cb54-1115" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: arithmetic_harmonic_means</span></span>
<span id="cb54-1116"><a href="#cb54-1116" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1117"><a href="#cb54-1117" aria-hidden="true" tabindex="-1"></a>reg2 <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> </span>
<span id="cb54-1118"><a href="#cb54-1118" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">resid =</span> sr <span class="sc">-</span> slope <span class="sc">*</span> h) <span class="sc">%&gt;%</span></span>
<span id="cb54-1119"><a href="#cb54-1119" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">seps =</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(resid <span class="sc">^</span>  <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">8</span>), </span>
<span id="cb54-1120"><a href="#cb54-1120" aria-hidden="true" tabindex="-1"></a>            <span class="at">H =</span> <span class="dv">9</span> <span class="sc">/</span> <span class="fu">sum</span>( <span class="dv">1</span> <span class="sc">/</span> h <span class="sc">^</span> <span class="dv">2</span>), </span>
<span id="cb54-1121"><a href="#cb54-1121" aria-hidden="true" tabindex="-1"></a>            <span class="at">A =</span> <span class="fu">sum</span>(h <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">9</span>, </span>
<span id="cb54-1122"><a href="#cb54-1122" aria-hidden="true" tabindex="-1"></a>            <span class="at">sdtilde =</span> seps <span class="sc">/</span> <span class="fu">sqrt</span>(N) <span class="sc">/</span> <span class="fu">sqrt</span>(H))</span>
<span id="cb54-1123"><a href="#cb54-1123" aria-hidden="true" tabindex="-1"></a>reg2</span>
<span id="cb54-1124"><a href="#cb54-1124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1125"><a href="#cb54-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1126"><a href="#cb54-1126" aria-hidden="true" tabindex="-1"></a>We check that the harmonic mean (<span class="in">`r round(reg2$H)`</span>) is lower than the</span>
<span id="cb54-1127"><a href="#cb54-1127" aria-hidden="true" tabindex="-1"></a>arithmetic mean (<span class="in">`r round(reg2$A)`</span>). Comparing with the OLS results:</span>
<span id="cb54-1128"><a href="#cb54-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1129"><a href="#cb54-1129" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1130"><a href="#cb54-1130" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: summary_ptx2</span></span>
<span id="cb54-1131"><a href="#cb54-1131" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1132"><a href="#cb54-1132" aria-hidden="true" tabindex="-1"></a>pxt2 <span class="sc">%&gt;%</span> stder</span>
<span id="cb54-1133"><a href="#cb54-1133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1134"><a href="#cb54-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1135"><a href="#cb54-1135" aria-hidden="true" tabindex="-1"></a>we confirm that the OLS estimator has a lower standard error than</span>
<span id="cb54-1136"><a href="#cb54-1136" aria-hidden="true" tabindex="-1"></a>the alternative estimator.</span>
<span id="cb54-1137"><a href="#cb54-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1138"><a href="#cb54-1138" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{best linear unbiased estimator!simple linear regression model|)}</span>
<span id="cb54-1139"><a href="#cb54-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1140"><a href="#cb54-1140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotic properties of the estimator {#sec-asymp_prop_ols}</span></span>
<span id="cb54-1141"><a href="#cb54-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1142"><a href="#cb54-1142" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- !!! consist on mal dit --&gt;</span></span>
<span id="cb54-1143"><a href="#cb54-1143" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{consistency|(}</span>
<span id="cb54-1144"><a href="#cb54-1144" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic properties|(}</span>
<span id="cb54-1145"><a href="#cb54-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1146"><a href="#cb54-1146" aria-hidden="true" tabindex="-1"></a>Asymptotic properties of an estimator deal with the behavior of this</span>
<span id="cb54-1147"><a href="#cb54-1147" aria-hidden="true" tabindex="-1"></a>estimator when the sample size increases without bound. Compared to</span>
<span id="cb54-1148"><a href="#cb54-1148" aria-hidden="true" tabindex="-1"></a>exact properties which are true and hold whatever the sample size is,</span>
<span id="cb54-1149"><a href="#cb54-1149" aria-hidden="true" tabindex="-1"></a>asymptotic properties are approximations, the better the larger the</span>
<span id="cb54-1150"><a href="#cb54-1150" aria-hidden="true" tabindex="-1"></a>sample size is. Two notions of convergence, that rely on two</span>
<span id="cb54-1151"><a href="#cb54-1151" aria-hidden="true" tabindex="-1"></a>fundamental theorems are used:</span>
<span id="cb54-1152"><a href="#cb54-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1153"><a href="#cb54-1153" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the convergence in probability, based on the **law of large</span>
<span id="cb54-1154"><a href="#cb54-1154" aria-hidden="true" tabindex="-1"></a>    numbers**,</span>
<span id="cb54-1155"><a href="#cb54-1155" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the convergence in distribution, based on the **central-limit**</span>
<span id="cb54-1156"><a href="#cb54-1156" aria-hidden="true" tabindex="-1"></a>    theorem.</span>
<span id="cb54-1157"><a href="#cb54-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1158"><a href="#cb54-1158" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{convergence!in probability|(}</span>
<span id="cb54-1159"><a href="#cb54-1159" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{law of large numbers|(}</span>
<span id="cb54-1160"><a href="#cb54-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1161"><a href="#cb54-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in probability</span></span>
<span id="cb54-1162"><a href="#cb54-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1163"><a href="#cb54-1163" aria-hidden="true" tabindex="-1"></a>We consider an estimator as a sequence of random numbers, indexed by the</span>
<span id="cb54-1164"><a href="#cb54-1164" aria-hidden="true" tabindex="-1"></a>size of the sample on which it has been estimated: $\left<span class="sc">\{</span>\hat{\beta}_N\right<span class="sc">\}</span>$.</span>
<span id="cb54-1165"><a href="#cb54-1165" aria-hidden="true" tabindex="-1"></a>This sequence converges in probability to a constant $\theta$ if:</span>
<span id="cb54-1166"><a href="#cb54-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1167"><a href="#cb54-1167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1168"><a href="#cb54-1168" aria-hidden="true" tabindex="-1"></a>\lim_{N\rightarrow \infty} \mbox{P}(\mid \hat{\beta}_N - \theta\mid &gt;</span>
<span id="cb54-1169"><a href="#cb54-1169" aria-hidden="true" tabindex="-1"></a>\nu) = 0 \;\forall \nu</span>
<span id="cb54-1170"><a href="#cb54-1170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1171"><a href="#cb54-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1172"><a href="#cb54-1172" aria-hidden="true" tabindex="-1"></a>This is denoted by:</span>
<span id="cb54-1173"><a href="#cb54-1173" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_N \xrightarrow{p} \theta \;\mbox{ or } \;\mbox{plim}\,\hat{\beta} = \theta$.</span>
<span id="cb54-1174"><a href="#cb54-1174" aria-hidden="true" tabindex="-1"></a>Convergence in probability implies convergence in mean square, which is</span>
<span id="cb54-1175"><a href="#cb54-1175" aria-hidden="true" tabindex="-1"></a>defined by:</span>
<span id="cb54-1176"><a href="#cb54-1176" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{convergence!in mean square|(}</span>
<span id="cb54-1177"><a href="#cb54-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1178"><a href="#cb54-1178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1179"><a href="#cb54-1179" aria-hidden="true" tabindex="-1"></a>\lim_{N\rightarrow + \infty} \mbox{E}\left( (\hat{\beta}_N - \theta) ^</span>
<span id="cb54-1180"><a href="#cb54-1180" aria-hidden="true" tabindex="-1"></a>2\right) = 0</span>
<span id="cb54-1181"><a href="#cb54-1181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1182"><a href="#cb54-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1183"><a href="#cb54-1183" aria-hidden="true" tabindex="-1"></a>and means that:</span>
<span id="cb54-1184"><a href="#cb54-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1185"><a href="#cb54-1185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1186"><a href="#cb54-1186" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-1187"><a href="#cb54-1187" aria-hidden="true" tabindex="-1"></a>\begin{array}{l}</span>
<span id="cb54-1188"><a href="#cb54-1188" aria-hidden="true" tabindex="-1"></a>\lim_{N\rightarrow  + \infty} \mbox{E}(\hat{\beta}_N) = \theta <span class="sc">\\</span></span>
<span id="cb54-1189"><a href="#cb54-1189" aria-hidden="true" tabindex="-1"></a>\lim_{N\rightarrow  + \infty} V(\hat{\beta}_N) = 0 <span class="sc">\\</span></span>
<span id="cb54-1190"><a href="#cb54-1190" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-1191"><a href="#cb54-1191" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-1192"><a href="#cb54-1192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1193"><a href="#cb54-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1194"><a href="#cb54-1194" aria-hidden="true" tabindex="-1"></a>If an estimator converges in mean square to its true value $\beta$,</span>
<span id="cb54-1195"><a href="#cb54-1195" aria-hidden="true" tabindex="-1"></a>we'll write $\hat{\beta}_N \xrightarrow{\mbox{m.s.}} \beta$ and we'll</span>
<span id="cb54-1196"><a href="#cb54-1196" aria-hidden="true" tabindex="-1"></a>also use $\mbox{plim} \, \hat{\beta}_N = \beta$, as convergence in mean</span>
<span id="cb54-1197"><a href="#cb54-1197" aria-hidden="true" tabindex="-1"></a>squares implies convergence in probability.^<span class="co">[</span><span class="ot">See for example @AMEM:85, pages 89-90.</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Amemiya} We'll also say in this case</span>
<span id="cb54-1198"><a href="#cb54-1198" aria-hidden="true" tabindex="-1"></a>that the estimator is consistent. Note that, on the opposite, an</span>
<span id="cb54-1199"><a href="#cb54-1199" aria-hidden="true" tabindex="-1"></a>estimator may be inconsistent for two reasons:</span>
<span id="cb54-1200"><a href="#cb54-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1201"><a href="#cb54-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the estimator doesn't converge in probability to any value,</span>
<span id="cb54-1202"><a href="#cb54-1202" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the estimator converges in probability to $\theta \neq \beta$.</span>
<span id="cb54-1203"><a href="#cb54-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1204"><a href="#cb54-1204" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{convergence!in mean square|)}</span>
<span id="cb54-1205"><a href="#cb54-1205" aria-hidden="true" tabindex="-1"></a>The consistency of an estimator shouldn't be confused with the property</span>
<span id="cb54-1206"><a href="#cb54-1206" aria-hidden="true" tabindex="-1"></a>of unbiasedness, even if we often encounter estimators which are</span>
<span id="cb54-1207"><a href="#cb54-1207" aria-hidden="true" tabindex="-1"></a>unbiased *and* consistent:</span>
<span id="cb54-1208"><a href="#cb54-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1209"><a href="#cb54-1209" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>unbiasedness is an exact property (true or false whatever the sample</span>
<span id="cb54-1210"><a href="#cb54-1210" aria-hidden="true" tabindex="-1"></a>    size), and it refers only to the expected value of the estimator and</span>
<span id="cb54-1211"><a href="#cb54-1211" aria-hidden="true" tabindex="-1"></a>    doesn't say anything about its variance,\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{unbiasedness}</span>
<span id="cb54-1212"><a href="#cb54-1212" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>consistency is an asymptotic property, which implies a limit for the</span>
<span id="cb54-1213"><a href="#cb54-1213" aria-hidden="true" tabindex="-1"></a>    expected value ($\beta$) and for the variance (0) of the estimator.</span>
<span id="cb54-1214"><a href="#cb54-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1215"><a href="#cb54-1215" aria-hidden="true" tabindex="-1"></a>Therefore, an unbiased estimator can be inconsistent and, conversely, a consistent estimator can be biased. Consider for example</span>
<span id="cb54-1216"><a href="#cb54-1216" aria-hidden="true" tabindex="-1"></a>that we have a random sample of N observations of a variable $x$ which</span>
<span id="cb54-1217"><a href="#cb54-1217" aria-hidden="true" tabindex="-1"></a>has a mean and a variance equal respectively to $\mu$ and $\sigma^2$. A</span>
<span id="cb54-1218"><a href="#cb54-1218" aria-hidden="true" tabindex="-1"></a>natural estimator of $\mu$ is the arithmetic mean:</span>
<span id="cb54-1219"><a href="#cb54-1219" aria-hidden="true" tabindex="-1"></a>$\bar{x}_N = \frac{1}{N} \sum_{n=1}^N x_n$, with expected value and</span>
<span id="cb54-1220"><a href="#cb54-1220" aria-hidden="true" tabindex="-1"></a>variance:</span>
<span id="cb54-1221"><a href="#cb54-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1222"><a href="#cb54-1222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1223"><a href="#cb54-1223" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-1224"><a href="#cb54-1224" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-1225"><a href="#cb54-1225" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\bar{x}_N) &amp;=&amp; \mbox{E}\left(\frac{1}{N} \sum_{n=1}^N x_n\right) = </span>
<span id="cb54-1226"><a href="#cb54-1226" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sum_{n=1}^N \mbox{E}(x_n)=\frac{1}{N}\sum_{n=1}^N \mu =</span>
<span id="cb54-1227"><a href="#cb54-1227" aria-hidden="true" tabindex="-1"></a>\mu <span class="sc">\\</span></span>
<span id="cb54-1228"><a href="#cb54-1228" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\bar{x}_N) &amp;=&amp; \mbox{V}\left(\frac{1}{N} \sum_{n=1}^N x_n\right) = </span>
<span id="cb54-1229"><a href="#cb54-1229" aria-hidden="true" tabindex="-1"></a>\frac{1}{N ^ 2}\sum_{n=1}^N \mbox{V}(x_n)=\frac{1}{N^2}\sum_{n=1}^N \sigma^2 =</span>
<span id="cb54-1230"><a href="#cb54-1230" aria-hidden="true" tabindex="-1"></a>\frac{\sigma ^ 2}{N}</span>
<span id="cb54-1231"><a href="#cb54-1231" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-1232"><a href="#cb54-1232" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-1233"><a href="#cb54-1233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1234"><a href="#cb54-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1235"><a href="#cb54-1235" aria-hidden="true" tabindex="-1"></a>This estimator is unbiased and consistent (the variance tends to 0 and</span>
<span id="cb54-1236"><a href="#cb54-1236" aria-hidden="true" tabindex="-1"></a>the expected value is equal to the population mean $\mu$). Consider now</span>
<span id="cb54-1237"><a href="#cb54-1237" aria-hidden="true" tabindex="-1"></a>two alternative estimators.<span class="ot">[^simple_regression_properties-2]</span> The first</span>
<span id="cb54-1238"><a href="#cb54-1238" aria-hidden="true" tabindex="-1"></a>one is:</span>
<span id="cb54-1239"><a href="#cb54-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1240"><a href="#cb54-1240" aria-hidden="true" tabindex="-1"></a><span class="ot">[^simple_regression_properties-2]: </span>These two estimators are inspired by</span>
<span id="cb54-1241"><a href="#cb54-1241" aria-hidden="true" tabindex="-1"></a>    @DAVI:MACK:04, page 97, and @DAVI:MACK:93, pages 123-124.\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Davidson}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{McKinnon}</span>
<span id="cb54-1242"><a href="#cb54-1242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1243"><a href="#cb54-1243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1244"><a href="#cb54-1244" aria-hidden="true" tabindex="-1"></a>\dot{x}_N = \frac{1}{N - 1} \sum_{n=1}^N x_n</span>
<span id="cb54-1245"><a href="#cb54-1245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1246"><a href="#cb54-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1247"><a href="#cb54-1247" aria-hidden="true" tabindex="-1"></a>Its first two moments can easily be obtained by writing $\dot{x}_N$ as a</span>
<span id="cb54-1248"><a href="#cb54-1248" aria-hidden="true" tabindex="-1"></a>function of $\bar{x}_N$: $\dot{x}_N = \frac{N}{N-1} \bar{x}_N$, so that</span>
<span id="cb54-1249"><a href="#cb54-1249" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\dot{x}_N) = \frac{N}{N-1} \mu$ and</span>
<span id="cb54-1250"><a href="#cb54-1250" aria-hidden="true" tabindex="-1"></a>$\mbox{V}(\dot{x}_N) = \left(\frac{N}{N-1}\right) ^ 2 \frac{\sigma^ 2}{N}$.</span>
<span id="cb54-1251"><a href="#cb54-1251" aria-hidden="true" tabindex="-1"></a>The estimator is upward biased, by a multiplicative factor of</span>
<span id="cb54-1252"><a href="#cb54-1252" aria-hidden="true" tabindex="-1"></a>$\frac{N}{N-1}$. The bias is severe in small samples (for example 25% if</span>
<span id="cb54-1253"><a href="#cb54-1253" aria-hidden="true" tabindex="-1"></a>$N$ is equal to 5), but becomes negligible as $N$ grows. As the variance</span>
<span id="cb54-1254"><a href="#cb54-1254" aria-hidden="true" tabindex="-1"></a>tends to 0 and the expected value to $\mu$, $\dot{x}_N$ is consistent.</span>
<span id="cb54-1255"><a href="#cb54-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1256"><a href="#cb54-1256" aria-hidden="true" tabindex="-1"></a>The second estimator is:</span>
<span id="cb54-1257"><a href="#cb54-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1258"><a href="#cb54-1258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1259"><a href="#cb54-1259" aria-hidden="true" tabindex="-1"></a>\tilde{x}_N = \frac{1}{2} x_1 + \frac{1}{2} \frac{1}{N - 1} \sum_{n=2}^N x_n</span>
<span id="cb54-1260"><a href="#cb54-1260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1261"><a href="#cb54-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1262"><a href="#cb54-1262" aria-hidden="true" tabindex="-1"></a>It consists of first taking the mean for the whole sample except the</span>
<span id="cb54-1263"><a href="#cb54-1263" aria-hidden="true" tabindex="-1"></a>first observation and then taking the simple average between it and the</span>
<span id="cb54-1264"><a href="#cb54-1264" aria-hidden="true" tabindex="-1"></a>first observation:</span>
<span id="cb54-1265"><a href="#cb54-1265" aria-hidden="true" tabindex="-1"></a>$\tilde{x}_N = \frac{1}{2} x_1 + \frac{1}{2} \bar{x}_{N-1}$. It is</span>
<span id="cb54-1266"><a href="#cb54-1266" aria-hidden="true" tabindex="-1"></a>unbiased, as:</span>
<span id="cb54-1267"><a href="#cb54-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1268"><a href="#cb54-1268" aria-hidden="true" tabindex="-1"></a>$$\mbox{E}(\tilde{x}_N) = \frac{1}{2}</span>
<span id="cb54-1269"><a href="#cb54-1269" aria-hidden="true" tabindex="-1"></a>\mbox{E}(x_1) + \frac{1}{2} \mbox{E}(\bar{x}_{N-1})=\frac{1}{2}\mu +</span>
<span id="cb54-1270"><a href="#cb54-1270" aria-hidden="true" tabindex="-1"></a>\frac{1}{2}\mu = \mu</span>
<span id="cb54-1271"><a href="#cb54-1271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1272"><a href="#cb54-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1273"><a href="#cb54-1273" aria-hidden="true" tabindex="-1"></a>and the variance is: $$\mbox{V}(\tilde{x}_N) = \frac{1}{4}</span>
<span id="cb54-1274"><a href="#cb54-1274" aria-hidden="true" tabindex="-1"></a>\mbox{V}(x_1) + \frac{1}{4} \mbox{V}(\bar{x}_{N-1})=</span>
<span id="cb54-1275"><a href="#cb54-1275" aria-hidden="true" tabindex="-1"></a>\frac{1}{4}\sigma^2 + \frac{1}{4}\frac{\sigma^2}{N - 1}</span>
<span id="cb54-1276"><a href="#cb54-1276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1277"><a href="#cb54-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1278"><a href="#cb54-1278" aria-hidden="true" tabindex="-1"></a>which tends to $\frac{1}{4}\sigma^2$ as $N \rightarrow +\infty$.</span>
<span id="cb54-1279"><a href="#cb54-1279" aria-hidden="true" tabindex="-1"></a>Therefore, this unbiased estimator is not consistent. The problem is</span>
<span id="cb54-1280"><a href="#cb54-1280" aria-hidden="true" tabindex="-1"></a>that the weight of the first observation is constant and, therefore, the</span>
<span id="cb54-1281"><a href="#cb54-1281" aria-hidden="true" tabindex="-1"></a>value obtained for $x_1$ influences the estimator, whatever the size of</span>
<span id="cb54-1282"><a href="#cb54-1282" aria-hidden="true" tabindex="-1"></a>the sample.</span>
<span id="cb54-1283"><a href="#cb54-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1284"><a href="#cb54-1284" aria-hidden="true" tabindex="-1"></a>The OLS estimator writes:</span>
<span id="cb54-1285"><a href="#cb54-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1286"><a href="#cb54-1286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1287"><a href="#cb54-1287" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_N = \beta + \frac{\sum_{n=1} ^ N (x_n - \bar{x})</span>
<span id="cb54-1288"><a href="#cb54-1288" aria-hidden="true" tabindex="-1"></a>\epsilon_n}{N \hat{\sigma}_x ^ 2} = \beta +</span>
<span id="cb54-1289"><a href="#cb54-1289" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{x\epsilon}}{\hat{\sigma}_x ^ 2}</span>
<span id="cb54-1290"><a href="#cb54-1290" aria-hidden="true" tabindex="-1"></a>$$ {#eq-beta_N}</span>
<span id="cb54-1291"><a href="#cb54-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1292"><a href="#cb54-1292" aria-hidden="true" tabindex="-1"></a>where $\bar{x}$, $\hat{\sigma}_x ^ 2$ and $\hat{\sigma}_{x\epsilon}$ are</span>
<span id="cb54-1293"><a href="#cb54-1293" aria-hidden="true" tabindex="-1"></a>the sample estimates of the population mean and variance of $x$ and of</span>
<span id="cb54-1294"><a href="#cb54-1294" aria-hidden="true" tabindex="-1"></a>the covariance between $x$ and $\epsilon$.^<span class="co">[</span><span class="ot">Note that the numerator of @eq-beta_N is not exactly the sample covariance, which is $\sum_n  (x_n - \bar{x})(\epsilon_n - \bar{\epsilon})/N$.</span><span class="co">]</span> As the sample size increases,</span>
<span id="cb54-1295"><a href="#cb54-1295" aria-hidden="true" tabindex="-1"></a>these three estimators converge to their population counterpart, namely</span>
<span id="cb54-1296"><a href="#cb54-1296" aria-hidden="true" tabindex="-1"></a>$\mu_x = \mbox{E}(x)$, $\sigma_x^2 = \mbox{V}(x)$ and</span>
<span id="cb54-1297"><a href="#cb54-1297" aria-hidden="true" tabindex="-1"></a>$\sigma_{x\epsilon}=\mbox{cov}(x, \epsilon)$. We therefore have:</span>
<span id="cb54-1298"><a href="#cb54-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1299"><a href="#cb54-1299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1300"><a href="#cb54-1300" aria-hidden="true" tabindex="-1"></a>\mbox{plim}\,\hat{\beta}_N = \beta +</span>
<span id="cb54-1301"><a href="#cb54-1301" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_{x\epsilon}}{\sigma_x ^2} = \theta</span>
<span id="cb54-1302"><a href="#cb54-1302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1303"><a href="#cb54-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1304"><a href="#cb54-1304" aria-hidden="true" tabindex="-1"></a>$\theta$ equals $\beta$, in which case the estimator is consistent, if</span>
<span id="cb54-1305"><a href="#cb54-1305" aria-hidden="true" tabindex="-1"></a>$x$ is uncorrelated in the population with $\epsilon$</span>
<span id="cb54-1306"><a href="#cb54-1306" aria-hidden="true" tabindex="-1"></a>($\sigma_{x\epsilon}=0$).</span>
<span id="cb54-1307"><a href="#cb54-1307" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{convergence!in probability|)}</span>
<span id="cb54-1308"><a href="#cb54-1308" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{law of large numbers|)}</span>
<span id="cb54-1309"><a href="#cb54-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1310"><a href="#cb54-1310" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in distribution: central-limit theorem {#sec-clt}</span></span>
<span id="cb54-1311"><a href="#cb54-1311" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{convergence!in distribution|(}</span>
<span id="cb54-1312"><a href="#cb54-1312" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{central-limit theorem|(}</span>
<span id="cb54-1313"><a href="#cb54-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1314"><a href="#cb54-1314" aria-hidden="true" tabindex="-1"></a>When $N\rightarrow +\infty$, $\hat{\beta}_N$ has a degenerate</span>
<span id="cb54-1315"><a href="#cb54-1315" aria-hidden="true" tabindex="-1"></a>distribution, as it converges to a constant (which is $\beta$ if the</span>
<span id="cb54-1316"><a href="#cb54-1316" aria-hidden="true" tabindex="-1"></a>estimator is consistent) as its variance tends to 0. In this subsection,</span>
<span id="cb54-1317"><a href="#cb54-1317" aria-hidden="true" tabindex="-1"></a>we seek to analyze the shape of the distribution of $\hat{\beta}$ as the</span>
<span id="cb54-1318"><a href="#cb54-1318" aria-hidden="true" tabindex="-1"></a>sample size grows. We therefore need to consider a transformation of</span>
<span id="cb54-1319"><a href="#cb54-1319" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_N$ which has a constant variance, and we'll see that it is</span>
<span id="cb54-1320"><a href="#cb54-1320" aria-hidden="true" tabindex="-1"></a>$\sqrt{N}(\hat{\beta} - \beta)$. Starting again with the equation that</span>
<span id="cb54-1321"><a href="#cb54-1321" aria-hidden="true" tabindex="-1"></a>relates $\hat{\beta}_N$ to the errors and defining</span>
<span id="cb54-1322"><a href="#cb54-1322" aria-hidden="true" tabindex="-1"></a>$w_n = \frac{x_n - \bar{x}}{\sqrt{N}\hat{\sigma}_x}$, we have:</span>
<span id="cb54-1323"><a href="#cb54-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1324"><a href="#cb54-1324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1325"><a href="#cb54-1325" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_N = \beta + \sum_{n=1}^N c_n \epsilon_n = \beta + \frac{\sum_{n=1}^N w_n \epsilon_n}{\sqrt{N}\hat{\sigma}_x}</span>
<span id="cb54-1326"><a href="#cb54-1326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1327"><a href="#cb54-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1328"><a href="#cb54-1328" aria-hidden="true" tabindex="-1"></a>Note that $w_n$ sums to 0 (as $c_n$), but that</span>
<span id="cb54-1329"><a href="#cb54-1329" aria-hidden="true" tabindex="-1"></a>$\sum_{n=1}^ Nw_n ^ 2=1$. Subtracting $\beta$ and multiplying by</span>
<span id="cb54-1330"><a href="#cb54-1330" aria-hidden="true" tabindex="-1"></a>$\sqrt{N}$, we get:</span>
<span id="cb54-1331"><a href="#cb54-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1332"><a href="#cb54-1332" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1333"><a href="#cb54-1333" aria-hidden="true" tabindex="-1"></a>z = \sqrt{N}(\hat{\beta}_N - \beta) = \sum_{n=1}^N w_n \frac{\epsilon_n}{\hat{\sigma}_x}</span>
<span id="cb54-1334"><a href="#cb54-1334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1335"><a href="#cb54-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1336"><a href="#cb54-1336" aria-hidden="true" tabindex="-1"></a>The distribution of $\sqrt{N}(\hat{\beta}_N - \beta)$ is the</span>
<span id="cb54-1337"><a href="#cb54-1337" aria-hidden="true" tabindex="-1"></a>distribution of a linear combination of $N$ random deviates</span>
<span id="cb54-1338"><a href="#cb54-1338" aria-hidden="true" tabindex="-1"></a>$\epsilon_n / \hat{\sigma}_x$, with an unknown distribution, a 0</span>
<span id="cb54-1339"><a href="#cb54-1339" aria-hidden="true" tabindex="-1"></a>expected value and a standard deviation equal to</span>
<span id="cb54-1340"><a href="#cb54-1340" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon / \hat{\sigma}_x$. The first two moments of</span>
<span id="cb54-1341"><a href="#cb54-1341" aria-hidden="true" tabindex="-1"></a>$z = \sqrt{N}(\hat{\beta}_N - \beta)$ don't depend on $N$</span>
<span id="cb54-1342"><a href="#cb54-1342" aria-hidden="true" tabindex="-1"></a>($\mbox{E}(z)=0$ and</span>
<span id="cb54-1343"><a href="#cb54-1343" aria-hidden="true" tabindex="-1"></a>$\mbox{V}(z) = \sum_n \omega_n ^ 2 \sigma_\epsilon ^ 2/ \hat{\sigma}_x ^ 2 = \sigma_\epsilon ^ 2/ \hat{\sigma}_x ^ 2$).</span>
<span id="cb54-1344"><a href="#cb54-1344" aria-hidden="true" tabindex="-1"></a>As $N$ tends to infinity, the distribution of</span>
<span id="cb54-1345"><a href="#cb54-1345" aria-hidden="true" tabindex="-1"></a>$\sqrt{N}(\hat{\beta}_N - \beta)$ still has a 0 expected value and a</span>
<span id="cb54-1346"><a href="#cb54-1346" aria-hidden="true" tabindex="-1"></a>standard deviation equals to $\sigma_\epsilon/\hat{\sigma}_x$. The</span>
<span id="cb54-1347"><a href="#cb54-1347" aria-hidden="true" tabindex="-1"></a>central-limit theorem states that the distribution of</span>
<span id="cb54-1348"><a href="#cb54-1348" aria-hidden="true" tabindex="-1"></a>$\sqrt{N}(\hat{\beta}_N - \beta)$ **converges in distribution** to a</span>
<span id="cb54-1349"><a href="#cb54-1349" aria-hidden="true" tabindex="-1"></a>normal distribution as $N$ tends to infinity, whatever the distribution</span>
<span id="cb54-1350"><a href="#cb54-1350" aria-hidden="true" tabindex="-1"></a>of $\epsilon$. This is denoted by:</span>
<span id="cb54-1351"><a href="#cb54-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1352"><a href="#cb54-1352" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1353"><a href="#cb54-1353" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\beta}_N - \beta) \xrightarrow{d} \mathcal{N}\left(0, \frac{\sigma_\epsilon}{\hat{\sigma}_x}\right)</span>
<span id="cb54-1354"><a href="#cb54-1354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1355"><a href="#cb54-1355" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic distribution}</span>
<span id="cb54-1356"><a href="#cb54-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1357"><a href="#cb54-1357" aria-hidden="true" tabindex="-1"></a>Stated differently, the **asymptotic distribution** of $\hat{\beta}$ is</span>
<span id="cb54-1358"><a href="#cb54-1358" aria-hidden="true" tabindex="-1"></a>a normal distribution with an expected value equal to $\beta$ and a standard deviation equal to</span>
<span id="cb54-1359"><a href="#cb54-1359" aria-hidden="true" tabindex="-1"></a>$\frac{\sigma_{\epsilon}}{\sqrt{N} \hat{\sigma}_x}$:</span>
<span id="cb54-1360"><a href="#cb54-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1361"><a href="#cb54-1361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1362"><a href="#cb54-1362" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_N \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}\right)</span>
<span id="cb54-1363"><a href="#cb54-1363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1364"><a href="#cb54-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1365"><a href="#cb54-1365" aria-hidden="true" tabindex="-1"></a>To illustrate the strength of the central-limit theorem, consider the</span>
<span id="cb54-1366"><a href="#cb54-1366" aria-hidden="true" tabindex="-1"></a>simple case of the arithmetic mean of $N$ independent random numbers</span>
<span id="cb54-1367"><a href="#cb54-1367" aria-hidden="true" tabindex="-1"></a>with an expected value equal to 0 and a standard deviation equal to 1:</span>
<span id="cb54-1368"><a href="#cb54-1368" aria-hidden="true" tabindex="-1"></a>$\bar{x}_n = \frac{\sum_{n=1} x_n}{N}$.<span class="ot">[^simple_regression_properties-3]</span></span>
<span id="cb54-1369"><a href="#cb54-1369" aria-hidden="true" tabindex="-1"></a>$\bar{x}_n$ has a 0 expected value and a variance equal to $1/N$. As we</span>
<span id="cb54-1370"><a href="#cb54-1370" aria-hidden="true" tabindex="-1"></a>already now, $\bar{x}_n$ converges in probability to 0 and has therefore</span>
<span id="cb54-1371"><a href="#cb54-1371" aria-hidden="true" tabindex="-1"></a>a degenerate distribution. Consider now:</span>
<span id="cb54-1372"><a href="#cb54-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1373"><a href="#cb54-1373" aria-hidden="true" tabindex="-1"></a><span class="ot">[^simple_regression_properties-3]: </span>Inspired by @DAVI:MACK:93\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Davidson}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{McKinnon}, pages</span>
<span id="cb54-1374"><a href="#cb54-1374" aria-hidden="true" tabindex="-1"></a>    126-127.</span>
<span id="cb54-1375"><a href="#cb54-1375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1376"><a href="#cb54-1376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1377"><a href="#cb54-1377" aria-hidden="true" tabindex="-1"></a>z_N = \sqrt{N} \bar{x}_n = \frac{\sum_{n=1} x_n}{\sqrt{N}}</span>
<span id="cb54-1378"><a href="#cb54-1378" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1379"><a href="#cb54-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1380"><a href="#cb54-1380" aria-hidden="true" tabindex="-1"></a>The expected value of $z_N$ is still 0, but its standard deviation is</span>
<span id="cb54-1381"><a href="#cb54-1381" aria-hidden="true" tabindex="-1"></a>now 1. Its third moment is:</span>
<span id="cb54-1382"><a href="#cb54-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1383"><a href="#cb54-1383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1384"><a href="#cb54-1384" aria-hidden="true" tabindex="-1"></a>E(z_N^3) = \frac{\mbox{E}\left((\sum_{n=1}^N x_n) ^ 3\right)}{N^{3/2}}</span>
<span id="cb54-1385"><a href="#cb54-1385" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1386"><a href="#cb54-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1387"><a href="#cb54-1387" aria-hidden="true" tabindex="-1"></a>Developing the sum for $N=3$, we have:</span>
<span id="cb54-1388"><a href="#cb54-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1389"><a href="#cb54-1389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1390"><a href="#cb54-1390" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1}^3 x_n\right) ^ 3 =(x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 + 2 x_1 x_2 +</span>
<span id="cb54-1391"><a href="#cb54-1391" aria-hidden="true" tabindex="-1"></a>2x_1 x_3 + 2 x_2 x_3)(x_1 + x_2 + x_3)</span>
<span id="cb54-1392"><a href="#cb54-1392" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1393"><a href="#cb54-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1394"><a href="#cb54-1394" aria-hidden="true" tabindex="-1"></a>Taking the expected value of this sum, we get terms like:</span>
<span id="cb54-1395"><a href="#cb54-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1396"><a href="#cb54-1396" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\mbox{E}(x_n x_m^2) = \mbox{E}(x_n)\mbox{V}(x_m^2)=0 \times 1 = 0$,</span>
<span id="cb54-1397"><a href="#cb54-1397" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\mbox{E}(x_n, x_m, x_l) = \mbox{E}(x_n)\mbox{E}(x_m)\mbox{E}(x_l)= 0 \times 0 \times 0 = 0$,</span>
<span id="cb54-1398"><a href="#cb54-1398" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\mbox{E}(x_n^3) = \mu_3$.</span>
<span id="cb54-1399"><a href="#cb54-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1400"><a href="#cb54-1400" aria-hidden="true" tabindex="-1"></a>Therefore, only the last category of terms remains while taking the</span>
<span id="cb54-1401"><a href="#cb54-1401" aria-hidden="true" tabindex="-1"></a>expected value of the sum. As we have $N$ of them, the third moment of</span>
<span id="cb54-1402"><a href="#cb54-1402" aria-hidden="true" tabindex="-1"></a>$z_N$ is therefore:</span>
<span id="cb54-1403"><a href="#cb54-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1404"><a href="#cb54-1404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1405"><a href="#cb54-1405" aria-hidden="true" tabindex="-1"></a>E(z_N^3) = \frac{N \mu_3}{N^{3/2}} = \frac{\mu_3}{\sqrt{N}}</span>
<span id="cb54-1406"><a href="#cb54-1406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1407"><a href="#cb54-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1408"><a href="#cb54-1408" aria-hidden="true" tabindex="-1"></a>Therefore, as $N$ tends to infinity, $E(z_N^3)$ tends to 0, whatever the</span>
<span id="cb54-1409"><a href="#cb54-1409" aria-hidden="true" tabindex="-1"></a>value of $\mu_3$.</span>
<span id="cb54-1410"><a href="#cb54-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1411"><a href="#cb54-1411" aria-hidden="true" tabindex="-1"></a>Consider now the fourth moment:</span>
<span id="cb54-1412"><a href="#cb54-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1413"><a href="#cb54-1413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1414"><a href="#cb54-1414" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-1415"><a href="#cb54-1415" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1}^3 x_n\right) ^ 4 &amp;=&amp; </span>
<span id="cb54-1416"><a href="#cb54-1416" aria-hidden="true" tabindex="-1"></a>(x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 + 2 x_1 x_2 + 2x_1 x_3 + 2 x_2 x_3) <span class="sc">\\</span></span>
<span id="cb54-1417"><a href="#cb54-1417" aria-hidden="true" tabindex="-1"></a>&amp;\times&amp;</span>
<span id="cb54-1418"><a href="#cb54-1418" aria-hidden="true" tabindex="-1"></a>(x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 + 2 x_1 x_2 + 2x_1 x_3 + 2 x_2 x_3)</span>
<span id="cb54-1419"><a href="#cb54-1419" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-1420"><a href="#cb54-1420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1421"><a href="#cb54-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1422"><a href="#cb54-1422" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>terms like $x_n x_m ^ 3$, $x_n x_m x_l ^ 2$ and $x_n x_m x_l x_p$</span>
<span id="cb54-1423"><a href="#cb54-1423" aria-hidden="true" tabindex="-1"></a>    have zero expected values,</span>
<span id="cb54-1424"><a href="#cb54-1424" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>terms like $x_n ^ 2 x_m ^ 2$ have an expected value of</span>
<span id="cb54-1425"><a href="#cb54-1425" aria-hidden="true" tabindex="-1"></a>    $1 \times 1 = 1$. For $N=3$, there are 18 of them and, more</span>
<span id="cb54-1426"><a href="#cb54-1426" aria-hidden="true" tabindex="-1"></a>    generally, for a given value of $N$, there are $3 N (N - 1)$ of</span>
<span id="cb54-1427"><a href="#cb54-1427" aria-hidden="true" tabindex="-1"></a>    them,</span>
<span id="cb54-1428"><a href="#cb54-1428" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>terms like $x_n ^ 4$ have an expected value of $\mu_4$ and there are</span>
<span id="cb54-1429"><a href="#cb54-1429" aria-hidden="true" tabindex="-1"></a>    $N$ of them.</span>
<span id="cb54-1430"><a href="#cb54-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1431"><a href="#cb54-1431" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb54-1432"><a href="#cb54-1432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1433"><a href="#cb54-1433" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1434"><a href="#cb54-1434" aria-hidden="true" tabindex="-1"></a>E(z_N^4) = \frac{3N(N-1) + N \mu_4}{N^2} = 3 \frac{N-1}{N} + \frac{\mu_4}{N}</span>
<span id="cb54-1435"><a href="#cb54-1435" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1436"><a href="#cb54-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1437"><a href="#cb54-1437" aria-hidden="true" tabindex="-1"></a>which tends to 3, as $N$ tends to infinity.</span>
<span id="cb54-1438"><a href="#cb54-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1439"><a href="#cb54-1439" aria-hidden="true" tabindex="-1"></a>Therefore, for "large" $N$, the distribution of $z_N$ doesn't depend on</span>
<span id="cb54-1440"><a href="#cb54-1440" aria-hidden="true" tabindex="-1"></a>the shape parameters of $x_n$ ($\mu_3$ and $\mu_4$) and its third and fourth moments tend to 0 and 3, which are the corresponding values for a normal distribution. These reasonings can easily be extended to higher moments, the general conclusion being that, when $N$ tends to infinity, all the moments of $z_N$ tend to those of the normal distribution. The asymptotic distribution of $z_N$ is therefore normal and doesn't depend on the characteristics of the distribution of $x_N$.</span>
<span id="cb54-1441"><a href="#cb54-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1442"><a href="#cb54-1442" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{convergence!in distribution|)}</span>
<span id="cb54-1443"><a href="#cb54-1443" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{central-limit theorem|)}</span>
<span id="cb54-1444"><a href="#cb54-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1445"><a href="#cb54-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1446"><a href="#cb54-1446" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simulations</span></span>
<span id="cb54-1447"><a href="#cb54-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1448"><a href="#cb54-1448" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!convergence of the OLS estimator|(}</span>
<span id="cb54-1449"><a href="#cb54-1449" aria-hidden="true" tabindex="-1"></a>The law of large numbers and the central-limit theorem can be</span>
<span id="cb54-1450"><a href="#cb54-1450" aria-hidden="true" tabindex="-1"></a>interestingly illustrated using simulations. Consider errors that follow</span>
<span id="cb54-1451"><a href="#cb54-1451" aria-hidden="true" tabindex="-1"></a>a standardized chi-square distribution with one degree of freedom.</span>
<span id="cb54-1452"><a href="#cb54-1452" aria-hidden="true" tabindex="-1"></a>Remember that a chi-squared with one degree of freedom is simply the</span>
<span id="cb54-1453"><a href="#cb54-1453" aria-hidden="true" tabindex="-1"></a>square of a standard normal deviate: $x = z ^ 2$. We thus have</span>
<span id="cb54-1454"><a href="#cb54-1454" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(x) = \mbox{E}(z ^ 2) = \mbox{V}(z) = 1$ and:</span>
<span id="cb54-1455"><a href="#cb54-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1456"><a href="#cb54-1456" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1457"><a href="#cb54-1457" aria-hidden="true" tabindex="-1"></a>\mbox{V}(x) = \mbox{E}\left((x - 1) ^ 2\right)=</span>
<span id="cb54-1458"><a href="#cb54-1458" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left(z ^ 4\right) - 1 = 3 - 1 = 2</span>
<span id="cb54-1459"><a href="#cb54-1459" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1460"><a href="#cb54-1460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1461"><a href="#cb54-1461" aria-hidden="true" tabindex="-1"></a>Therefore, $v = \frac{x-1}{\sqrt{2}}$ has a zero expected value and a</span>
<span id="cb54-1462"><a href="#cb54-1462" aria-hidden="true" tabindex="-1"></a>standard deviation equal to 1. One can show that its third and fourth</span>
<span id="cb54-1463"><a href="#cb54-1463" aria-hidden="true" tabindex="-1"></a>centered moments are $2\sqrt{2}$ and $15$. Therefore, the distribution</span>
<span id="cb54-1464"><a href="#cb54-1464" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb54-1465"><a href="#cb54-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1466"><a href="#cb54-1466" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>highly asymmetric, i.e., it has a long tail on the right side of the</span>
<span id="cb54-1467"><a href="#cb54-1467" aria-hidden="true" tabindex="-1"></a>    distribution, and a negative median, which is lower than the mean</span>
<span id="cb54-1468"><a href="#cb54-1468" aria-hidden="true" tabindex="-1"></a>    (equal to zero),</span>
<span id="cb54-1469"><a href="#cb54-1469" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>highly leptokurtic, the fourth moment (15) is much larger than the</span>
<span id="cb54-1470"><a href="#cb54-1470" aria-hidden="true" tabindex="-1"></a>    value of 3 of the normal distribution; it has therefore a much</span>
<span id="cb54-1471"><a href="#cb54-1471" aria-hidden="true" tabindex="-1"></a>    higher mode and fatter tails than a normal distribution.</span>
<span id="cb54-1472"><a href="#cb54-1472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1473"><a href="#cb54-1473" aria-hidden="true" tabindex="-1"></a>Now, going back to the <span class="in">`prtime`</span> data, we generate a sample using the</span>
<span id="cb54-1474"><a href="#cb54-1474" aria-hidden="true" tabindex="-1"></a>following DGP:</span>
<span id="cb54-1475"><a href="#cb54-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1476"><a href="#cb54-1476" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1477"><a href="#cb54-1477" aria-hidden="true" tabindex="-1"></a>y_n = \alpha + \beta x_n + \epsilon_n</span>
<span id="cb54-1478"><a href="#cb54-1478" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1479"><a href="#cb54-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1480"><a href="#cb54-1480" aria-hidden="true" tabindex="-1"></a>with $\alpha = <span class="in">`r round(alpha, 1)`</span>$, $\beta = <span class="in">`r round(beta, 3)`</span>$ and:</span>
<span id="cb54-1481"><a href="#cb54-1481" aria-hidden="true" tabindex="-1"></a>$\epsilon_n = \sigma_\epsilon \frac{z_n ^ 2 - 1}{\sqrt{2}}$, where</span>
<span id="cb54-1482"><a href="#cb54-1482" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon = 0.08$ and $z_n$ is a random draw on a standard normal</span>
<span id="cb54-1483"><a href="#cb54-1483" aria-hidden="true" tabindex="-1"></a>distribution.</span>
<span id="cb54-1484"><a href="#cb54-1484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1485"><a href="#cb54-1485" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1486"><a href="#cb54-1486" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: a_sample_with_chisq</span></span>
<span id="cb54-1487"><a href="#cb54-1487" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1488"><a href="#cb54-1488" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb54-1489"><a href="#cb54-1489" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(h)</span>
<span id="cb54-1490"><a href="#cb54-1490" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb54-1491"><a href="#cb54-1491" aria-hidden="true" tabindex="-1"></a>asmpl <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">h =</span> x,</span>
<span id="cb54-1492"><a href="#cb54-1492" aria-hidden="true" tabindex="-1"></a>                <span class="at">eps =</span> seps <span class="sc">*</span> (<span class="fu">rnorm</span>(N) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>),</span>
<span id="cb54-1493"><a href="#cb54-1493" aria-hidden="true" tabindex="-1"></a>                <span class="at">sr =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> h <span class="sc">+</span> eps)</span>
<span id="cb54-1494"><a href="#cb54-1494" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h, asmpl)</span>
<span id="cb54-1495"><a href="#cb54-1495" aria-hidden="true" tabindex="-1"></a>v <span class="sc">%&gt;%</span> residuals <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">2</span>)</span>
<span id="cb54-1496"><a href="#cb54-1496" aria-hidden="true" tabindex="-1"></a>v <span class="sc">%&gt;%</span> residuals <span class="sc">%&gt;%</span> sum</span>
<span id="cb54-1497"><a href="#cb54-1497" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1498"><a href="#cb54-1498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1499"><a href="#cb54-1499" aria-hidden="true" tabindex="-1"></a>The sum of the residuals is still equal to zero, but we can see in</span>
<span id="cb54-1500"><a href="#cb54-1500" aria-hidden="true" tabindex="-1"></a>@fig-smpls4hbeta, where a scatterplot is drawn for four random samples that the distribution of the errors (and therefore the distribution of the</span>
<span id="cb54-1501"><a href="#cb54-1501" aria-hidden="true" tabindex="-1"></a>residuals) is highly asymmetric (we have only a couple of positive</span>
<span id="cb54-1502"><a href="#cb54-1502" aria-hidden="true" tabindex="-1"></a>values, some of them being very large).</span>
<span id="cb54-1503"><a href="#cb54-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1504"><a href="#cb54-1504" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1505"><a href="#cb54-1505" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-smpls4hbeta</span></span>
<span id="cb54-1506"><a href="#cb54-1506" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1507"><a href="#cb54-1507" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Four samples with $\\chi^2$ errors"</span></span>
<span id="cb54-1508"><a href="#cb54-1508" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb54-1509"><a href="#cb54-1509" aria-hidden="true" tabindex="-1"></a>smpls <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">smpl =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb54-1510"><a href="#cb54-1510" aria-hidden="true" tabindex="-1"></a>                <span class="at">h =</span> <span class="fu">rep</span>(x, R),</span>
<span id="cb54-1511"><a href="#cb54-1511" aria-hidden="true" tabindex="-1"></a>                <span class="at">eps =</span> seps <span class="sc">*</span> (<span class="fu">rnorm</span>(N <span class="sc">*</span> R) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>),</span>
<span id="cb54-1512"><a href="#cb54-1512" aria-hidden="true" tabindex="-1"></a>                <span class="at">sr =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> h <span class="sc">+</span> eps)</span>
<span id="cb54-1513"><a href="#cb54-1513" aria-hidden="true" tabindex="-1"></a>smpls <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> h, <span class="at">y =</span> sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-1514"><a href="#cb54-1514" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> smpl) <span class="sc">+</span></span>
<span id="cb54-1515"><a href="#cb54-1515" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb54-1516"><a href="#cb54-1516" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> alpha, <span class="at">slope =</span> beta)</span>
<span id="cb54-1517"><a href="#cb54-1517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1518"><a href="#cb54-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1519"><a href="#cb54-1519" aria-hidden="true" tabindex="-1"></a>We then generate a large number of samples and for each of them, we</span>
<span id="cb54-1520"><a href="#cb54-1520" aria-hidden="true" tabindex="-1"></a>compute the estimator of the slope and we plot the empirical</span>
<span id="cb54-1521"><a href="#cb54-1521" aria-hidden="true" tabindex="-1"></a>distribution of $\hat{\beta}$ using a histogram. We consider different</span>
<span id="cb54-1522"><a href="#cb54-1522" aria-hidden="true" tabindex="-1"></a>sample sizes; we use the "repeated in fixed sample" hypothesis,^<span class="co">[</span><span class="ot">See @DAVI:MACK:93, pp. 116-117 for details.</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Davidson}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{McKinnon} i.e., we</span>
<span id="cb54-1523"><a href="#cb54-1523" aria-hidden="true" tabindex="-1"></a>increase the size of the sample by duplicating the same values of $x$.</span>
<span id="cb54-1524"><a href="#cb54-1524" aria-hidden="true" tabindex="-1"></a>The histograms are presented in @fig-empdisthbeta, along with the normal</span>
<span id="cb54-1525"><a href="#cb54-1525" aria-hidden="true" tabindex="-1"></a>density curve.</span>
<span id="cb54-1526"><a href="#cb54-1526" aria-hidden="true" tabindex="-1"></a>The distribution of $\hat{\beta}$ is centered on $\beta$, whatever the</span>
<span id="cb54-1527"><a href="#cb54-1527" aria-hidden="true" tabindex="-1"></a>sample size, which illustrates the fact that the estimator is unbiased.</span>
<span id="cb54-1528"><a href="#cb54-1528" aria-hidden="true" tabindex="-1"></a>As the sample size is growing, we can see two changes in the shape of</span>
<span id="cb54-1529"><a href="#cb54-1529" aria-hidden="true" tabindex="-1"></a>the histogram:</span>
<span id="cb54-1530"><a href="#cb54-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1531"><a href="#cb54-1531" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>it is more and more concentrated around the mean value of</span>
<span id="cb54-1532"><a href="#cb54-1532" aria-hidden="true" tabindex="-1"></a>    $\hat{\beta}$, which is due to the fact that the standard deviation</span>
<span id="cb54-1533"><a href="#cb54-1533" aria-hidden="true" tabindex="-1"></a>    of $\hat{\beta}$ is inversely proportional to sample size,</span>
<span id="cb54-1534"><a href="#cb54-1534" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the adjustment by the normal density curve is very bad in small</span>
<span id="cb54-1535"><a href="#cb54-1535" aria-hidden="true" tabindex="-1"></a>    samples; especially, the distribution of the estimator is highly</span>
<span id="cb54-1536"><a href="#cb54-1536" aria-hidden="true" tabindex="-1"></a>    leptokurtic, but the adjustment gets much better for larger samples.</span>
<span id="cb54-1537"><a href="#cb54-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1538"><a href="#cb54-1538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1539"><a href="#cb54-1539" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1540"><a href="#cb54-1540" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-empdisthbeta</span></span>
<span id="cb54-1541"><a href="#cb54-1541" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1542"><a href="#cb54-1542" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Empirical distribution of $\\hat{\\beta}$ for different sample sizes and adjustment by a normal density"</span></span>
<span id="cb54-1543"><a href="#cb54-1543" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb54-1544"><a href="#cb54-1544" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(h)</span>
<span id="cb54-1545"><a href="#cb54-1545" aria-hidden="true" tabindex="-1"></a>sx <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb54-1546"><a href="#cb54-1546" aria-hidden="true" tabindex="-1"></a>seps <span class="ot">&lt;-</span> <span class="fl">0.08</span></span>
<span id="cb54-1547"><a href="#cb54-1547" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.2</span></span>
<span id="cb54-1548"><a href="#cb54-1548" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.032</span></span>
<span id="cb54-1549"><a href="#cb54-1549" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fl">1E04</span></span>
<span id="cb54-1550"><a href="#cb54-1550" aria-hidden="true" tabindex="-1"></a>Ns <span class="ot">&lt;-</span> N <span class="sc">*</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">11</span>, <span class="dv">22</span>)</span>
<span id="cb54-1551"><a href="#cb54-1551" aria-hidden="true" tabindex="-1"></a>smpls <span class="ot">&lt;-</span> <span class="fu">lapply</span>(Ns, <span class="cf">function</span>(N)</span>
<span id="cb54-1552"><a href="#cb54-1552" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tibble</span>(<span class="at">smpl =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb54-1553"><a href="#cb54-1553" aria-hidden="true" tabindex="-1"></a>           <span class="at">x =</span> <span class="fu">rep</span>(x, R <span class="sc">*</span> N <span class="sc">/</span> <span class="dv">9</span>),</span>
<span id="cb54-1554"><a href="#cb54-1554" aria-hidden="true" tabindex="-1"></a>           <span class="at">eps =</span> seps <span class="sc">*</span> (<span class="fu">rnorm</span>(R <span class="sc">*</span> N) <span class="sc">^</span> <span class="dv">2</span>  <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>),</span>
<span id="cb54-1555"><a href="#cb54-1555" aria-hidden="true" tabindex="-1"></a>           <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps) <span class="sc">%&gt;%</span></span>
<span id="cb54-1556"><a href="#cb54-1556" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(smpl) <span class="sc">%&gt;%</span></span>
<span id="cb54-1557"><a href="#cb54-1557" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">hbeta =</span> <span class="fu">sum</span>( (y <span class="sc">-</span> <span class="fu">mean</span>(y)) <span class="sc">*</span> (x <span class="sc">-</span> <span class="fu">mean</span>(x))) <span class="sc">/</span></span>
<span id="cb54-1558"><a href="#cb54-1558" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">sum</span>( (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>),</span>
<span id="cb54-1559"><a href="#cb54-1559" aria-hidden="true" tabindex="-1"></a>              <span class="at">theta =</span> <span class="fu">sqrt</span>(N) <span class="sc">*</span> (hbeta <span class="sc">-</span> beta)) <span class="sc">%&gt;%</span></span>
<span id="cb54-1560"><a href="#cb54-1560" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(hbeta, theta)) <span class="sc">%&gt;%</span></span>
<span id="cb54-1561"><a href="#cb54-1561" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Reduce</span>(<span class="at">f =</span> <span class="st">"cbind"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb54-1562"><a href="#cb54-1562" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_names</span>(<span class="fu">as.character</span>(<span class="fu">t</span>(<span class="fu">outer</span>(Ns, <span class="fu">c</span>(<span class="st">"hbeta"</span>, <span class="st">"theta"</span>), paste, <span class="at">sep =</span> <span class="st">"_"</span>)))) <span class="sc">%&gt;%</span></span>
<span id="cb54-1563"><a href="#cb54-1563" aria-hidden="true" tabindex="-1"></a>    as_tibble <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span>R) <span class="sc">%&gt;%</span></span>
<span id="cb54-1564"><a href="#cb54-1564" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_longer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>, <span class="at">names_to =</span> <span class="fu">c</span>(<span class="st">"size"</span>, <span class="st">"rnd"</span>), <span class="at">names_sep =</span> <span class="st">"_"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb54-1565"><a href="#cb54-1565" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_wider</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">names_from =</span> rnd, <span class="at">values_from =</span> value) <span class="sc">%&gt;%</span> <span class="co"># 1:3 remplacé par 1:2 2022/07/12</span></span>
<span id="cb54-1566"><a href="#cb54-1566" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">size =</span> <span class="fu">factor</span>(size, <span class="at">levels =</span> Ns))</span>
<span id="cb54-1567"><a href="#cb54-1567" aria-hidden="true" tabindex="-1"></a>xs <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.02</span>, <span class="fl">0.045</span>, <span class="fl">0.0001</span>)</span>
<span id="cb54-1568"><a href="#cb54-1568" aria-hidden="true" tabindex="-1"></a>datanorm <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x=</span> <span class="fu">rep</span>(xs, <span class="dv">4</span>),</span>
<span id="cb54-1569"><a href="#cb54-1569" aria-hidden="true" tabindex="-1"></a>                   <span class="at">size =</span> <span class="fu">rep</span>(Ns, <span class="at">each =</span> <span class="fu">length</span>(xs)),</span>
<span id="cb54-1570"><a href="#cb54-1570" aria-hidden="true" tabindex="-1"></a>                   <span class="at">dens =</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> beta, <span class="at">sd =</span> seps <span class="sc">/</span> sx <span class="sc">/</span> <span class="fu">sqrt</span>(size))) <span class="sc">%&gt;%</span></span>
<span id="cb54-1571"><a href="#cb54-1571" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">size =</span> <span class="fu">factor</span>(size, <span class="at">levels =</span> Ns))                   </span>
<span id="cb54-1572"><a href="#cb54-1572" aria-hidden="true" tabindex="-1"></a>smpls <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(hbeta)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>,</span>
<span id="cb54-1573"><a href="#cb54-1573" aria-hidden="true" tabindex="-1"></a>                                              <span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)),</span>
<span id="cb54-1574"><a href="#cb54-1574" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="fl">0.02</span>, <span class="fl">0.045</span>, <span class="fl">0.001</span>)) <span class="sc">+</span></span>
<span id="cb54-1575"><a href="#cb54-1575" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">data =</span> datanorm, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> dens)) <span class="sc">+</span> </span>
<span id="cb54-1576"><a href="#cb54-1576" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb54-1577"><a href="#cb54-1577" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> size)</span>
<span id="cb54-1578"><a href="#cb54-1578" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1579"><a href="#cb54-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1580"><a href="#cb54-1580" aria-hidden="true" tabindex="-1"></a>Next, we plot in @fig-empdisttheta the distribution of</span>
<span id="cb54-1581"><a href="#cb54-1581" aria-hidden="true" tabindex="-1"></a>$\sqrt{N}(\hat{\beta}-\beta)$, which has constant mean and standard</span>
<span id="cb54-1582"><a href="#cb54-1582" aria-hidden="true" tabindex="-1"></a>deviation (respectively 0 and $\sigma_\epsilon/\sigma_x)$. Therefore,</span>
<span id="cb54-1583"><a href="#cb54-1583" aria-hidden="true" tabindex="-1"></a>only the shape of the distribution changes when the sample size</span>
<span id="cb54-1584"><a href="#cb54-1584" aria-hidden="true" tabindex="-1"></a>increases. We can see more precisely on this figure the strength of the</span>
<span id="cb54-1585"><a href="#cb54-1585" aria-hidden="true" tabindex="-1"></a>central-limit theorem, even for errors that follow a distribution very</span>
<span id="cb54-1586"><a href="#cb54-1586" aria-hidden="true" tabindex="-1"></a>different from the normal.</span>
<span id="cb54-1587"><a href="#cb54-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1588"><a href="#cb54-1588" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1589"><a href="#cb54-1589" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-empdisttheta</span></span>
<span id="cb54-1590"><a href="#cb54-1590" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1591"><a href="#cb54-1591" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Empirical distribution of $\\sqrt{N}(\\hat{\\beta}-\\beta)$ for different sample sizes and adjustment by a normal density"</span></span>
<span id="cb54-1592"><a href="#cb54-1592" aria-hidden="true" tabindex="-1"></a>smpls <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(theta)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>,</span>
<span id="cb54-1593"><a href="#cb54-1593" aria-hidden="true" tabindex="-1"></a>                                              <span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)),</span>
<span id="cb54-1594"><a href="#cb54-1594" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.03</span>, <span class="fl">0.03</span>, <span class="fl">0.0025</span>)) <span class="sc">+</span></span>
<span id="cb54-1595"><a href="#cb54-1595" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb54-1596"><a href="#cb54-1596" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> size) <span class="sc">+</span></span>
<span id="cb54-1597"><a href="#cb54-1597" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> seps <span class="sc">/</span> sx), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.03</span>, <span class="fl">0.03</span>))</span>
<span id="cb54-1598"><a href="#cb54-1598" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1599"><a href="#cb54-1599" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!convergence of the OLS estimator|)}</span>
<span id="cb54-1600"><a href="#cb54-1600" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{consistency|)}</span>
<span id="cb54-1601"><a href="#cb54-1601" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic properties|)}</span>
<span id="cb54-1602"><a href="#cb54-1602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1603"><a href="#cb54-1603" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb54-1604"><a href="#cb54-1604" aria-hidden="true" tabindex="-1"></a><span class="fu">## Confidence interval and tests {#sec-confint_test_slm}</span></span>
<span id="cb54-1605"><a href="#cb54-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1606"><a href="#cb54-1606" aria-hidden="true" tabindex="-1"></a>With the set of hypotheses we have made concerning the errors of the</span>
<span id="cb54-1607"><a href="#cb54-1607" aria-hidden="true" tabindex="-1"></a>model, the distribution of the estimator is completely defined by:</span>
<span id="cb54-1608"><a href="#cb54-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1609"><a href="#cb54-1609" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1610"><a href="#cb54-1610" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_N \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_{\epsilon}}{\sqrt{N}\hat{\sigma}_x}\right)</span>
<span id="cb54-1611"><a href="#cb54-1611" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1612"><a href="#cb54-1612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1613"><a href="#cb54-1613" aria-hidden="true" tabindex="-1"></a>where $\stackrel{a}{\sim}$ means that the normal distribution is</span>
<span id="cb54-1614"><a href="#cb54-1614" aria-hidden="true" tabindex="-1"></a>asymptotic and is actually a very good approximation if the sample size</span>
<span id="cb54-1615"><a href="#cb54-1615" aria-hidden="true" tabindex="-1"></a>is large enough (which is the case in general in microeconometrics</span>
<span id="cb54-1616"><a href="#cb54-1616" aria-hidden="true" tabindex="-1"></a>studies), whatever the distribution of the errors. Moreover, as</span>
<span id="cb54-1617"><a href="#cb54-1617" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_N = \beta + \sum_{n=1}^N c_n \epsilon_n$ (the estimator is</span>
<span id="cb54-1618"><a href="#cb54-1618" aria-hidden="true" tabindex="-1"></a>a linear combination of the errors), if the errors are normal, then the</span>
<span id="cb54-1619"><a href="#cb54-1619" aria-hidden="true" tabindex="-1"></a>distribution of $\hat{\beta}$ is **exactly normal** (see</span>
<span id="cb54-1620"><a href="#cb54-1620" aria-hidden="true" tabindex="-1"></a>@sec-exact_ols_distribution). Removing from $\hat{\beta}$ its expected value and dividing</span>
<span id="cb54-1621"><a href="#cb54-1621" aria-hidden="true" tabindex="-1"></a>by its standard deviation, we get a standard normal variable:</span>
<span id="cb54-1622"><a href="#cb54-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1623"><a href="#cb54-1623" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1624"><a href="#cb54-1624" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\beta}_N-\beta}{\sigma_{\hat{\beta}_N}}=</span>
<span id="cb54-1625"><a href="#cb54-1625" aria-hidden="true" tabindex="-1"></a>\frac{\sqrt{N}\hat{\sigma}_x}{\sigma_\epsilon}(\hat{\beta}_N-\beta)</span>
<span id="cb54-1626"><a href="#cb54-1626" aria-hidden="true" tabindex="-1"></a>\stackrel{a}{\sim}</span>
<span id="cb54-1627"><a href="#cb54-1627" aria-hidden="true" tabindex="-1"></a>\mathcal{N}(0, 1)</span>
<span id="cb54-1628"><a href="#cb54-1628" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1629"><a href="#cb54-1629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1630"><a href="#cb54-1630" aria-hidden="true" tabindex="-1"></a>This result enables to perform two tasks:</span>
<span id="cb54-1631"><a href="#cb54-1631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1632"><a href="#cb54-1632" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>testing hypothesis,</span>
<span id="cb54-1633"><a href="#cb54-1633" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>constructing a confidence interval, either for the coefficients or</span>
<span id="cb54-1634"><a href="#cb54-1634" aria-hidden="true" tabindex="-1"></a>    for the predictions of the model.</span>
<span id="cb54-1635"><a href="#cb54-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1636"><a href="#cb54-1636" aria-hidden="true" tabindex="-1"></a><span class="fu">### Testing hypothesis</span></span>
<span id="cb54-1637"><a href="#cb54-1637" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!simple linear model|(}</span>
<span id="cb54-1638"><a href="#cb54-1638" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{critical value|(}</span>
<span id="cb54-1639"><a href="#cb54-1639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1640"><a href="#cb54-1640" aria-hidden="true" tabindex="-1"></a>We want to test the hypothesis that $\mbox{H}_0: \beta = \beta_0$, the</span>
<span id="cb54-1641"><a href="#cb54-1641" aria-hidden="true" tabindex="-1"></a>alternative hypothesis being $\mbox{H}_1: \beta \neq \beta_0$. Denote</span>
<span id="cb54-1642"><a href="#cb54-1642" aria-hidden="true" tabindex="-1"></a>$z_{\alpha/2}$ the critical value of a standard normal distribution at</span>
<span id="cb54-1643"><a href="#cb54-1643" aria-hidden="true" tabindex="-1"></a>the $\alpha$% error level. It is defined by:</span>
<span id="cb54-1644"><a href="#cb54-1644" aria-hidden="true" tabindex="-1"></a>$\mbox{P}(\mid z \mid &gt; z_{\alpha/2}) = \alpha$ or:</span>
<span id="cb54-1645"><a href="#cb54-1645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1646"><a href="#cb54-1646" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1647"><a href="#cb54-1647" aria-hidden="true" tabindex="-1"></a>\mbox{P}(\mid z \mid \leq z_{\alpha/2}) = 1 - \alpha</span>
<span id="cb54-1648"><a href="#cb54-1648" aria-hidden="true" tabindex="-1"></a>$$ {#eq-critvalue}</span>
<span id="cb54-1649"><a href="#cb54-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1650"><a href="#cb54-1650" aria-hidden="true" tabindex="-1"></a>Consider for example $\alpha = 5$%. To obtain the critical value, the</span>
<span id="cb54-1651"><a href="#cb54-1651" aria-hidden="true" tabindex="-1"></a><span class="in">`qnorm`</span> function can be used, which takes a probability $p$ as argument</span>
<span id="cb54-1652"><a href="#cb54-1652" aria-hidden="true" tabindex="-1"></a>and returns a quantile $q$. By default, it returns the value such that</span>
<span id="cb54-1653"><a href="#cb54-1653" aria-hidden="true" tabindex="-1"></a>$\mbox{P}(z &lt; q) = p$, but the value of $\mbox{P}(z &gt; q) = p$ is</span>
<span id="cb54-1654"><a href="#cb54-1654" aria-hidden="true" tabindex="-1"></a>returned if the <span class="in">`lower.tail`</span> argument is set to <span class="in">`FALSE`</span>:\idxfun{qnorm}{stats}</span>
<span id="cb54-1655"><a href="#cb54-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1656"><a href="#cb54-1656" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1657"><a href="#cb54-1657" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1658"><a href="#cb54-1658" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: qnorm</span></span>
<span id="cb54-1659"><a href="#cb54-1659" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.025</span>)</span>
<span id="cb54-1660"><a href="#cb54-1660" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb54-1661"><a href="#cb54-1661" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.025</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb54-1662"><a href="#cb54-1662" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1663"><a href="#cb54-1663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1664"><a href="#cb54-1664" aria-hidden="true" tabindex="-1"></a>In this case, the critical value is $1.96$, which means that, drawing in</span>
<span id="cb54-1665"><a href="#cb54-1665" aria-hidden="true" tabindex="-1"></a>a standard normal distribution, one gets on average 95% of values lower,</span>
<span id="cb54-1666"><a href="#cb54-1666" aria-hidden="true" tabindex="-1"></a>in absolute values, than 1.96. The preceding command indicates</span>
<span id="cb54-1667"><a href="#cb54-1667" aria-hidden="true" tabindex="-1"></a>respectively that:</span>
<span id="cb54-1668"><a href="#cb54-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1669"><a href="#cb54-1669" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>2.5% of the values of a normal distribution are lower than $-1.96$,</span>
<span id="cb54-1670"><a href="#cb54-1670" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>97.5% of the values of a normal distribution are lower than 1.96,</span>
<span id="cb54-1671"><a href="#cb54-1671" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>2.5% of the values of a normal distribution are greater than 1.96.</span>
<span id="cb54-1672"><a href="#cb54-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1673"><a href="#cb54-1673" aria-hidden="true" tabindex="-1"></a>The 5% critical value is presented in @fig-normal.</span>
<span id="cb54-1674"><a href="#cb54-1674" aria-hidden="true" tabindex="-1"></a>If $\mbox{H}_0$ is true,</span>
<span id="cb54-1675"><a href="#cb54-1675" aria-hidden="true" tabindex="-1"></a>$(\hat{\beta}_N-\beta_0)/\sigma_{\hat{\beta}_N}$ is a draw in a</span>
<span id="cb54-1676"><a href="#cb54-1676" aria-hidden="true" tabindex="-1"></a>standard normal distribution and we therefore should have an absolute</span>
<span id="cb54-1677"><a href="#cb54-1677" aria-hidden="true" tabindex="-1"></a>value lower than 1.96, 95% of the time. Obviously, $\hat{\beta}$ will</span>
<span id="cb54-1678"><a href="#cb54-1678" aria-hidden="true" tabindex="-1"></a>almost never be exactly equal to $\beta_0$, even if $\mbox{H}_0$ is true</span>
<span id="cb54-1679"><a href="#cb54-1679" aria-hidden="true" tabindex="-1"></a>because of sampling error. We have therefore the following decision</span>
<span id="cb54-1680"><a href="#cb54-1680" aria-hidden="true" tabindex="-1"></a>rule, say at the 95% confidence level:</span>
<span id="cb54-1681"><a href="#cb54-1681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1682"><a href="#cb54-1682" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>if the absolute value of the computed statistic</span>
<span id="cb54-1683"><a href="#cb54-1683" aria-hidden="true" tabindex="-1"></a>    $(\hat{\beta}_N-\beta_0)/\sigma_{\hat{\beta}_N}$ is greater</span>
<span id="cb54-1684"><a href="#cb54-1684" aria-hidden="true" tabindex="-1"></a>    than the critical value, we'll say that the difference between</span>
<span id="cb54-1685"><a href="#cb54-1685" aria-hidden="true" tabindex="-1"></a>    $\hat{\beta}$ and $\beta_0$ is too large to be caused by sampling</span>
<span id="cb54-1686"><a href="#cb54-1686" aria-hidden="true" tabindex="-1"></a>    error; we therefore reject the hypothesis,</span>
<span id="cb54-1687"><a href="#cb54-1687" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>if the absolute value of the computed statistic</span>
<span id="cb54-1688"><a href="#cb54-1688" aria-hidden="true" tabindex="-1"></a>    $(\hat{\beta}_N-\beta_0)/\sigma_{\hat{\beta}_N}$ is lower than</span>
<span id="cb54-1689"><a href="#cb54-1689" aria-hidden="true" tabindex="-1"></a>    the critical value, we'll say that the difference between</span>
<span id="cb54-1690"><a href="#cb54-1690" aria-hidden="true" tabindex="-1"></a>    $\hat{\beta}$ and $\beta_0$ is small enough to be caused by sampling</span>
<span id="cb54-1691"><a href="#cb54-1691" aria-hidden="true" tabindex="-1"></a>    error; we therefore don't reject the hypothesis.</span>
<span id="cb54-1692"><a href="#cb54-1692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1693"><a href="#cb54-1693" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{critical value|)}</span>
<span id="cb54-1694"><a href="#cb54-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1695"><a href="#cb54-1695" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1696"><a href="#cb54-1696" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-normal</span></span>
<span id="cb54-1697"><a href="#cb54-1697" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1698"><a href="#cb54-1698" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Normal distribution and 5% critical value"</span></span>
<span id="cb54-1699"><a href="#cb54-1699" aria-hidden="true" tabindex="-1"></a>dd <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="fl">0.01</span>), <span class="at">y =</span> <span class="fu">dnorm</span>(x))</span>
<span id="cb54-1700"><a href="#cb54-1700" aria-hidden="true" tabindex="-1"></a>dd <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_path</span>() <span class="sc">+</span></span>
<span id="cb54-1701"><a href="#cb54-1701" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&gt;=</span> <span class="fl">1.96</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"lightgray"</span>) <span class="sc">+</span></span>
<span id="cb54-1702"><a href="#cb54-1702" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&lt;=</span> <span class="sc">-</span><span class="fl">1.96</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"lightgray"</span>) <span class="sc">+</span></span>
<span id="cb54-1703"><a href="#cb54-1703" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_text</span>(<span class="at">data =</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.96</span>, <span class="dv">0</span>, <span class="fl">1.96</span>), <span class="at">y =</span> <span class="sc">-</span> <span class="fl">0.01</span>, <span class="at">text =</span> <span class="fu">c</span>(<span class="st">"-1.96"</span>, <span class="st">"0"</span>, <span class="st">"+1.96"</span>)), <span class="fu">aes</span>(<span class="at">label =</span> text)) <span class="sc">+</span></span>
<span id="cb54-1704"><a href="#cb54-1704" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb54-1705"><a href="#cb54-1705" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_void</span>()</span>
<span id="cb54-1706"><a href="#cb54-1706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1707"><a href="#cb54-1707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1708"><a href="#cb54-1708" aria-hidden="true" tabindex="-1"></a>Consider as an example $\hat{\beta} = 3.46$, $\beta_0 = 4$ and</span>
<span id="cb54-1709"><a href="#cb54-1709" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon = 0.3$. The computed statistic is</span>
<span id="cb54-1710"><a href="#cb54-1710" aria-hidden="true" tabindex="-1"></a>$\frac{3.46 - 4}{0.3} = - 1.8$.</span>
<span id="cb54-1711"><a href="#cb54-1711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1712"><a href="#cb54-1712" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1713"><a href="#cb54-1713" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: TRUE</span></span>
<span id="cb54-1714"><a href="#cb54-1714" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: t_stat</span></span>
<span id="cb54-1715"><a href="#cb54-1715" aria-hidden="true" tabindex="-1"></a>hbeta <span class="ot">&lt;-</span> <span class="fl">3.46</span> ; betao <span class="ot">&lt;-</span> <span class="dv">4</span> ; shbeta <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb54-1716"><a href="#cb54-1716" aria-hidden="true" tabindex="-1"></a>stat <span class="ot">&lt;-</span> (hbeta <span class="sc">-</span> betao) <span class="sc">/</span> shbeta</span>
<span id="cb54-1717"><a href="#cb54-1717" aria-hidden="true" tabindex="-1"></a>stat</span>
<span id="cb54-1718"><a href="#cb54-1718" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1719"><a href="#cb54-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1720"><a href="#cb54-1720" aria-hidden="true" tabindex="-1"></a>It is lower, in absolute value, than $1.96$; we therefore don't reject</span>
<span id="cb54-1721"><a href="#cb54-1721" aria-hidden="true" tabindex="-1"></a>the null hypothesis at the 5% error level.</span>
<span id="cb54-1722"><a href="#cb54-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1723"><a href="#cb54-1723" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{probability value|(}</span>
<span id="cb54-1724"><a href="#cb54-1724" aria-hidden="true" tabindex="-1"></a>A more general tool is the **probability value**. It is the probability</span>
<span id="cb54-1725"><a href="#cb54-1725" aria-hidden="true" tabindex="-1"></a>of drawing a value at least as large as the one we obtained (in absolute</span>
<span id="cb54-1726"><a href="#cb54-1726" aria-hidden="true" tabindex="-1"></a>value) if the hypothesis is true. It is given by:</span>
<span id="cb54-1727"><a href="#cb54-1727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1728"><a href="#cb54-1728" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1729"><a href="#cb54-1729" aria-hidden="true" tabindex="-1"></a>p = 2 \left<span class="co">[</span><span class="ot">1 -\Phi\left(\left| \frac{\hat{\beta}-\beta_0}{\sigma_{\hat{\beta}}} \right|\right)\right</span><span class="co">]</span></span>
<span id="cb54-1730"><a href="#cb54-1730" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1731"><a href="#cb54-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1732"><a href="#cb54-1732" aria-hidden="true" tabindex="-1"></a>Probability values are computed using the <span class="in">`pnorm`</span> function, which takes</span>
<span id="cb54-1733"><a href="#cb54-1733" aria-hidden="true" tabindex="-1"></a>as argument a value of the variable ($q$) and computes the probability</span>
<span id="cb54-1734"><a href="#cb54-1734" aria-hidden="true" tabindex="-1"></a>for a given value of its argument. The default behavior of <span class="in">`pnorm`</span> is</span>
<span id="cb54-1735"><a href="#cb54-1735" aria-hidden="true" tabindex="-1"></a>to return $p = \mbox{P}(z &lt; q)$, but the upper tail, given by</span>
<span id="cb54-1736"><a href="#cb54-1736" aria-hidden="true" tabindex="-1"></a>$\mbox{P}(z &gt; x)$ is returned by setting the <span class="in">`lower.tail`</span> argument to</span>
<span id="cb54-1737"><a href="#cb54-1737" aria-hidden="true" tabindex="-1"></a><span class="in">`FALSE`</span>.\idxfun{pnorm}{stats}</span>
<span id="cb54-1738"><a href="#cb54-1738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1739"><a href="#cb54-1739" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1740"><a href="#cb54-1740" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1741"><a href="#cb54-1741" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: pnorm</span></span>
<span id="cb54-1742"><a href="#cb54-1742" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(stat)</span>
<span id="cb54-1743"><a href="#cb54-1743" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fu">abs</span>(stat))</span>
<span id="cb54-1744"><a href="#cb54-1744" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="fu">abs</span>(stat))</span>
<span id="cb54-1745"><a href="#cb54-1745" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fu">abs</span>(stat), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb54-1746"><a href="#cb54-1746" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span> <span class="sc">*</span> <span class="fu">pnorm</span>(<span class="fu">abs</span>(stat), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb54-1747"><a href="#cb54-1747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1748"><a href="#cb54-1748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1749"><a href="#cb54-1749" aria-hidden="true" tabindex="-1"></a>The computed statistics can be of both signs, so the last formula is the</span>
<span id="cb54-1750"><a href="#cb54-1750" aria-hidden="true" tabindex="-1"></a>most robust: first take the absolute value of the statistic, then</span>
<span id="cb54-1751"><a href="#cb54-1751" aria-hidden="true" tabindex="-1"></a>compute the upper tail for a normal distribution and finally multiply it</span>
<span id="cb54-1752"><a href="#cb54-1752" aria-hidden="true" tabindex="-1"></a>by 2. The p-value is greater than $5$%; therefore, the hypothesis is not</span>
<span id="cb54-1753"><a href="#cb54-1753" aria-hidden="true" tabindex="-1"></a>rejected at the 5%. The interest of the p-value is that, once it is</span>
<span id="cb54-1754"><a href="#cb54-1754" aria-hidden="true" tabindex="-1"></a>computed, it is very easy to get the decision, whatever the error level</span>
<span id="cb54-1755"><a href="#cb54-1755" aria-hidden="true" tabindex="-1"></a>(and even whatever the distribution). The 5-10% critical values and the</span>
<span id="cb54-1756"><a href="#cb54-1756" aria-hidden="true" tabindex="-1"></a>p-value are represented in @fig-pvalue.</span>
<span id="cb54-1757"><a href="#cb54-1757" aria-hidden="true" tabindex="-1"></a>The absolute value of the statistic is $1.80$, the critical values at</span>
<span id="cb54-1758"><a href="#cb54-1758" aria-hidden="true" tabindex="-1"></a>the 5 and 10% are $1.96$ and $1.64$. Then:</span>
<span id="cb54-1759"><a href="#cb54-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1760"><a href="#cb54-1760" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1761"><a href="#cb54-1761" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pvalue</span></span>
<span id="cb54-1762"><a href="#cb54-1762" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-1763"><a href="#cb54-1763" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Critical value and p-value"</span></span>
<span id="cb54-1764"><a href="#cb54-1764" aria-hidden="true" tabindex="-1"></a>dx <span class="ot">&lt;-</span> <span class="fl">0.02</span></span>
<span id="cb54-1765"><a href="#cb54-1765" aria-hidden="true" tabindex="-1"></a>dd <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="fl">0.01</span>), <span class="at">y =</span> <span class="fu">dnorm</span>(x))</span>
<span id="cb54-1766"><a href="#cb54-1766" aria-hidden="true" tabindex="-1"></a>dd <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_path</span>() <span class="sc">+</span></span>
<span id="cb54-1767"><a href="#cb54-1767" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&gt;=</span> <span class="fl">1.645</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"grey50"</span>) <span class="sc">+</span></span>
<span id="cb54-1768"><a href="#cb54-1768" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&lt;=</span> <span class="sc">-</span><span class="fl">1.645</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"grey50"</span>) <span class="sc">+</span></span>
<span id="cb54-1769"><a href="#cb54-1769" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&gt;=</span> <span class="fl">1.8</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"grey70"</span>) <span class="sc">+</span></span>
<span id="cb54-1770"><a href="#cb54-1770" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&lt;=</span> <span class="sc">-</span><span class="fl">1.8</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"grey70"</span>) <span class="sc">+</span></span>
<span id="cb54-1771"><a href="#cb54-1771" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&gt;=</span> <span class="fl">1.96</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"grey90"</span>) <span class="sc">+</span></span>
<span id="cb54-1772"><a href="#cb54-1772" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="at">data =</span> <span class="fu">filter</span>(dd, x <span class="sc">&lt;=</span> <span class="sc">-</span><span class="fl">1.96</span>), <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> y), <span class="at">fill =</span> <span class="st">"grey90"</span>) <span class="sc">+</span></span>
<span id="cb54-1773"><a href="#cb54-1773" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_text</span>(<span class="at">data =</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.96</span> <span class="sc">-</span> dx, <span class="sc">-</span><span class="fl">1.8</span> <span class="sc">+</span> dx, <span class="sc">-</span><span class="fl">1.645</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> dx, <span class="fl">1.645</span> <span class="sc">-</span> <span class="dv">3</span> <span class="sc">*</span> dx, <span class="fl">1.8</span> <span class="sc">-</span> dx, <span class="fl">1.96</span> <span class="sc">+</span> dx), <span class="at">y =</span> <span class="fu">c</span>(<span class="sc">-</span> <span class="fl">0.01</span>, <span class="sc">-</span><span class="fl">0.03</span>, <span class="sc">-</span><span class="fl">0.01</span>, <span class="sc">-</span><span class="fl">0.01</span>, <span class="sc">-</span><span class="fl">0.03</span>, <span class="sc">-</span><span class="fl">0.01</span>),</span>
<span id="cb54-1774"><a href="#cb54-1774" aria-hidden="true" tabindex="-1"></a>                            <span class="at">text =</span> <span class="fu">c</span>(<span class="st">"-1.96"</span>, <span class="st">"-1.80"</span>, <span class="st">"-1.64"</span>, <span class="st">"+1.64"</span>, <span class="st">"+1.80"</span>, <span class="st">"+1.96"</span>)),</span>
<span id="cb54-1775"><a href="#cb54-1775" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">label =</span> text), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb54-1776"><a href="#cb54-1776" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb54-1777"><a href="#cb54-1777" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_void</span>()</span>
<span id="cb54-1778"><a href="#cb54-1778" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1779"><a href="#cb54-1779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1780"><a href="#cb54-1780" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the absolute value of the statistic being lower than the 5% critical</span>
<span id="cb54-1781"><a href="#cb54-1781" aria-hidden="true" tabindex="-1"></a>    value; the hypothesis is not rejected at the 5% level,</span>
<span id="cb54-1782"><a href="#cb54-1782" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the absolute value of the statistic being greater than the 10%</span>
<span id="cb54-1783"><a href="#cb54-1783" aria-hidden="true" tabindex="-1"></a>    critical value; the hypothesis is rejected at the 10% level.</span>
<span id="cb54-1784"><a href="#cb54-1784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1785"><a href="#cb54-1785" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Next page --&gt;</span></span>
<span id="cb54-1786"><a href="#cb54-1786" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb54-1787"><a href="#cb54-1787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1788"><a href="#cb54-1788" aria-hidden="true" tabindex="-1"></a>The p-value is equal to 7.2%:</span>
<span id="cb54-1789"><a href="#cb54-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1790"><a href="#cb54-1790" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the p-value is greater than 5%; the hypothesis is not rejected at</span>
<span id="cb54-1791"><a href="#cb54-1791" aria-hidden="true" tabindex="-1"></a>    the 5% level,</span>
<span id="cb54-1792"><a href="#cb54-1792" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the p-value is lower than 10%; the hypothesis is rejected at the 10%</span>
<span id="cb54-1793"><a href="#cb54-1793" aria-hidden="true" tabindex="-1"></a>    level.</span>
<span id="cb54-1794"><a href="#cb54-1794" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-1795"><a href="#cb54-1795" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{probability value|)}    </span>
<span id="cb54-1796"><a href="#cb54-1796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1797"><a href="#cb54-1797" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!simple linear model|)}</span>
<span id="cb54-1798"><a href="#cb54-1798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1799"><a href="#cb54-1799" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confidence interval {#sec-confint_simple_ols}</span></span>
<span id="cb54-1800"><a href="#cb54-1800" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!simple linear regression model|(}</span>
<span id="cb54-1801"><a href="#cb54-1801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1802"><a href="#cb54-1802" aria-hidden="true" tabindex="-1"></a>Knowing the distribution of the estimator enables one to go beyond the point</span>
<span id="cb54-1803"><a href="#cb54-1803" aria-hidden="true" tabindex="-1"></a>estimation of the unknown parameter and to introduce the uncertainty by</span>
<span id="cb54-1804"><a href="#cb54-1804" aria-hidden="true" tabindex="-1"></a>giving an interval of values which contains the real value of the</span>
<span id="cb54-1805"><a href="#cb54-1805" aria-hidden="true" tabindex="-1"></a>unknown parameter with a given confidence. This is called a **confidence</span>
<span id="cb54-1806"><a href="#cb54-1806" aria-hidden="true" tabindex="-1"></a>interval**. To obtain it, we start with @eq-critvalue:</span>
<span id="cb54-1807"><a href="#cb54-1807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1808"><a href="#cb54-1808" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1809"><a href="#cb54-1809" aria-hidden="true" tabindex="-1"></a>\mbox{P}\left(\left|\frac{\hat{\beta} -</span>
<span id="cb54-1810"><a href="#cb54-1810" aria-hidden="true" tabindex="-1"></a>\beta}{\sigma_{\hat{\beta}}}\right|&lt;z_{\alpha/2}\right) = 1 - \alpha</span>
<span id="cb54-1811"><a href="#cb54-1811" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1812"><a href="#cb54-1812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1813"><a href="#cb54-1813" aria-hidden="true" tabindex="-1"></a>Developing this expression, we get:</span>
<span id="cb54-1814"><a href="#cb54-1814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1815"><a href="#cb54-1815" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1816"><a href="#cb54-1816" aria-hidden="true" tabindex="-1"></a>\mbox{P}\left(\hat{\beta} - \sigma_{\hat{\beta}} z_{\alpha/2} &lt; \beta &lt; \hat{\beta} + \sigma_{\hat{\beta}} z_{\alpha/2}\right) = 1 - \alpha</span>
<span id="cb54-1817"><a href="#cb54-1817" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1818"><a href="#cb54-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1819"><a href="#cb54-1819" aria-hidden="true" tabindex="-1"></a>which gives, in our example:\idxfun{round}{base}</span>
<span id="cb54-1820"><a href="#cb54-1820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1821"><a href="#cb54-1821" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1822"><a href="#cb54-1822" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confidence_interval</span></span>
<span id="cb54-1823"><a href="#cb54-1823" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1824"><a href="#cb54-1824" aria-hidden="true" tabindex="-1"></a>ic <span class="ot">&lt;-</span> <span class="fu">round</span>(hbeta <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="fl">1.96</span> <span class="sc">*</span> shbeta, <span class="dv">3</span>)</span>
<span id="cb54-1825"><a href="#cb54-1825" aria-hidden="true" tabindex="-1"></a>ic</span>
<span id="cb54-1826"><a href="#cb54-1826" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1827"><a href="#cb54-1827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1828"><a href="#cb54-1828" aria-hidden="true" tabindex="-1"></a>This confidence interval indicates that there is a probability of 95% that the true value of</span>
<span id="cb54-1829"><a href="#cb54-1829" aria-hidden="true" tabindex="-1"></a>$\beta$ is between <span class="in">`r ic[1]`</span> and <span class="in">`r ic[2]`</span>.</span>
<span id="cb54-1830"><a href="#cb54-1830" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!simple linear regression model|)}</span>
<span id="cb54-1831"><a href="#cb54-1831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1832"><a href="#cb54-1832" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exact distribution, the Student distribution</span></span>
<span id="cb54-1833"><a href="#cb54-1833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1834"><a href="#cb54-1834" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Student t distribution|(}</span>
<span id="cb54-1835"><a href="#cb54-1835" aria-hidden="true" tabindex="-1"></a>In real settings,</span>
<span id="cb54-1836"><a href="#cb54-1836" aria-hidden="true" tabindex="-1"></a>$\sigma_{\hat{\beta}}=\frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_x}$ is</span>
<span id="cb54-1837"><a href="#cb54-1837" aria-hidden="true" tabindex="-1"></a>unknown because $\sigma_\epsilon$ is an unknown parameter. Replacing</span>
<span id="cb54-1838"><a href="#cb54-1838" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon$ by the unbiased estimator of $\dot{\sigma}_\epsilon$</span>
<span id="cb54-1839"><a href="#cb54-1839" aria-hidden="true" tabindex="-1"></a>we get $\dot{\sigma}_{\hat{\beta}}$, the standard error of the estimation of the slope:</span>
<span id="cb54-1840"><a href="#cb54-1840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1841"><a href="#cb54-1841" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1842"><a href="#cb54-1842" aria-hidden="true" tabindex="-1"></a>\dot{\sigma}_{\hat{\beta}} = \frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_x}</span>
<span id="cb54-1843"><a href="#cb54-1843" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1844"><a href="#cb54-1844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1845"><a href="#cb54-1845" aria-hidden="true" tabindex="-1"></a>As $\sigma_\epsilon$ is estimated, some more noise is added, so that the</span>
<span id="cb54-1846"><a href="#cb54-1846" aria-hidden="true" tabindex="-1"></a>distribution of $\hat{\beta}$ is no longer a normal, but a Student t</span>
<span id="cb54-1847"><a href="#cb54-1847" aria-hidden="true" tabindex="-1"></a>with $N-2$ degrees of freedom (see @sec-exact_ols_distribution):</span>
<span id="cb54-1848"><a href="#cb54-1848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1849"><a href="#cb54-1849" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1850"><a href="#cb54-1850" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\beta}_N-\beta}{\dot{\sigma}_{\hat{\beta}_N}}=</span>
<span id="cb54-1851"><a href="#cb54-1851" aria-hidden="true" tabindex="-1"></a>\frac{\sqrt{N}\hat{\sigma}_x}{\dot{\sigma}_\epsilon}(\hat{\beta}_N-\beta)</span>
<span id="cb54-1852"><a href="#cb54-1852" aria-hidden="true" tabindex="-1"></a>\sim t_{N-2}</span>
<span id="cb54-1853"><a href="#cb54-1853" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1854"><a href="#cb54-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1855"><a href="#cb54-1855" aria-hidden="true" tabindex="-1"></a>The Student distribution has a 0 expected value and a variance equal to</span>
<span id="cb54-1856"><a href="#cb54-1856" aria-hidden="true" tabindex="-1"></a>$\frac{N - 2}{N -4}$, which tends to 1 for large $N$. Moreover, the</span>
<span id="cb54-1857"><a href="#cb54-1857" aria-hidden="true" tabindex="-1"></a>Student distribution converges in distribution to a normal distribution.</span>
<span id="cb54-1858"><a href="#cb54-1858" aria-hidden="true" tabindex="-1"></a>Therefore, for large $N$, the same inference as the one presented for</span>
<span id="cb54-1859"><a href="#cb54-1859" aria-hidden="true" tabindex="-1"></a>known $\sigma_\epsilon$ can be applied, using the normal distribution as</span>
<span id="cb54-1860"><a href="#cb54-1860" aria-hidden="true" tabindex="-1"></a>a good approximation. For small samples, however, critical values of the</span>
<span id="cb54-1861"><a href="#cb54-1861" aria-hidden="true" tabindex="-1"></a>Student distribution should be used. The relevant 95% critical values</span>
<span id="cb54-1862"><a href="#cb54-1862" aria-hidden="true" tabindex="-1"></a>are computed below for numbers of degrees of freedom equal to 5, 10, 50,</span>
<span id="cb54-1863"><a href="#cb54-1863" aria-hidden="true" tabindex="-1"></a>100 and 1000:</span>
<span id="cb54-1864"><a href="#cb54-1864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1865"><a href="#cb54-1865" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1866"><a href="#cb54-1866" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: qt</span></span>
<span id="cb54-1867"><a href="#cb54-1867" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.025</span>, <span class="at">df =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">1000</span>), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">3</span>)</span>
<span id="cb54-1868"><a href="#cb54-1868" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1869"><a href="#cb54-1869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1870"><a href="#cb54-1870" aria-hidden="true" tabindex="-1"></a>Therefore, the normal distribution can be safely used if the</span>
<span id="cb54-1871"><a href="#cb54-1871" aria-hidden="true" tabindex="-1"></a>sample has at least a few hundreds of observations.</span>
<span id="cb54-1872"><a href="#cb54-1872" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Student t distribution|)}</span>
<span id="cb54-1873"><a href="#cb54-1873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1874"><a href="#cb54-1874" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference with R</span></span>
<span id="cb54-1875"><a href="#cb54-1875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1876"><a href="#cb54-1876" aria-hidden="true" tabindex="-1"></a>**R** has different functions that extract information about the</span>
<span id="cb54-1877"><a href="#cb54-1877" aria-hidden="true" tabindex="-1"></a>statistical properties of the fitted model. To illustrate their use, we</span>
<span id="cb54-1878"><a href="#cb54-1878" aria-hidden="true" tabindex="-1"></a>use once again the price-time model:\idxfun{lm}{stats}</span>
<span id="cb54-1879"><a href="#cb54-1879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1880"><a href="#cb54-1880" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1881"><a href="#cb54-1881" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_price_time_reestimation</span></span>
<span id="cb54-1882"><a href="#cb54-1882" aria-hidden="true" tabindex="-1"></a>pxt <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h, prtime)</span>
<span id="cb54-1883"><a href="#cb54-1883" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1884"><a href="#cb54-1884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1885"><a href="#cb54-1885" aria-hidden="true" tabindex="-1"></a>Detailed results of the model are computed using the <span class="in">`summary`</span> method</span>
<span id="cb54-1886"><a href="#cb54-1886" aria-hidden="true" tabindex="-1"></a>for <span class="in">`lm`</span> objects:</span>
<span id="cb54-1887"><a href="#cb54-1887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1888"><a href="#cb54-1888" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1889"><a href="#cb54-1889" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: summary_lm</span></span>
<span id="cb54-1890"><a href="#cb54-1890" aria-hidden="true" tabindex="-1"></a>spxt <span class="ot">&lt;-</span> <span class="fu">summary</span>(pxt)</span>
<span id="cb54-1891"><a href="#cb54-1891" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1892"><a href="#cb54-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1893"><a href="#cb54-1893" aria-hidden="true" tabindex="-1"></a>which returns an object of class <span class="in">`summary.lm`</span>. Moreover, <span class="in">`summary.lm`</span></span>
<span id="cb54-1894"><a href="#cb54-1894" aria-hidden="true" tabindex="-1"></a>prints nicely. It is therefore customary to use <span class="in">`summary`</span> without</span>
<span id="cb54-1895"><a href="#cb54-1895" aria-hidden="true" tabindex="-1"></a>storing the result in an object but only to visualize the detailed</span>
<span id="cb54-1896"><a href="#cb54-1896" aria-hidden="true" tabindex="-1"></a>results of the fitted model:</span>
<span id="cb54-1897"><a href="#cb54-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1900"><a href="#cb54-1900" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb54-1901"><a href="#cb54-1901" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: print_summary_lm</span></span>
<span id="cb54-1902"><a href="#cb54-1902" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pxt)</span>
<span id="cb54-1903"><a href="#cb54-1903" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1904"><a href="#cb54-1904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1905"><a href="#cb54-1905" aria-hidden="true" tabindex="-1"></a>The output first indicates the "call", i.e., the function that has been</span>
<span id="cb54-1906"><a href="#cb54-1906" aria-hidden="true" tabindex="-1"></a>used to estimate the model. Then, the distribution of the residuals is</span>
<span id="cb54-1907"><a href="#cb54-1907" aria-hidden="true" tabindex="-1"></a>summarized using the **five numbers** (the range <span class="in">`min`</span> and <span class="in">`max`</span>, the two</span>
<span id="cb54-1908"><a href="#cb54-1908" aria-hidden="true" tabindex="-1"></a>quartiles and the median).<span class="ot">[^simple_regression_properties-4]</span> Note that</span>
<span id="cb54-1909"><a href="#cb54-1909" aria-hidden="true" tabindex="-1"></a>the mean is not indicated, as it is necessarily 0 for a model fitted by</span>
<span id="cb54-1910"><a href="#cb54-1910" aria-hidden="true" tabindex="-1"></a>OLS. Next, the table of coefficients is printed, containing:</span>
<span id="cb54-1911"><a href="#cb54-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1912"><a href="#cb54-1912" aria-hidden="true" tabindex="-1"></a><span class="ot">[^simple_regression_properties-4]: </span>Any series can be summarized this way</span>
<span id="cb54-1913"><a href="#cb54-1913" aria-hidden="true" tabindex="-1"></a>    using the <span class="in">`fivenum`</span> function.</span>
<span id="cb54-1914"><a href="#cb54-1914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1915"><a href="#cb54-1915" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the names of the effects,</span>
<span id="cb54-1916"><a href="#cb54-1916" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the value of the estimates ($\hat{\beta}$),</span>
<span id="cb54-1917"><a href="#cb54-1917" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>their standard errors ($\dot{\sigma}_{\hat{\beta}}$),</span>
<span id="cb54-1918"><a href="#cb54-1918" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the Student statistic which is the ratio of the previous two columns</span>
<span id="cb54-1919"><a href="#cb54-1919" aria-hidden="true" tabindex="-1"></a>    and is a special case of the test statistic</span>
<span id="cb54-1920"><a href="#cb54-1920" aria-hidden="true" tabindex="-1"></a>    $(\hat{\beta}-\beta_0) /\dot{\sigma}_{\hat{\beta}}$ where</span>
<span id="cb54-1921"><a href="#cb54-1921" aria-hidden="true" tabindex="-1"></a>    $\beta_0=0$,</span>
<span id="cb54-1922"><a href="#cb54-1922" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the probability value of this statistic.</span>
<span id="cb54-1923"><a href="#cb54-1923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1924"><a href="#cb54-1924" aria-hidden="true" tabindex="-1"></a>Thes kinds of tests are often considered as tests of significance of the</span>
<span id="cb54-1925"><a href="#cb54-1925" aria-hidden="true" tabindex="-1"></a>corresponding covariate. If the hypothesis that $\beta_0=0$ is rejected,</span>
<span id="cb54-1926"><a href="#cb54-1926" aria-hidden="true" tabindex="-1"></a>we would say that the coefficient is "significant", which means more</span>
<span id="cb54-1927"><a href="#cb54-1927" aria-hidden="true" tabindex="-1"></a>precisely that it is significantly different from 0. As we have a very</span>
<span id="cb54-1928"><a href="#cb54-1928" aria-hidden="true" tabindex="-1"></a>small sample, it is worth considering the critical value of a Student</span>
<span id="cb54-1929"><a href="#cb54-1929" aria-hidden="true" tabindex="-1"></a>instead of a normal distribution. We get here:</span>
<span id="cb54-1930"><a href="#cb54-1930" aria-hidden="true" tabindex="-1"></a>\idxfun{qt}{stats}\idxfun{df.residual}{stats}</span>
<span id="cb54-1931"><a href="#cb54-1931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1932"><a href="#cb54-1932" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1933"><a href="#cb54-1933" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: critical_value_t</span></span>
<span id="cb54-1934"><a href="#cb54-1934" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-1935"><a href="#cb54-1935" aria-hidden="true" tabindex="-1"></a>cv <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="fl">0.025</span>, <span class="at">df =</span> <span class="fu">df.residual</span>(pxt), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb54-1936"><a href="#cb54-1936" aria-hidden="true" tabindex="-1"></a>cv</span>
<span id="cb54-1937"><a href="#cb54-1937" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1938"><a href="#cb54-1938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1939"><a href="#cb54-1939" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>for the intercept, the t statistic is much lower than the critical</span>
<span id="cb54-1940"><a href="#cb54-1940" aria-hidden="true" tabindex="-1"></a>    value and the probability value is far greater than 5%; therefore,</span>
<span id="cb54-1941"><a href="#cb54-1941" aria-hidden="true" tabindex="-1"></a>    the hypothesis that $\alpha = 0$ is not rejected,</span>
<span id="cb54-1942"><a href="#cb54-1942" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>for the slope, the t statistic is much higher than the critical</span>
<span id="cb54-1943"><a href="#cb54-1943" aria-hidden="true" tabindex="-1"></a>    value and the probability value is far lower than 5%; therefore, the</span>
<span id="cb54-1944"><a href="#cb54-1944" aria-hidden="true" tabindex="-1"></a>    hypothesis that $\beta = 0$ is rejected.</span>
<span id="cb54-1945"><a href="#cb54-1945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1946"><a href="#cb54-1946" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb54-1947"><a href="#cb54-1947" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- new page --&gt;</span></span>
<span id="cb54-1948"><a href="#cb54-1948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1949"><a href="#cb54-1949" aria-hidden="true" tabindex="-1"></a>This table of coefficients is a matrix that is stored in the</span>
<span id="cb54-1950"><a href="#cb54-1950" aria-hidden="true" tabindex="-1"></a><span class="in">`summary.lm`</span> object with the <span class="in">`coefficients`</span> name. As such, it can be</span>
<span id="cb54-1951"><a href="#cb54-1951" aria-hidden="true" tabindex="-1"></a>extracted using <span class="in">`spxt$coefficients`</span> or using the <span class="in">`coef`</span> method of</span>
<span id="cb54-1952"><a href="#cb54-1952" aria-hidden="true" tabindex="-1"></a><span class="in">`summary.lm`</span>: <span class="in">`coef(spxt)`</span>.\idxfun{coef}{stats}</span>
<span id="cb54-1953"><a href="#cb54-1953" aria-hidden="true" tabindex="-1"></a>Finally, the printed output ends with some general indicators (often</span>
<span id="cb54-1954"><a href="#cb54-1954" aria-hidden="true" tabindex="-1"></a>**GOF** for goodness-of-fit indicators) as the residual standard error</span>
<span id="cb54-1955"><a href="#cb54-1955" aria-hidden="true" tabindex="-1"></a>($\dot{\sigma}_\epsilon$, which can be extracted using the <span class="in">`sigma`</span></span>
<span id="cb54-1956"><a href="#cb54-1956" aria-hidden="true" tabindex="-1"></a>function), two measures of the coefficient of determination, and the $F$</span>
<span id="cb54-1957"><a href="#cb54-1957" aria-hidden="true" tabindex="-1"></a>statistic that is relevant for the multiple regression model.</span>
<span id="cb54-1958"><a href="#cb54-1958" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{goodness-of-fit indicators!R}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{residual standard error!R}</span>
<span id="cb54-1959"><a href="#cb54-1959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1960"><a href="#cb54-1960" aria-hidden="true" tabindex="-1"></a>The <span class="in">`confint`</span> function computes the confidence interval for the</span>
<span id="cb54-1961"><a href="#cb54-1961" aria-hidden="true" tabindex="-1"></a>coefficients:\idxfun{confint}{stats}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!R}</span>
<span id="cb54-1962"><a href="#cb54-1962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1963"><a href="#cb54-1963" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-1964"><a href="#cb54-1964" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confint</span></span>
<span id="cb54-1965"><a href="#cb54-1965" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(pxt, <span class="at">level =</span> <span class="fl">0.9</span>)</span>
<span id="cb54-1966"><a href="#cb54-1966" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-1967"><a href="#cb54-1967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1968"><a href="#cb54-1968" aria-hidden="true" tabindex="-1"></a>we set here the <span class="in">`level`</span> argument to <span class="in">`0.9`</span> (the default value being</span>
<span id="cb54-1969"><a href="#cb54-1969" aria-hidden="true" tabindex="-1"></a><span class="in">`0.95`</span>) and the results indicate that there is a 90% probability that</span>
<span id="cb54-1970"><a href="#cb54-1970" aria-hidden="true" tabindex="-1"></a>the true value of the slope is between</span>
<span id="cb54-1971"><a href="#cb54-1971" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(confint(pxt, level = .9)[2, 1], 3)`</span> and</span>
<span id="cb54-1972"><a href="#cb54-1972" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(confint(pxt, level = .9)[2, 2], 3)`</span>.\idxfun{confint}{stats}\idxfun{round}{base}</span>
<span id="cb54-1973"><a href="#cb54-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1974"><a href="#cb54-1974" aria-hidden="true" tabindex="-1"></a><span class="fu">### Delta method</span></span>
<span id="cb54-1975"><a href="#cb54-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1976"><a href="#cb54-1976" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{delta method|(}</span>
<span id="cb54-1977"><a href="#cb54-1977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1978"><a href="#cb54-1978" aria-hidden="true" tabindex="-1"></a>It's often the case that the parameters of interest are not the fitted</span>
<span id="cb54-1979"><a href="#cb54-1979" aria-hidden="true" tabindex="-1"></a>parameters, but some functions of them. In the price-time model, the</span>
<span id="cb54-1980"><a href="#cb54-1980" aria-hidden="true" tabindex="-1"></a>fitted parameters are $\alpha$ and $\beta$, but the structural</span>
<span id="cb54-1981"><a href="#cb54-1981" aria-hidden="true" tabindex="-1"></a>parameters (the lower and higher values of the travel time) are $a$</span>
<span id="cb54-1982"><a href="#cb54-1982" aria-hidden="true" tabindex="-1"></a>and $b$:</span>
<span id="cb54-1983"><a href="#cb54-1983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1984"><a href="#cb54-1984" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1985"><a href="#cb54-1985" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-1986"><a href="#cb54-1986" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-1987"><a href="#cb54-1987" aria-hidden="true" tabindex="-1"></a>a &amp;=&amp; F^a(\alpha, \beta) = -\frac{\alpha}{\beta} <span class="sc">\\</span></span>
<span id="cb54-1988"><a href="#cb54-1988" aria-hidden="true" tabindex="-1"></a>b &amp;=&amp; F^b(\alpha, \beta) = \frac{1 - \alpha}{\beta}</span>
<span id="cb54-1989"><a href="#cb54-1989" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-1990"><a href="#cb54-1990" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-1991"><a href="#cb54-1991" aria-hidden="true" tabindex="-1"></a>$$ {#eq-structpar}</span>
<span id="cb54-1992"><a href="#cb54-1992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1993"><a href="#cb54-1993" aria-hidden="true" tabindex="-1"></a>The structural parameters are easily retrieved using @eq-structpar. The</span>
<span id="cb54-1994"><a href="#cb54-1994" aria-hidden="true" tabindex="-1"></a>so-called delta method can be used to compute their standard deviations.</span>
<span id="cb54-1995"><a href="#cb54-1995" aria-hidden="true" tabindex="-1"></a>Denoting $f$ the first derivatives of $F$, we write a first-order Taylor</span>
<span id="cb54-1996"><a href="#cb54-1996" aria-hidden="true" tabindex="-1"></a>expansion for $F^a$ and $F^b$:</span>
<span id="cb54-1997"><a href="#cb54-1997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-1998"><a href="#cb54-1998" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-1999"><a href="#cb54-1999" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-2000"><a href="#cb54-2000" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-2001"><a href="#cb54-2001" aria-hidden="true" tabindex="-1"></a>a &amp;=&amp; F^a(\alpha_0, \beta_0) +</span>
<span id="cb54-2002"><a href="#cb54-2002" aria-hidden="true" tabindex="-1"></a>(\alpha - \alpha_0) f^a_\alpha(\alpha_0, \beta_0) +</span>
<span id="cb54-2003"><a href="#cb54-2003" aria-hidden="true" tabindex="-1"></a>(\beta - \beta_0) f^a_\beta(\alpha_0, \beta_0)<span class="sc">\\</span></span>
<span id="cb54-2004"><a href="#cb54-2004" aria-hidden="true" tabindex="-1"></a>b &amp;=&amp; F^b(\alpha_0, \beta_0) +</span>
<span id="cb54-2005"><a href="#cb54-2005" aria-hidden="true" tabindex="-1"></a>(\alpha - \alpha_0) f^b_\alpha(\alpha_0, \beta_0) +</span>
<span id="cb54-2006"><a href="#cb54-2006" aria-hidden="true" tabindex="-1"></a>(\beta - \beta_0) f^b_\beta(\alpha_0, \beta_0)<span class="sc">\\</span></span>
<span id="cb54-2007"><a href="#cb54-2007" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-2008"><a href="#cb54-2008" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-2009"><a href="#cb54-2009" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2010"><a href="#cb54-2010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2011"><a href="#cb54-2011" aria-hidden="true" tabindex="-1"></a>So that the variances of the fitted structural parameters are approximately:</span>
<span id="cb54-2012"><a href="#cb54-2012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2013"><a href="#cb54-2013" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2014"><a href="#cb54-2014" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-2015"><a href="#cb54-2015" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb54-2016"><a href="#cb54-2016" aria-hidden="true" tabindex="-1"></a>  \hat{\sigma}_{\hat{a}} ^ 2 &amp;=&amp;</span>
<span id="cb54-2017"><a href="#cb54-2017" aria-hidden="true" tabindex="-1"></a>  f^a_\alpha(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\alpha}} ^ 2 +</span>
<span id="cb54-2018"><a href="#cb54-2018" aria-hidden="true" tabindex="-1"></a>  f^a_\beta(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\beta}} ^ 2 +</span>
<span id="cb54-2019"><a href="#cb54-2019" aria-hidden="true" tabindex="-1"></a>  2 f^a_\alpha(\alpha_0, \beta_0)f^a_\beta(\alpha_0, \beta_0)</span>
<span id="cb54-2020"><a href="#cb54-2020" aria-hidden="true" tabindex="-1"></a>  \hat{\sigma}_{\hat{\alpha}\hat{\beta}} <span class="sc">\\</span></span>
<span id="cb54-2021"><a href="#cb54-2021" aria-hidden="true" tabindex="-1"></a>  \hat{\sigma}_{\hat{b}} ^ 2 &amp;= &amp;</span>
<span id="cb54-2022"><a href="#cb54-2022" aria-hidden="true" tabindex="-1"></a>  f^b_\alpha(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\alpha}} ^ 2 +</span>
<span id="cb54-2023"><a href="#cb54-2023" aria-hidden="true" tabindex="-1"></a>  f^b_\beta(\alpha_0, \beta_0) ^ 2  \hat{\sigma}_{\hat{\beta}} ^ 2 +</span>
<span id="cb54-2024"><a href="#cb54-2024" aria-hidden="true" tabindex="-1"></a>  2 f^b_\alpha(\alpha_0, \beta_0)f^b_\beta(\alpha_0, \beta_0)</span>
<span id="cb54-2025"><a href="#cb54-2025" aria-hidden="true" tabindex="-1"></a>  \hat{\sigma}_{\hat{\alpha}\hat{\beta}} <span class="sc">\\</span></span>
<span id="cb54-2026"><a href="#cb54-2026" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb54-2027"><a href="#cb54-2027" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-2028"><a href="#cb54-2028" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2029"><a href="#cb54-2029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2030"><a href="#cb54-2030" aria-hidden="true" tabindex="-1"></a>Replacing ($\alpha_0, \beta_0$) by ($\hat{\alpha}, \hat{\beta}$) and</span>
<span id="cb54-2031"><a href="#cb54-2031" aria-hidden="true" tabindex="-1"></a>using the formulas for the variances and covariance of $\hat{\alpha}$</span>
<span id="cb54-2032"><a href="#cb54-2032" aria-hidden="true" tabindex="-1"></a>and $\hat{\beta}$ given in @eq-covariance_gamma, we get:</span>
<span id="cb54-2033"><a href="#cb54-2033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2034"><a href="#cb54-2034" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2035"><a href="#cb54-2035" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb54-2036"><a href="#cb54-2036" aria-hidden="true" tabindex="-1"></a>  \begin{array}{rcl}</span>
<span id="cb54-2037"><a href="#cb54-2037" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{a}} &amp;=&amp;</span>
<span id="cb54-2038"><a href="#cb54-2038" aria-hidden="true" tabindex="-1"></a>\frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_x}\frac{1}{\hat{\beta}}</span>
<span id="cb54-2039"><a href="#cb54-2039" aria-hidden="true" tabindex="-1"></a>  \sqrt{\hat{\sigma}_x ^ 2 + \left(\bar{x} +</span>
<span id="cb54-2040"><a href="#cb54-2040" aria-hidden="true" tabindex="-1"></a>                           \frac{\hat{\alpha}}{\hat{\beta}}\right) ^ 2} <span class="sc">\\</span></span>
<span id="cb54-2041"><a href="#cb54-2041" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{b}} &amp;=&amp;</span>
<span id="cb54-2042"><a href="#cb54-2042" aria-hidden="true" tabindex="-1"></a>\frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_x}\frac{1}{\hat{\beta}}</span>
<span id="cb54-2043"><a href="#cb54-2043" aria-hidden="true" tabindex="-1"></a>  \sqrt{\hat{\sigma}_x ^ 2 + \left(\bar{x} -</span>
<span id="cb54-2044"><a href="#cb54-2044" aria-hidden="true" tabindex="-1"></a>                           \frac{1 - \hat{\alpha}}{\hat{\beta}}\right) ^ 2}</span>
<span id="cb54-2045"><a href="#cb54-2045" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb54-2046"><a href="#cb54-2046" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb54-2047"><a href="#cb54-2047" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2048"><a href="#cb54-2048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2049"><a href="#cb54-2049" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2050"><a href="#cb54-2050" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: delta_method</span></span>
<span id="cb54-2051"><a href="#cb54-2051" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-2052"><a href="#cb54-2052" aria-hidden="true" tabindex="-1"></a>bx <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb54-2053"><a href="#cb54-2053" aria-hidden="true" tabindex="-1"></a>sx <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>( (x <span class="sc">-</span> bx) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb54-2054"><a href="#cb54-2054" aria-hidden="true" tabindex="-1"></a>halpha <span class="ot">&lt;-</span> <span class="fu">coef</span>(pxt)[<span class="dv">1</span>] <span class="sc">%&gt;%</span> unname</span>
<span id="cb54-2055"><a href="#cb54-2055" aria-hidden="true" tabindex="-1"></a>hbeta <span class="ot">&lt;-</span> <span class="fu">coef</span>(pxt)[<span class="dv">2</span>] <span class="sc">%&gt;%</span> unname</span>
<span id="cb54-2056"><a href="#cb54-2056" aria-hidden="true" tabindex="-1"></a>hseps <span class="ot">&lt;-</span> <span class="fu">sigma</span>(pxt)</span>
<span id="cb54-2057"><a href="#cb54-2057" aria-hidden="true" tabindex="-1"></a>ab <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span> halpha <span class="sc">/</span> hbeta, (<span class="dv">1</span> <span class="sc">-</span> halpha) <span class="sc">/</span> hbeta)</span>
<span id="cb54-2058"><a href="#cb54-2058" aria-hidden="true" tabindex="-1"></a>sab <span class="ot">&lt;-</span> hseps <span class="sc">/</span> sx <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">nobs</span>(pxt)) <span class="sc">/</span> hbeta <span class="sc">*</span></span>
<span id="cb54-2059"><a href="#cb54-2059" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sqrt</span>(<span class="fu">c</span>(sx <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> (bx <span class="sc">+</span> halpha <span class="sc">/</span> hbeta) <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb54-2060"><a href="#cb54-2060" aria-hidden="true" tabindex="-1"></a>           sx <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> (bx <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> halpha) <span class="sc">/</span> hbeta) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb54-2061"><a href="#cb54-2061" aria-hidden="true" tabindex="-1"></a>         )</span>
<span id="cb54-2062"><a href="#cb54-2062" aria-hidden="true" tabindex="-1"></a>ab</span>
<span id="cb54-2063"><a href="#cb54-2063" aria-hidden="true" tabindex="-1"></a>sab</span>
<span id="cb54-2064"><a href="#cb54-2064" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2065"><a href="#cb54-2065" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{sigma}{stats}\idxfun{unname}{base}\idxfun{nobs}{stats}</span>
<span id="cb54-2066"><a href="#cb54-2066" aria-hidden="true" tabindex="-1"></a>which finally leads to the 95% confidence interval:\idxfun{matrix}{base}</span>
<span id="cb54-2067"><a href="#cb54-2067" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval}</span>
<span id="cb54-2068"><a href="#cb54-2068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2069"><a href="#cb54-2069" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2070"><a href="#cb54-2070" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-2071"><a href="#cb54-2071" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confint_ab_hide</span></span>
<span id="cb54-2072"><a href="#cb54-2072" aria-hidden="true" tabindex="-1"></a>maxvt <span class="ot">&lt;-</span> (<span class="fu">matrix</span>(ab, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span> sab, sab), <span class="dv">2</span>) <span class="sc">*</span> cv)[<span class="dv">2</span>, ]</span>
<span id="cb54-2073"><a href="#cb54-2073" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2074"><a href="#cb54-2074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2075"><a href="#cb54-2075" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2076"><a href="#cb54-2076" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confint_ab</span></span>
<span id="cb54-2077"><a href="#cb54-2077" aria-hidden="true" tabindex="-1"></a><span class="fu">matrix</span>(ab, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span> sab, sab), <span class="dv">2</span>) <span class="sc">*</span> cv</span>
<span id="cb54-2078"><a href="#cb54-2078" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2079"><a href="#cb54-2079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2080"><a href="#cb54-2080" aria-hidden="true" tabindex="-1"></a>There is therefore a 95% probability that the maximum value of time is</span>
<span id="cb54-2081"><a href="#cb54-2081" aria-hidden="true" tabindex="-1"></a>between <span class="in">`r round(maxvt[1], 1)`</span> and <span class="in">`r round(maxvt[2], 1)`</span> euros and the hypotheses that the minimum value of time is 0 is not rejected.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{delta method|)}</span>
<span id="cb54-2082"><a href="#cb54-2082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2083"><a href="#cb54-2083" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confidence interval for the prediction</span></span>
<span id="cb54-2084"><a href="#cb54-2084" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!prediction|(}</span>
<span id="cb54-2085"><a href="#cb54-2085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2086"><a href="#cb54-2086" aria-hidden="true" tabindex="-1"></a>Once the model is estimated, a prediction for every observation can be</span>
<span id="cb54-2087"><a href="#cb54-2087" aria-hidden="true" tabindex="-1"></a>computed using the formula of the conditional expectation of $y$ for</span>
<span id="cb54-2088"><a href="#cb54-2088" aria-hidden="true" tabindex="-1"></a>$x = x_n$, which is:</span>
<span id="cb54-2089"><a href="#cb54-2089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2090"><a href="#cb54-2090" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2091"><a href="#cb54-2091" aria-hidden="true" tabindex="-1"></a>\mbox{E}(y \mid x = x_n) = \alpha + \beta x_n + \mbox{E}(\epsilon \mid</span>
<span id="cb54-2092"><a href="#cb54-2092" aria-hidden="true" tabindex="-1"></a>x = x_n) = \alpha + \beta x_n</span>
<span id="cb54-2093"><a href="#cb54-2093" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2094"><a href="#cb54-2094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2095"><a href="#cb54-2095" aria-hidden="true" tabindex="-1"></a>As $\hat{\alpha}$ and $\hat{\beta}$ are unbiased estimators of $\alpha$</span>
<span id="cb54-2096"><a href="#cb54-2096" aria-hidden="true" tabindex="-1"></a>and $\beta$, $\hat{y}_n = \hat{\alpha} + \hat{\beta} x_n$ is an unbiased</span>
<span id="cb54-2097"><a href="#cb54-2097" aria-hidden="true" tabindex="-1"></a>estimator of $\mbox{E}(y \mid x = x_n)$. Applying the formula for the</span>
<span id="cb54-2098"><a href="#cb54-2098" aria-hidden="true" tabindex="-1"></a>variance of a sum, we have:</span>
<span id="cb54-2099"><a href="#cb54-2099" aria-hidden="true" tabindex="-1"></a>$\mbox{V}(\hat{y}) = \mbox{V}(\hat{\alpha}) + x_n ^ 2 \mbox{V}(\hat{\beta}) + 2 x_n \mbox{cov}(\hat{\alpha}, \hat{\beta})$.</span>
<span id="cb54-2100"><a href="#cb54-2100" aria-hidden="true" tabindex="-1"></a>Using @eq-covariance_gamma, we get:</span>
<span id="cb54-2101"><a href="#cb54-2101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2102"><a href="#cb54-2102" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2103"><a href="#cb54-2103" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{y}_n} ^ 2 =\frac{\sigma_\epsilon ^ 2}{N \sigma_x ^ 2}</span>
<span id="cb54-2104"><a href="#cb54-2104" aria-hidden="true" tabindex="-1"></a>\left( \sigma_x ^ 2 + (x_n - \bar{x}) ^ 2\right)</span>
<span id="cb54-2105"><a href="#cb54-2105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2106"><a href="#cb54-2106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2107"><a href="#cb54-2107" aria-hidden="true" tabindex="-1"></a>which finally leads to the formula of the standard deviation of the</span>
<span id="cb54-2108"><a href="#cb54-2108" aria-hidden="true" tabindex="-1"></a>predictions:</span>
<span id="cb54-2109"><a href="#cb54-2109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2110"><a href="#cb54-2110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2111"><a href="#cb54-2111" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{y}_n} = \frac{\sigma_\epsilon}{\sqrt{N}}</span>
<span id="cb54-2112"><a href="#cb54-2112" aria-hidden="true" tabindex="-1"></a>\sqrt{1 + \frac{(x_n - \bar{x}) ^ 2}{\hat{\sigma}_x ^ 2}}</span>
<span id="cb54-2113"><a href="#cb54-2113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2114"><a href="#cb54-2114" aria-hidden="true" tabindex="-1"></a>$\sigma_{\hat{y}_n}$ increases with the deviation of $x_n$ from the sample mean.</span>
<span id="cb54-2115"><a href="#cb54-2115" aria-hidden="true" tabindex="-1"></a>Moreover, $\sigma_{\hat{y}}$ tends to 0 when $N$ tends to infinity,</span>
<span id="cb54-2116"><a href="#cb54-2116" aria-hidden="true" tabindex="-1"></a>which means that $\hat{y}_n$ is a consistent estimator of $\mbox{E}(y \mid x = x_n)$.</span>
<span id="cb54-2117"><a href="#cb54-2117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2118"><a href="#cb54-2118" aria-hidden="true" tabindex="-1"></a>Consider now the standard deviation of</span>
<span id="cb54-2119"><a href="#cb54-2119" aria-hidden="true" tabindex="-1"></a>$y_n = \mbox{E}(y \mid x = x_n) + \epsilon_n$. To the variation due to</span>
<span id="cb54-2120"><a href="#cb54-2120" aria-hidden="true" tabindex="-1"></a>the estimation of $\alpha$ and $\beta$, we have to add the one</span>
<span id="cb54-2121"><a href="#cb54-2121" aria-hidden="true" tabindex="-1"></a>associated with $\epsilon_n$. Therefore, the variance of $y_n$ is the</span>
<span id="cb54-2122"><a href="#cb54-2122" aria-hidden="true" tabindex="-1"></a>sum of $\sigma_{\hat{y}_n} ^ 2$ and $\sigma_\epsilon^2$ and therefore its</span>
<span id="cb54-2123"><a href="#cb54-2123" aria-hidden="true" tabindex="-1"></a>standard deviation is:</span>
<span id="cb54-2124"><a href="#cb54-2124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2125"><a href="#cb54-2125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb54-2126"><a href="#cb54-2126" aria-hidden="true" tabindex="-1"></a>\sigma_{y_n} = \sqrt{\sigma_{\hat{y}_n} ^ 2 + \sigma_{\epsilon} ^ 2}=</span>
<span id="cb54-2127"><a href="#cb54-2127" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_{\epsilon}}{\sqrt{N}}</span>
<span id="cb54-2128"><a href="#cb54-2128" aria-hidden="true" tabindex="-1"></a>\sqrt{1 + \frac{(x_n - \bar{x}) ^ 2}{\hat{\sigma}_x ^ 2} + N}</span>
<span id="cb54-2129"><a href="#cb54-2129" aria-hidden="true" tabindex="-1"></a>$$ Note that when $N \rightarrow \infty$, $\sigma_{y_n}$, contrary</span>
<span id="cb54-2130"><a href="#cb54-2130" aria-hidden="true" tabindex="-1"></a>Note that $\sigma_{\hat{y}_n}$ tends to $\sigma_\epsilon$ and not to 0.</span>
<span id="cb54-2131"><a href="#cb54-2131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2132"><a href="#cb54-2132" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{prediction interval}</span>
<span id="cb54-2133"><a href="#cb54-2133" aria-hidden="true" tabindex="-1"></a>A **confidence interval** for $\hat{y}_n$ is obtained by adding and</span>
<span id="cb54-2134"><a href="#cb54-2134" aria-hidden="true" tabindex="-1"></a>subtracting to the point estimator the estimated standard deviation</span>
<span id="cb54-2135"><a href="#cb54-2135" aria-hidden="true" tabindex="-1"></a>$\sigma_{\hat{y}_n}$ times the critical value (here a Student $t$ with</span>
<span id="cb54-2136"><a href="#cb54-2136" aria-hidden="true" tabindex="-1"></a>$N-2=7$ degrees of freedom). A **prediction interval** for $y_n$ is</span>
<span id="cb54-2137"><a href="#cb54-2137" aria-hidden="true" tabindex="-1"></a>obtained the same way, but using $\sigma_{y_n}$ instead of</span>
<span id="cb54-2138"><a href="#cb54-2138" aria-hidden="true" tabindex="-1"></a>$\sigma_{\hat{y}_n}$.</span>
<span id="cb54-2139"><a href="#cb54-2139" aria-hidden="true" tabindex="-1"></a>The following code computes the two standard deviations and the relevant</span>
<span id="cb54-2140"><a href="#cb54-2140" aria-hidden="true" tabindex="-1"></a>limits of the confidence / prediction intervals:\idxfun{nobs}{stats}\idxfun{df.residual}{stats}\idxfun{qt}{stats}\idxfun{mutate}{dplyr}\idxfun{fitted}{stats}\idxfun{sigma}{stats}\idxfun{mutate}{dplyr}\idxfun{select}{dplyr}</span>
<span id="cb54-2141"><a href="#cb54-2141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2142"><a href="#cb54-2142" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2143"><a href="#cb54-2143" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confidence_prediction_interval</span></span>
<span id="cb54-2144"><a href="#cb54-2144" aria-hidden="true" tabindex="-1"></a>mux <span class="ot">&lt;-</span> <span class="fu">mean</span>(prtime<span class="sc">$</span>h)</span>
<span id="cb54-2145"><a href="#cb54-2145" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nobs</span>(pxt)</span>
<span id="cb54-2146"><a href="#cb54-2146" aria-hidden="true" tabindex="-1"></a>sx2 <span class="ot">&lt;-</span> <span class="fu">sum</span>( (prtime<span class="sc">$</span>h <span class="sc">-</span> mux) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> N</span>
<span id="cb54-2147"><a href="#cb54-2147" aria-hidden="true" tabindex="-1"></a>tcv <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="fu">df.residual</span>(pxt))</span>
<span id="cb54-2148"><a href="#cb54-2148" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span></span>
<span id="cb54-2149"><a href="#cb54-2149" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">fitted =</span> <span class="fu">fitted</span>(pxt),</span>
<span id="cb54-2150"><a href="#cb54-2150" aria-hidden="true" tabindex="-1"></a>           <span class="at">sehy =</span> <span class="fu">sigma</span>(pxt) <span class="sc">/</span> <span class="fu">sqrt</span>(N) <span class="sc">*</span></span>
<span id="cb54-2151"><a href="#cb54-2151" aria-hidden="true" tabindex="-1"></a>             <span class="fu">sqrt</span>( <span class="dv">1</span> <span class="sc">+</span> (prtime<span class="sc">$</span>h <span class="sc">-</span> mux) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> sx2),</span>
<span id="cb54-2152"><a href="#cb54-2152" aria-hidden="true" tabindex="-1"></a>           <span class="at">sey =</span> <span class="fu">sigma</span>(pxt) <span class="sc">/</span> <span class="fu">sqrt</span>(N) <span class="sc">*</span></span>
<span id="cb54-2153"><a href="#cb54-2153" aria-hidden="true" tabindex="-1"></a>             <span class="fu">sqrt</span>( <span class="dv">1</span> <span class="sc">+</span> (prtime<span class="sc">$</span>h <span class="sc">-</span> mux) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> sx2 <span class="sc">+</span> N),</span>
<span id="cb54-2154"><a href="#cb54-2154" aria-hidden="true" tabindex="-1"></a>           <span class="at">lowhy =</span> fitted <span class="sc">-</span> tcv <span class="sc">*</span> sehy, <span class="at">uphy  =</span> fitted <span class="sc">+</span> tcv <span class="sc">*</span> sehy,</span>
<span id="cb54-2155"><a href="#cb54-2155" aria-hidden="true" tabindex="-1"></a>           <span class="at">lowy  =</span> fitted <span class="sc">-</span> tcv <span class="sc">*</span>sey, <span class="at">upy   =</span> fitted <span class="sc">+</span> tcv <span class="sc">*</span> sey)</span>
<span id="cb54-2156"><a href="#cb54-2156" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">select</span>(fitted, lowhy, uphy, lowy, upy) <span class="sc">%&gt;%</span> <span class="fu">head</span>(<span class="dv">3</span>)</span>
<span id="cb54-2157"><a href="#cb54-2157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2158"><a href="#cb54-2158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2159"><a href="#cb54-2159" aria-hidden="true" tabindex="-1"></a>These values can also be obtained with the <span class="in">`predict`</span> function, with the</span>
<span id="cb54-2160"><a href="#cb54-2160" aria-hidden="true" tabindex="-1"></a><span class="in">`interval`</span> argument set to <span class="in">`"confidence"`</span> or <span class="in">`"prediction"`</span>:\idxfun{predict}{stats}</span>
<span id="cb54-2161"><a href="#cb54-2161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2162"><a href="#cb54-2162" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2163"><a href="#cb54-2163" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confidence_prediction_lm</span></span>
<span id="cb54-2164"><a href="#cb54-2164" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb54-2165"><a href="#cb54-2165" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: hide</span></span>
<span id="cb54-2166"><a href="#cb54-2166" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(pxt, <span class="at">interval =</span> <span class="st">"confidence"</span>)</span>
<span id="cb54-2167"><a href="#cb54-2167" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(pxt, <span class="at">interval =</span> <span class="st">"prediction"</span>)</span>
<span id="cb54-2168"><a href="#cb54-2168" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2169"><a href="#cb54-2169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2170"><a href="#cb54-2170" aria-hidden="true" tabindex="-1"></a>In @fig-confpredit, the two intervals are represented:</span>
<span id="cb54-2171"><a href="#cb54-2171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2172"><a href="#cb54-2172" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>for the confidence interval, we use <span class="in">`geom_smooth`</span> with</span>
<span id="cb54-2173"><a href="#cb54-2173" aria-hidden="true" tabindex="-1"></a>    <span class="in">`method = "lm"`</span> and the default <span class="in">`TRUE`</span> value for the <span class="in">`se`</span> argument;</span>
<span id="cb54-2174"><a href="#cb54-2174" aria-hidden="true" tabindex="-1"></a>    in this case, we have a gray zone which figures the confidence</span>
<span id="cb54-2175"><a href="#cb54-2175" aria-hidden="true" tabindex="-1"></a>    interval (by default at the 95% level, but another level can be used by setting the <span class="in">`level`</span> argument to the desired level),</span>
<span id="cb54-2176"><a href="#cb54-2176" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>for the confidence interval, we use <span class="in">`geom_errorbar`</span> that draws</span>
<span id="cb54-2177"><a href="#cb54-2177" aria-hidden="true" tabindex="-1"></a>    vertical segments, which represent here the limits of the confidence</span>
<span id="cb54-2178"><a href="#cb54-2178" aria-hidden="true" tabindex="-1"></a>    interval we have computed.\idxfun{ggplot}{ggplot2}\idxfun{geom<span class="sc">\_</span>point}{ggplot2}\idxfun{geom<span class="sc">\_</span>smooth}{ggplot2}\idxfun{geom<span class="sc">\_</span>errorbar}{ggplot2}</span>
<span id="cb54-2179"><a href="#cb54-2179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2180"><a href="#cb54-2180" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2181"><a href="#cb54-2181" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-confpredit</span></span>
<span id="cb54-2182"><a href="#cb54-2182" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Confidence and prediction intervals"</span></span>
<span id="cb54-2183"><a href="#cb54-2183" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb54-2184"><a href="#cb54-2184" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-2185"><a href="#cb54-2185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb54-2186"><a href="#cb54-2186" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lowy, <span class="at">ymax =</span> upy))</span>
<span id="cb54-2187"><a href="#cb54-2187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2188"><a href="#cb54-2188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2189"><a href="#cb54-2189" aria-hidden="true" tabindex="-1"></a>As an example, consider trips from Bordeaux to Paris. Reported transport</span>
<span id="cb54-2190"><a href="#cb54-2190" aria-hidden="true" tabindex="-1"></a>time is 242 minutes, which is approximately 4 hours. The high speed</span>
<span id="cb54-2191"><a href="#cb54-2191" aria-hidden="true" tabindex="-1"></a>track, opened in 2018 reduces this transport time to a minimum of 2 hours</span>
<span id="cb54-2192"><a href="#cb54-2192" aria-hidden="true" tabindex="-1"></a>and 6 minutes. We consider 3 hours as the mean transport time, and we</span>
<span id="cb54-2193"><a href="#cb54-2193" aria-hidden="true" tabindex="-1"></a>consider the average price to be 75 euros. Assuming that the conditions</span>
<span id="cb54-2194"><a href="#cb54-2194" aria-hidden="true" tabindex="-1"></a>on the air transport market are unchanged, what prediction can we make</span>
<span id="cb54-2195"><a href="#cb54-2195" aria-hidden="true" tabindex="-1"></a>about the change of the modal share of rail? We first construct a tibble</span>
<span id="cb54-2196"><a href="#cb54-2196" aria-hidden="true" tabindex="-1"></a>called <span class="in">`bordeaux`</span> with two lines: the first contains the actual</span>
<span id="cb54-2197"><a href="#cb54-2197" aria-hidden="true" tabindex="-1"></a>features of the Paris-Bordeaux trip, and the second the new features.\idxfun{filter}{dplyr}\idxfun{select}{dplyr}\idxfun{add<span class="sc">\_</span>row}{dplyr}\idxfun{mutate}{dplyr}</span>
<span id="cb54-2198"><a href="#cb54-2198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2199"><a href="#cb54-2199" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2200"><a href="#cb54-2200" aria-hidden="true" tabindex="-1"></a><span class="co">#| label : initial_bordeaux</span></span>
<span id="cb54-2201"><a href="#cb54-2201" aria-hidden="true" tabindex="-1"></a>bordeaux <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">filter</span>(town <span class="sc">==</span> <span class="st">"Bordeaux"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb54-2202"><a href="#cb54-2202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(town, pr, pa, tr, ta) <span class="sc">%&gt;%</span> </span>
<span id="cb54-2203"><a href="#cb54-2203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">town =</span> <span class="st">"BordeauxSim"</span>, <span class="at">pr =</span> <span class="dv">75</span>, </span>
<span id="cb54-2204"><a href="#cb54-2204" aria-hidden="true" tabindex="-1"></a>          <span class="at">pa =</span> <span class="fl">82.6</span>, <span class="at">tr =</span> <span class="dv">180</span>, <span class="at">ta =</span> <span class="dv">165</span>) <span class="sc">%&gt;%</span></span>
<span id="cb54-2205"><a href="#cb54-2205" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">h =</span> (pa <span class="sc">-</span> pr) <span class="sc">/</span> ( (tr <span class="sc">-</span> ta) <span class="sc">/</span> <span class="dv">60</span>))</span>
<span id="cb54-2206"><a href="#cb54-2206" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2207"><a href="#cb54-2207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2208"><a href="#cb54-2208" aria-hidden="true" tabindex="-1"></a>The prediction of train's modal share is obtained using the <span class="in">`predict`</span></span>
<span id="cb54-2209"><a href="#cb54-2209" aria-hidden="true" tabindex="-1"></a>function with the <span class="in">`new`</span> argument which is a data frame containing the</span>
<span id="cb54-2210"><a href="#cb54-2210" aria-hidden="true" tabindex="-1"></a>values of the covariates for which we want to compute predictions (this</span>
<span id="cb54-2211"><a href="#cb54-2211" aria-hidden="true" tabindex="-1"></a>is the <span class="in">`bordeaux`</span> table in our example):\idxfun{predict}{stats}\idxfun{as<span class="sc">\_</span>tibble}{tibble}</span>
<span id="cb54-2212"><a href="#cb54-2212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2213"><a href="#cb54-2213" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2214"><a href="#cb54-2214" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: predict_bordeaux</span></span>
<span id="cb54-2215"><a href="#cb54-2215" aria-hidden="true" tabindex="-1"></a>prd <span class="ot">&lt;-</span> <span class="fu">predict</span>(pxt, <span class="at">new =</span> bordeaux, <span class="at">interval =</span> <span class="st">"confidence"</span>) <span class="sc">%&gt;%</span> as_tibble</span>
<span id="cb54-2216"><a href="#cb54-2216" aria-hidden="true" tabindex="-1"></a>prd</span>
<span id="cb54-2217"><a href="#cb54-2217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2218"><a href="#cb54-2218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2219"><a href="#cb54-2219" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb54-2220"><a href="#cb54-2220" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- new page --&gt;</span></span>
<span id="cb54-2221"><a href="#cb54-2221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2222"><a href="#cb54-2222" aria-hidden="true" tabindex="-1"></a>The model predicts that the train's share increases from</span>
<span id="cb54-2223"><a href="#cb54-2223" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(pull(prd, fit)[1], 3)`</span> to <span class="in">`r round(pull(prd, fit)[2], 3)`</span>, but</span>
<span id="cb54-2224"><a href="#cb54-2224" aria-hidden="true" tabindex="-1"></a>the confidence intervals are quite large and overlap. Present and</span>
<span id="cb54-2225"><a href="#cb54-2225" aria-hidden="true" tabindex="-1"></a>predicted market shares are represented by a triangle and by a circle in</span>
<span id="cb54-2226"><a href="#cb54-2226" aria-hidden="true" tabindex="-1"></a>@fig-bordeaux, along with the confidence intervals.</span>
<span id="cb54-2227"><a href="#cb54-2227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2228"><a href="#cb54-2228" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb54-2229"><a href="#cb54-2229" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bordeaux</span></span>
<span id="cb54-2230"><a href="#cb54-2230" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb54-2231"><a href="#cb54-2231" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Predictions for train's model share"</span></span>
<span id="cb54-2232"><a href="#cb54-2232" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb54-2233"><a href="#cb54-2233" aria-hidden="true" tabindex="-1"></a>bordeaux <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(bordeaux, prd)</span>
<span id="cb54-2234"><a href="#cb54-2234" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb54-2235"><a href="#cb54-2235" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb54-2236"><a href="#cb54-2236" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> bordeaux, <span class="at">shape =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">size =</span> <span class="dv">4</span>, <span class="fu">aes</span>(<span class="at">y =</span> fit)) <span class="sc">+</span></span>
<span id="cb54-2237"><a href="#cb54-2237" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_errorbar</span>(<span class="at">data =</span> bordeaux, <span class="fu">aes</span>(<span class="at">ymin =</span> lwr, <span class="at">ymax =</span> upr, <span class="at">y =</span> <span class="cn">NULL</span>))</span>
<span id="cb54-2238"><a href="#cb54-2238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-2239"><a href="#cb54-2239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2240"><a href="#cb54-2240" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!prediction|)}</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>