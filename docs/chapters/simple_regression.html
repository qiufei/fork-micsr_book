<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Microeconometrics with R - 1&nbsp; Simple linear regression model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/simple_regression_properties.html" rel="next">
<link href="../OLS.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Microeconometrics with R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../OLS.html" class="sidebar-item-text sidebar-link">Ordinary least squares estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/multiple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coefficients.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../beyond_OLS.html" class="sidebar-item-text sidebar-link">Beyond the OLS estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/maximum_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/non_spherical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/endogeneity.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/treateff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/spatial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Spatial econometrics</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../special_responses.html" class="sidebar-item-text sidebar-link">Special responses</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/binomial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Binomial models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/tobit.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Censored and truncated models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/count.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Count data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/duration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Duration models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discrete choice models</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-condexp_covariance" id="toc-sec-condexp_covariance" class="nav-link active" data-scroll-target="#sec-condexp_covariance"><span class="toc-section-number">1.1</span>  Conditional expectation and covariance</a>
  <ul class="collapse">
<li><a href="#linear-models-two-examples" id="toc-linear-models-two-examples" class="nav-link" data-scroll-target="#linear-models-two-examples">Linear models: two examples</a></li>
  <li><a href="#linear-model-conditional-expectation-and-covariance" id="toc-linear-model-conditional-expectation-and-covariance" class="nav-link" data-scroll-target="#linear-model-conditional-expectation-and-covariance">Linear model, conditional expectation and covariance</a></li>
  </ul>
</li>
  <li><a href="#sec-simple_model_data" id="toc-sec-simple_model_data" class="nav-link" data-scroll-target="#sec-simple_model_data"><span class="toc-section-number">1.2</span>  Model and data set</a></li>
  <li><a href="#sec-comp_simple_ols" id="toc-sec-comp_simple_ols" class="nav-link" data-scroll-target="#sec-comp_simple_ols"><span class="toc-section-number">1.3</span>  Computation of the OLS estimator</a></li>
  <li>
<a href="#sec-simple_geometry" id="toc-sec-simple_geometry" class="nav-link" data-scroll-target="#sec-simple_geometry"><span class="toc-section-number">1.4</span>  Geometry of least squares, variance decomposition and of determination</a>
  <ul class="collapse">
<li><a href="#vectors-variance-and-covariance" id="toc-vectors-variance-and-covariance" class="nav-link" data-scroll-target="#vectors-variance-and-covariance">Vectors, variance and covariance</a></li>
  <li><a href="#sec-geometry_ols" id="toc-sec-geometry_ols" class="nav-link" data-scroll-target="#sec-geometry_ols">Geometry of least squares</a></li>
  <li><a href="#sec-vardecomp_R2" id="toc-sec-vardecomp_R2" class="nav-link" data-scroll-target="#sec-vardecomp_R2">Variance decomposition and the <span class="math inline">\(R^2\)</span></a></li>
  </ul>
</li>
  <li><a href="#sec-comp_ols_simple_R" id="toc-sec-comp_ols_simple_R" class="nav-link" data-scroll-target="#sec-comp_ols_simple_R"><span class="toc-section-number">1.5</span>  Computation with R</a></li>
  <li>
<a href="#sec-dgp_simulations" id="toc-sec-dgp_simulations" class="nav-link" data-scroll-target="#sec-dgp_simulations"><span class="toc-section-number">1.6</span>  Data generator process and simulations</a>
  <ul class="collapse">
<li><a href="#data-generator-process" id="toc-data-generator-process" class="nav-link" data-scroll-target="#data-generator-process">Data generator process</a></li>
  <li><a href="#sec-random_numbers_simulations" id="toc-sec-random_numbers_simulations" class="nav-link" data-scroll-target="#sec-random_numbers_simulations">Random numbers and simulations</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-simple_ols" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><!-- see zeileis Kleiber on structural models --><!-- the adoptees data set is quite heavy --><!-- introduce the epsilon_O notation ? --><!-- \newcommand{\indexfunction}[2]{\index[functions]{#1}} --><p>Obviously there will be rarely a deterministic relationship between a response and one or several covariates. Education has a positive causal effect on wage, but there are many other variables that affect wages: sex, ethnicity, experience, abilities, to name a few. Even if a large set of relevant covariates are observed, some are not (this is in particular the case for abilities). Therefore, we won’t try to modelize the value of <span class="math inline">\(y\)</span> as a function of covariates <span class="math inline">\(x\)</span> and some unknown parameters <span class="math inline">\(\gamma\)</span>: <span class="math inline">\(y = f(x, \gamma)\)</span> but, more modestly, the <strong>conditional expectation</strong> of <span class="math inline">\(y\)</span>: </p>
<p><span id="eq-condexpsimp"><span class="math display">\[
\mbox{E}(y | x) = f(x, \gamma)
\tag{1.1}\]</span></span></p>
<p>Back to our education / wage model, the conditional expectation is the mean value of the wage in the population for a given value of education. It is therefore a function of <span class="math inline">\(x\)</span> which takes as many values as there are distinct values of <span class="math inline">\(x\)</span>.</p>
<p>In this chapter, we discuss the simplest econometric model, which is the simple linear regression model. This is a <strong>simple</strong> model because there is only one covariate <span class="math inline">\(x\)</span>. This is a <strong>linear</strong> model because the <span class="math inline">\(f\)</span> function is assumed to be linear. <a href="#eq-condexpsimp">Equation&nbsp;<span>1.1</span></a> can then be rewritten as follow:</p>
<p><span id="eq-lincondexp"><span class="math display">\[
\mbox{E}(y | x) = \alpha + \beta x
\tag{1.2}\]</span></span></p>
<p><a href="#sec-condexp_covariance"><span>Section&nbsp;1.1</span></a> details the notion of conditional expectation and its relation with the notion of covariance. <a href="#sec-simple_model_data"><span>Section&nbsp;1.2</span></a> presents the model and the data set that will be used throughout this chapter. <a href="#sec-comp_simple_ols"><span>Section&nbsp;1.3</span></a> is devoted to the computation of the ordinary least squares estimator. <a href="#sec-simple_geometry"><span>Section&nbsp;1.4</span></a> presents the geometry of least squares. <a href="#sec-comp_ols_simple_R"><span>Section&nbsp;1.5</span></a> explains how to compute the ordinary least squares estimator using <strong>R</strong>. Finally <a href="#sec-dgp_simulations"><span>Section&nbsp;1.6</span></a> presents the notion of data generator process and explains how simulations can be usefully performed.</p>
<section id="sec-condexp_covariance" class="level2" data-number="1.1"><h2 data-number="1.1" class="anchored" data-anchor-id="sec-condexp_covariance">
<span class="header-section-number">1.1</span> Conditional expectation and covariance</h2>
<p></p>
<p>To estimate the two unknown parameters <span class="math inline">\(\alpha\)</span> (the <strong>intercept</strong>) and <span class="math inline">\(\beta\)</span> (the <strong>slope</strong>), we need a data set that contains different individuals for which <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are observed. Typically, we’ll consider a random sample, which is a small subset of the whole population consisting of a set of individuals randomly drawn from this population.</p>
<section id="linear-models-two-examples" class="level3"><h3 class="anchored" data-anchor-id="linear-models-two-examples">Linear models: two examples</h3>
<p>To understand what the hypothesis of the simple linear model implies, we consider two data sets. The first one is called <code>birthwt</code> <span class="citation" data-cites="MULL:97">(<a href="#ref-MULL:97" role="doc-biblioref">Mullahy 1997</a>)</span> and contains birth weights (<code>birthwt</code> in ounces) of babies and smoking habits of mothers. The <code>cigarettes</code> variable is the number of cigarettes smoked per day during pregnancy (for about 85% of the mothers, it is equal to 0). We first compute a binary variable for smoking mothers:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">birthwt</span> <span class="op">&lt;-</span> <span class="va">birthwt</span> <span class="op">%&gt;%</span> <span class="fu">mutate</span><span class="op">(</span>smoke <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">cigarettes</span> <span class="op">&gt;</span> <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and consider a simple linear regression model with <code>birthwt</code> as the response and <code>smoke</code> as a covariate. As the covariate takes only two values, so does the conditional expectation. In a linear regression model, <a href="#eq-lincondexp">Equation&nbsp;<span>1.2</span></a> will therefore returns two values, <span class="math inline">\(\alpha\)</span> for <span class="math inline">\(x = 0\)</span> and <span class="math inline">\(\alpha + \beta\)</span> for <span class="math inline">\(x = 1\)</span>. <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\alpha + \beta\)</span> are therefore the expected birth weights for respectively non-smoking and smoking mothers. Note that, because we have as many parameters as values of the covariate, the linear hypothesis is necessarily supported. Natural estimators of the conditional expectations are the conditional sample means, i.e., the average birth weights in the sample for non-smoking and smoking women:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cond_means</span> <span class="op">&lt;-</span> <span class="va">birthwt</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">group_by</span><span class="op">(</span><span class="va">smoke</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">summarise</span><span class="op">(</span>birthwt <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">birthwt</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">cond_means</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
  smoke birthwt
  &lt;dbl&gt;   &lt;dbl&gt;
1     0    120.
2     1    111.</code></pre>
</div>
</div>
<p>Therefore, our estimation of <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(\hat{\alpha} = 120.1\)</span> and the estimation of <span class="math inline">\(\alpha + \beta\)</span> is <span class="math inline">\(111.1\)</span>, so that <span class="math inline">\(\hat{\beta} = 111.1 - 120.1 = -8.9\)</span> ounces, which is the estimation of birth weight’s loss caused by smoking during pregnancy. This can be illustrated using a scatterplot (see <a href="#fig-birthwt">Figure&nbsp;<span>1.1</span></a>), with <code>smoke</code> on the horizontal axis and <code>birthwt</code> on the vertical axis, all the points having only two possible abscissa values, 0 and 1. The conditional means are represented by square points, and we also drew the line that contains these two points. The intercept is therefore the y-value of the square point for <span class="math inline">\(x = 0\)</span> which is <span class="math inline">\(\hat{\alpha} = 120.1\)</span>. The slope of the line is the ratio of the vertical and the horizontal distances between the two large points, which are respectively <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(1 - 0 = 1\)</span> and is therefore equal to <span class="math inline">\(\hat{\beta}\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-birthwt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-birthwt-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.1: Birth weight for non-smoking and smoking mothers</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Consider now the relationship between education and wage. The <code>adoptees</code> data set <span class="citation" data-cites="PLUG:04">(<a href="#ref-PLUG:04" role="doc-biblioref">Plug 2004</a>)</span> contains the number of years of education <code>educ</code> and the annual income <code>income</code> (in thousands of US$) for 16481 individuals. We restrict the sample to individuals with an education level between 12 (high school degree) and 15 years (bachelor’s degree).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">adoptees</span> <span class="op">&lt;-</span> <span class="va">adoptees</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">educ</span> <span class="op">&gt;=</span> <span class="fl">12</span>, <span class="va">educ</span> <span class="op">&lt;=</span> <span class="fl">15</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and we compute the mean income for the four values of education:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">adoptees</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">educ</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">summarise</span><span class="op">(</span>income <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">income</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 2
   educ income
  &lt;dbl&gt;  &lt;dbl&gt;
1    12   58.9
2    13   68.8
3    14   70.5
4    15   78.4</code></pre>
</div>
</div>
<p>The scatterplot is presented in <a href="#fig-adoptees">Figure&nbsp;<span>1.2</span></a>:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-adoptees" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-adoptees-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.2: Education and income</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>This time, we have four distinct values of the covariate and we can estimate four conditional sample means, but there are only two parameters to estimate for the simple linear model. Therefore, we can’t estimate directly <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> using the conditional means, except in the improbable case where the four conditional means lie on a straight line, which means that, for one additional year of education, the average wage increases exactly by the same amount. We can see in <a href="#fig-adoptees">Figure&nbsp;<span>1.2</span></a> that it is not the case: the increase of income is 9.9 for a 13th year of education, 1.7 for a 14th year and 8 for a 15th year. We therefore need a formal method of estimation which enables us to obtain values of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>. The remaining of this chapter is devoted to the presentation of this estimator, which is called the <strong>ordinary least squares</strong> (<strong>OLS</strong>) estimator.</p>
</section><section id="linear-model-conditional-expectation-and-covariance" class="level3"><h3 class="anchored" data-anchor-id="linear-model-conditional-expectation-and-covariance">Linear model, conditional expectation and covariance</h3>
<p>For now, it is very important to understand the relation between the conditional expectation and the covariance. The covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is, denoting <span class="math inline">\(\mu_x = \mbox{E}(x)\)</span> and <span class="math inline">\(\mu_y = \mbox{E}(y)\)</span>:</p>
<p><span class="math display">\[
\sigma_{xy} = \mbox{cov}(x, y) = \mbox{E}\left[(x - \mu_x)(y - \mu_y)\right] = \mbox{E}(xy) - \mu_x\mu_y
\]</span> It is the expectation of the product of the two variables in deviations from their respective expectations or the expectation of the product minus the product of the expectations. The expectation is for the two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and the law of repeated expectation states that this expectation can be written as:</p>
<p><span class="math display">\[
\mbox{cov}(x, y) = \mbox{E}_x\left[\mbox{E}_y\left((y - \mu_y)(x - \mu_x)\mid x\right)\right] = \mbox{E}_x\left[(x - \mu_x)\mbox{E}_y(y - \mu_y\mid x)\right]
\]</span></p>
<p>Therefore, the covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is also the covariance between <span class="math inline">\(x\)</span> and the conditional expectation of <span class="math inline">\(y\)</span> (which is a function of <span class="math inline">\(x\)</span>).</p>
<p>As an example, consider the case where <span class="math inline">\(y\)</span> is the hourly wage and <span class="math inline">\(x\)</span> the education level, which takes the values 0 (highschool), 3 (bachelor) and 5 (graduate), the frequencies for the three levels of education being 0.4, 0.3 and 0.3. Let denote <span class="math inline">\(h\)</span>, <span class="math inline">\(b\)</span> and <span class="math inline">\(g\)</span> these three education levels and assume that <span class="math inline">\(\mbox{E}(y\mid x) = 8 + 0.5 x\)</span>, which means that one more year of education increases the expected wage by $0.5. The expected level of education is <span class="math inline">\(\mu_x = 0.4 \times 0 + 0.3 \times 3 + 0.3 \times 5 = 2.4\)</span>. The expected wage for the three levels of education are <span class="math inline">\(\mu_{xh} = 8\)</span>, <span class="math inline">\(\mu_{xb} = 9.5\)</span> and <span class="math inline">\(\mu_{xg} = 10.5\)</span>. The expected wage is then <span class="math inline">\(\mu_y = 0.4 \times 8 + 0.3 \times 9.5 + 0.3 \times 10.5 = 9.2\)</span>. We first compute the first term of the covariance between wage and education for <span class="math inline">\(x = x_h = 0\)</span>:</p>
<p><span id="eq-cov_one_term"><span class="math display">\[
\begin{array}{rcl}
\mbox{E}\left[ (x - \mu_x)(y -
\mu_y)\mid x = x_h\right] &amp;=&amp;
\mbox{E}\left[ (x_h - \mu_x)(y -
\mu_y)\mid x = x_h\right] \\
&amp;=&amp;
(x_h - \mu_x) \mbox{E}\left[ (y -
\mu_y)\mid x = x_h\right] \\
&amp;=&amp;
(x_h - \mu_x)(\mbox{E}(y\mid x = x_h) - \mu_y)\\
&amp;=&amp; (0 - 2.4) \times (8 - 9.2) = 2.88
\end{array}
\tag{1.3}\]</span></span></p>
<p>Similarly, we get <span class="math inline">\((3 - 2.4) \times (9.5 - 9.2) = 0.18\)</span> for the bachelor level and <span class="math inline">\((5 - 2.4)\times (10.5 - 9.2) = 3.38\)</span> for the master level. Finally the covariance is obtained as the weight average of these three terms:</p>
<p><span class="math display">\[
\sigma_{xy} = 0.4 \times 2.88 + 0.3 \times 0.18 + 0.3 \times 3.38 = 2.22
\]</span></p>
<p>Now consider the case where the conditional expectation of the hourly wage is the same for the three education levels: <span class="math inline">\(\mbox{E}(y \mid x = x_h) = \mbox{E}(y \mid x = x_b) = \mbox{E}(y \mid x = x_m)\)</span>. These three conditional expectations are therefore equal to the unconditional expectation <span class="math inline">\(\mu_y\)</span> and <a href="#eq-cov_one_term">Equation&nbsp;<span>1.3</span></a> is clearly 0 and so is the covariance. Therefore, a constant conditional expectation of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> implies that the covariance is 0. Note that the opposite is not true. For example, if <span class="math inline">\(\mu_{yh} = 8\)</span>, <span class="math inline">\(\mu_{yb} = 13.2\)</span> and <span class="math inline">\(\mu_{ym} = 6.2\)</span>, the relation between the conditional mean of the wage and the level of education is no more linear, and it is not monotonous but inversely U-shaped (the wage first increases with education and then decreases). The unconditional mean is still: <span class="math inline">\(\mu_y = 0.4 \times 8 + 0.3 \times 13.2 + 0.3 \times 6.2 = 9.2\)</span>, but the covariance is then:</p>
<p><span class="math display">\[
\sigma_{xy} = 0.4 \times (0-2.4) \times (8 - 9.2) + 0.3 \times (3-2.4) \times (13.2 - 9.2) + 0.3 \times (5 - 2.4) \times (6.2 - 9.2) = 0
\]</span></p>
<p>Therefore, a 0 covariance doesn’t imply that the conditional mean is constant. However, in the special case where the conditional mean is a linear function of the conditional variable, the two properties of 0 covariance and constant conditional means are equivalent.</p>
<p>Consider now the sample counterpart of this theoretical covariance. Consider a sample of 10 individuals for which the levels of education and the hourly wages are:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="table table-sm table-striped"><tbody>
<tr class="odd">
<td style="text-align: left;">education</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">5</td>
</tr>
<tr class="even">
<td style="text-align: left;">wage</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">9.5</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">11</td>
</tr>
</tbody></table>
</div>
</div>
<p>The sample means of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are : <span class="math inline">\(\bar{x} = 2.4\)</span> and <span class="math inline">\(\bar{y} = 9.2\)</span>. The conditional means of <span class="math inline">\(y\)</span> for a given value of <span class="math inline">\(x\)</span> in the sample are simply the means for the three subsamples defined by an education level:</p>
<ul>
<li>
<span class="math inline">\(x_h=0\)</span>&nbsp;: <span class="math inline">\(\bar{y}_h = \frac{8+7+8+9}{4} = 8\)</span>,</li>
<li>
<span class="math inline">\(x_b=3\)</span>&nbsp;: <span class="math inline">\(\bar{y}_b = \frac{9+9.5+10}{3} = 9.5\)</span>,</li>
<li>
<span class="math inline">\(x_m=5\)</span>&nbsp;: <span class="math inline">\(\bar{y}_m = \frac{10, 10.5, 11}{3} = 10.5\)</span>,</li>
</ul>
<p>and the covariance is obtained as the mean of the products of the two variables in deviations from their mean, or equivalently by the difference between the mean of the products and the products of the means:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[
\hat{\sigma}_{xy} = \frac{\sum_{n=1}^N (x_n - \bar{x}) (y_n -
\bar{y})}{N}=\frac{\sum_{n=1}^N x_n y_n}{N}-\bar{x}\bar{y}
\]</span></p>
<p>The sum of the products is <span class="math inline">\(\sum_n x_n y_n = 243\)</span> and the covariance is then:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy}=\frac{243}{10} - 2.4 \times 9.2 = 2.22
\]</span> We also can consider the three subsamples defined by education levels, denoting <span class="math inline">\(n=1, ..., H\)</span>, <span class="math inline">\(n=H+1, ..., H+B\)</span>, <span class="math inline">\(n=H+B+1, ..., N\)</span> (with <span class="math inline">\(H=4\)</span>, <span class="math inline">\(B=3\)</span> and <span class="math inline">\(N=10\)</span>) observations for, respectively, a high-school, a bachelor and a master level. We then have:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy} = \frac{\sum_{n=1}^H (x_n - \bar{x})(y_n -
\bar{y}) + \sum_{n=H+1}^{H+B} (x_n - \bar{x})(y_n - \bar{y}) +
\sum_{n=H+B+1}^N (x_n - \bar{x})(y_n - \bar{y})}{N}
\]</span> For example, for <span class="math inline">\(n=1, ..., H\)</span>, the value of <span class="math inline">\(x\)</span> is the same: <span class="math inline">\(x_n=x_h=0\)</span>. Therefore, we can write <span class="math inline">\(\sum_{n=1}^H (x_n - \bar{x})(y_n -\bar{y}) = (x_h - \bar{x}) \sum_{n=1}^H (y_n -\bar{y})\)</span> and more generally:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy} = \frac{(x_h - \bar{x})\sum_{n=1}^H (y_n -
\bar{y}) + (x_b - \bar{x})\sum_{n=H+1}^B (y_n - \bar{y}) +
(x_m - \bar{x})\sum_{n=H+B+1}^N (y_n - \bar{y})}{N}
\]</span></p>
<p>Moreover <span class="math inline">\(\frac{\sum_{n=1}^H (y_n - \bar{y})}{H}=(\bar{y}_h-\bar{y})\)</span> and then:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy} = \frac{(x_h - \bar{x})H(\bar{y}_h-\bar{y})+ (x_b - \bar{x})B(\bar{y}_b -\bar{y})+
(x_m - \bar{x})M(\bar{y}_m - \bar{y})}{N}
\]</span></p>
<p>Denoting <span class="math inline">\(f_h = \frac{H}{N}\)</span>, <span class="math inline">\(f_b = \frac{B}{N}\)</span> and <span class="math inline">\(f_m=\frac{M}{N}\)</span> the empirical frequencies of the three education levels in the sample:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy} = f_h(x_h - \bar{x})(\bar{y}_h-\bar{y})+ f_b(x_b - \bar{x})(\bar{y}_b -\bar{y})+
(x_m - \bar{x})(\bar{y}_m - \bar{y})
\]</span></p>
<p>Finally, denoting <span class="math inline">\(k=h, b, m\)</span> the education levels:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy}=\hat{\sigma}_{x\bar{y}_x}=
\sum_k f_k (x_k- \bar{x})(\bar{y}_k-\bar{y})
\]</span></p>
<p>Therefore, the covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is also the covariance between <span class="math inline">\(x\)</span> and the mean values of <span class="math inline">\(y\)</span> for given values of <span class="math inline">\(x\)</span>. We have here: <span class="math inline">\(\bar{y}_h = 8\)</span>, <span class="math inline">\(\bar{y}_b = 9.5\)</span> and <span class="math inline">\(\bar{y}_m = 10.5\)</span>. The covariance is then:</p>
<p><span class="math display">\[
\hat{\sigma}_{xy} = 0.4 \times (0 - 2.4) \times (8 - 9.2) +
0.3 \times (3 - 2.4) \times (9.5 - 9.2) +
0.3 \times (5 - 2.4) \times (10.5 - 9.2) =
2.22
\]</span></p>
<p>Consider now the case where the mean wage is the same for every education level: <span class="math inline">\(\bar{y}_b=\bar{y}_l=\bar{y}_m=\bar{y}=9.2\)</span>. In this case, the covariance is obviously 0 because <span class="math inline">\(\bar{y}_k-\bar{y}=0\;\forall k\)</span>. </p>
</section></section><section id="sec-simple_model_data" class="level2" data-number="1.2"><h2 data-number="1.2" class="anchored" data-anchor-id="sec-simple_model_data">
<span class="header-section-number">1.2</span> Model and data set</h2>
<p>We’ll consider in this chapter the question of mode shares for inter-urban transportation. More precisely, considering that a trip can be made using one out of two transport modes (air and rail), how can we modelize the market shares of both modes? We’ll use in this section a popular model in transportation economics which is the price-time model. <code>price_time</code> contains aggregate data about rail and air transportation between Paris and 13 French towns in 1995, it is reproduced from <span class="citation" data-cites="BONN:04">Bonnel (<a href="#ref-BONN:04" role="doc-biblioref">2004</a>)</span> pp.&nbsp;364-366.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">price_time</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 13 × 7
  town     trafic_rail trafic_air price_rail price_air time_rail
  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Bordeaux        2005       1400       48.3      82.6       242
2 Brest            471        428       51.6      98.6       308
3 Clermont         429        243       32.7      89.4       266
# ℹ 10 more rows
# ℹ 1 more variable: time_air &lt;dbl&gt;</code></pre>
</div>
</div>
<p>For the sake of simplicity, we’ll use shorter names for the variables:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="va">price_time</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">set_names</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Variables are prices (<code>pr</code> and <code>pa</code>) in euros, transport times (<code>tr</code> and <code>ta</code>) in minutes and thousands of trips (<code>qf</code> and <code>qa</code>) for the two modes (<code>r</code> for rail and <code>a</code> for air). We first compute the market shares of rail:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="fu">mutate</span><span class="op">(</span><span class="va">prtime</span>, sr <span class="op">=</span> <span class="va">qr</span> <span class="op">/</span> <span class="op">(</span><span class="va">qr</span> <span class="op">+</span> <span class="va">qa</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sr</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">summary</span></span>
<span><span class="co">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">##   0.136   0.339   0.524   0.555   0.868   0.943</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Rail’s market share exhibits huge variations in the sample, ranging from 14 to 94%. For an individual, the relevant cost of a trip is the generalized cost, which is the sum of the monetary cost and the value of the travel time. Denoting <span class="math inline">\(h^i\)</span> the time value of individual <span class="math inline">\(i\)</span>, in euros per hour, the generalized cost for the two modes are:</p>
<p><span id="eq-generalized_costs"><span class="math display">\[
\left\{
\begin{array}{rcl}
c_{a} ^ i &amp;=&amp; p_{a} + h ^ i t_{a} / 60\\
c_{r} ^ i &amp;=&amp; p_{r} + h ^ i t_{r} / 60\\
\end{array}
\right.
\tag{1.4}\]</span></span></p>
<p>Plane is typically faster and more expensive than train, which means that in the time-value&nbsp;/&nbsp;generalized cost plane, the generalized cost for rail will be represented by a line with a lower intercept (the price of train is lower) and with a higher slope (transport time is higher) than the one that corresponds to air. Generalized cost for both modes and for the two towns of Bordeaux and Nice are presented in <a href="#fig-costmode">Figure&nbsp;<span>1.3</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-costmode" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-costmode-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.3: Generalized cost for train and plane</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Every individual will choose the mode with the lowest generalized cost. For example, <span class="math inline">\(i\)</span> will choose the train if <span class="math inline">\(c_r^i &lt; c_a^i\)</span>. This will depend on the individual value of time: an individual with a high value of time will choose the plane as an individual with a lower travel time will choose the train. For given values of prices and travel times, one can compute a value of travel time <span class="math inline">\(h^*\)</span> which equates the generalized costs of the two modes in <a href="#eq-generalized_costs">Equation&nbsp;<span>1.4</span></a>:</p>
<p><span class="math display">\[
h^* = 60\frac{p_a - p_r}{(t_r - t_a)}
\]</span></p>
<p>For Bordeaux and Nice, these time values are respectively 26.7 and 9 euros per hour. Nice is actually very far from Paris and only people with a very low value of time would spend 7.5 hours in the train instead of taking a plane. We now compute this threshold value for every city:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="fu">mutate</span><span class="op">(</span><span class="va">prtime</span>,  h <span class="op">=</span> <span class="op">(</span><span class="va">pa</span> <span class="op">-</span> <span class="va">pr</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="va">tr</span> <span class="op">-</span> <span class="va">ta</span><span class="op">)</span> <span class="op">/</span> <span class="fl">60</span><span class="op">)</span> <span class="op">)</span></span>
<span><span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">h</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">summary</span></span>
<span><span class="co">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">##     9.0    15.9    20.4    59.0    67.2   314.3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are huge variations of the threshold value of time, as it ranges from 9 (Nice) to 314 (Nantes) euros per hour. Before considering a theoretical model that links the market share of train with the threshold value of time, let’s have a first glance at <a href="#fig-hsfsmpl">Figure&nbsp;<span>1.4</span></a> of this relationship using a scatterplot.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hsfsmpl" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-hsfsmpl-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.4: Share of rail in function of the threshold time value</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The relationship between the threshold value of time and rail’s market share seems approximately linear, except for cities where the market share of train is very high (more than 75%). For now, we’ll remove these four cities from the sample and plot on <a href="#fig-hsfsubsmpl">Figure&nbsp;<span>1.5</span></a> the scatterplot for this restricted sample.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prtime</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">prtime</span>, <span class="va">sr</span> <span class="op">&lt;</span> <span class="fl">0.75</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hsfsubsmpl" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-hsfsubsmpl-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.5: Share of rail in function of the threshold time value on a subsample</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Now, we consider the distribution of the values of time. If <span class="math inline">\(h\)</span> follows a given distribution between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, train’s market share is the share of the population for which the value of time is between <span class="math inline">\(a\)</span> and <span class="math inline">\(h^*\)</span> (and plane’s market share is the share of the population for which the value of time is between <span class="math inline">\(h^*\)</span> and <span class="math inline">\(b\)</span>). The simplest probability distribution is the uniform distribution, which is defined by a constant density equal to <span class="math inline">\(\frac{1}{b-a}\)</span> between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. It is represented in <a href="#fig-unifdist">Figure&nbsp;<span>1.6</span></a>. The area of the rectangle of width <span class="math inline">\([a,b]\)</span> and height <span class="math inline">\([0, \frac{1}{b-a}]\)</span> is 100%, because the whole population has a time value between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. This rectangle has two components:</p>
<ul>
<li>a first rectangle of width <span class="math inline">\([a, h ^ *]\)</span> which includes people for which time value is below <span class="math inline">\(h^*\)</span> and therefore take the train,</li>
<li>a second rectangle of width <span class="math inline">\([h ^*, b]\)</span> which includes people for which time value is higher than <span class="math inline">\(h^*\)</span> and therefore take the plane.</li>
</ul>
<p>Stated differently: <span class="math inline">\(s_f = \frac{h^* - a}{b - a} = -\frac{a}{b - a} + \frac{1}{b-a} h^*\)</span> and this model therefore predicts a linear relationship between <span class="math inline">\(h^*\)</span> and <span class="math inline">\(s_f\)</span>, the intercept being <span class="math inline">\(-\frac{a}{b - a}\)</span> and the slope <span class="math inline">\(\frac{1}{b - a}\)</span>. Of course, rail’s market share depends on other variables than the threshold value of time, so that the linear relationship concerns the conditional expectation of rail’s market share. With <span class="math inline">\(y=s_f\)</span> and <span class="math inline">\(x=h^*\)</span>, we therefore have a linear model of the form: <span class="math inline">\(\mbox{E}(y | x) = \alpha + \beta x\)</span>. Moreover, the two parameters to be estimated <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are functions of the structural parameters of the model <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> which are the minimum and the maximum of the value of time.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-unifdist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="./tikz/fig/price_time.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.6: Model shares with a uniform distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div style="page-break-after: always;"></div>
</section><section id="sec-comp_simple_ols" class="level2" data-number="1.3"><h2 data-number="1.3" class="anchored" data-anchor-id="sec-comp_simple_ols">
<span class="header-section-number">1.3</span> Computation of the OLS estimator</h2>
<p>The model we seek to estimate is: <span class="math inline">\(\mbox{E}(y_n \mid x_n) = \alpha+\beta x_n\)</span>. The difference between the observed value of <span class="math inline">\(y\)</span> and its conditional expectation is called the <strong>error</strong> for observation <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
y_n - \mbox{E}(y_n \mid x_n) = \epsilon_n
\]</span></p>
<p>The linear regression model can therefore be rewritten as:</p>
<p><span class="math display">\[
y_n = \mbox{E}(y_n \mid x_n) + \epsilon_n = \alpha + \beta x_n +  \epsilon_n
\]</span></p>
<p><span class="math inline">\(\epsilon_n\)</span> is the error for observation <span class="math inline">\(n\)</span> when the values of the unknown parameters <span class="math inline">\((\alpha, \beta)\)</span> are set to their true values <span class="math inline">\((\alpha_o, \beta_o)\)</span>. For given values of <span class="math inline">\((\alpha, \beta)\)</span>, obtained using an estimation method, <span class="math inline">\(\epsilon_n\)</span> will be called the <strong>residual</strong> for observation <span class="math inline">\(n\)</span>. The residual for an observation is therefore the vertical distance between the point for observation <span class="math inline">\(n\)</span> and the regression line. </p>
<p>We seek to draw a straight line that is as closest as possible to all the points of our sample as in <a href="#fig-hsfsubsmpl">Figure&nbsp;<span>1.5</span></a>. In the simple linear regression model, the distance between a point and the line is defined by the vertical distance, which is the residual for this observation. For the whole sample, we need to aggregate this individual measures of distance. Summing them is not an issue, as there are positive and negative values of the residuals and the sum may be very close to zero even if the individual residuals are very high in absolute values. One solution would be to use the sum of the absolute values of the residuals,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> but with the OLS estimator, we’ll consider the sum of the squares of the residuals (also called the residual sum of squares (<strong>RSS</strong>). Taking the squares, as taking the absolute values, removes the sign of the individual residuals and it results in an estimator which has nice mathematical and statistical properties. We’ll therefore consider a function <span class="math inline">\(f\)</span> which depends on the value of the response and the covariate in the sample (two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of length <span class="math inline">\(N\)</span>) and on two unknown parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, respectively the intercept and the slope of the regression line:</p>
<p><span class="math display">\[
f(\alpha, \beta |x, y)=\sum_{n = 1} ^ N (y_n - \alpha - \beta x_n) ^ 2
\]</span></p>
<p>Note that we write <span class="math inline">\(f\)</span> as a function of the two unknown parameters conditional on the values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> for a given sample. First-order conditions for the minimization of <span class="math inline">\(f\)</span> are:</p>
<p><span id="eq-gradientols"><span class="math display">\[
\left\{
\begin{array}{rcl}
\displaystyle \frac{\partial f}{\partial \alpha}  &amp;=&amp;
-2 \sum_{n = 1} ^ N \left(y_n - \alpha - \beta x_n\right) = 0 \\
\displaystyle \frac{\partial f}{\partial \beta}  &amp;=&amp;
-2\sum_{n=1}^N x_n\left(y_n-\alpha-\beta x_n\right)=0
\end{array}
\right.
\tag{1.5}\]</span></span></p>
<p>Or, dividing by <span class="math inline">\(-2\)</span>:</p>
<p><span id="eq-cpoalpha"><span class="math display">\[
\sum_{n = 1} ^ N\left(y_n - \alpha -\beta x_n\right) = \sum_{n = 1} ^
N \epsilon_n = 0
\tag{1.6}\]</span></span></p>
<p><span id="eq-cpobeta"><span class="math display">\[
\sum_{n = 1} ^ N x_n\left(y_n - \alpha - \beta x_n\right) = \sum_{n =
1} ^ N x_n \epsilon_n = 0
\tag{1.7}\]</span></span></p>
<p><a href="#eq-cpoalpha">Equation&nbsp;<span>1.6</span></a> indicates that the sum (or the mean) of the residuals in the sample is 0. Dividing this expression by <span class="math inline">\(N\)</span> also implies that, denoting <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> the sample means of the response and of the covariate:</p>
<p><span id="eq-cpoalpha2"><span class="math display">\[
\bar{y} = \alpha + \beta \bar{x},
\tag{1.8}\]</span></span></p>
<p>which means that the sample mean is on the regression line. Denoting <span class="math inline">\(\hat{\epsilon}_n\)</span> the residuals of the OLS estimator, <a href="#eq-cpobeta">Equation&nbsp;<span>1.7</span></a> states that <span class="math inline">\(\sum_n x_n \hat{\epsilon} / N=0\)</span>, i.e., that the average cross-product of the covariate and the residual is 0. But, as the sample mean of the residuals <span class="math inline">\(\bar{\hat{\epsilon}}\)</span> is 0, this expression is also the covariance between the covariate and the residuals:</p>
<p><span class="math display">\[
\hat{\sigma}_{x\hat{\epsilon}} = \frac{\sum_{n = 1} ^ N (x_n -
\bar{x})(\hat{\epsilon}_n - \bar{\hat{\epsilon}})}{N} =
\frac{\sum_{n = 1} ^ N x_n \hat{\epsilon}_n}{N} - \bar{x}\bar{\hat{\epsilon}}=
\frac{\sum_{n = 1} ^ N x_n \hat{\epsilon}_n}{N}=
0
\]</span></p>
<p>which means that the regression line is such that there is no correlation between the covariate and the residuals in the sample. Subtracting <span class="math inline">\(\bar{y} - \alpha - \beta \bar{x}\)</span>, which is 0 (see <a href="#eq-cpoalpha2">Equation&nbsp;<span>1.8</span></a>) from <a href="#eq-cpobeta">Equation&nbsp;<span>1.7</span></a>, one gets:</p>
<p><span class="math display">\[
\sum_{n = 1}^N x_n\left[\left(y_n - \bar{y}\right)-\beta \left(x_n -
\bar{x}\right)\right] = 0
\]</span></p>
<p>Moreover, <span class="math inline">\(\sum_{n = 1} ^ N \bar{x}\left[\left(y_n - \bar{y}\right) - \beta\left(x_n - \bar{x}\right)\right] = 0\)</span> and so:</p>
<p><span id="eq-normal_equation"><span class="math display">\[
\sum_{n=1} ^ N \left(x_n - \bar{x}\right)\left[\left(y_n -
\bar{y}\right) - \beta\left(x_n - \bar{x}\right)\right] = 0
\tag{1.9}\]</span></span></p>
<p>Solving for <span class="math inline">\(\beta\)</span>, we finally get the estimator of the slope:</p>
<p><span id="eq-slrbeta"><span class="math display">\[
\hat{\beta} = \frac{\sum_{n = 1} ^ N
(x_n - \bar{x})(y_n - \bar{y})}{\sum_{n = 1} ^ N
(x_n - \bar{x}) ^ 2} =
\frac{S_{xy}}{S_{xx}}=
\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x ^ 2}=
\hat{\rho}_{xy} \frac{\hat{\sigma}_y}{\hat{\sigma}_x}
\tag{1.10}\]</span></span></p>
<p><a href="#eq-slrbeta">Equation&nbsp;<span>1.10</span></a> gives three formulations for this estimator:</p>
<ul>
<li>the first indicates that it is the ratio of the covariation of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(S_{xy}=\sum_{n = 1} ^ N (x_n - \bar{x})(y_n - \bar{y})\)</span> and the variation of <span class="math inline">\(x\)</span>: <span class="math inline">\(S_{xx}= \sum_{n = 1} ^ N (x_n - \bar{x}) ^ 2\)</span>,</li>
<li>the second is obtained by dividing both sides of the ratio by the sample size, so that the estimator is now the ratio of the sample covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and the sample variance of <span class="math inline">\(x\)</span>,</li>
<li>the third is obtained by introducing the coefficient of correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(\hat{\rho}_{xy} =\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x\hat{\sigma}_y}\)</span>, so that the estimator is also expressed as the product of the coefficient of correlation and the ratio of the standard deviations.</li>
</ul>
<p>This last formulation is particularly intuitive: <span class="math inline">\(\hat{\rho}_{xy}\)</span> is a pure measure of the correlation between the covariate and the response. This number has no unit and lies in the <span class="math inline">\(-1\)</span>/<span class="math inline">\(+1\)</span> interval, a value of <span class="math inline">\(-1\)</span> (<span class="math inline">\(+1\)</span>) indicating a perfect negative (positive) correlation and the value of 0 no correlation. The ratio of the standard deviations gives the relevant unit to the slope, which is the unit of <span class="math inline">\(y\)</span> divided by the unit of <span class="math inline">\(x\)</span>. With this estimator of the slope in hand, we easily get the estimator of the intercept using <a href="#eq-cpoalpha2">Equation&nbsp;<span>1.8</span></a>:</p>
<p><span id="eq-slralpha"><span class="math display">\[
\hat{\alpha}=\bar{y}-\hat{\beta} \hat{x}
\tag{1.11}\]</span></span></p>
<p>Consider now that prior to the estimation, we sustracted from the covariate and the response their sample means. We then used <span class="math inline">\(\tilde{y}_n = y_n - \bar{y}\)</span> and <span class="math inline">\(\tilde{x}_n = x_n - \bar{x}\)</span> as the response and the covariate. In this case, as the mean of these two transformed variables are zero, the intercept is 0 (from <a href="#eq-slralpha">Equation&nbsp;<span>1.11</span></a>) and the slope is simply <span class="math inline">\(\sum \tilde{y}_n \tilde{x}_n / \sum \tilde{x}_n ^ 2\)</span>.</p>
<p>Once the parameters of the regression line have been computed, one can define the <strong>fitted value</strong> for observation <span class="math inline">\(n\)</span> as the value returned by the regression line for <span class="math inline">\(x_n\)</span>, which is:</p>
<p><span class="math display">\[
\hat{y}_n = \hat{\alpha} + \hat{\beta} x_n
\]</span></p>
<p>By definition, we have for the fitted model: <span class="math inline">\(y_n = \hat{y}_n + \hat{\epsilon}_n\)</span>. Note that, for the “true” model, we have <span class="math inline">\(y_n = \mbox{E}(y | x = x_n) + \epsilon_n\)</span>, so that the residuals are an estimation of the errors and the fitted values <span class="math inline">\(\hat{y}_n\)</span> are an estimation of the conditional expectations of <span class="math inline">\(y\)</span>. Moreover, denoting <span class="math inline">\(\bar{\hat{y}}\)</span> the sample mean of the fitted values:</p>
<p><span class="math display">\[
\frac{\sum_n (\hat{y}_n - \bar{\hat{y}})(\hat{\epsilon}_n -
\bar{\hat{\epsilon}})}{N}=
\frac{\sum_n \hat{y}_n \hat{\epsilon}_n}{N} -
\bar{\hat{y}}\bar{\hat{\epsilon}}
=
\frac{\sum_n \hat{y}_n \hat{\epsilon}_n}{N}
=
\frac{\sum_n (\hat{\alpha} + \hat{\beta} x_n) \hat{\epsilon}_n}{N}
= 0
\]</span></p>
<p>Therefore, there is no correlation between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span>, which is clear as <span class="math inline">\(\hat{y}\)</span> is a linear function of <span class="math inline">\(x\)</span> and <span class="math inline">\(x\)</span> is uncorrelated with <span class="math inline">\(\hat{\epsilon}\)</span>. <span class="math inline">\((\hat{\alpha}, \hat{\beta})\)</span> is an optimum of the objective function. To check that this optimum is a minimum, we have to compute the second derivatives. From <a href="#eq-gradientols">Equation&nbsp;<span>1.5</span></a>, we get: <span class="math inline">\(\frac{\partial^2 f}{\partial \alpha ^ 2}= 2N\)</span>, <span class="math inline">\(\frac{\partial^2 f}{\partial \beta ^ 2}= 2\sum_n x_n ^ 2\)</span> and <span class="math inline">\(\frac{\partial^2 f}{\partial \alpha \partial \beta}= 2\sum_n x_n\)</span>. We need, for a maximum, positive direct second derivatives, which is obviously the case, and also a positive determinant of the matrix of second derivatives:</p>
<p><span class="math display">\[
D = \frac{\partial^2 f}{\partial \alpha ^ 2}\frac{\partial^2 f}{\partial \beta ^ 2}
- \left(\frac{\partial^2 f}{\partial \alpha \partial \beta}\right)^2
= 4 N \sum_n x_n ^ 2 - 4  \left(\sum_n x_n\right) ^ 2 = 4N^2\left(\frac{\sum_n x_n ^ 2}{N} - \bar{x}^2\right)
&gt; 0
\]</span></p>
<p>which is the case as the term in brackets is the variance of <span class="math inline">\(x\)</span> and is therefore positive.</p>
</section><section id="sec-simple_geometry" class="level2" data-number="1.4"><h2 data-number="1.4" class="anchored" data-anchor-id="sec-simple_geometry">
<span class="header-section-number">1.4</span> Geometry of least squares, variance decomposition and of determination</h2>
<p> The OLS estimator relies on variances and covariances of several observable (<span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>) and computed (<span class="math inline">\(\hat{y}\)</span>, <span class="math inline">\(\hat{\epsilon}\)</span>) variables. Its properties can be nicely illustrated using vector algebra, each variable being represented by a vector, and by plotting these vectors.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<section id="vectors-variance-and-covariance" class="level3"><h3 class="anchored" data-anchor-id="vectors-variance-and-covariance">Vectors, variance and covariance</h3>
<p>Every variable <span class="math inline">\(z\)</span> used in a regression is a vector of <span class="math inline">\(\mathcal{R}_N\)</span>, i.e., a set of <span class="math inline">\(N\)</span> real values: <span class="math inline">\(z^\top = (z_1, z_2, \ldots, z_N)\)</span>. The length (or norm) of the vector is: <span class="math inline">\(\|z\| = \sqrt{\sum_{n=1}^N z_n ^ 2}\)</span>. Remember that the OLS estimator can always be computed with data measured in deviations from their sample mean. Then, <span class="math inline">\(\|z\| ^ 2 / N\)</span> is the variance of the variable, or the norm of the vector is <span class="math inline">\(\sqrt{N}\)</span> times the standard deviation of the corresponding variable. The inner (or scalar) product of two vectors is denoted by <span class="math inline">\(z ^ \top w = w ^ \top z = \sum_{n=1} ^ N z_n w\index[author]{Davidson}\index[author]{McKinnon}_n\)</span> (note that the inner product is commutative). For corresponding variables expressed in deviations from their respective means it is, up to the <span class="math inline">\(1/N\)</span> factor, the covariance between the two variables. Denoting <span class="math inline">\(\theta\)</span> the angle formed by the two vectors, we also have:<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="math inline">\(z ^ \top w = \cos \theta \|z\| \|w\|\)</span>.</p>
<div class="cell" data-layout-align="center">

</div>
<p>Consider as an example: <span class="math inline">\(x = ( 4 , 3 )\)</span>, <span class="math inline">\(z = ( 4.5 , 6 )\)</span> and <span class="math inline">\(w = ( -6 , 4.5 )\)</span>. The three vectors are plotted in <a href="#fig-mafig">Figure&nbsp;<span>1.7</span></a>. The norm of <span class="math inline">\(x\)</span> is <span class="math inline">\(\|x\|=\sqrt{ 4 ^ 2 + 3 ^ 2} = 5\)</span>. Similarly, <span class="math inline">\(\|z\| = 7.5\)</span> and <span class="math inline">\(\|w\| = 7.5\)</span>. The cosinus of the angle formed by <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> with the horizontal axis is <span class="math inline">\(\cos \theta_x = 4 / 5 = 0.8\)</span> and <span class="math inline">\(\cos \theta_z = 4.5 / 7.5 = 0.6\)</span>. The angle formed by <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> is therefore: <span class="math inline">\(\theta = \arccos 0.6 - \arccos 0.8 = 0.284\)</span>, with <span class="math inline">\(\cos 0.284 = 0.96\)</span>. We can then check that <span class="math inline">\(z ^ \top x = 4 \times 4.5 + 3 \times 6 = 36\)</span>, which is equal to: <span class="math inline">\(\cos \theta \|z\| \|w\| = 0.96 \times 7.5 \times 5\)</span>. As the absolute value of <span class="math inline">\(\cos \theta\)</span> is necessarily lower than or equal to 1, the inner product of two vectors is lower than the product of the norms of the two vectors, and <span class="math inline">\(\cos \theta = \frac{x ^ \top z}{\|x\| \|z\|}\)</span> is the ratio of the covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> and the product of their standard deviations, which is the coefficient of correlation between the two underlying variables <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>. Consider now <span class="math inline">\(z\)</span> and <span class="math inline">\(w\)</span>. Their inner product is: <span class="math inline">\(z ^ \top x = 4.5 \times -6 + 6 \times 4.5 = 0\)</span>. This is because <span class="math inline">\(z\)</span> and <span class="math inline">\(w\)</span> are two orthogonal vectors, which means that the two underlying variables are uncorrelated.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mafig" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="./tikz/fig/vectors2D.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.7: Vector algebra</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-geometry_ols" class="level3"><h3 class="anchored" data-anchor-id="sec-geometry_ols">Geometry of least squares</h3>
<p>The geometry of the simple linear regression model is represented in <a href="#fig-smplregmodel">Figure&nbsp;<span>1.8</span></a>. With <span class="math inline">\(N = 2\)</span>, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are two vectors in a plane. For the “true” model, the <span class="math inline">\(y\)</span> vector is the sum of two vectors: <span class="math inline">\(\beta x\)</span> (which is the conditional expectation of <span class="math inline">\(y\)</span>) and <span class="math inline">\(\epsilon\)</span>, which is the vector of errors. For the estimated model, <span class="math inline">\(y\)</span> is the sum of the fitted values <span class="math inline">\(\hat{y} = \hat{\beta} x\)</span> and the residuals <span class="math inline">\(\hat{\epsilon}\)</span>. Using the OLS estimator, we must minimize the sum of squares of the residuals, i.e., the norm of the <span class="math inline">\(\hat{\epsilon}\)</span> vector. Obviously, this implies that <span class="math inline">\(\hat{\epsilon}\)</span> should be orthogonal to <span class="math inline">\(\hat{y}\)</span> and therefore to <span class="math inline">\(x\)</span>, which implies that the residuals are uncorrelated to the fitted values (and to the covariate) in the sample. Note also that, except in the unlikely case where <span class="math inline">\(\hat{\beta} = \beta\)</span>, <span class="math inline">\(\|\hat{\epsilon}\| &lt; \|\epsilon\|\)</span> which means that the residuals have a smaller variance than the errors. Note finally that what determines <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span> is not <span class="math inline">\(x\)</span> per se, but the subspace defined by it, in our case the horizontal straight line. For example, consider the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(z = 0.5 x\)</span>; we would then obtain exactly the same values for <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span>, the only difference being that the estimator of <span class="math inline">\(\beta\)</span> would be two times larger.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-smplregmodel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="./tikz/fig/OLS2D.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.8: Geometry of the simple regression model</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p>
</section><section id="sec-vardecomp_R2" class="level3"><h3 class="anchored" data-anchor-id="sec-vardecomp_R2">Variance decomposition and the <span class="math inline">\(R^2\)</span>
</h3>
<p></p>
<p>For one observation <span class="math inline">\(n\)</span>, we have:</p>
<p><span id="eq-varobs"><span class="math display">\[
y_n - \bar{y}=\left(y_n - \hat{y}_n\right)+\left(\hat{y}_n - \bar{y}\right)
\tag{1.12}\]</span></span></p>
<p>The difference between <span class="math inline">\(y\)</span> for individual <span class="math inline">\(n\)</span> and the sample mean is therefore the sum of:</p>
<ul>
<li>a residual variation: <span class="math inline">\(\left(y_n - \hat{y}_n\right) = \hat{\epsilon}_n\)</span>,</li>
<li>an explained variation: <span class="math inline">\(\hat{y}_n - \bar{y}=\hat{\beta}(x_n - \bar{x})\)</span>.</li>
</ul>
<p>Taking the square of <a href="#eq-varobs">Equation&nbsp;<span>1.12</span></a> and summing for all <span class="math inline">\(n\)</span>, we get:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\sum_{n = 1} ^ N(y_n - \bar{y})^2 &amp; = &amp; \sum_{n = 1}^ N
\left(\hat{\epsilon}_n + \hat{\beta}(x_n - \bar{x})\right) ^ 2 \\
&amp; = &amp; \sum_{n = 1} ^ N \hat{\epsilon}_n ^ 2 + \hat{\beta} ^ 2 \sum_{n
= 1} ^ N (x_n - \bar{x}) ^ 2 \\
&amp; + &amp; 2 \hat{\beta}\sum_{n = 1} ^ N \hat{\epsilon}_n x_n -
2\hat{\beta}\bar{x}\sum_{n = 1} ^ N \hat{\epsilon}_n
\end{array}
\]</span></p>
<p>But <span class="math inline">\(\sum_{n = 1} ^ N \hat{\epsilon}_n x_n = 0\)</span> (<a href="#eq-cpobeta">Equation&nbsp;<span>1.7</span></a>) and <span class="math inline">\(\sum_{n = 1} ^ N \hat{\epsilon}_n = 0\)</span> (<a href="#eq-cpoalpha">Equation&nbsp;<span>1.6</span></a>), so that:</p>
<p><span id="eq-variance_decomposition"><span class="math display">\[
\sum_{n = 1} ^ N (y_n - \bar{y}) ^ 2 = \hat{\beta} ^ 2
\sum_{n = 1} ^ N (x_n - \bar{x}) ^ 2+\sum_{n = 1} ^ N
\hat{\epsilon}_n ^ 2
\tag{1.13}\]</span></span></p>
<p>This equation indicates that the total sum of squares (TSS) of the response equals the sum of the explained sum of squares (ESS) and of the residual sum of squares (RSS). This latter term is also called the <strong>deviance</strong>, and it is the objective function for the OLS estimator. Dividing by <span class="math inline">\(N\)</span>, we also have on the left-hand size the variance of <span class="math inline">\(y\)</span> and on the right-hand side the sum of the variances of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span>. This is the formula of the <strong>variance decomposition</strong> of the response. It can be easily understood using <a href="#fig-smplregmodel">Figure&nbsp;<span>1.8</span></a>. It is clear from this figure that <span class="math inline">\(y = \hat{y} + \hat{\epsilon}\)</span> and that <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span> are orthogonal. Therefore, applying the Pythagorean theorem, we have <span class="math inline">\(\|y\|^2 = \|\hat{y}\|^2 + \|\hat{\epsilon}\|^2\)</span>. Up to the <span class="math inline">\(1/N\)</span> factor, if the response is measured in deviation from its sample mean, we have on the left the total variance of <span class="math inline">\(y\)</span> and on the right the sum of the variances of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span>. Note that it is a unique feature of the ordinary least squares estimator. Any other estimator will generally result with a vector of residuals which won’t be orthogonal to <span class="math inline">\(\hat{y}\)</span> (or to <span class="math inline">\(x\)</span>) and therefore <a href="#eq-variance_decomposition">Equation&nbsp;<span>1.13</span></a> won’t apply.</p>
<p>The coefficient of determination, denoted by <span class="math inline">\(R^2\)</span>, measures the share of the variance of the response which is explained by the model, or one minus the share of the residual variation. We then have:</p>
<p><span id="eq-r2_slrm"><span class="math display">\[
R^2=\frac{\hat{\beta}^2\sum_{n=1}^N(x_n-\bar{x})^2}{\sum_{n=1}^N (y_n-\bar{y})^2} = 1 - \frac{\sum_{n=1}^N\hat{\epsilon}_n^2}{\sum_{n=1}^N (y_n-\bar{y})^2}
\tag{1.14}\]</span></span></p>
<p>using <a href="#eq-slrbeta">Equation&nbsp;<span>1.10</span></a>, we finally get:</p>
<p><span class="math display">\[
  R^2=\left[\frac{\sum_{n=1}^N(x_n-\bar{x})(y_n-\bar{y})}{\sum_{n=1}^N
    (x_n-\bar{x})^2}\right]^2
\frac{\sum_{n=1}^N(x_n-\bar{x})^2}{\sum_{n=1}^N (y_n-\bar{y})^2} =
\left[\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x ^ 2}\right] ^ 2
\left[\frac{\hat{\sigma}_x ^ 2}{\hat{\sigma}_y ^ 2}\right] =
\frac{\hat{\sigma}_{xy} ^ 2}{\hat{\sigma}_x ^ 2\hat{\sigma}_y ^ 2}
=\hat{\rho}_{xy} ^ 2
\]</span></p>
<p><span class="math inline">\(R^2\)</span> is therefore simply the square of the coefficient of correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We have seen previously that, denoting <span class="math inline">\(\theta\)</span> the angle formed by two vectors, <span class="math inline">\(\cos \theta\)</span> is the coefficient of correlation between the two underlying variables (if they are measured in deviations from their means). Therefore, in <a href="#fig-smplregmodel">Figure&nbsp;<span>1.8</span></a>, <span class="math inline">\(R^2\)</span> is represented by the square of the cosine of the angle formed by the two vectors <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>. As this angle tends to 0 (the two vectors point almost in the same direction), <span class="math inline">\(R^2\)</span> tends to 1. On the contrary, if this angle tends to <span class="math inline">\(\pi/2\)</span>, the two vectors become almost orthogonal, and <span class="math inline">\(R^2\)</span> tends to 0. </p>
<div style="page-break-after: always;"></div>
</section></section><section id="sec-comp_ols_simple_R" class="level2" data-number="1.5"><h2 data-number="1.5" class="anchored" data-anchor-id="sec-comp_ols_simple_R">
<span class="header-section-number">1.5</span> Computation with R</h2>
<p>The slope of the regression line can easily be computed “by hand”, using any of the formula indicated in <a href="#eq-slrbeta">Equation&nbsp;<span>1.10</span></a>, using the <code><a href="https://dplyr.tidyverse.org/reference/summarise.html">dplyr::summarise</a></code> function. We first compute the variations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and the covariation of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the two standard deviations and the coefficient of correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">stats</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">summarise</span><span class="op">(</span>N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">prtime</span><span class="op">)</span>, xb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">h</span><span class="op">)</span>, yb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sr</span><span class="op">)</span>,</span>
<span>              Sxx <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">h</span> <span class="op">-</span> <span class="va">xb</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>, Syy <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">sr</span> <span class="op">-</span> <span class="va">yb</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>              Sxy <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">h</span> <span class="op">-</span> <span class="va">xb</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">sr</span> <span class="op">-</span> <span class="va">yb</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              sx <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">Sxx</span> <span class="op">/</span> <span class="va">N</span><span class="op">)</span>, sy <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">Syy</span> <span class="op">/</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>              sxy <span class="op">=</span> <span class="va">Sxy</span> <span class="op">/</span> <span class="va">N</span>, rxy <span class="op">=</span> <span class="va">sxy</span> <span class="op">/</span> <span class="op">(</span><span class="va">sx</span> <span class="op">*</span> <span class="va">sy</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then compute the estimator of the slope using any of the three formulas:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hbeta</span> <span class="op">&lt;-</span> <span class="va">stats</span><span class="op">$</span><span class="va">Sxy</span> <span class="op">/</span> <span class="va">stats</span><span class="op">$</span><span class="va">Sxx</span></span>
<span><span class="va">hbeta</span></span>
<span><span class="co">## [1] 0.02393</span></span>
<span><span class="va">stats</span><span class="op">$</span><span class="va">sxy</span> <span class="op">/</span> <span class="va">stats</span><span class="op">$</span><span class="va">sx</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="co">## [1] 0.02393</span></span>
<span><span class="va">stats</span><span class="op">$</span><span class="va">rxy</span> <span class="op">*</span> <span class="va">stats</span><span class="op">$</span><span class="va">sy</span> <span class="op">/</span> <span class="va">stats</span><span class="op">$</span><span class="va">sx</span></span>
<span><span class="co">## [1] 0.02393</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The estimation of the intercept is obtained using <a href="#eq-slralpha">Equation&nbsp;<span>1.11</span></a>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">halpha</span> <span class="op">&lt;-</span> <span class="va">stats</span><span class="op">$</span><span class="va">yb</span> <span class="op">-</span> <span class="va">hbeta</span> <span class="op">*</span> <span class="va">stats</span><span class="op">$</span><span class="va">xb</span></span>
<span><span class="va">halpha</span></span>
<span><span class="co">## [1] -0.01548</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Much more simply, the OLS estimator can be obtained using the <code>lm</code> function (for linear model). It is a very important function in <strong>R</strong>, not only because it implements efficiently the most important estimator used in econometrics, but also because <strong>R</strong> functions that implement other estimators often mimic the features of the <code>lm</code> function. Therefore, once one is at ease with using the <code>lm</code> function, using other estimating function of <strong>R</strong> will be straightforward. <code>lm</code> is a function that has many arguments, but the first two are fundamental and almost mandatory:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<ul>
<li>
<code>formula</code> is a symbolic description of the model to be estimated,</li>
<li>
<code>data</code> is a data frame that contains the variables used in the formula.</li>
</ul>
<p>Here, our formula writes <code>sr ~ h</code>, which means <code>sr</code> as a function of <code>h</code>. The data frame is <code>prtime</code>. The result of the <code>lm</code> function may be directly printed (the result is then lost), or saved in an object, which can be later printed or manipulated:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span>, <span class="va">prtime</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = sr ~ h, data = prtime)

Coefficients:
(Intercept)            h  
    -0.0155       0.0239  </code></pre>
</div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">h</span>, <span class="va">prtime</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>writing directly <code>pxt</code> is like writing <code>print(pxt)</code>, the side effect is to print a short description of the results, namely a remind of the function call and the name and the values of the fitted coefficients. <code>lm</code> returns an object of class <code>lm</code> which is a list of 12 elements; their names can be retrieved using the <code>names</code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        </code></pre>
</div>
</div>
<p>An element of this list can be extracted using the <code>$</code> operator. For example:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pxt</span><span class="op">$</span><span class="va">coefficients</span></span>
<span><span class="co">## (Intercept)           h </span></span>
<span><span class="co">##    -0.01548     0.02393</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>returns the named vector of coefficients. <code>pxt$residuals</code> and <code>pxt$fitted.values</code> return two vectors of length <span class="math inline">\(N\)</span> containing the residuals and the fitted values. However, it is not advised to use the <code>$</code> operator to retrieve the elements of a fitted model. Specific functions, called extractors, should be used instead. For example, to retrieve the coefficients, the residuals and the fitted values as previously, we would use:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are other functions that extract important information about the model, as the number of observation (<code>nobs</code>) and the sum of square residuals (<code>deviance</code>):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="co">## [1] 9</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.03606</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The results of the estimation are presented in <a href="#fig-reserror">Figure&nbsp;<span>1.9</span></a>; we’ve added to the scatterplot:</p>
<ul>
<li>the sample mean, indicated by a large circle,</li>
<li>the regression line,</li>
<li>the residuals, represented by arrows: upward arrows represent positive residuals (e.g., Brest and Toulon), and downward arrows represent negative residuals (e.g., Strasbourg and Nice).</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-reserror" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-reserror-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.9: Regression line and residuals</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Individual coefficients can be extracted using the <code>[</code> operator. As <code>coef</code> returns a named vector, one can either indicate the position or the name of the coefficient to be extracted:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">int</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unname.html">unname</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">slope</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unname.html">unname</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pxt</span><span class="op">)</span><span class="op">[</span><span class="st">"h"</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">int</span>, <span class="va">slope</span><span class="op">)</span></span>
<span><span class="co">## [1] -0.01548  0.02393</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we use the <code>unname</code> function in order to remove the name of the extracted coefficient. Once the intercept (<code>int</code>) and the slope (<code>slope</code>) are extracted, the structural parameters can be retrieved, as the intercept is <span class="math inline">\(\alpha = -\frac{a}{b-a}\)</span> and the slope <span class="math inline">\(\beta =\frac{1}{b-a}\)</span>. Therefore, <span class="math inline">\(a = -\frac{\alpha}{\beta}\)</span> and <span class="math inline">\(b = \frac{1}{\beta} + a\)</span>. We finally get:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ahat</span> <span class="op">=</span> <span class="op">-</span> <span class="va">int</span> <span class="op">/</span> <span class="va">slope</span></span>
<span><span class="va">bhat</span> <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">slope</span> <span class="op">+</span> <span class="va">ahat</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">ahat</span>, <span class="va">bhat</span><span class="op">)</span></span>
<span><span class="co">## [1]  0.647 42.431</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Time values therefore lie between 0.65 and 42.43 euros per hour. The mean (and median) time value is the mean of the two extreme values, which is 21.54 euros per hour.</p>
</section><section id="sec-dgp_simulations" class="level2" data-number="1.6"><h2 data-number="1.6" class="anchored" data-anchor-id="sec-dgp_simulations">
<span class="header-section-number">1.6</span> Data generator process and simulations</h2>
<p>Inferential statistics rely on the notions of population and sample. The population is a large and exhaustive set of observations, and a sample is a small subset of observations drawn in this population. These notions are relevant in medical sciences and often in economic studies. For example, the first data set we used concerned smoking habits and birth weight. The population of interest is all American pregnant women in 1988, and a sample of 1388 of them was drawn from this population. The second data set concerned the relation between education and wage. The population was American labor force in 1992, and a sample of 16,481 workers was drawn from this population. On the contrary, our third data set doesn’t fit with these notions of population and sample. The sample consists of major towns in France that are connected on a regular basis by rail and air to Paris. It contains 13 observations, which are not 13 observations randomly drawn from a large set of cities, but which are more or less all the relevant cities.</p>
<section id="data-generator-process" class="level3"><h3 class="anchored" data-anchor-id="data-generator-process">Data generator process</h3>
<p></p>
<p>An interesting alternative is the notion of <strong>data generator process</strong> (<strong>DGP</strong>). It describes how the data are assumed to have been generated. We assume in the linear regression model that:</p>
<ul>
<li>
<span class="math inline">\(\mbox{E}(y|x = x_n) = \alpha + \beta x_n\)</span>: the expected value of <span class="math inline">\(y\)</span> for a given value of <span class="math inline">\(x\)</span> is a linear function of <span class="math inline">\(x\)</span>,</li>
<li>
<span class="math inline">\(y_n = \mbox{E}(y|x = x_n) + \epsilon_n\)</span>: the observed value of <span class="math inline">\(y_n\)</span> is obtained by adding to the conditional expectation of <span class="math inline">\(y\)</span> for <span class="math inline">\(x = x_n\)</span> a random variable <span class="math inline">\(\epsilon\)</span> called the error.</li>
</ul>
<p>From <a href="#eq-slrbeta">Equation&nbsp;<span>1.10</span></a>, we can write the estimator of the slope as:</p>
<p><span id="eq-linear_estimator"><span class="math display">\[
\hat{\beta} = \frac{\sum_{n = 1} ^ N   (x_{n} - \bar{x})(y_{n} - \bar{y})}
{\sum_{n = 1} ^ N   (x_{n} - \bar{x})^ 2}
= \sum_{n = 1} ^ N \frac{(x_{n} - \bar{x})}{\sum_{n = 1} ^ N
(x_{n} - \bar{x}) ^ 2} y_{n}
= \sum_{n} c_{n} y_{n}
\tag{1.15}\]</span></span></p>
<p>with <span class="math inline">\(c_{n} = \frac{x_{n} - \bar{x}}{\sum_{n = 1} ^ N (x_{n} - \bar{x}) ^ 2}\)</span>. The OLS estimator is therefore a linear estimator, i.e., a linear combination of the values of <span class="math inline">\(y\)</span>. The coefficients of this linear combination <span class="math inline">\(c_n\)</span> are such that <span class="math inline">\(\sum_{n = 1} ^ N c_{n} = 0\)</span> and that:</p>
<p><span class="math display">\[
\sum_{n = 1} ^ N  c_{n} ^ 2 = \frac{\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2}
{\left(\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2\right) ^ 2}  =
\frac{1}{\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2} = \frac{1}{N\hat{\sigma}_x^2}
\]</span></p>
<p>Replacing <span class="math inline">\(y_{n}\)</span> by <span class="math inline">\(\alpha + \beta x_{n} + \epsilon_{n}\)</span>, we then express <span class="math inline">\(\hat{\beta}\)</span> as a function of <span class="math inline">\(\epsilon_n\)</span>:</p>
<p><span class="math display">\[
\hat{\beta}=\sum_{n = 1} ^ N c_{n} (\alpha + \beta
x_{n} + \epsilon_{n}) = \alpha \sum_{n = 1} ^ N c_{n}+\beta\sum_{n = 1}
^ N \frac{x_{n}(x_{n} - \bar{x})}{\sum_{n = 1} ^ N
(x_{n} - \bar{x}) ^ 2}+\sum_{n = 1} ^ N c_{n} \epsilon_{n}
\]</span></p>
<p>As <span class="math inline">\(\sum_{n = 1} ^ N x_{n}(x_{n} - \bar{x}) = \sum_{n = 1} ^ N (x_{n} - \bar{x}) ^ 2\)</span> and <span class="math inline">\(\sum_{n = 1} ^ N c_{n} = 0\)</span>, we finally get:</p>
<p><span id="eq-ols_lin_comb_errors"><span class="math display">\[
\hat{\beta} = \beta + \sum_{n = 1} ^ N c_{n} \epsilon_{n}
\tag{1.16}\]</span></span></p>
<div style="page-break-after: always;"></div>
<!-- new page -->
<p>The deviation of the estimator of the slope of the OLS regression line <span class="math inline">\(\hat{\beta}\)</span> from the true value <span class="math inline">\(\beta\)</span> is therefore a linear combination of the <span class="math inline">\(N\)</span> errors. Consider our sample used to estimate the price-time model. From a DGP perspective, this sample has been generated using the formula: <span class="math inline">\(y=\alpha + \beta x + \epsilon\)</span>. Consider now that the “true” values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are <span class="math inline">\(-0.2\)</span> and <span class="math inline">\(0.032\)</span>, we can in this case compute the vector of errors for our sample (<span class="math inline">\(\epsilon = y - \alpha - \beta x\)</span>):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fl">0.2</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fl">0.032</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sr</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">h</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">=</span> <span class="va">y</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">-</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then compute the OLS estimator, and we retrieve <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="va">hat_eps</span> <span class="op">&lt;-</span> <span class="va">z</span> <span class="op">%&gt;%</span> <span class="va">residuals</span></span>
<span><span class="va">hat_y</span> <span class="op">&lt;-</span> <span class="va">z</span> <span class="op">%&gt;%</span> <span class="va">fitted</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The observed data set, in the DGP perspective, is represented in <a href="#fig-dgp">Figure&nbsp;<span>1.10</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-dgp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-dgp-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.10: Data generating process</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The “true” model is represented by the plain line. The errors are represented by the plain arrows (positive errors for upward arrows, negative errors for downward arrows). Each value of <span class="math inline">\(y_n\)</span> is the sum of the conditional expectation of <span class="math inline">\(y\)</span> for the value of <span class="math inline">\(x\)</span>: <span class="math inline">\(E(y|x = x_n) = \alpha + \beta x_n\)</span> (the value returned by the plain line for the given value of <span class="math inline">\(x\)</span>) and the error <span class="math inline">\(\epsilon_n\)</span> represented by the plain arrow. For our specific sample, we have a specific vector of <span class="math inline">\(\epsilon_n\)</span>, which means a specific set of points and a specific regression line, the dashed line on the figure. Each value of <span class="math inline">\(y_n\)</span> is then also the sum of the fitted value (the one returned by the regression line for <span class="math inline">\(x=x_n\)</span>) and the residual, represented by a dashed arrow.</p>
<p>We can check that the residuals sum to 0 and are uncorrelated with the covariate and the fitted values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hat_eps</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.735e-18</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hat_eps</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.556e-16</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hat_eps</span> <span class="op">*</span> <span class="va">hat_y</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.903e-18</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which is not the case for the errors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eps</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.3818</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">## [1] 4.036</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">*</span> <span class="va">hat_y</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.09069</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can also check that the residuals are “smaller” than the errors by computing their standard deviations:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">eps</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.08496</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">hat_eps</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.06713</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section><section id="sec-random_numbers_simulations" class="level3"><h3 class="anchored" data-anchor-id="sec-random_numbers_simulations">Random numbers and simulations</h3>
<p></p>
<p>Now consider an other fictive sample for the same values of <span class="math inline">\(x\)</span>, i.e., consider that the values of the threshold values of time are the same, but that the other factors that influence rail’s shares (the <span class="math inline">\(\epsilon\)</span>) are different. To generate such a fictive sample, we need to generate the <span class="math inline">\(\epsilon\)</span> vector, using a function that generates random numbers.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> With <strong>R</strong>, these functions have a name composed of the letter <code>r</code> (for random) and the abbreviated name of the statistical distribution: for example <code>runif</code> draws numbers in a uniform distribution (by default within a <span class="math inline">\(0-1\)</span> range) and <code>rnorm</code> draws numbers in a normal distribution (by default with zero expectation and unit standard deviation). These functions have a mandatory argument which is the number of draws. For example, to get five numbers drawn from a standard normal distribution: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">## [1]  0.01875 -0.18425 -1.37133 -0.59917  0.29455</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the same command once more, we get a completely different sequence:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">## [1]  0.3898 -1.2081 -0.3637 -1.6267 -0.2565</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As stated previously, the <code>rnorm</code> function doesn’t draw random numbers but computes a sequence of numbers that looks like a random sequence. Imagine that <code>rnorm</code> actually computes a sequence of thousands of numbers, what is obtained using <code>rnorm(5)</code> is 5 consecutive numbers in this sequence, for example from the 5107th to the 5111th number. The position of the first element is called the <strong>seed</strong>, and it can be set to an integer using the <code>set.seed</code> function. Using the same seed while starting a simulation, we would then get exactly the same pseudo-random numbers each time and therefore the same results:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">7L</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">## [1]  2.2872 -1.1968 -0.6943 -0.4123 -0.9707</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">## [1] -0.9473  0.7481 -0.1170  0.1527  2.1900</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">7L</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">## [1]  2.2872 -1.1968 -0.6943 -0.4123 -0.9707</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">## [1] -0.9473  0.7481 -0.1170  0.1527  2.1900</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> The DGP is completely described by specifying the distribution of <span class="math inline">\(\epsilon\)</span>. We’ll consider here a normal distribution with mean 0 and standard deviation <span class="math inline">\(\sigma_\epsilon = 0.08\)</span>. A pseudo-random sample can then be constructed as follow:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="va">nrow</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">prtime</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">h</span><span class="op">)</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fl">0.2</span> ; <span class="va">beta</span> <span class="op">&lt;-</span> <span class="fl">0.032</span> ; <span class="va">seps</span> <span class="op">&lt;-</span> <span class="fl">0.08</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, sd <span class="op">=</span> <span class="va">seps</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span></span>
<span><span class="va">asmpl</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span><span class="va">y</span>, <span class="va">x</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which gives the following OLS estimates:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">asmpl</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">## (Intercept)           x </span></span>
<span><span class="co">##    -0.25218     0.03841</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The notion of DGP enables to perform simulations, that have two purposes:</p>
<ul>
<li>the first (and the most important in practice) is that, in some situations, it is impossible to get analytical results for the properties of an estimator, and those properties can in this case be obtained using simulations,</li>
<li>the second is pedagogical, as theoretical results can be confirmed and illustrated using simulations.</li>
</ul>
<p>A simulation consists of creating a large number of samples, computing some interesting numbers for every sample and calculating some relevant statistics for these numbers. For example, we’ll compute the slope of the OLS estimate for every sample, and we’ll calculate the mean and the standard deviation for this slope. A minimal simulation is presented in <a href="#fig-foursmpls">Figure&nbsp;<span>1.11</span></a>, where we present four different samples obtained for four different sets of errors.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-foursmpls" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="simple_regression_files/figure-html/fig-foursmpls-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.11: 4 different samples</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For every sample, there is a specific set of errors (arrows) and therefore specific data points and regression lines. The estimated intercepts range from <span class="math inline">\(-0.212\)</span> to <span class="math inline">\(-0.015\)</span> and the slopes from <span class="math inline">\(0.024\)</span> to <span class="math inline">\(0.032\)</span>. The important point is that <span class="math inline">\(\beta\)</span> is, in practice, an unknown fixed parameter. <span class="math inline">\(\hat{\beta}\)</span> depends on the <span class="math inline">\(N\)</span> observations of the sample and therefore the <span class="math inline">\(N\)</span> values of the errors. Each sample is characterized by a specific vector of errors and therefore by a different value of the estimator. </p>
<p>More generally, we denote <span class="math inline">\(R\)</span> the number of replications and we construct a data frame with <span class="math inline">\(R \times N\)</span> lines. The columns are the 13 values of <span class="math inline">\(x\)</span> (fixed in this simulation), the vector <span class="math inline">\(\epsilon\)</span> drawn in a normal distribution with a standard deviation equal to 0.08 and <span class="math inline">\(y = \alpha + \beta x + \epsilon\)</span>, with <span class="math inline">\(\alpha = 0.2\)</span> and <span class="math inline">\(\beta = 0.032\)</span> as previously. Finally we add a variable <code>id</code> that is a vector containing integers from 1 to <span class="math inline">\(R\)</span> repeated 13 times that will be used to identify the sample:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fl">0.2</span> ; <span class="va">beta</span> <span class="op">&lt;-</span> <span class="fl">0.032</span> ; <span class="va">seps</span> <span class="op">&lt;-</span> <span class="fl">0.08</span></span>
<span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fl">1E03</span> ; <span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">datas</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>                x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">R</span><span class="op">)</span>, </span>
<span>                eps <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">R</span> <span class="op">*</span> <span class="va">N</span>, sd <span class="op">=</span> <span class="va">seps</span><span class="op">)</span>,</span>
<span>                y <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Running regressions on every sample requires to use <code>lm(y ~ x)</code> not on the whole data set, but on every subset defined by a value of <code>id</code>. The <code>lm</code> function has a <code>subset</code> argument that must be a logical expression used to select only a subset of the data frame. For example, to get the fitted model for the first sample, the logical expression is <code>id == 1</code> and one can use:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">datas</span>, subset <span class="op">=</span> <span class="va">id</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The slope can also be calculated using <a href="#eq-linear_estimator">Equation&nbsp;<span>1.15</span></a>, which shows that the OLS estimator is a linear combination of the values of the response:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datas</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">id</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>cn <span class="op">=</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span>slope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">cn</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">pull</span></span>
<span><span class="co">## [1] 0.03287</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To get values of the estimator for every sample, we have to loop on this command for <code>id</code> from 1 to <code>R</code>. A good practice in <strong>R</strong> is to avoid the use of explicit loops whenever it’s possible. The <code>group_by</code> / <code>summarise</code> couple of functions from the <code>dplyr</code> package can be used instead. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="va">datas</span>  <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">id</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>cn <span class="op">=</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span>slope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">cn</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span>, intercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">slope</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">results</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1,000 × 3
     id  slope intercept
  &lt;int&gt;  &lt;dbl&gt;     &lt;dbl&gt;
1     1 0.0329    -0.166
2     2 0.0309    -0.181
3     3 0.0348    -0.213
4     4 0.0267    -0.129
5     5 0.0270    -0.127
# ℹ 995 more rows</code></pre>
</div>
</div>
<p>The result is now a tibble with <code>R</code> lines, as the computation of the OLS estimator is performed for every sample. As we now have a large number of values of <span class="math inline">\(\hat{\beta}\)</span>, we can analyze its statistical properties, for example its mean and its standard deviation for the 1000 random samples: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">results</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
    mean      sd
   &lt;dbl&gt;   &lt;dbl&gt;
1 0.0320 0.00437</code></pre>
</div>
</div>
<p>We’ll see in <a href="simple_regression_properties.html"><span>Chapter&nbsp;2</span></a> that the expected value of <span class="math inline">\(\hat{\beta}\)</span> is <span class="math inline">\(\beta=0.032\)</span> (we’ll say that the OLS estimator is unbiased) and that its standard deviation is <span class="math inline">\(\sigma_\epsilon / \sigma_x / \sqrt{N} = 0.0041\)</span>. The mean and standard deviations for our 1000 values of <span class="math inline">\(\hat{\beta}\)</span> are estimates of these two theoretical values. We can see that the mean of <span class="math inline">\(\hat{\beta}\)</span> is actually equal to 0.032 and that the standard deviation equals 0.00437, which is close to the theoretical value.</p>
<p>A more general method to perform simulation is to use list-columns of data frames. To conduct such an analysis step by step, we start with a very small example, with <span class="math inline">\(R = 2\)</span> and <span class="math inline">\(N = 3\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fl">2</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">small_data</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span>,</span>
<span>                     x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">]</span>, <span class="va">R</span><span class="op">)</span>, </span>
<span>                     eps <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">R</span> <span class="op">*</span> <span class="va">N</span>, sd <span class="op">=</span> <span class="va">seps</span><span class="op">)</span>,</span>
<span>                     y <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span><span class="op">)</span></span>
<span><span class="va">small_data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 4
     id     x     eps     y
  &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
1     1  26.7 -0.0624 0.593
2     1  20.5  0.0243 0.479
3     1  27.0  0.111  0.775
4     2  26.7 -0.0124 0.643
5     2  20.5 -0.0356 0.419
# ℹ 1 more row</code></pre>
</div>
</div>
<p>Starting with <code>small_data</code>, we use <code>tidy::nest</code> to nest the data frame using the <code>id</code> variable (<code>.by = id</code>); the name of the results is by default <code>data</code>, but it can be customized using the <code>.key</code> argument:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datalst</span> <span class="op">&lt;-</span> <span class="va">small_data</span> <span class="op">%&gt;%</span> <span class="fu">nest</span><span class="op">(</span>.by <span class="op">=</span> <span class="va">id</span>, .key <span class="op">=</span> <span class="st">"smpl"</span><span class="op">)</span></span>
<span><span class="va">datalst</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
     id smpl            
  &lt;int&gt; &lt;list&gt;          
1     1 &lt;tibble [3 × 3]&gt;
2     2 &lt;tibble [3 × 3]&gt;</code></pre>
</div>
</div>
<p>Each line now corresponds to a value of <code>id</code> and <code>smpl</code> is a list column that contains a tibble, </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datalst</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">smpl</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
# A tibble: 3 × 3
      x     eps     y
  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
1  26.7 -0.0624 0.593
2  20.5  0.0243 0.479
3  27.0  0.111  0.775

[[2]]
# A tibble: 3 × 3
      x     eps     y
  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
1  26.7 -0.0124 0.643
2  20.5 -0.0356 0.419
3  27.0  0.141  0.805</code></pre>
</div>
</div>
<p>We now write a function that takes as argument a data frame containing the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> columns and returns a one-line tibble that contains the fitted intercept and slope: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">s</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">s</span><span class="op">$</span><span class="va">x</span> ; <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">s</span><span class="op">$</span><span class="va">y</span></span>
<span>  <span class="fu">tibble</span><span class="op">(</span>slope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span>  </span>
<span>           <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span> <span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>         intercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">slope</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This function can be applied for a given sample:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ols</span><span class="op">(</span><span class="va">small_data</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
   slope intercept
   &lt;dbl&gt;     &lt;dbl&gt;
1 0.0329    -0.198</code></pre>
</div>
</div>
<p>But, as the <code>smpl</code> column of <code>datalst</code> is a list, the <code>ols</code> function can be applied to all the elements of the list, which can be done using the <code><a href="https://purrr.tidyverse.org/reference/map.html">purrr::map</a></code> function which takes as argument a list and a function: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datalst</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">smpl</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">map</span><span class="op">(</span><span class="va">ols</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
# A tibble: 1 × 2
   slope intercept
   &lt;dbl&gt;     &lt;dbl&gt;
1 0.0329    -0.198

[[2]]
# A tibble: 1 × 2
   slope intercept
   &lt;dbl&gt;     &lt;dbl&gt;
1 0.0483    -0.573</code></pre>
</div>
</div>
<p>and returns a list. But this can more easily be done using <code>mutate</code> or <code>transmute</code>, so that the result is a tibble: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="va">datalst</span> <span class="op">%&gt;%</span> <span class="fu">transmute</span><span class="op">(</span>smpl <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">smpl</span>, <span class="va">ols</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">results</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 1
  smpl            
  &lt;list&gt;          
1 &lt;tibble [1 × 2]&gt;
2 &lt;tibble [1 × 2]&gt;</code></pre>
</div>
</div>
<p>Finally, <code><a href="https://tidyr.tidyverse.org/reference/unnest.html">tidyr::unnest</a></code> expands the list column containing tibbles into rows and columns. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">results</span> <span class="op">%&gt;%</span> <span class="fu">unnest</span><span class="op">(</span>cols <span class="op">=</span> <span class="va">smpl</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
   slope intercept
   &lt;dbl&gt;     &lt;dbl&gt;
1 0.0329    -0.198
2 0.0483    -0.573</code></pre>
</div>
</div>
<p>which returns a standard tibble with one row for each draw and one column for each computed statistic. Going back to the full example with <span class="math inline">\(R = 1000\)</span>, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">datas</span> <span class="op">%&gt;%</span> <span class="fu">nest</span><span class="op">(</span>.by <span class="op">=</span> <span class="va">id</span>, .key <span class="op">=</span> <span class="st">"smpl"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">transmute</span><span class="op">(</span>smpl <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">smpl</span>, <span class="va">ols</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">unnest</span><span class="op">(</span>cols <span class="op">=</span> <span class="va">smpl</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">slope</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
    mean      sd
   &lt;dbl&gt;   &lt;dbl&gt;
1 0.0320 0.00437</code></pre>
</div>
</div>
<p></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-BONN:04" class="csl-entry" role="doc-biblioentry">
Bonnel, Patrick. 2004. <em>Prévoir La Demande de Transports</em>. Presses de l’école nationale des ponts et chaussées.
</div>
<div id="ref-DAVI:MACK:04" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, and James G. MacKinnon. 2004. <em>Econometric Theory and Methods</em>. Oxford University Press.
</div>
<div id="ref-MULL:97" class="csl-entry" role="doc-biblioentry">
Mullahy, John. 1997. <span>“Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior.”</span> <em>The Review of Economics and Statistics</em> 79 (4): 586–93. <a href="http://www.jstor.org/stable/2951410">http://www.jstor.org/stable/2951410</a>.
</div>
<div id="ref-PLUG:04" class="csl-entry" role="doc-biblioentry">
Plug, Erik. 2004. <span>“Estimating the Effect of Mother’s Schooling on Children’s Schooling Using a Sample of Adoptees.”</span> <em>American Economic Review</em> 94 (1): 358–68. <a href="https://doi.org/10.1257/000282804322970850">https://doi.org/10.1257/000282804322970850</a>.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Note that we use a biased estimator of the covariance as the denominator is <span class="math inline">\(N\)</span> and not <span class="math inline">\(N-1\)</span>. The difference is negligible if <span class="math inline">\(N\)</span> is large.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is obviously a very raw method to remove the non-linearity. Real time-price models use a much relevant solution, i.e., a transformation of the two variables in order to deal with the non-linearity.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This estimator is called the least absolute deviations estimator.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For a detailed presentation of the geometry of least squares, see <span class="citation" data-cites="DAVI:MACK:04">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:04" role="doc-biblioref">2004</a>)</span>, chapter 2.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See <span class="citation" data-cites="DAVI:MACK:04">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:04" role="doc-biblioref">2004</a>)</span>, p.&nbsp;48.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Actually, the second argument <code>data</code> is not mandatory because estimation can be performed without using a data frame.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>More precisely, a function that generates a sequence of numbers that looks like random numbers.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../OLS.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Ordinary least squares estimator</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/simple_regression_properties.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb60" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Simple linear regression model {#sec-simple_ols}</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setup_simple_regression</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../_commonR.R"</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- see zeileis Kleiber on structural models --&gt;</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- the adoptees data set is quite heavy --&gt;</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- introduce the epsilon_O notation ? --&gt;</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \newcommand{\indexfunction}[2]{\index[functions]{#1}} --&gt;</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>Obviously there will be rarely a deterministic relationship between</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>a response and one or several covariates. Education has a positive</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>causal effect on wage, but there are many other variables that affect</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>wages: sex, ethnicity, experience, abilities, to name a few. Even if a</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>large set of relevant covariates are observed, some are not (this</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>is in particular the case for abilities). Therefore, we won't try to</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>modelize the value of $y$ as a function of covariates $x$ and some</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>unknown parameters $\gamma$: $y = f(x, \gamma)$ but, more modestly,</span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>the **conditional expectation** of $y$: \index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{conditional expectation|(}</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>\mbox{E}(y | x) = f(x, \gamma)</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>$$ {#eq-condexpsimp}</span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>Back to our education / wage model, the conditional expectation is the</span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>mean value of the wage in the population for a given value of education. It is therefore a function of $x$ which takes as many values as there are distinct</span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>values of $x$. </span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>In this chapter, we discuss the simplest econometric model, which is</span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a>the simple linear regression model. This is a **simple** model because</span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>there is only one covariate $x$. This is a **linear** model</span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>because the $f$ function is assumed to be linear. @eq-condexpsimp can then</span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>be rewritten as follow:</span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>\mbox{E}(y | x) = \alpha + \beta x</span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>$$ {#eq-lincondexp}</span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a>@sec-condexp_covariance details the notion of conditional expectation and its relation with the notion of covariance. @sec-simple_model_data presents the model and the data set that will be used throughout this chapter. @sec-comp_simple_ols is devoted to the computation of the ordinary least squares estimator. @sec-simple_geometry presents the geometry of least squares. @sec-comp_ols_simple_R explains how to compute the ordinary least squares estimator using **R**. Finally @sec-dgp_simulations presents the notion of data generator process and explains how simulations can be usefully performed.</span>
<span id="cb60-45"><a href="#cb60-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-46"><a href="#cb60-46" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conditional expectation and covariance {#sec-condexp_covariance}</span></span>
<span id="cb60-47"><a href="#cb60-47" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance|(}</span>
<span id="cb60-48"><a href="#cb60-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-49"><a href="#cb60-49" aria-hidden="true" tabindex="-1"></a>To estimate the two unknown parameters $\alpha$ (the **intercept**) and $\beta$ (the **slope**), we need a</span>
<span id="cb60-50"><a href="#cb60-50" aria-hidden="true" tabindex="-1"></a>data set that contains different individuals for which $x$ and $y$ are</span>
<span id="cb60-51"><a href="#cb60-51" aria-hidden="true" tabindex="-1"></a>observed. Typically, we'll consider a random sample, which is a small</span>
<span id="cb60-52"><a href="#cb60-52" aria-hidden="true" tabindex="-1"></a>subset of the whole population consisting of a set of individuals</span>
<span id="cb60-53"><a href="#cb60-53" aria-hidden="true" tabindex="-1"></a>randomly drawn from this population.  </span>
<span id="cb60-54"><a href="#cb60-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-55"><a href="#cb60-55" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear models: two examples</span></span>
<span id="cb60-56"><a href="#cb60-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-57"><a href="#cb60-57" aria-hidden="true" tabindex="-1"></a>To understand what the</span>
<span id="cb60-58"><a href="#cb60-58" aria-hidden="true" tabindex="-1"></a>hypothesis of the simple linear model implies, we consider two data</span>
<span id="cb60-59"><a href="#cb60-59" aria-hidden="true" tabindex="-1"></a>sets. The first one is called <span class="in">`birthwt`</span>\idxdata{birthwt}{micsr} <span class="co">[</span><span class="ot">@MULL:97</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Mullbauer} and contains birth weights</span>
<span id="cb60-60"><a href="#cb60-60" aria-hidden="true" tabindex="-1"></a>(<span class="in">`birthwt`</span> in ounces) of babies and smoking habits of mothers. The</span>
<span id="cb60-61"><a href="#cb60-61" aria-hidden="true" tabindex="-1"></a><span class="in">`cigarettes`</span>\idxdata{cigarettes}{micsr} variable is the number of cigarettes smoked per day</span>
<span id="cb60-62"><a href="#cb60-62" aria-hidden="true" tabindex="-1"></a>during pregnancy (for about 85% of the mothers, it is equal to 0). We first compute a binary variable for smoking mothers:</span>
<span id="cb60-63"><a href="#cb60-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-64"><a href="#cb60-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-65"><a href="#cb60-65" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-66"><a href="#cb60-66" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: smoke_computation</span></span>
<span id="cb60-67"><a href="#cb60-67" aria-hidden="true" tabindex="-1"></a>birthwt <span class="ot">&lt;-</span> birthwt <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">smoke =</span> <span class="fu">ifelse</span>(cigarettes <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb60-68"><a href="#cb60-68" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-69"><a href="#cb60-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-70"><a href="#cb60-70" aria-hidden="true" tabindex="-1"></a>and consider a simple linear regression model with <span class="in">`birthwt`</span> as the</span>
<span id="cb60-71"><a href="#cb60-71" aria-hidden="true" tabindex="-1"></a>response and <span class="in">`smoke`</span> as a covariate. As the covariate takes only two</span>
<span id="cb60-72"><a href="#cb60-72" aria-hidden="true" tabindex="-1"></a>values, so does the conditional expectation. In a linear regression</span>
<span id="cb60-73"><a href="#cb60-73" aria-hidden="true" tabindex="-1"></a>model, @eq-lincondexp will therefore returns two values, $\alpha$ for</span>
<span id="cb60-74"><a href="#cb60-74" aria-hidden="true" tabindex="-1"></a>$x = 0$ and $\alpha + \beta$ for $x = 1$. $\alpha$ and $\alpha +</span>
<span id="cb60-75"><a href="#cb60-75" aria-hidden="true" tabindex="-1"></a>\beta$ are therefore the expected birth weights for respectively</span>
<span id="cb60-76"><a href="#cb60-76" aria-hidden="true" tabindex="-1"></a>non-smoking and smoking mothers. Note that, because we have as many</span>
<span id="cb60-77"><a href="#cb60-77" aria-hidden="true" tabindex="-1"></a>parameters as values of the covariate, the linear hypothesis</span>
<span id="cb60-78"><a href="#cb60-78" aria-hidden="true" tabindex="-1"></a>is necessarily supported. Natural estimators of the conditional expectations</span>
<span id="cb60-79"><a href="#cb60-79" aria-hidden="true" tabindex="-1"></a>are the conditional sample means, i.e., the average birth weights in the</span>
<span id="cb60-80"><a href="#cb60-80" aria-hidden="true" tabindex="-1"></a>sample for non-smoking and smoking women:</span>
<span id="cb60-81"><a href="#cb60-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-82"><a href="#cb60-82" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-83"><a href="#cb60-83" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: smoke_effect</span></span>
<span id="cb60-84"><a href="#cb60-84" aria-hidden="true" tabindex="-1"></a>cond_means <span class="ot">&lt;-</span> birthwt <span class="sc">%&gt;%</span></span>
<span id="cb60-85"><a href="#cb60-85" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(smoke) <span class="sc">%&gt;%</span></span>
<span id="cb60-86"><a href="#cb60-86" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">birthwt =</span> <span class="fu">mean</span>(birthwt))</span>
<span id="cb60-87"><a href="#cb60-87" aria-hidden="true" tabindex="-1"></a>cond_means</span>
<span id="cb60-88"><a href="#cb60-88" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-89"><a href="#cb60-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-90"><a href="#cb60-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-91"><a href="#cb60-91" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb60-92"><a href="#cb60-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: smoke_effect_hiden</span></span>
<span id="cb60-93"><a href="#cb60-93" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> cond_means[<span class="dv">1</span>, <span class="dv">2</span>, drop <span class="ot">=</span> <span class="cn">TRUE</span>]</span>
<span id="cb60-94"><a href="#cb60-94" aria-hidden="true" tabindex="-1"></a>alpha_beta <span class="ot">&lt;-</span> cond_means[<span class="dv">2</span>, <span class="dv">2</span>, drop <span class="ot">=</span> <span class="cn">TRUE</span>]</span>
<span id="cb60-95"><a href="#cb60-95" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> alpha_beta <span class="sc">-</span> alpha</span>
<span id="cb60-96"><a href="#cb60-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-97"><a href="#cb60-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-98"><a href="#cb60-98" aria-hidden="true" tabindex="-1"></a>Therefore, our estimation of $\alpha$ is </span>
<span id="cb60-99"><a href="#cb60-99" aria-hidden="true" tabindex="-1"></a>$\hat{\alpha} = <span class="in">`r round(alpha, 1)`</span>$ and the estimation of $\alpha +</span>
<span id="cb60-100"><a href="#cb60-100" aria-hidden="true" tabindex="-1"></a>\beta$ is $<span class="in">`r round(alpha_beta, 1)`</span>$, so that $\hat{\beta} =</span>
<span id="cb60-101"><a href="#cb60-101" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(alpha_beta, 1)`</span> - <span class="in">`r round(alpha, 1)`</span> = <span class="in">`r round(beta, 1)`</span>$</span>
<span id="cb60-102"><a href="#cb60-102" aria-hidden="true" tabindex="-1"></a>ounces, which is the estimation of birth weight's loss caused by smoking during pregnancy.</span>
<span id="cb60-103"><a href="#cb60-103" aria-hidden="true" tabindex="-1"></a>This can be illustrated using a scatterplot (see @fig-birthwt)\idxdata{cigarettes}{micsr}, with</span>
<span id="cb60-104"><a href="#cb60-104" aria-hidden="true" tabindex="-1"></a><span class="in">`smoke`</span> on the horizontal axis and <span class="in">`birthwt`</span> on the vertical axis, all</span>
<span id="cb60-105"><a href="#cb60-105" aria-hidden="true" tabindex="-1"></a>the points having only two possible abscissa values, 0 and 1. The conditional means</span>
<span id="cb60-106"><a href="#cb60-106" aria-hidden="true" tabindex="-1"></a>are represented by square points, and we also drew the line that</span>
<span id="cb60-107"><a href="#cb60-107" aria-hidden="true" tabindex="-1"></a>contains these two points. The intercept is therefore the y-value of</span>
<span id="cb60-108"><a href="#cb60-108" aria-hidden="true" tabindex="-1"></a>the square point for $x = 0$ which is $\hat{\alpha} = `r round(alpha,</span>
<span id="cb60-109"><a href="#cb60-109" aria-hidden="true" tabindex="-1"></a>1)`$. The slope of the line is the ratio of the vertical and the</span>
<span id="cb60-110"><a href="#cb60-110" aria-hidden="true" tabindex="-1"></a>horizontal distances between the two large points, which are respectively</span>
<span id="cb60-111"><a href="#cb60-111" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}$ and $1 - 0 = 1$ and is therefore equal to $\hat{\beta}$.</span>
<span id="cb60-112"><a href="#cb60-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-115"><a href="#cb60-115" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-116"><a href="#cb60-116" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-birthwt</span></span>
<span id="cb60-117"><a href="#cb60-117" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Birth weight for non-smoking and smoking mothers"</span></span>
<span id="cb60-118"><a href="#cb60-118" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-119"><a href="#cb60-119" aria-hidden="true" tabindex="-1"></a>birthwt <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(smoke, birthwt)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb60-120"><a href="#cb60-120" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> beta, <span class="at">intercept =</span> alpha) <span class="sc">+</span> </span>
<span id="cb60-121"><a href="#cb60-121" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> cond_means, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">shape =</span> <span class="dv">15</span>) <span class="sc">+</span></span>
<span id="cb60-122"><a href="#cb60-122" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">200</span>))</span>
<span id="cb60-123"><a href="#cb60-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-124"><a href="#cb60-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-125"><a href="#cb60-125" aria-hidden="true" tabindex="-1"></a>Consider now the relationship between education and wage. The</span>
<span id="cb60-126"><a href="#cb60-126" aria-hidden="true" tabindex="-1"></a><span class="in">`adoptees`</span>\idxdata{adoptees}{micsr.data} data set <span class="co">[</span><span class="ot">@PLUG:04</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Plug} contains the number of years of education <span class="in">`educ`</span></span>
<span id="cb60-127"><a href="#cb60-127" aria-hidden="true" tabindex="-1"></a>and the annual income <span class="in">`income`</span> (in thousands of US$) for 16481</span>
<span id="cb60-128"><a href="#cb60-128" aria-hidden="true" tabindex="-1"></a>individuals. We restrict the sample to individuals with an education level</span>
<span id="cb60-129"><a href="#cb60-129" aria-hidden="true" tabindex="-1"></a>between 12 (high school degree) and 15 years (bachelor's degree).</span>
<span id="cb60-130"><a href="#cb60-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-131"><a href="#cb60-131" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-132"><a href="#cb60-132" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: adoptees_filter</span></span>
<span id="cb60-133"><a href="#cb60-133" aria-hidden="true" tabindex="-1"></a>adoptees <span class="ot">&lt;-</span> adoptees <span class="sc">%&gt;%</span></span>
<span id="cb60-134"><a href="#cb60-134" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(educ <span class="sc">&gt;=</span> <span class="dv">12</span>, educ <span class="sc">&lt;=</span> <span class="dv">15</span>)</span>
<span id="cb60-135"><a href="#cb60-135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-136"><a href="#cb60-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-137"><a href="#cb60-137" aria-hidden="true" tabindex="-1"></a>and we compute the mean income for the four values of education:</span>
<span id="cb60-138"><a href="#cb60-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-139"><a href="#cb60-139" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-140"><a href="#cb60-140" aria-hidden="true" tabindex="-1"></a><span class="co">#| adoptees_mean_income</span></span>
<span id="cb60-141"><a href="#cb60-141" aria-hidden="true" tabindex="-1"></a>adoptees <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(educ) <span class="sc">%&gt;%</span></span>
<span id="cb60-142"><a href="#cb60-142" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">income =</span> <span class="fu">mean</span>(income))</span>
<span id="cb60-143"><a href="#cb60-143" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-144"><a href="#cb60-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-145"><a href="#cb60-145" aria-hidden="true" tabindex="-1"></a>The scatterplot is presented in @fig-adoptees:</span>
<span id="cb60-146"><a href="#cb60-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-147"><a href="#cb60-147" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-148"><a href="#cb60-148" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-adoptees</span></span>
<span id="cb60-149"><a href="#cb60-149" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Education and income"</span></span>
<span id="cb60-150"><a href="#cb60-150" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-151"><a href="#cb60-151" aria-hidden="true" tabindex="-1"></a>cond_means <span class="ot">&lt;-</span> adoptees <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(educ) <span class="sc">%&gt;%</span></span>
<span id="cb60-152"><a href="#cb60-152" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">income =</span> <span class="fu">mean</span>(income))</span>
<span id="cb60-153"><a href="#cb60-153" aria-hidden="true" tabindex="-1"></a>adoptees <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(educ, income)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb60-154"><a href="#cb60-154" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> cond_means, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb60-155"><a href="#cb60-155" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="fl">5E01</span>, <span class="fl">1E02</span>))</span>
<span id="cb60-156"><a href="#cb60-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-157"><a href="#cb60-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-158"><a href="#cb60-158" aria-hidden="true" tabindex="-1"></a>This time, we have four distinct values of the covariate and we can estimate four conditional sample means, but there are only two parameters to estimate for the simple linear model. Therefore, we can't estimate directly $\alpha$ and $\beta$ using the conditional means, except in the improbable case where the four conditional means lie on a straight line, which means that, for one additional year of education, the average wage increases exactly by the same amount. We can see in @fig-adoptees\idxdata{adoptees}{micsr.data} that it is not the case: the increase of income is <span class="in">`r round(diff(cond_means$income[1:2]), 1)`</span> for a 13th year of education, <span class="in">`r round(diff(cond_means$income[2:3]), 1)`</span> for a 14th year and <span class="in">`r round(diff(cond_means$income[3:4]), 1)`</span> for a 15th year. We therefore need a formal method of estimation which enables us to obtain values of $\hat{\alpha}$ and $\hat{\beta}$. The remaining of this chapter is devoted to the presentation of this estimator, which is called the **ordinary least squares** (**OLS**) estimator. </span>
<span id="cb60-159"><a href="#cb60-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-160"><a href="#cb60-160" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear model, conditional expectation and covariance</span></span>
<span id="cb60-161"><a href="#cb60-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-162"><a href="#cb60-162" aria-hidden="true" tabindex="-1"></a>For now, it is very important to understand the relation between the conditional expectation and the covariance. The covariance between $x$ and $y$ is, denoting $\mu_x = \mbox{E}(x)$ and $\mu_y = \mbox{E}(y)$:</span>
<span id="cb60-163"><a href="#cb60-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-164"><a href="#cb60-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-165"><a href="#cb60-165" aria-hidden="true" tabindex="-1"></a>\sigma_{xy} = \mbox{cov}(x, y) = \mbox{E}\left<span class="co">[</span><span class="ot">(x - \mu_x)(y - \mu_y)\right</span><span class="co">]</span> = \mbox{E}(xy) - \mu_x\mu_y</span>
<span id="cb60-166"><a href="#cb60-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-167"><a href="#cb60-167" aria-hidden="true" tabindex="-1"></a>It is the expectation of the product of the two variables in deviations from their respective expectations or the expectation of the product minus the product of the expectations. The expectation is for the two variables $x$ and $y$ and the law of repeated expectation states that this expectation can be written as:</span>
<span id="cb60-168"><a href="#cb60-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-169"><a href="#cb60-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-170"><a href="#cb60-170" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(x, y) = \mbox{E}_x\left[\mbox{E}_y\left((y - \mu_y)(x - \mu_x)\mid x\right)\right] = \mbox{E}_x\left[(x - \mu_x)\mbox{E}_y(y - \mu_y\mid x)\right]</span>
<span id="cb60-171"><a href="#cb60-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-172"><a href="#cb60-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-173"><a href="#cb60-173" aria-hidden="true" tabindex="-1"></a>Therefore, the covariance between $x$ and $y$ is also the covariance between $x$ and the conditional expectation of $y$ (which is a function of $x$).</span>
<span id="cb60-174"><a href="#cb60-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-175"><a href="#cb60-175" aria-hidden="true" tabindex="-1"></a>As an example, consider the case where $y$ is the hourly wage and $x$ the education level, which takes the values 0 (highschool), 3 (bachelor) and 5 (graduate), the frequencies for the three levels of education being 0.4, 0.3 and 0.3. Let denote $h$, $b$ and $g$ these three education levels and assume that $\mbox{E}(y\mid x) = 8 + 0.5 x$, which means that one more year of education increases the expected wage by $0.5. </span>
<span id="cb60-176"><a href="#cb60-176" aria-hidden="true" tabindex="-1"></a>The expected level of education is $\mu_x = 0.4 \times 0 + 0.3 \times 3 + 0.3 \times 5 = 2.4$. </span>
<span id="cb60-177"><a href="#cb60-177" aria-hidden="true" tabindex="-1"></a>The expected wage for the three levels of education are $\mu_{xh} = 8$, $\mu_{xb} = 9.5$ and $\mu_{xg} = 10.5$. The expected wage is then $\mu_y = 0.4 \times 8 + 0.3 \times 9.5 + 0.3 \times 10.5 = 9.2$. We first compute the first term of the covariance between wage and education for $x = x_h = 0$:</span>
<span id="cb60-178"><a href="#cb60-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-179"><a href="#cb60-179" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb60-180"><a href="#cb60-180" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb60-181"><a href="#cb60-181" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left[ (x - \mu_x)(y -</span>
<span id="cb60-182"><a href="#cb60-182" aria-hidden="true" tabindex="-1"></a>\mu_y)\mid x = x_h\right] &amp;=&amp; </span>
<span id="cb60-183"><a href="#cb60-183" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left[ (x_h - \mu_x)(y -</span>
<span id="cb60-184"><a href="#cb60-184" aria-hidden="true" tabindex="-1"></a>\mu_y)\mid x = x_h\right] <span class="sc">\\</span></span>
<span id="cb60-185"><a href="#cb60-185" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; </span>
<span id="cb60-186"><a href="#cb60-186" aria-hidden="true" tabindex="-1"></a>(x_h - \mu_x) \mbox{E}\left[ (y -</span>
<span id="cb60-187"><a href="#cb60-187" aria-hidden="true" tabindex="-1"></a>\mu_y)\mid x = x_h\right] <span class="sc">\\</span></span>
<span id="cb60-188"><a href="#cb60-188" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;</span>
<span id="cb60-189"><a href="#cb60-189" aria-hidden="true" tabindex="-1"></a>(x_h - \mu_x)(\mbox{E}(y\mid x = x_h) - \mu_y)<span class="sc">\\</span></span>
<span id="cb60-190"><a href="#cb60-190" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; (0 - 2.4) \times (8 - 9.2) = 2.88</span>
<span id="cb60-191"><a href="#cb60-191" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb60-192"><a href="#cb60-192" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cov_one_term}</span>
<span id="cb60-193"><a href="#cb60-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-194"><a href="#cb60-194" aria-hidden="true" tabindex="-1"></a>Similarly, we get $(3 - 2.4) \times (9.5 - 9.2) = 0.18$ for the bachelor level and $(5 - 2.4)\times (10.5 - 9.2) = 3.38$ for the master level. Finally the covariance is obtained as the weight average of these three terms:</span>
<span id="cb60-195"><a href="#cb60-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-196"><a href="#cb60-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-197"><a href="#cb60-197" aria-hidden="true" tabindex="-1"></a>\sigma_{xy} = 0.4 \times 2.88 + 0.3 \times 0.18 + 0.3 \times 3.38 = 2.22</span>
<span id="cb60-198"><a href="#cb60-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-199"><a href="#cb60-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-200"><a href="#cb60-200" aria-hidden="true" tabindex="-1"></a>Now consider the case where the conditional expectation of the hourly wage is the same for the three education levels: $\mbox{E}(y \mid x = x_h) = \mbox{E}(y \mid x = x_b) = \mbox{E}(y \mid x = x_m)$. These three conditional expectations are therefore equal to the unconditional expectation $\mu_y$ and @eq-cov_one_term is clearly 0 and so is the covariance. Therefore, a constant conditional expectation of $y$ given $x$ implies that the covariance is 0. Note that the opposite is not true. For example, if $\mu_{yh} = 8$, $\mu_{yb} = 13.2$ and $\mu_{ym} = 6.2$, the relation between the conditional mean of the wage and the level of education is no more linear, and it is not monotonous but inversely U-shaped (the wage first increases with education and then decreases). The unconditional mean is still: $\mu_y = 0.4 \times 8 + 0.3 \times 13.2 + 0.3 \times 6.2 = 9.2$, but the covariance is then:</span>
<span id="cb60-201"><a href="#cb60-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-202"><a href="#cb60-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-203"><a href="#cb60-203" aria-hidden="true" tabindex="-1"></a>\sigma_{xy} = 0.4 \times (0-2.4) \times (8 - 9.2) + 0.3 \times (3-2.4) \times (13.2 - 9.2) + 0.3 \times (5 - 2.4) \times (6.2 - 9.2) = 0</span>
<span id="cb60-204"><a href="#cb60-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-205"><a href="#cb60-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-206"><a href="#cb60-206" aria-hidden="true" tabindex="-1"></a>Therefore, a 0 covariance doesn't imply that the conditional mean is constant. However, in the special case where the conditional mean is a linear function of the conditional variable, the two properties of 0 covariance and constant conditional means are equivalent.</span>
<span id="cb60-207"><a href="#cb60-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-208"><a href="#cb60-208" aria-hidden="true" tabindex="-1"></a>Consider now the sample counterpart of this theoretical covariance. Consider a sample of 10 individuals for which the levels of education and the hourly wages are:</span>
<span id="cb60-209"><a href="#cb60-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-210"><a href="#cb60-210" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo = FALSE, message = FALSE}</span></span>
<span id="cb60-211"><a href="#cb60-211" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"tibble"</span>)</span>
<span id="cb60-212"><a href="#cb60-212" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="fl">9.5</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">10.5</span>, <span class="dv">11</span>)</span>
<span id="cb60-213"><a href="#cb60-213" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">5</span>), <span class="at">each =</span> <span class="dv">3</span>))</span>
<span id="cb60-214"><a href="#cb60-214" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb60-215"><a href="#cb60-215" aria-hidden="true" tabindex="-1"></a>xb <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb60-216"><a href="#cb60-216" aria-hidden="true" tabindex="-1"></a>yb <span class="ot">&lt;-</span> <span class="fu">mean</span>(y)</span>
<span id="cb60-217"><a href="#cb60-217" aria-hidden="true" tabindex="-1"></a>ybi <span class="ot">&lt;-</span> <span class="fu">tapply</span>(y, x, mean)[<span class="fu">as.character</span>(x)]</span>
<span id="cb60-218"><a href="#cb60-218" aria-hidden="true" tabindex="-1"></a>Sxy <span class="ot">&lt;-</span> <span class="fu">sum</span>(x <span class="sc">*</span> y)</span>
<span id="cb60-219"><a href="#cb60-219" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">education =</span> <span class="fu">as.character</span>(x), <span class="at">wage =</span> <span class="fu">as.character</span>(y)) <span class="sc">%&gt;%</span> as.matrix <span class="sc">%&gt;%</span> t <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span>
<span id="cb60-220"><a href="#cb60-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-221"><a href="#cb60-221" aria-hidden="true" tabindex="-1"></a>The sample means of $x$ and $y$ are : $\bar{x} = <span class="in">`r mean(x)`</span>$ and</span>
<span id="cb60-222"><a href="#cb60-222" aria-hidden="true" tabindex="-1"></a>$\bar{y} = <span class="in">`r mean(y)`</span>$.</span>
<span id="cb60-223"><a href="#cb60-223" aria-hidden="true" tabindex="-1"></a>The conditional means of $y$ for a given value of $x$ in the sample are simply the means for the three subsamples defined by an education level:</span>
<span id="cb60-224"><a href="#cb60-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-225"><a href="#cb60-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x_h=0$\ : $\bar{y}_h = \frac{8+7+8+9}{4} = </span>
<span id="cb60-226"><a href="#cb60-226" aria-hidden="true" tabindex="-1"></a><span class="in">`r mean(c(8, 7, 8, 9))`</span>$,</span>
<span id="cb60-227"><a href="#cb60-227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x_b=3$\ : $\bar{y}_b = \frac{9+9.5+10}{3} = </span>
<span id="cb60-228"><a href="#cb60-228" aria-hidden="true" tabindex="-1"></a><span class="in">`r mean(c(9, 9.5, 10))`</span>$,</span>
<span id="cb60-229"><a href="#cb60-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x_m=5$\ : $\bar{y}_m = \frac{10, 10.5, 11}{3} = </span>
<span id="cb60-230"><a href="#cb60-230" aria-hidden="true" tabindex="-1"></a><span class="in">`r mean(c(10, 10.5, 11))`</span>$,</span>
<span id="cb60-231"><a href="#cb60-231" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-232"><a href="#cb60-232" aria-hidden="true" tabindex="-1"></a>and the covariance is obtained as the mean of the products of the two variables in deviations from their mean, or equivalently by the difference between the mean of the products and the products of the means:^<span class="co">[</span><span class="ot">Note that we use a biased estimator of the covariance as the denominator is $N$ and not $N-1$. The difference is negligible if $N$ is large.</span><span class="co">]</span></span>
<span id="cb60-233"><a href="#cb60-233" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-234"><a href="#cb60-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-235"><a href="#cb60-235" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy} = \frac{\sum_{n=1}^N (x_n - \bar{x}) (y_n -</span>
<span id="cb60-236"><a href="#cb60-236" aria-hidden="true" tabindex="-1"></a>\bar{y})}{N}=\frac{\sum_{n=1}^N x_n y_n}{N}-\bar{x}\bar{y}</span>
<span id="cb60-237"><a href="#cb60-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-238"><a href="#cb60-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-239"><a href="#cb60-239" aria-hidden="true" tabindex="-1"></a>The sum of the products is $\sum_n x_n y_n = 243$ and the covariance is then:</span>
<span id="cb60-240"><a href="#cb60-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-241"><a href="#cb60-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-242"><a href="#cb60-242" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy}=\frac{<span class="in">`r Sxy`</span>}{<span class="in">`r N`</span>} - <span class="in">`r xb`</span> \times <span class="in">`r yb`</span> = <span class="in">`r Sxy/N-xb*yb`</span></span>
<span id="cb60-243"><a href="#cb60-243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-244"><a href="#cb60-244" aria-hidden="true" tabindex="-1"></a>We also can consider the three subsamples defined by education levels, denoting </span>
<span id="cb60-245"><a href="#cb60-245" aria-hidden="true" tabindex="-1"></a>$n=1, ..., H$, $n=H+1, ..., H+B$, $n=H+B+1, ..., N$ (with $H=4$, $B=3$ and $N=10$) observations for, respectively, a high-school, a bachelor and a master level. We then have:</span>
<span id="cb60-246"><a href="#cb60-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-247"><a href="#cb60-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-248"><a href="#cb60-248" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy} = \frac{\sum_{n=1}^H (x_n - \bar{x})(y_n -</span>
<span id="cb60-249"><a href="#cb60-249" aria-hidden="true" tabindex="-1"></a>\bar{y}) + \sum_{n=H+1}^{H+B} (x_n - \bar{x})(y_n - \bar{y}) + </span>
<span id="cb60-250"><a href="#cb60-250" aria-hidden="true" tabindex="-1"></a>\sum_{n=H+B+1}^N (x_n - \bar{x})(y_n - \bar{y})}{N}</span>
<span id="cb60-251"><a href="#cb60-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-252"><a href="#cb60-252" aria-hidden="true" tabindex="-1"></a>For example, for $n=1, ..., H$, the value of $x$ is the same: $x_n=x_h=0$. Therefore, we can write</span>
<span id="cb60-253"><a href="#cb60-253" aria-hidden="true" tabindex="-1"></a>$\sum_{n=1}^H (x_n - \bar{x})(y_n -\bar{y}) = (x_h - \bar{x})</span>
<span id="cb60-254"><a href="#cb60-254" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^H (y_n -\bar{y})$ and more generally:</span>
<span id="cb60-255"><a href="#cb60-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-256"><a href="#cb60-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-257"><a href="#cb60-257" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy} = \frac{(x_h - \bar{x})\sum_{n=1}^H (y_n -</span>
<span id="cb60-258"><a href="#cb60-258" aria-hidden="true" tabindex="-1"></a>\bar{y}) + (x_b - \bar{x})\sum_{n=H+1}^B (y_n - \bar{y}) + </span>
<span id="cb60-259"><a href="#cb60-259" aria-hidden="true" tabindex="-1"></a>(x_m - \bar{x})\sum_{n=H+B+1}^N (y_n - \bar{y})}{N}</span>
<span id="cb60-260"><a href="#cb60-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-261"><a href="#cb60-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-262"><a href="#cb60-262" aria-hidden="true" tabindex="-1"></a>Moreover $\frac{\sum_{n=1}^H (y_n - \bar{y})}{H}=(\bar{y}_h-\bar{y})$ and then:</span>
<span id="cb60-263"><a href="#cb60-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-264"><a href="#cb60-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-265"><a href="#cb60-265" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy} = \frac{(x_h - \bar{x})H(\bar{y}_h-\bar{y})+ (x_b - \bar{x})B(\bar{y}_b -\bar{y})+ </span>
<span id="cb60-266"><a href="#cb60-266" aria-hidden="true" tabindex="-1"></a>(x_m - \bar{x})M(\bar{y}_m - \bar{y})}{N}</span>
<span id="cb60-267"><a href="#cb60-267" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-268"><a href="#cb60-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-269"><a href="#cb60-269" aria-hidden="true" tabindex="-1"></a>Denoting $f_h = \frac{H}{N}$, $f_b = \frac{B}{N}$ and </span>
<span id="cb60-270"><a href="#cb60-270" aria-hidden="true" tabindex="-1"></a>$f_m=\frac{M}{N}$ the empirical frequencies of the three education levels in the sample:</span>
<span id="cb60-271"><a href="#cb60-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-272"><a href="#cb60-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-273"><a href="#cb60-273" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy} = f_h(x_h - \bar{x})(\bar{y}_h-\bar{y})+ f_b(x_b - \bar{x})(\bar{y}_b -\bar{y})+ </span>
<span id="cb60-274"><a href="#cb60-274" aria-hidden="true" tabindex="-1"></a>(x_m - \bar{x})(\bar{y}_m - \bar{y})</span>
<span id="cb60-275"><a href="#cb60-275" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-276"><a href="#cb60-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-277"><a href="#cb60-277" aria-hidden="true" tabindex="-1"></a>Finally, denoting $k=h, b, m$ the education levels:</span>
<span id="cb60-278"><a href="#cb60-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-279"><a href="#cb60-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-280"><a href="#cb60-280" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy}=\hat{\sigma}_{x\bar{y}_x}=</span>
<span id="cb60-281"><a href="#cb60-281" aria-hidden="true" tabindex="-1"></a>\sum_k f_k (x_k- \bar{x})(\bar{y}_k-\bar{y})</span>
<span id="cb60-282"><a href="#cb60-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-283"><a href="#cb60-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-284"><a href="#cb60-284" aria-hidden="true" tabindex="-1"></a>Therefore, the covariance between $x$ and $y$ is also the covariance between $x$ and the mean values of $y$ for given values of $x$. We have here: $\bar{y}_h = <span class="in">`r mean(y[1:4])`</span>$, </span>
<span id="cb60-285"><a href="#cb60-285" aria-hidden="true" tabindex="-1"></a>$\bar{y}_b = `r mean(y[5:7])`$ and $\bar{y}_m = <span class="in">`r mean(y[8:10])`</span>$. The covariance is then:</span>
<span id="cb60-286"><a href="#cb60-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-287"><a href="#cb60-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-288"><a href="#cb60-288" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{xy} = 0.4 \times (0 - 2.4) \times (8 - 9.2) + </span>
<span id="cb60-289"><a href="#cb60-289" aria-hidden="true" tabindex="-1"></a>0.3 \times (3 - 2.4) \times (9.5 - 9.2) + </span>
<span id="cb60-290"><a href="#cb60-290" aria-hidden="true" tabindex="-1"></a>0.3 \times (5 - 2.4) \times (10.5 - 9.2) =</span>
<span id="cb60-291"><a href="#cb60-291" aria-hidden="true" tabindex="-1"></a><span class="in">`r 0.4 * 2.4 * 1.2 + .3 * 0.6 * 0.3 + 0.3 * 2.6 * 1.3`</span></span>
<span id="cb60-292"><a href="#cb60-292" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-293"><a href="#cb60-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-294"><a href="#cb60-294" aria-hidden="true" tabindex="-1"></a>Consider now the case where the mean wage is the same for every education level:</span>
<span id="cb60-295"><a href="#cb60-295" aria-hidden="true" tabindex="-1"></a>$\bar{y}_b=\bar{y}_l=\bar{y}_m=\bar{y}=9.2$. In this case, the covariance is obviously 0 because $\bar{y}_k-\bar{y}=0\;\forall k$.</span>
<span id="cb60-296"><a href="#cb60-296" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{conditional expectation|)}</span>
<span id="cb60-297"><a href="#cb60-297" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance|)}</span>
<span id="cb60-298"><a href="#cb60-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-299"><a href="#cb60-299" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model and data set {#sec-simple_model_data}</span></span>
<span id="cb60-300"><a href="#cb60-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-301"><a href="#cb60-301" aria-hidden="true" tabindex="-1"></a>We'll consider in this chapter the question of mode shares for</span>
<span id="cb60-302"><a href="#cb60-302" aria-hidden="true" tabindex="-1"></a>inter-urban transportation. More precisely, considering that a trip</span>
<span id="cb60-303"><a href="#cb60-303" aria-hidden="true" tabindex="-1"></a>can be made using one out of two transport modes (air and rail), how can we modelize</span>
<span id="cb60-304"><a href="#cb60-304" aria-hidden="true" tabindex="-1"></a>the market shares of both modes? We'll use in this section a popular</span>
<span id="cb60-305"><a href="#cb60-305" aria-hidden="true" tabindex="-1"></a>model in transportation economics which is the price-time model.</span>
<span id="cb60-306"><a href="#cb60-306" aria-hidden="true" tabindex="-1"></a><span class="in">`price_time`</span>\idxdata{price<span class="sc">\_</span>time}{micsr.data} contains aggregate data about rail</span>
<span id="cb60-307"><a href="#cb60-307" aria-hidden="true" tabindex="-1"></a>and air transportation between Paris and 13 French towns in 1995, it</span>
<span id="cb60-308"><a href="#cb60-308" aria-hidden="true" tabindex="-1"></a>is reproduced from @BONN:04 pp. 364-366\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Bonnel}.</span>
<span id="cb60-309"><a href="#cb60-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-310"><a href="#cb60-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-311"><a href="#cb60-311" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: price_time</span></span>
<span id="cb60-312"><a href="#cb60-312" aria-hidden="true" tabindex="-1"></a>price_time <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb60-313"><a href="#cb60-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-314"><a href="#cb60-314" aria-hidden="true" tabindex="-1"></a>For the sake of simplicity, we'll use shorter names for the variables:</span>
<span id="cb60-315"><a href="#cb60-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-316"><a href="#cb60-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-317"><a href="#cb60-317" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: price_time_names</span></span>
<span id="cb60-318"><a href="#cb60-318" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> price_time <span class="sc">%&gt;%</span></span>
<span id="cb60-319"><a href="#cb60-319" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_names</span>(<span class="fu">c</span>(<span class="st">"town"</span>, <span class="st">"qr"</span>, <span class="st">"qa"</span>, <span class="st">"pr"</span>, <span class="st">"pa"</span>, <span class="st">"tr"</span>, <span class="st">"ta"</span>))</span>
<span id="cb60-320"><a href="#cb60-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-321"><a href="#cb60-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-322"><a href="#cb60-322" aria-hidden="true" tabindex="-1"></a>Variables are prices (<span class="in">`pr`</span> and <span class="in">`pa`</span>) in euros, transport times (<span class="in">`tr`</span> and</span>
<span id="cb60-323"><a href="#cb60-323" aria-hidden="true" tabindex="-1"></a><span class="in">`ta`</span>) in minutes and thousands of trips (<span class="in">`qf`</span> and <span class="in">`qa`</span>) for the two</span>
<span id="cb60-324"><a href="#cb60-324" aria-hidden="true" tabindex="-1"></a>modes (<span class="in">`r`</span> for rail and <span class="in">`a`</span> for air).</span>
<span id="cb60-325"><a href="#cb60-325" aria-hidden="true" tabindex="-1"></a>We first compute the market shares of rail:</span>
<span id="cb60-326"><a href="#cb60-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-327"><a href="#cb60-327" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-328"><a href="#cb60-328" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: shares_computation</span></span>
<span id="cb60-329"><a href="#cb60-329" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-330"><a href="#cb60-330" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> <span class="fu">mutate</span>(prtime, <span class="at">sr =</span> qr <span class="sc">/</span> (qr <span class="sc">+</span> qa))</span>
<span id="cb60-331"><a href="#cb60-331" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sr) <span class="sc">%&gt;%</span> summary</span>
<span id="cb60-332"><a href="#cb60-332" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-333"><a href="#cb60-333" aria-hidden="true" tabindex="-1"></a>Rail's market share exhibits huge variations in the sample, ranging from </span>
<span id="cb60-334"><a href="#cb60-334" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(min(prtime$sr) * 100)`</span> to</span>
<span id="cb60-335"><a href="#cb60-335" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(max(prtime$sr) * 100)`</span>%.</span>
<span id="cb60-336"><a href="#cb60-336" aria-hidden="true" tabindex="-1"></a>For an individual, the relevant cost of a trip is the generalized</span>
<span id="cb60-337"><a href="#cb60-337" aria-hidden="true" tabindex="-1"></a>cost, which is the sum of the monetary cost and the value of the</span>
<span id="cb60-338"><a href="#cb60-338" aria-hidden="true" tabindex="-1"></a>travel time. Denoting $h^i$ the time value of individual $i$, in euros</span>
<span id="cb60-339"><a href="#cb60-339" aria-hidden="true" tabindex="-1"></a>per hour, the generalized cost for the two modes are:</span>
<span id="cb60-340"><a href="#cb60-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-341"><a href="#cb60-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-342"><a href="#cb60-342" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb60-343"><a href="#cb60-343" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb60-344"><a href="#cb60-344" aria-hidden="true" tabindex="-1"></a>c_{a} ^ i &amp;=&amp; p_{a} + h ^ i t_{a} / 60<span class="sc">\\</span></span>
<span id="cb60-345"><a href="#cb60-345" aria-hidden="true" tabindex="-1"></a>c_{r} ^ i &amp;=&amp; p_{r} + h ^ i t_{r} / 60<span class="sc">\\</span></span>
<span id="cb60-346"><a href="#cb60-346" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb60-347"><a href="#cb60-347" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb60-348"><a href="#cb60-348" aria-hidden="true" tabindex="-1"></a>$$ {#eq-generalized_costs}</span>
<span id="cb60-349"><a href="#cb60-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-350"><a href="#cb60-350" aria-hidden="true" tabindex="-1"></a>Plane is typically faster and more expensive than train, which means</span>
<span id="cb60-351"><a href="#cb60-351" aria-hidden="true" tabindex="-1"></a>that in the time-value\ /\ generalized cost plane, the generalized cost</span>
<span id="cb60-352"><a href="#cb60-352" aria-hidden="true" tabindex="-1"></a>for rail will be represented by a line with a lower intercept (the price of train is</span>
<span id="cb60-353"><a href="#cb60-353" aria-hidden="true" tabindex="-1"></a>lower) and with a higher slope (transport time is higher) than the one that corresponds to air. Generalized</span>
<span id="cb60-354"><a href="#cb60-354" aria-hidden="true" tabindex="-1"></a>cost for both modes and for the two towns of Bordeaux and Nice are</span>
<span id="cb60-355"><a href="#cb60-355" aria-hidden="true" tabindex="-1"></a>presented in @fig-costmode.</span>
<span id="cb60-356"><a href="#cb60-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-359"><a href="#cb60-359" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-360"><a href="#cb60-360" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-costmode</span></span>
<span id="cb60-361"><a href="#cb60-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Generalized cost for train and plane"</span></span>
<span id="cb60-362"><a href="#cb60-362" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-363"><a href="#cb60-363" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">filter</span>(town <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"Bordeaux"</span>, <span class="st">"Nice"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb60-364"><a href="#cb60-364" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">h =</span> (pa <span class="sc">-</span> pr) <span class="sc">/</span> ( (tr <span class="sc">-</span> ta) <span class="sc">/</span> <span class="dv">60</span>))</span>
<span id="cb60-365"><a href="#cb60-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-366"><a href="#cb60-366" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">h =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>),</span>
<span id="cb60-367"><a href="#cb60-367" aria-hidden="true" tabindex="-1"></a>            <span class="at">Bordeaux_Plane =</span> z[<span class="dv">1</span>, <span class="st">"pa"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">+</span> z[<span class="dv">1</span>, <span class="st">"ta"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">*</span> h <span class="sc">/</span> <span class="dv">60</span>,</span>
<span id="cb60-368"><a href="#cb60-368" aria-hidden="true" tabindex="-1"></a>            <span class="at">Bordeaux_Train =</span> z[<span class="dv">1</span>, <span class="st">"pr"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">+</span> z[<span class="dv">1</span>, <span class="st">"tr"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">*</span> h <span class="sc">/</span> <span class="dv">60</span>,</span>
<span id="cb60-369"><a href="#cb60-369" aria-hidden="true" tabindex="-1"></a>            <span class="at">Nice_Plane =</span> z[<span class="dv">2</span>, <span class="st">"pa"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">+</span> z[<span class="dv">2</span>, <span class="st">"ta"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">*</span> h <span class="sc">/</span> <span class="dv">60</span>,</span>
<span id="cb60-370"><a href="#cb60-370" aria-hidden="true" tabindex="-1"></a>            <span class="at">Nice_Train =</span> z[<span class="dv">2</span>, <span class="st">"pr"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">+</span> z[<span class="dv">2</span>, <span class="st">"tr"</span>, <span class="at">drop =</span> <span class="cn">TRUE</span>] <span class="sc">*</span> h <span class="sc">/</span> <span class="dv">60</span></span>
<span id="cb60-371"><a href="#cb60-371" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb60-372"><a href="#cb60-372" aria-hidden="true" tabindex="-1"></a>v <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="at">names_sep =</span> <span class="st">"_"</span>, <span class="at">names_to =</span> <span class="fu">c</span>(<span class="st">"town"</span>, <span class="st">"mode"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb60-373"><a href="#cb60-373" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, value)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">linetype =</span> mode, <span class="at">linewidth =</span> town)) <span class="sc">+</span></span>
<span id="cb60-374"><a href="#cb60-374" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">40</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">200</span>)) <span class="sc">+</span></span>
<span id="cb60-375"><a href="#cb60-375" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of time"</span>, <span class="at">y =</span> <span class="st">"Generalized cost"</span>, <span class="at">town =</span> <span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb60-376"><a href="#cb60-376" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linewidth_discrete</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb60-377"><a href="#cb60-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-378"><a href="#cb60-378" aria-hidden="true" tabindex="-1"></a>Every individual will choose the mode with the lowest generalized cost. For</span>
<span id="cb60-379"><a href="#cb60-379" aria-hidden="true" tabindex="-1"></a>example, $i$ will choose the train if $c_r^i &lt; c_a^i$. This will</span>
<span id="cb60-380"><a href="#cb60-380" aria-hidden="true" tabindex="-1"></a>depend on the individual value of time: an individual with a high value of time</span>
<span id="cb60-381"><a href="#cb60-381" aria-hidden="true" tabindex="-1"></a>will choose the plane as an individual with a lower travel time will</span>
<span id="cb60-382"><a href="#cb60-382" aria-hidden="true" tabindex="-1"></a>choose the train. For given values of prices and travel times, one can</span>
<span id="cb60-383"><a href="#cb60-383" aria-hidden="true" tabindex="-1"></a>compute a value of travel time $h^*$ which equates the generalized</span>
<span id="cb60-384"><a href="#cb60-384" aria-hidden="true" tabindex="-1"></a>costs of the two modes in @eq-generalized_costs:</span>
<span id="cb60-385"><a href="#cb60-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-386"><a href="#cb60-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-387"><a href="#cb60-387" aria-hidden="true" tabindex="-1"></a>h^* = 60\frac{p_a - p_r}{(t_r - t_a)}</span>
<span id="cb60-388"><a href="#cb60-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-389"><a href="#cb60-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-390"><a href="#cb60-390" aria-hidden="true" tabindex="-1"></a>For Bordeaux and Nice, these time values are respectively </span>
<span id="cb60-391"><a href="#cb60-391" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(z$h[1], 1)`</span> and <span class="in">`r round(z$h[2], 1)`</span> euros per hour.</span>
<span id="cb60-392"><a href="#cb60-392" aria-hidden="true" tabindex="-1"></a>Nice is actually very far from Paris and only people with a very low</span>
<span id="cb60-393"><a href="#cb60-393" aria-hidden="true" tabindex="-1"></a>value of time would spend 7.5 hours in the train instead of taking a plane. We now</span>
<span id="cb60-394"><a href="#cb60-394" aria-hidden="true" tabindex="-1"></a>compute this threshold value for every city:</span>
<span id="cb60-395"><a href="#cb60-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-396"><a href="#cb60-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-397"><a href="#cb60-397" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: threshold_value_time</span></span>
<span id="cb60-398"><a href="#cb60-398" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-399"><a href="#cb60-399" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> <span class="fu">mutate</span>(prtime,  <span class="at">h =</span> (pa <span class="sc">-</span> pr) <span class="sc">/</span> ( (tr <span class="sc">-</span> ta) <span class="sc">/</span> <span class="dv">60</span>) )</span>
<span id="cb60-400"><a href="#cb60-400" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(h) <span class="sc">%&gt;%</span> summary</span>
<span id="cb60-401"><a href="#cb60-401" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-402"><a href="#cb60-402" aria-hidden="true" tabindex="-1"></a>There are huge variations of the threshold value of time, as it ranges</span>
<span id="cb60-403"><a href="#cb60-403" aria-hidden="true" tabindex="-1"></a>from <span class="in">`r round(pull(top_n(prtime, -1, h), h))`</span> </span>
<span id="cb60-404"><a href="#cb60-404" aria-hidden="true" tabindex="-1"></a>(<span class="in">`r pull(top_n(prtime, -1, h), town)`</span>) to <span class="in">`r round(pull(top_n(prtime, 1, h), h))`</span> </span>
<span id="cb60-405"><a href="#cb60-405" aria-hidden="true" tabindex="-1"></a>(<span class="in">`r pull(top_n(prtime, 1, h), town)`</span>) euros per hour.</span>
<span id="cb60-406"><a href="#cb60-406" aria-hidden="true" tabindex="-1"></a>Before considering a theoretical model that links the market share of</span>
<span id="cb60-407"><a href="#cb60-407" aria-hidden="true" tabindex="-1"></a>train with the threshold value of time, let's have a first glance at @fig-hsfsmpl of this relationship using a scatterplot.</span>
<span id="cb60-408"><a href="#cb60-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-411"><a href="#cb60-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-412"><a href="#cb60-412" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-hsfsmpl </span></span>
<span id="cb60-413"><a href="#cb60-413" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Share of rail in function of the threshold time value"</span></span>
<span id="cb60-414"><a href="#cb60-414" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-415"><a href="#cb60-415" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb60-416"><a href="#cb60-416" aria-hidden="true" tabindex="-1"></a>    ggrepel<span class="sc">::</span><span class="fu">geom_label_repel</span>(<span class="fu">aes</span>(<span class="at">label =</span> town))</span>
<span id="cb60-417"><a href="#cb60-417" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-418"><a href="#cb60-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-419"><a href="#cb60-419" aria-hidden="true" tabindex="-1"></a>The relationship between the threshold value of time and rail's</span>
<span id="cb60-420"><a href="#cb60-420" aria-hidden="true" tabindex="-1"></a>market share seems approximately linear, except for cities where the</span>
<span id="cb60-421"><a href="#cb60-421" aria-hidden="true" tabindex="-1"></a>market share of train is very high (more than 75%). For now, we'll</span>
<span id="cb60-422"><a href="#cb60-422" aria-hidden="true" tabindex="-1"></a>remove these four cities from the sample and plot on @fig-hsfsubsmpl the scatterplot for this restricted sample.^<span class="co">[</span><span class="ot">This is obviously a very raw method to remove the non-linearity. Real time-price models use a much relevant solution, i.e., a transformation of the two variables in order to deal with the non-linearity.</span><span class="co">]</span> </span>
<span id="cb60-423"><a href="#cb60-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-424"><a href="#cb60-424" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-425"><a href="#cb60-425" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: subset_price_time</span></span>
<span id="cb60-426"><a href="#cb60-426" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> <span class="fu">filter</span>(prtime, sr <span class="sc">&lt;</span> <span class="fl">0.75</span>)</span>
<span id="cb60-427"><a href="#cb60-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-428"><a href="#cb60-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-431"><a href="#cb60-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-432"><a href="#cb60-432" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-hsfsubsmpl</span></span>
<span id="cb60-433"><a href="#cb60-433" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Share of rail in function of the threshold time value on a subsample"</span></span>
<span id="cb60-434"><a href="#cb60-434" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-435"><a href="#cb60-435" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb60-436"><a href="#cb60-436" aria-hidden="true" tabindex="-1"></a>    ggrepel<span class="sc">::</span><span class="fu">geom_label_repel</span>(<span class="fu">aes</span>(<span class="at">label =</span> town))</span>
<span id="cb60-437"><a href="#cb60-437" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-438"><a href="#cb60-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-439"><a href="#cb60-439" aria-hidden="true" tabindex="-1"></a>Now, we consider the distribution of the values of time. If $h$ follows</span>
<span id="cb60-440"><a href="#cb60-440" aria-hidden="true" tabindex="-1"></a>a given distribution between $a$ and $b$, train's market share is the</span>
<span id="cb60-441"><a href="#cb60-441" aria-hidden="true" tabindex="-1"></a>share of the population for which the value of time is between $a$ and $h^*$</span>
<span id="cb60-442"><a href="#cb60-442" aria-hidden="true" tabindex="-1"></a>(and plane's market share is the share of the population for which</span>
<span id="cb60-443"><a href="#cb60-443" aria-hidden="true" tabindex="-1"></a>the value of time is between $h^*$ and $b$). The simplest probability</span>
<span id="cb60-444"><a href="#cb60-444" aria-hidden="true" tabindex="-1"></a>distribution is the uniform distribution, which is defined by a</span>
<span id="cb60-445"><a href="#cb60-445" aria-hidden="true" tabindex="-1"></a>constant density equal to $\frac{1}{b-a}$ between $a$ and $b$. It is</span>
<span id="cb60-446"><a href="#cb60-446" aria-hidden="true" tabindex="-1"></a>represented in @fig-unifdist.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{uniform distribution}</span>
<span id="cb60-447"><a href="#cb60-447" aria-hidden="true" tabindex="-1"></a>The area of the rectangle of width $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ and height $[0,</span>
<span id="cb60-448"><a href="#cb60-448" aria-hidden="true" tabindex="-1"></a>\frac{1}{b-a}]$ is 100%, because the whole population has a time value</span>
<span id="cb60-449"><a href="#cb60-449" aria-hidden="true" tabindex="-1"></a>between $a$ and $b$. This rectangle has two components:</span>
<span id="cb60-450"><a href="#cb60-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-451"><a href="#cb60-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a first rectangle of width $<span class="co">[</span><span class="ot">a, h ^ *</span><span class="co">]</span>$ which includes people for</span>
<span id="cb60-452"><a href="#cb60-452" aria-hidden="true" tabindex="-1"></a>  which time value is below  $h^*$ and therefore take the train,</span>
<span id="cb60-453"><a href="#cb60-453" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a second rectangle of width $<span class="co">[</span><span class="ot">h ^*, b</span><span class="co">]</span>$ which includes people for</span>
<span id="cb60-454"><a href="#cb60-454" aria-hidden="true" tabindex="-1"></a>  which time value is higher than $h^*$ and therefore take the plane.</span>
<span id="cb60-455"><a href="#cb60-455" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-456"><a href="#cb60-456" aria-hidden="true" tabindex="-1"></a>Stated differently: $s_f = \frac{h^* - a}{b - a} = -\frac{a}{b - a} +</span>
<span id="cb60-457"><a href="#cb60-457" aria-hidden="true" tabindex="-1"></a>\frac{1}{b-a} h^*$ and this model therefore predicts a linear</span>
<span id="cb60-458"><a href="#cb60-458" aria-hidden="true" tabindex="-1"></a>relationship between $h^*$ and $s_f$, the intercept being</span>
<span id="cb60-459"><a href="#cb60-459" aria-hidden="true" tabindex="-1"></a>$-\frac{a}{b - a}$ and the slope $\frac{1}{b - a}$. Of course, rail's market share depends on other variables than the threshold value of time, so that the linear relationship concerns the conditional expectation of rail's market share. With $y=s_f$ and $x=h^*$, we therefore have a linear model of the form: $\mbox{E}(y | x) = \alpha + \beta x$. Moreover, the two parameters to be estimated $\alpha$ and $\beta$ are functions of the structural parameters of the model $a$ and $b$ which are the minimum and the maximum of the value of time.</span>
<span id="cb60-460"><a href="#cb60-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-461"><a href="#cb60-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-464"><a href="#cb60-464" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-465"><a href="#cb60-465" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-unifdist</span></span>
<span id="cb60-466"><a href="#cb60-466" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Model shares with a uniform distribution"</span></span>
<span id="cb60-467"><a href="#cb60-467" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-468"><a href="#cb60-468" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"./tikz/fig/price_time.png"</span>, <span class="at">auto_pdf =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-469"><a href="#cb60-469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-470"><a href="#cb60-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-471"><a href="#cb60-471" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb60-472"><a href="#cb60-472" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computation of the OLS estimator {#sec-comp_simple_ols}</span></span>
<span id="cb60-473"><a href="#cb60-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-474"><a href="#cb60-474" aria-hidden="true" tabindex="-1"></a>The model we seek to estimate is: $\mbox{E}(y_n \mid x_n) = \alpha+\beta x_n$. The difference between the observed value of $y$ and</span>
<span id="cb60-475"><a href="#cb60-475" aria-hidden="true" tabindex="-1"></a>its conditional expectation is called the **error** for observation</span>
<span id="cb60-476"><a href="#cb60-476" aria-hidden="true" tabindex="-1"></a>$n$:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{errors}</span>
<span id="cb60-477"><a href="#cb60-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-478"><a href="#cb60-478" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-479"><a href="#cb60-479" aria-hidden="true" tabindex="-1"></a>y_n - \mbox{E}(y_n \mid x_n) = \epsilon_n</span>
<span id="cb60-480"><a href="#cb60-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-481"><a href="#cb60-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-482"><a href="#cb60-482" aria-hidden="true" tabindex="-1"></a>The linear regression model can therefore be rewritten as:</span>
<span id="cb60-483"><a href="#cb60-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-484"><a href="#cb60-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-485"><a href="#cb60-485" aria-hidden="true" tabindex="-1"></a>y_n = \mbox{E}(y_n \mid x_n) + \epsilon_n = \alpha + \beta x_n +  \epsilon_n</span>
<span id="cb60-486"><a href="#cb60-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-487"><a href="#cb60-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-488"><a href="#cb60-488" aria-hidden="true" tabindex="-1"></a>$\epsilon_n$ is the error for observation $n$ when the values of the</span>
<span id="cb60-489"><a href="#cb60-489" aria-hidden="true" tabindex="-1"></a>unknown parameters $(\alpha, \beta)$ are set to their true values</span>
<span id="cb60-490"><a href="#cb60-490" aria-hidden="true" tabindex="-1"></a>$(\alpha_o, \beta_o)$. For given values of $(\alpha, \beta)$, obtained</span>
<span id="cb60-491"><a href="#cb60-491" aria-hidden="true" tabindex="-1"></a>using an estimation method, $\epsilon_n$ will be called the</span>
<span id="cb60-492"><a href="#cb60-492" aria-hidden="true" tabindex="-1"></a>**residual** for observation $n$. The residual for an observation is</span>
<span id="cb60-493"><a href="#cb60-493" aria-hidden="true" tabindex="-1"></a>therefore the vertical distance between the point for observation $n$</span>
<span id="cb60-494"><a href="#cb60-494" aria-hidden="true" tabindex="-1"></a>and the regression line. \index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{residuals}</span>
<span id="cb60-495"><a href="#cb60-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-496"><a href="#cb60-496" aria-hidden="true" tabindex="-1"></a>We seek to draw a straight line that is as closest as</span>
<span id="cb60-497"><a href="#cb60-497" aria-hidden="true" tabindex="-1"></a>possible to all the points of our sample as in @fig-hsfsubsmpl. In the</span>
<span id="cb60-498"><a href="#cb60-498" aria-hidden="true" tabindex="-1"></a>simple linear regression model, the distance between a</span>
<span id="cb60-499"><a href="#cb60-499" aria-hidden="true" tabindex="-1"></a>point and the line is defined by the vertical distance, which is the residual for</span>
<span id="cb60-500"><a href="#cb60-500" aria-hidden="true" tabindex="-1"></a>this observation. For the whole sample, we need to aggregate this</span>
<span id="cb60-501"><a href="#cb60-501" aria-hidden="true" tabindex="-1"></a>individual measures of distance. Summing them is not an issue, as there</span>
<span id="cb60-502"><a href="#cb60-502" aria-hidden="true" tabindex="-1"></a>are positive and negative values of the residuals and the sum may be</span>
<span id="cb60-503"><a href="#cb60-503" aria-hidden="true" tabindex="-1"></a>very close to zero even if the individual residuals are very high in</span>
<span id="cb60-504"><a href="#cb60-504" aria-hidden="true" tabindex="-1"></a>absolute values. One solution would be to use the sum of the absolute</span>
<span id="cb60-505"><a href="#cb60-505" aria-hidden="true" tabindex="-1"></a>values of the residuals,^<span class="co">[</span><span class="ot">This estimator is called the least absolute deviations estimator.</span><span class="co">]</span> but with the OLS estimator, we'll consider the sum of the squares of the</span>
<span id="cb60-506"><a href="#cb60-506" aria-hidden="true" tabindex="-1"></a>residuals (also called the residual sum of squares (**RSS**). Taking the squares, as taking the absolute values,</span>
<span id="cb60-507"><a href="#cb60-507" aria-hidden="true" tabindex="-1"></a>removes the sign of the individual residuals and it results in an</span>
<span id="cb60-508"><a href="#cb60-508" aria-hidden="true" tabindex="-1"></a>estimator which has nice mathematical and statistical</span>
<span id="cb60-509"><a href="#cb60-509" aria-hidden="true" tabindex="-1"></a>properties. We'll therefore consider a function $f$ which</span>
<span id="cb60-510"><a href="#cb60-510" aria-hidden="true" tabindex="-1"></a>depends on the value of the response and the covariate in the sample</span>
<span id="cb60-511"><a href="#cb60-511" aria-hidden="true" tabindex="-1"></a>(two vectors $x$ and $y$ of length $N$) and on two unknown parameters</span>
<span id="cb60-512"><a href="#cb60-512" aria-hidden="true" tabindex="-1"></a>$\alpha$ and $\beta$, respectively the intercept and the slope of the</span>
<span id="cb60-513"><a href="#cb60-513" aria-hidden="true" tabindex="-1"></a>regression line:</span>
<span id="cb60-514"><a href="#cb60-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-515"><a href="#cb60-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-516"><a href="#cb60-516" aria-hidden="true" tabindex="-1"></a>f(\alpha, \beta |x, y)=\sum_{n = 1} ^ N (y_n - \alpha - \beta x_n) ^ 2</span>
<span id="cb60-517"><a href="#cb60-517" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-518"><a href="#cb60-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-519"><a href="#cb60-519" aria-hidden="true" tabindex="-1"></a>Note that we write $f$ as a function of the two unknown parameters</span>
<span id="cb60-520"><a href="#cb60-520" aria-hidden="true" tabindex="-1"></a>conditional on the values of $x$ and $y$ for a given sample.</span>
<span id="cb60-521"><a href="#cb60-521" aria-hidden="true" tabindex="-1"></a>First-order conditions for the minimization of $f$ are:</span>
<span id="cb60-522"><a href="#cb60-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-523"><a href="#cb60-523" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-524"><a href="#cb60-524" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb60-525"><a href="#cb60-525" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb60-526"><a href="#cb60-526" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\partial f}{\partial \alpha}  &amp;=&amp; </span>
<span id="cb60-527"><a href="#cb60-527" aria-hidden="true" tabindex="-1"></a>-2 \sum_{n = 1} ^ N \left(y_n - \alpha - \beta x_n\right) = 0 <span class="sc">\\</span></span>
<span id="cb60-528"><a href="#cb60-528" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\partial f}{\partial \beta}  &amp;=&amp; </span>
<span id="cb60-529"><a href="#cb60-529" aria-hidden="true" tabindex="-1"></a>-2\sum_{n=1}^N x_n\left(y_n-\alpha-\beta x_n\right)=0 </span>
<span id="cb60-530"><a href="#cb60-530" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb60-531"><a href="#cb60-531" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb60-532"><a href="#cb60-532" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gradientols}</span>
<span id="cb60-533"><a href="#cb60-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-534"><a href="#cb60-534" aria-hidden="true" tabindex="-1"></a>Or, dividing by $-2$:</span>
<span id="cb60-535"><a href="#cb60-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-536"><a href="#cb60-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-537"><a href="#cb60-537" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N\left(y_n - \alpha -\beta x_n\right) = \sum_{n = 1} ^</span>
<span id="cb60-538"><a href="#cb60-538" aria-hidden="true" tabindex="-1"></a>N \epsilon_n = 0</span>
<span id="cb60-539"><a href="#cb60-539" aria-hidden="true" tabindex="-1"></a>$${#eq-cpoalpha}</span>
<span id="cb60-540"><a href="#cb60-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-541"><a href="#cb60-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-542"><a href="#cb60-542" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-543"><a href="#cb60-543" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N x_n\left(y_n - \alpha - \beta x_n\right) = \sum_{n =</span>
<span id="cb60-544"><a href="#cb60-544" aria-hidden="true" tabindex="-1"></a>1} ^ N x_n \epsilon_n = 0</span>
<span id="cb60-545"><a href="#cb60-545" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cpobeta}</span>
<span id="cb60-546"><a href="#cb60-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-547"><a href="#cb60-547" aria-hidden="true" tabindex="-1"></a>@eq-cpoalpha indicates that the sum (or the mean) of the residuals in the sample is 0. Dividing</span>
<span id="cb60-548"><a href="#cb60-548" aria-hidden="true" tabindex="-1"></a>this expression by $N$ also implies that, denoting $\bar{y}$ and</span>
<span id="cb60-549"><a href="#cb60-549" aria-hidden="true" tabindex="-1"></a>$\bar{x}$ the sample means of the response and of the covariate:</span>
<span id="cb60-550"><a href="#cb60-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-551"><a href="#cb60-551" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-552"><a href="#cb60-552" aria-hidden="true" tabindex="-1"></a>\bar{y} = \alpha + \beta \bar{x}, </span>
<span id="cb60-553"><a href="#cb60-553" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cpoalpha2}</span>
<span id="cb60-554"><a href="#cb60-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-555"><a href="#cb60-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-556"><a href="#cb60-556" aria-hidden="true" tabindex="-1"></a>which means that the sample mean is on the</span>
<span id="cb60-557"><a href="#cb60-557" aria-hidden="true" tabindex="-1"></a>regression line.</span>
<span id="cb60-558"><a href="#cb60-558" aria-hidden="true" tabindex="-1"></a>Denoting $\hat{\epsilon}_n$ the residuals of the OLS estimator, @eq-cpobeta states that $\sum_n x_n \hat{\epsilon} / N=0$, i.e., that the</span>
<span id="cb60-559"><a href="#cb60-559" aria-hidden="true" tabindex="-1"></a>average cross-product of the covariate and the residual is 0. But, as</span>
<span id="cb60-560"><a href="#cb60-560" aria-hidden="true" tabindex="-1"></a>the sample mean of the residuals $\bar{\hat{\epsilon}}$ is 0, this expression is also the</span>
<span id="cb60-561"><a href="#cb60-561" aria-hidden="true" tabindex="-1"></a>covariance between the covariate and the residuals:</span>
<span id="cb60-562"><a href="#cb60-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-563"><a href="#cb60-563" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-564"><a href="#cb60-564" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{x\hat{\epsilon}} = \frac{\sum_{n = 1} ^ N (x_n -</span>
<span id="cb60-565"><a href="#cb60-565" aria-hidden="true" tabindex="-1"></a>\bar{x})(\hat{\epsilon}_n - \bar{\hat{\epsilon}})}{N} = </span>
<span id="cb60-566"><a href="#cb60-566" aria-hidden="true" tabindex="-1"></a>\frac{\sum_{n = 1} ^ N x_n \hat{\epsilon}_n}{N} - \bar{x}\bar{\hat{\epsilon}}=</span>
<span id="cb60-567"><a href="#cb60-567" aria-hidden="true" tabindex="-1"></a>\frac{\sum_{n = 1} ^ N x_n \hat{\epsilon}_n}{N}=</span>
<span id="cb60-568"><a href="#cb60-568" aria-hidden="true" tabindex="-1"></a>0</span>
<span id="cb60-569"><a href="#cb60-569" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-570"><a href="#cb60-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-571"><a href="#cb60-571" aria-hidden="true" tabindex="-1"></a>which means that the regression line is such that there is no correlation</span>
<span id="cb60-572"><a href="#cb60-572" aria-hidden="true" tabindex="-1"></a>between the covariate and the residuals in the sample.</span>
<span id="cb60-573"><a href="#cb60-573" aria-hidden="true" tabindex="-1"></a>Subtracting $\bar{y} - \alpha - \beta \bar{x}$, which is</span>
<span id="cb60-574"><a href="#cb60-574" aria-hidden="true" tabindex="-1"></a>0 (see @eq-cpoalpha2) from @eq-cpobeta, one gets:</span>
<span id="cb60-575"><a href="#cb60-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-576"><a href="#cb60-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-577"><a href="#cb60-577" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1}^N x_n\left[\left(y_n - \bar{y}\right)-\beta \left(x_n -</span>
<span id="cb60-578"><a href="#cb60-578" aria-hidden="true" tabindex="-1"></a>\bar{x}\right)\right] = 0 </span>
<span id="cb60-579"><a href="#cb60-579" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-580"><a href="#cb60-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-581"><a href="#cb60-581" aria-hidden="true" tabindex="-1"></a>Moreover, $\sum_{n = 1} ^ N \bar{x}\left[\left(y_n - \bar{y}\right) -</span>
<span id="cb60-582"><a href="#cb60-582" aria-hidden="true" tabindex="-1"></a>\beta\left(x_n - \bar{x}\right)\right] = 0$ and so:</span>
<span id="cb60-583"><a href="#cb60-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-584"><a href="#cb60-584" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-585"><a href="#cb60-585" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N \left(x_n - \bar{x}\right)\left[\left(y_n -</span>
<span id="cb60-586"><a href="#cb60-586" aria-hidden="true" tabindex="-1"></a>\bar{y}\right) - \beta\left(x_n - \bar{x}\right)\right] = 0 </span>
<span id="cb60-587"><a href="#cb60-587" aria-hidden="true" tabindex="-1"></a>$$ {#eq-normal_equation}</span>
<span id="cb60-588"><a href="#cb60-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-589"><a href="#cb60-589" aria-hidden="true" tabindex="-1"></a>Solving for $\beta$, we finally get the estimator of the slope:</span>
<span id="cb60-590"><a href="#cb60-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-591"><a href="#cb60-591" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-592"><a href="#cb60-592" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_{n = 1} ^ N</span>
<span id="cb60-593"><a href="#cb60-593" aria-hidden="true" tabindex="-1"></a>(x_n - \bar{x})(y_n - \bar{y})}{\sum_{n = 1} ^ N</span>
<span id="cb60-594"><a href="#cb60-594" aria-hidden="true" tabindex="-1"></a>(x_n - \bar{x}) ^ 2} = </span>
<span id="cb60-595"><a href="#cb60-595" aria-hidden="true" tabindex="-1"></a>\frac{S_{xy}}{S_{xx}}=</span>
<span id="cb60-596"><a href="#cb60-596" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x ^ 2}=</span>
<span id="cb60-597"><a href="#cb60-597" aria-hidden="true" tabindex="-1"></a>\hat{\rho}_{xy} \frac{\hat{\sigma}_y}{\hat{\sigma}_x}</span>
<span id="cb60-598"><a href="#cb60-598" aria-hidden="true" tabindex="-1"></a>$$ {#eq-slrbeta}</span>
<span id="cb60-599"><a href="#cb60-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-600"><a href="#cb60-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-601"><a href="#cb60-601" aria-hidden="true" tabindex="-1"></a>@eq-slrbeta gives three formulations for this estimator:</span>
<span id="cb60-602"><a href="#cb60-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-603"><a href="#cb60-603" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the first indicates that it is the ratio of the covariation of</span>
<span id="cb60-604"><a href="#cb60-604" aria-hidden="true" tabindex="-1"></a>  $x$ and $y$: $S_{xy}=\sum_{n = 1} ^ N (x_n - \bar{x})(y_n -</span>
<span id="cb60-605"><a href="#cb60-605" aria-hidden="true" tabindex="-1"></a>  \bar{y})$ and the variation of $x$: </span>
<span id="cb60-606"><a href="#cb60-606" aria-hidden="true" tabindex="-1"></a>  $S_{xx}= \sum_{n = 1} ^ N (x_n - \bar{x}) ^ 2$,</span>
<span id="cb60-607"><a href="#cb60-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the second is obtained by dividing both sides of the ratio by the sample</span>
<span id="cb60-608"><a href="#cb60-608" aria-hidden="true" tabindex="-1"></a>  size, so that the estimator is now the ratio of the sample covariance</span>
<span id="cb60-609"><a href="#cb60-609" aria-hidden="true" tabindex="-1"></a>  between $x$ and $y$ and the sample variance of $x$,</span>
<span id="cb60-610"><a href="#cb60-610" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the third is obtained by introducing the coefficient of</span>
<span id="cb60-611"><a href="#cb60-611" aria-hidden="true" tabindex="-1"></a>  correlation between $x$ and $y$: $\hat{\rho}_{xy}</span>
<span id="cb60-612"><a href="#cb60-612" aria-hidden="true" tabindex="-1"></a>  =\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x\hat{\sigma}_y}$, so that the</span>
<span id="cb60-613"><a href="#cb60-613" aria-hidden="true" tabindex="-1"></a>  estimator is also expressed as the product of the coefficient of</span>
<span id="cb60-614"><a href="#cb60-614" aria-hidden="true" tabindex="-1"></a>  correlation and the ratio of the standard deviations. </span>
<span id="cb60-615"><a href="#cb60-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-616"><a href="#cb60-616" aria-hidden="true" tabindex="-1"></a>This last formulation is particularly intuitive: $\hat{\rho}_{xy}$ is</span>
<span id="cb60-617"><a href="#cb60-617" aria-hidden="true" tabindex="-1"></a>a pure measure of the correlation between the covariate and the</span>
<span id="cb60-618"><a href="#cb60-618" aria-hidden="true" tabindex="-1"></a>response. This number has no unit and lies in the $-1$/$+1$ interval, a value of $-1$ ($+1$)</span>
<span id="cb60-619"><a href="#cb60-619" aria-hidden="true" tabindex="-1"></a>indicating a perfect negative (positive) correlation and the value of 0 no correlation. The ratio of the standard deviations gives the relevant</span>
<span id="cb60-620"><a href="#cb60-620" aria-hidden="true" tabindex="-1"></a>unit to the slope, which is the unit of $y$ divided by the unit of $x$.</span>
<span id="cb60-621"><a href="#cb60-621" aria-hidden="true" tabindex="-1"></a>With this estimator of the slope in hand, we easily get the estimator</span>
<span id="cb60-622"><a href="#cb60-622" aria-hidden="true" tabindex="-1"></a>of the intercept using @eq-cpoalpha2:</span>
<span id="cb60-623"><a href="#cb60-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-624"><a href="#cb60-624" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-625"><a href="#cb60-625" aria-hidden="true" tabindex="-1"></a>\hat{\alpha}=\bar{y}-\hat{\beta} \hat{x}</span>
<span id="cb60-626"><a href="#cb60-626" aria-hidden="true" tabindex="-1"></a>$$ {#eq-slralpha}</span>
<span id="cb60-627"><a href="#cb60-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-628"><a href="#cb60-628" aria-hidden="true" tabindex="-1"></a>Consider now that prior to the estimation, we sustracted from the covariate</span>
<span id="cb60-629"><a href="#cb60-629" aria-hidden="true" tabindex="-1"></a>and the response their sample means. We then used $\tilde{y}_n = y_n -</span>
<span id="cb60-630"><a href="#cb60-630" aria-hidden="true" tabindex="-1"></a>\bar{y}$ and $\tilde{x}_n = x_n - \bar{x}$ as the response and the</span>
<span id="cb60-631"><a href="#cb60-631" aria-hidden="true" tabindex="-1"></a>covariate. In this case, as the mean of these two transformed</span>
<span id="cb60-632"><a href="#cb60-632" aria-hidden="true" tabindex="-1"></a>variables are zero, the intercept is 0 (from @eq-slralpha) and the</span>
<span id="cb60-633"><a href="#cb60-633" aria-hidden="true" tabindex="-1"></a>slope is simply $\sum \tilde{y}_n \tilde{x}_n / \sum \tilde{x}_n ^ 2$. </span>
<span id="cb60-634"><a href="#cb60-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-635"><a href="#cb60-635" aria-hidden="true" tabindex="-1"></a>Once the parameters of the regression line have been computed, one can</span>
<span id="cb60-636"><a href="#cb60-636" aria-hidden="true" tabindex="-1"></a>define the **fitted value** for observation $n$ as the value returned by</span>
<span id="cb60-637"><a href="#cb60-637" aria-hidden="true" tabindex="-1"></a>the regression line for $x_n$, which is:</span>
<span id="cb60-638"><a href="#cb60-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-639"><a href="#cb60-639" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-640"><a href="#cb60-640" aria-hidden="true" tabindex="-1"></a>\hat{y}_n = \hat{\alpha} + \hat{\beta} x_n</span>
<span id="cb60-641"><a href="#cb60-641" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-642"><a href="#cb60-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-643"><a href="#cb60-643" aria-hidden="true" tabindex="-1"></a>By definition, we have for the fitted model: $y_n = \hat{y}_n +</span>
<span id="cb60-644"><a href="#cb60-644" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_n$. Note that, for the "true" model, we have $y_n =</span>
<span id="cb60-645"><a href="#cb60-645" aria-hidden="true" tabindex="-1"></a>\mbox{E}(y | x = x_n) + \epsilon_n$, so that the residuals</span>
<span id="cb60-646"><a href="#cb60-646" aria-hidden="true" tabindex="-1"></a>are an estimation of the errors and the fitted values $\hat{y}_n$ are an estimation of the</span>
<span id="cb60-647"><a href="#cb60-647" aria-hidden="true" tabindex="-1"></a>conditional expectations of $y$. Moreover, denoting $\bar{\hat{y}}$ the sample mean of the fitted values:</span>
<span id="cb60-648"><a href="#cb60-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-649"><a href="#cb60-649" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-650"><a href="#cb60-650" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n (\hat{y}_n - \bar{\hat{y}})(\hat{\epsilon}_n -</span>
<span id="cb60-651"><a href="#cb60-651" aria-hidden="true" tabindex="-1"></a>\bar{\hat{\epsilon}})}{N}=</span>
<span id="cb60-652"><a href="#cb60-652" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n \hat{y}_n \hat{\epsilon}_n}{N} -</span>
<span id="cb60-653"><a href="#cb60-653" aria-hidden="true" tabindex="-1"></a>\bar{\hat{y}}\bar{\hat{\epsilon}}</span>
<span id="cb60-654"><a href="#cb60-654" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb60-655"><a href="#cb60-655" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n \hat{y}_n \hat{\epsilon}_n}{N}</span>
<span id="cb60-656"><a href="#cb60-656" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb60-657"><a href="#cb60-657" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n (\hat{\alpha} + \hat{\beta} x_n) \hat{\epsilon}_n}{N}</span>
<span id="cb60-658"><a href="#cb60-658" aria-hidden="true" tabindex="-1"></a>= 0</span>
<span id="cb60-659"><a href="#cb60-659" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-660"><a href="#cb60-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-661"><a href="#cb60-661" aria-hidden="true" tabindex="-1"></a>Therefore, there is no correlation between $\hat{y}$ and $\hat{\epsilon}$, which is clear as $\hat{y}$ is a linear function of $x$ and $x$ is uncorrelated with $\hat{\epsilon}$.</span>
<span id="cb60-662"><a href="#cb60-662" aria-hidden="true" tabindex="-1"></a>$(\hat{\alpha}, \hat{\beta})$ is an optimum of the objective function. To check that this optimum is a minimum, we have to compute the second derivatives. From @eq-gradientols, we get: $\frac{\partial^2 f}{\partial \alpha ^ 2}= 2N$, $\frac{\partial^2 f}{\partial \beta ^ 2}= 2\sum_n x_n ^ 2$ and $\frac{\partial^2 f}{\partial \alpha \partial \beta}= 2\sum_n x_n$. We need, for a maximum, positive direct second derivatives, which is obviously the case, and also a positive determinant of the matrix of second derivatives:</span>
<span id="cb60-663"><a href="#cb60-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-664"><a href="#cb60-664" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-665"><a href="#cb60-665" aria-hidden="true" tabindex="-1"></a>D = \frac{\partial^2 f}{\partial \alpha ^ 2}\frac{\partial^2 f}{\partial \beta ^ 2}</span>
<span id="cb60-666"><a href="#cb60-666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\left(\frac{\partial^2 f}{\partial \alpha \partial \beta}\right)^2 </span>
<span id="cb60-667"><a href="#cb60-667" aria-hidden="true" tabindex="-1"></a>= 4 N \sum_n x_n ^ 2 - 4  \left(\sum_n x_n\right) ^ 2 = 4N^2\left(\frac{\sum_n x_n ^ 2}{N} - \bar{x}^2\right)</span>
<span id="cb60-668"><a href="#cb60-668" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 0</span></span>
<span id="cb60-669"><a href="#cb60-669" aria-hidden="true" tabindex="-1"></a><span class="at">$$</span></span>
<span id="cb60-670"><a href="#cb60-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-671"><a href="#cb60-671" aria-hidden="true" tabindex="-1"></a>which is the case as the term in brackets is the variance of $x$ and is therefore positive.</span>
<span id="cb60-672"><a href="#cb60-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-673"><a href="#cb60-673" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geometry of least squares, variance decomposition and \mbox{coefficient} of determination {#sec-simple_geometry}</span></span>
<span id="cb60-674"><a href="#cb60-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-675"><a href="#cb60-675" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{geometry of least squares!simple linear regression model|(}</span>
<span id="cb60-676"><a href="#cb60-676" aria-hidden="true" tabindex="-1"></a>The OLS estimator relies on variances and covariances of several observable ($y$ and $x$) and computed ($\hat{y}$, $\hat{\epsilon}$) variables. Its properties can be nicely illustrated using vector algebra, each variable being represented by a vector, and by plotting these vectors.^<span class="co">[</span><span class="ot">For a detailed presentation of the geometry of least squares, see @DAVI:MACK:04, chapter 2.</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Davidson}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{McKinnon}</span>
<span id="cb60-677"><a href="#cb60-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-678"><a href="#cb60-678" aria-hidden="true" tabindex="-1"></a><span class="fu">### Vectors, variance and covariance</span></span>
<span id="cb60-679"><a href="#cb60-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-680"><a href="#cb60-680" aria-hidden="true" tabindex="-1"></a>Every variable $z$ used in a regression is a vector of $\mathcal{R}_N$, i.e., a</span>
<span id="cb60-681"><a href="#cb60-681" aria-hidden="true" tabindex="-1"></a>set of $N$ real values: $z^\top = (z_1, z_2, \ldots, z_N)$.</span>
<span id="cb60-682"><a href="#cb60-682" aria-hidden="true" tabindex="-1"></a>The length (or norm) of the vector is: $\|z\| = \sqrt{\sum_{n=1}^N z_n ^ 2}$. Remember</span>
<span id="cb60-683"><a href="#cb60-683" aria-hidden="true" tabindex="-1"></a>that the OLS estimator can always be computed with data measured in</span>
<span id="cb60-684"><a href="#cb60-684" aria-hidden="true" tabindex="-1"></a>deviations from their sample mean. Then, $\|z\| ^ 2 / N$ is the</span>
<span id="cb60-685"><a href="#cb60-685" aria-hidden="true" tabindex="-1"></a>variance of the variable, or the norm of the vector is $\sqrt{N}$ times</span>
<span id="cb60-686"><a href="#cb60-686" aria-hidden="true" tabindex="-1"></a>the standard deviation of the corresponding variable.</span>
<span id="cb60-687"><a href="#cb60-687" aria-hidden="true" tabindex="-1"></a>The inner (or scalar) product of two vectors is denoted by</span>
<span id="cb60-688"><a href="#cb60-688" aria-hidden="true" tabindex="-1"></a>$z ^ \top w = w ^ \top z = \sum_{n=1} ^ N z_n w\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Davidson}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{McKinnon}_n$ (note that the inner</span>
<span id="cb60-689"><a href="#cb60-689" aria-hidden="true" tabindex="-1"></a>product is commutative). For corresponding variables expressed in</span>
<span id="cb60-690"><a href="#cb60-690" aria-hidden="true" tabindex="-1"></a>deviations from their respective means it is, up to the $1/N$ factor, the</span>
<span id="cb60-691"><a href="#cb60-691" aria-hidden="true" tabindex="-1"></a>covariance between the two variables. Denoting $\theta$ the angle formed</span>
<span id="cb60-692"><a href="#cb60-692" aria-hidden="true" tabindex="-1"></a>by the two vectors, we also have:^<span class="co">[</span><span class="ot">See @DAVI:MACK:04, p. 48.</span><span class="co">]</span></span>
<span id="cb60-693"><a href="#cb60-693" aria-hidden="true" tabindex="-1"></a>$z ^ \top w = \cos \theta \|z\| \|w\|$.</span>
<span id="cb60-694"><a href="#cb60-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-695"><a href="#cb60-695" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-696"><a href="#cb60-696" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vectors_functions</span></span>
<span id="cb60-697"><a href="#cb60-697" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-698"><a href="#cb60-698" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb60-699"><a href="#cb60-699" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">4.5</span>, <span class="dv">6</span>)</span>
<span id="cb60-700"><a href="#cb60-700" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="fl">4.5</span>)</span>
<span id="cb60-701"><a href="#cb60-701" aria-hidden="true" tabindex="-1"></a>norm_z <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(z <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb60-702"><a href="#cb60-702" aria-hidden="true" tabindex="-1"></a>norm_x <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(x <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb60-703"><a href="#cb60-703" aria-hidden="true" tabindex="-1"></a>norm_w <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(w <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb60-704"><a href="#cb60-704" aria-hidden="true" tabindex="-1"></a>theta_x <span class="ot">&lt;-</span> <span class="fu">acos</span>(x[<span class="dv">1</span>] <span class="sc">/</span> norm_x)</span>
<span id="cb60-705"><a href="#cb60-705" aria-hidden="true" tabindex="-1"></a>theta_z <span class="ot">&lt;-</span> <span class="fu">acos</span>(z[<span class="dv">1</span>] <span class="sc">/</span> norm_z)</span>
<span id="cb60-706"><a href="#cb60-706" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> theta_z <span class="sc">-</span> theta_x</span>
<span id="cb60-707"><a href="#cb60-707" aria-hidden="true" tabindex="-1"></a>dg <span class="ot">&lt;-</span> <span class="cf">function</span>(x) x <span class="sc">/</span> pi <span class="sc">*</span> <span class="dv">180</span></span>
<span id="cb60-708"><a href="#cb60-708" aria-hidden="true" tabindex="-1"></a>prtv <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">paste</span>(<span class="st">"("</span>, x[<span class="dv">1</span>], <span class="st">","</span>, x[<span class="dv">2</span>], <span class="st">")"</span>)</span>
<span id="cb60-709"><a href="#cb60-709" aria-hidden="true" tabindex="-1"></a>nrm <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sqrt</span>(<span class="fu">sum</span>(x <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb60-710"><a href="#cb60-710" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-711"><a href="#cb60-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-712"><a href="#cb60-712" aria-hidden="true" tabindex="-1"></a>Consider as an example: $x = <span class="in">`r prtv(x)`</span>$, $z = <span class="in">`r prtv(z)`</span>$ and</span>
<span id="cb60-713"><a href="#cb60-713" aria-hidden="true" tabindex="-1"></a>$w = <span class="in">`r prtv(w)`</span>$. The three vectors are plotted in @fig-mafig.</span>
<span id="cb60-714"><a href="#cb60-714" aria-hidden="true" tabindex="-1"></a>The norm of $x$ is</span>
<span id="cb60-715"><a href="#cb60-715" aria-hidden="true" tabindex="-1"></a>$\|x\|=\sqrt{ <span class="in">`r x[1]`</span> ^ 2 + <span class="in">`r x[2]`</span> ^ 2} = <span class="in">`r nrm(x)`</span>$. Similarly,</span>
<span id="cb60-716"><a href="#cb60-716" aria-hidden="true" tabindex="-1"></a>$\|z\| = <span class="in">`r nrm(z)`</span>$ and $\|w\| = <span class="in">`r nrm(w)`</span>$.</span>
<span id="cb60-717"><a href="#cb60-717" aria-hidden="true" tabindex="-1"></a>The cosinus of the angle formed by $x$ and $z$ with the horizontal axis</span>
<span id="cb60-718"><a href="#cb60-718" aria-hidden="true" tabindex="-1"></a>is $\cos \theta_x = <span class="in">`r x[1]`</span> / <span class="in">`r nrm(x)`</span> = <span class="in">`r round(x[1] / nrm(x), 3)`</span>$</span>
<span id="cb60-719"><a href="#cb60-719" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb60-720"><a href="#cb60-720" aria-hidden="true" tabindex="-1"></a>$\cos \theta_z = <span class="in">`r z[1]`</span> / <span class="in">`r nrm(z)`</span> = <span class="in">`r round(z[1] / nrm(z), 3)`</span>$.</span>
<span id="cb60-721"><a href="#cb60-721" aria-hidden="true" tabindex="-1"></a>The angle formed by $x$ and $z$ is therefore:</span>
<span id="cb60-722"><a href="#cb60-722" aria-hidden="true" tabindex="-1"></a>$\theta = \arccos <span class="in">`r z[1] / nrm(z)`</span> - \arccos <span class="in">`r x[1] / nrm(x)`</span> = <span class="in">`r round(theta_z - theta_x, 3)`</span>$,</span>
<span id="cb60-723"><a href="#cb60-723" aria-hidden="true" tabindex="-1"></a>with $\cos <span class="in">`r round(theta, 3)`</span> = <span class="in">`r cos(theta)`</span>$. We can then check that</span>
<span id="cb60-724"><a href="#cb60-724" aria-hidden="true" tabindex="-1"></a>$z ^ \top x = <span class="in">`r x[1]`</span> \times <span class="in">`r z[1]`</span> + <span class="in">`r x[2]`</span> \times <span class="in">`r z[2]`</span> = <span class="in">`r sum(x * z)`</span>$,</span>
<span id="cb60-725"><a href="#cb60-725" aria-hidden="true" tabindex="-1"></a>which is equal to:</span>
<span id="cb60-726"><a href="#cb60-726" aria-hidden="true" tabindex="-1"></a>$\cos \theta \|z\| \|w\| = <span class="in">`r cos(theta)`</span> \times <span class="in">`r nrm(z)`</span> \times <span class="in">`r nrm(x)`</span>$.</span>
<span id="cb60-727"><a href="#cb60-727" aria-hidden="true" tabindex="-1"></a>As the absolute value of $\cos \theta$ is necessarily lower than or equal to 1,</span>
<span id="cb60-728"><a href="#cb60-728" aria-hidden="true" tabindex="-1"></a>the inner product of two vectors is lower than the product of the norms</span>
<span id="cb60-729"><a href="#cb60-729" aria-hidden="true" tabindex="-1"></a>of the two vectors, and $\cos \theta = \frac{x ^ \top z}{\|x\| \|z\|}$ is</span>
<span id="cb60-730"><a href="#cb60-730" aria-hidden="true" tabindex="-1"></a>the ratio of the covariance between $x$ and $z$ and the product of their</span>
<span id="cb60-731"><a href="#cb60-731" aria-hidden="true" tabindex="-1"></a>standard deviations, which is the coefficient of correlation between the</span>
<span id="cb60-732"><a href="#cb60-732" aria-hidden="true" tabindex="-1"></a>two underlying variables $x$ and $z$. Consider now $z$ and $w$. Their</span>
<span id="cb60-733"><a href="#cb60-733" aria-hidden="true" tabindex="-1"></a>inner product is:</span>
<span id="cb60-734"><a href="#cb60-734" aria-hidden="true" tabindex="-1"></a>$z ^ \top x = <span class="in">`r z[1]`</span> \times <span class="in">`r w[1]`</span> + <span class="in">`r z[2]`</span> \times <span class="in">`r w[2]`</span> = <span class="in">`r sum(z * w)`</span>$.</span>
<span id="cb60-735"><a href="#cb60-735" aria-hidden="true" tabindex="-1"></a>This is because $z$ and $w$ are two orthogonal vectors, which means that</span>
<span id="cb60-736"><a href="#cb60-736" aria-hidden="true" tabindex="-1"></a>the two underlying variables are uncorrelated.</span>
<span id="cb60-737"><a href="#cb60-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-738"><a href="#cb60-738" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-739"><a href="#cb60-739" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-mafig</span></span>
<span id="cb60-740"><a href="#cb60-740" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Vector algebra"</span></span>
<span id="cb60-741"><a href="#cb60-741" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-742"><a href="#cb60-742" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"./tikz/fig/vectors2D.png"</span>, <span class="at">auto_pdf =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-743"><a href="#cb60-743" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-744"><a href="#cb60-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-745"><a href="#cb60-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-746"><a href="#cb60-746" aria-hidden="true" tabindex="-1"></a><span class="fu">### Geometry of least squares {#sec-geometry_ols}</span></span>
<span id="cb60-747"><a href="#cb60-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-748"><a href="#cb60-748" aria-hidden="true" tabindex="-1"></a>The geometry of the simple linear regression model is represented in @fig-smplregmodel.</span>
<span id="cb60-749"><a href="#cb60-749" aria-hidden="true" tabindex="-1"></a>With $N = 2$, $x$ and $y$ are two vectors in a plane. For the "true" model, the $y$ vector is the sum of two vectors: $\beta x$ (which is the conditional expectation of $y$) and $\epsilon$, which is the vector of errors. For the estimated model, $y$ is the sum of the fitted values $\hat{y} = \hat{\beta} x$ and the residuals $\hat{\epsilon}$. Using the OLS estimator, we must minimize the sum of squares of the residuals, i.e., the norm of the $\hat{\epsilon}$ vector. Obviously, this implies that $\hat{\epsilon}$ should be orthogonal to $\hat{y}$ and therefore to $x$, which implies that the residuals are uncorrelated to the fitted values (and to the covariate) in the sample. </span>
<span id="cb60-750"><a href="#cb60-750" aria-hidden="true" tabindex="-1"></a>Note also that, except in the unlikely case where $\hat{\beta} = \beta$, $\|\hat{\epsilon}\| &lt; \|\epsilon\|$ which means that the residuals have a smaller variance than the errors. Note finally that what determines $\hat{y}$ and $\hat{\epsilon}$ is not $x$</span>
<span id="cb60-751"><a href="#cb60-751" aria-hidden="true" tabindex="-1"></a>per se, but the subspace defined by it, in our case the horizontal straight line. For</span>
<span id="cb60-752"><a href="#cb60-752" aria-hidden="true" tabindex="-1"></a>example, consider the regression of $y$ on $z = 0.5 x$; we would then obtain exactly the same values for $\hat{y}$ and $\hat{\epsilon}$, the only difference being that the estimator of $\beta$ would be two times larger.</span>
<span id="cb60-753"><a href="#cb60-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-754"><a href="#cb60-754" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-755"><a href="#cb60-755" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-smplregmodel</span></span>
<span id="cb60-756"><a href="#cb60-756" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Geometry of the simple regression model"</span></span>
<span id="cb60-757"><a href="#cb60-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-758"><a href="#cb60-758" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"./tikz/fig/OLS2D.png"</span>, <span class="at">auto_pdf =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-759"><a href="#cb60-759" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-760"><a href="#cb60-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-761"><a href="#cb60-761" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{geometry of least squares!simple linear regression model|)}</span>
<span id="cb60-762"><a href="#cb60-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-763"><a href="#cb60-763" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variance decomposition and the $R^2$ {#sec-vardecomp_R2}</span></span>
<span id="cb60-764"><a href="#cb60-764" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{coefficient of determination|(}</span>
<span id="cb60-765"><a href="#cb60-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-766"><a href="#cb60-766" aria-hidden="true" tabindex="-1"></a>For one observation $n$, we have: </span>
<span id="cb60-767"><a href="#cb60-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-768"><a href="#cb60-768" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-769"><a href="#cb60-769" aria-hidden="true" tabindex="-1"></a>y_n - \bar{y}=\left(y_n - \hat{y}_n\right)+\left(\hat{y}_n - \bar{y}\right)</span>
<span id="cb60-770"><a href="#cb60-770" aria-hidden="true" tabindex="-1"></a>$$  {#eq-varobs}</span>
<span id="cb60-771"><a href="#cb60-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-772"><a href="#cb60-772" aria-hidden="true" tabindex="-1"></a>The difference between $y$ for individual $n$ and the sample mean</span>
<span id="cb60-773"><a href="#cb60-773" aria-hidden="true" tabindex="-1"></a>is therefore the sum of:</span>
<span id="cb60-774"><a href="#cb60-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-775"><a href="#cb60-775" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a residual variation: $\left(y_n - \hat{y}_n\right) = \hat{\epsilon}_n$,</span>
<span id="cb60-776"><a href="#cb60-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>an explained variation: $\hat{y}_n - \bar{y}=\hat{\beta}(x_n -</span>
<span id="cb60-777"><a href="#cb60-777" aria-hidden="true" tabindex="-1"></a>  \bar{x})$.</span>
<span id="cb60-778"><a href="#cb60-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-779"><a href="#cb60-779" aria-hidden="true" tabindex="-1"></a>Taking the square of @eq-varobs and summing for all $n$, we get:</span>
<span id="cb60-780"><a href="#cb60-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-781"><a href="#cb60-781" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-782"><a href="#cb60-782" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb60-783"><a href="#cb60-783" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N(y_n - \bar{y})^2 &amp; = &amp; \sum_{n = 1}^ N</span>
<span id="cb60-784"><a href="#cb60-784" aria-hidden="true" tabindex="-1"></a>\left(\hat{\epsilon}_n + \hat{\beta}(x_n - \bar{x})\right) ^ 2 <span class="sc">\\</span></span>
<span id="cb60-785"><a href="#cb60-785" aria-hidden="true" tabindex="-1"></a>&amp; = &amp; \sum_{n = 1} ^ N \hat{\epsilon}_n ^ 2 + \hat{\beta} ^ 2 \sum_{n</span>
<span id="cb60-786"><a href="#cb60-786" aria-hidden="true" tabindex="-1"></a>= 1} ^ N (x_n - \bar{x}) ^ 2 <span class="sc">\\</span></span>
<span id="cb60-787"><a href="#cb60-787" aria-hidden="true" tabindex="-1"></a>&amp; + &amp; 2 \hat{\beta}\sum_{n = 1} ^ N \hat{\epsilon}_n x_n -</span>
<span id="cb60-788"><a href="#cb60-788" aria-hidden="true" tabindex="-1"></a>2\hat{\beta}\bar{x}\sum_{n = 1} ^ N \hat{\epsilon}_n</span>
<span id="cb60-789"><a href="#cb60-789" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb60-790"><a href="#cb60-790" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-791"><a href="#cb60-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-792"><a href="#cb60-792" aria-hidden="true" tabindex="-1"></a>But $\sum_{n = 1} ^ N \hat{\epsilon}_n x_n = 0$ (@eq-cpobeta) and $\sum_{n = 1} ^ N</span>
<span id="cb60-793"><a href="#cb60-793" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_n = 0$ (@eq-cpoalpha), so that:</span>
<span id="cb60-794"><a href="#cb60-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-795"><a href="#cb60-795" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-796"><a href="#cb60-796" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N (y_n - \bar{y}) ^ 2 = \hat{\beta} ^ 2</span>
<span id="cb60-797"><a href="#cb60-797" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N (x_n - \bar{x}) ^ 2+\sum_{n = 1} ^ N</span>
<span id="cb60-798"><a href="#cb60-798" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_n ^ 2</span>
<span id="cb60-799"><a href="#cb60-799" aria-hidden="true" tabindex="-1"></a>$$ {#eq-variance_decomposition}</span>
<span id="cb60-800"><a href="#cb60-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-801"><a href="#cb60-801" aria-hidden="true" tabindex="-1"></a>This equation indicates that the total sum of squares (TSS) of the response equals the sum of the explained sum of squares (ESS) and of the residual sum of squares (RSS). This latter term is also called the **deviance**, and it is the objective function for the OLS estimator. Dividing by $N$, we also have on the left-hand size the variance of $y$ and on the right-hand side the sum of the variances of $\hat{y}$ and $\hat{\epsilon}$. This is the formula of the **variance decomposition** of the response. It can be easily understood using @fig-smplregmodel. It is clear from this figure that $y = \hat{y} + \hat{\epsilon}$ and that $\hat{y}$ and $\hat{\epsilon}$ are orthogonal. Therefore, applying the Pythagorean theorem, we have $\|y\|^2 = \|\hat{y}\|^2 + \|\hat{\epsilon}\|^2$. Up to the $1/N$ factor, if the response is measured in deviation from its sample mean, we have on the left the total variance of $y$ and on the right the sum of the variances of $\hat{y}$ and $\hat{\epsilon}$. Note that it is a unique feature of the ordinary least squares estimator. Any other estimator will generally result with a vector of residuals which won't be orthogonal to $\hat{y}$ (or to $x$) and therefore @eq-variance_decomposition won't apply.</span>
<span id="cb60-802"><a href="#cb60-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-803"><a href="#cb60-803" aria-hidden="true" tabindex="-1"></a>The coefficient of determination, denoted by $R^2$, measures the share of the variance of the response which is explained by the model, or one minus the share of the residual variation. We then have:</span>
<span id="cb60-804"><a href="#cb60-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-805"><a href="#cb60-805" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-806"><a href="#cb60-806" aria-hidden="true" tabindex="-1"></a>R^2=\frac{\hat{\beta}^2\sum_{n=1}^N(x_n-\bar{x})^2}{\sum_{n=1}^N (y_n-\bar{y})^2} = 1 - \frac{\sum_{n=1}^N\hat{\epsilon}_n^2}{\sum_{n=1}^N (y_n-\bar{y})^2}</span>
<span id="cb60-807"><a href="#cb60-807" aria-hidden="true" tabindex="-1"></a>$$ {#eq-r2_slrm}</span>
<span id="cb60-808"><a href="#cb60-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-809"><a href="#cb60-809" aria-hidden="true" tabindex="-1"></a>using @eq-slrbeta, we finally get:</span>
<span id="cb60-810"><a href="#cb60-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-811"><a href="#cb60-811" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-812"><a href="#cb60-812" aria-hidden="true" tabindex="-1"></a>  R^2=\left[\frac{\sum_{n=1}^N(x_n-\bar{x})(y_n-\bar{y})}{\sum_{n=1}^N</span>
<span id="cb60-813"><a href="#cb60-813" aria-hidden="true" tabindex="-1"></a>    (x_n-\bar{x})^2}\right]^2</span>
<span id="cb60-814"><a href="#cb60-814" aria-hidden="true" tabindex="-1"></a>\frac{\sum_{n=1}^N(x_n-\bar{x})^2}{\sum_{n=1}^N (y_n-\bar{y})^2} = </span>
<span id="cb60-815"><a href="#cb60-815" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot">\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x ^ 2}\right</span><span class="co">]</span> ^ 2 </span>
<span id="cb60-816"><a href="#cb60-816" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot">\frac{\hat{\sigma}_x ^ 2}{\hat{\sigma}_y ^ 2}\right</span><span class="co">]</span> = </span>
<span id="cb60-817"><a href="#cb60-817" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{xy} ^ 2}{\hat{\sigma}_x ^ 2\hat{\sigma}_y ^ 2}</span>
<span id="cb60-818"><a href="#cb60-818" aria-hidden="true" tabindex="-1"></a>=\hat{\rho}_{xy} ^ 2</span>
<span id="cb60-819"><a href="#cb60-819" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-820"><a href="#cb60-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-821"><a href="#cb60-821" aria-hidden="true" tabindex="-1"></a>$R^2$ is therefore simply the square of the coefficient of</span>
<span id="cb60-822"><a href="#cb60-822" aria-hidden="true" tabindex="-1"></a>correlation between $x$ and $y$. We have seen previously that, denoting $\theta$ the angle formed by two vectors, $\cos \theta$ is the coefficient of correlation between the two underlying variables (if they are measured in deviations from their means). Therefore, in @fig-smplregmodel, $R^2$ is represented by the square of the cosine of the angle formed by the two vectors $\hat{y}$ and $y$. As this angle tends to 0 (the two vectors point almost in the same direction), $R^2$ tends to 1. On the contrary, if this angle tends to $\pi/2$, the two vectors become almost orthogonal, and $R^2$ tends to 0.</span>
<span id="cb60-823"><a href="#cb60-823" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{coefficient of determination|)}</span>
<span id="cb60-824"><a href="#cb60-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-825"><a href="#cb60-825" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb60-826"><a href="#cb60-826" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computation with R {#sec-comp_ols_simple_R}</span></span>
<span id="cb60-827"><a href="#cb60-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-828"><a href="#cb60-828" aria-hidden="true" tabindex="-1"></a>The slope of the regression line can easily be computed "by hand", using any of the</span>
<span id="cb60-829"><a href="#cb60-829" aria-hidden="true" tabindex="-1"></a>formula indicated in @eq-slrbeta, using the <span class="in">`dplyr::summarise`</span> function. We first compute the variations of $x$ and $y$ and the covariation of $x$ and $y$, the two standard deviations and the coefficient of correlation between $x$ and $y$.</span>
<span id="cb60-830"><a href="#cb60-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-831"><a href="#cb60-831" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-832"><a href="#cb60-832" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: moments_computation</span></span>
<span id="cb60-833"><a href="#cb60-833" aria-hidden="true" tabindex="-1"></a>stats <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span></span>
<span id="cb60-834"><a href="#cb60-834" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">N =</span> <span class="fu">nrow</span>(prtime), <span class="at">xb =</span> <span class="fu">mean</span>(h), <span class="at">yb =</span> <span class="fu">mean</span>(sr),</span>
<span id="cb60-835"><a href="#cb60-835" aria-hidden="true" tabindex="-1"></a>              <span class="at">Sxx =</span> <span class="fu">sum</span>( (h <span class="sc">-</span> xb) <span class="sc">^</span> <span class="dv">2</span>), <span class="at">Syy =</span> <span class="fu">sum</span>( (sr <span class="sc">-</span> yb) <span class="sc">^</span> <span class="dv">2</span>),</span>
<span id="cb60-836"><a href="#cb60-836" aria-hidden="true" tabindex="-1"></a>              <span class="at">Sxy =</span> <span class="fu">sum</span>( (h <span class="sc">-</span> xb) <span class="sc">*</span> (sr <span class="sc">-</span> yb)),</span>
<span id="cb60-837"><a href="#cb60-837" aria-hidden="true" tabindex="-1"></a>              <span class="at">sx =</span> <span class="fu">sqrt</span>(Sxx <span class="sc">/</span> N), <span class="at">sy =</span> <span class="fu">sqrt</span>(Syy <span class="sc">/</span> N),</span>
<span id="cb60-838"><a href="#cb60-838" aria-hidden="true" tabindex="-1"></a>              <span class="at">sxy =</span> Sxy <span class="sc">/</span> N, <span class="at">rxy =</span> sxy <span class="sc">/</span> (sx <span class="sc">*</span> sy))</span>
<span id="cb60-839"><a href="#cb60-839" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-840"><a href="#cb60-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-841"><a href="#cb60-841" aria-hidden="true" tabindex="-1"></a>We can then compute the estimator of the slope using any of the three formulas:</span>
<span id="cb60-842"><a href="#cb60-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-843"><a href="#cb60-843" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-844"><a href="#cb60-844" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: slope_computation</span></span>
<span id="cb60-845"><a href="#cb60-845" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-846"><a href="#cb60-846" aria-hidden="true" tabindex="-1"></a>hbeta <span class="ot">&lt;-</span> stats<span class="sc">$</span>Sxy <span class="sc">/</span> stats<span class="sc">$</span>Sxx</span>
<span id="cb60-847"><a href="#cb60-847" aria-hidden="true" tabindex="-1"></a>hbeta</span>
<span id="cb60-848"><a href="#cb60-848" aria-hidden="true" tabindex="-1"></a>stats<span class="sc">$</span>sxy <span class="sc">/</span> stats<span class="sc">$</span>sx <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb60-849"><a href="#cb60-849" aria-hidden="true" tabindex="-1"></a>stats<span class="sc">$</span>rxy <span class="sc">*</span> stats<span class="sc">$</span>sy <span class="sc">/</span> stats<span class="sc">$</span>sx</span>
<span id="cb60-850"><a href="#cb60-850" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-851"><a href="#cb60-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-852"><a href="#cb60-852" aria-hidden="true" tabindex="-1"></a>The estimation of the intercept is obtained using @eq-slralpha:</span>
<span id="cb60-853"><a href="#cb60-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-854"><a href="#cb60-854" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-855"><a href="#cb60-855" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: intercept_computation</span></span>
<span id="cb60-856"><a href="#cb60-856" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-857"><a href="#cb60-857" aria-hidden="true" tabindex="-1"></a>halpha <span class="ot">&lt;-</span> stats<span class="sc">$</span>yb <span class="sc">-</span> hbeta <span class="sc">*</span> stats<span class="sc">$</span>xb</span>
<span id="cb60-858"><a href="#cb60-858" aria-hidden="true" tabindex="-1"></a>halpha</span>
<span id="cb60-859"><a href="#cb60-859" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-860"><a href="#cb60-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-861"><a href="#cb60-861" aria-hidden="true" tabindex="-1"></a>Much more simply, the OLS estimator can be obtained using the <span class="in">`lm`</span> function (for linear model). It is a very important function in **R**, not only because it implements efficiently the most important estimator used in econometrics, but also because **R** functions that implement other estimators often mimic the features of the `lm`\idxfun{lm}{stats} function. Therefore, once one is at ease with using the `lm` function, using other estimating function of **R** will be straightforward. <span class="in">`lm`</span> is a function that has many arguments, but the first two are fundamental and almost mandatory:^<span class="co">[</span><span class="ot">Actually, the second argument `data` is not mandatory because estimation can be performed without using a data frame.</span><span class="co">]</span></span>
<span id="cb60-862"><a href="#cb60-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-863"><a href="#cb60-863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`formula`</span> is a symbolic description of the model to be</span>
<span id="cb60-864"><a href="#cb60-864" aria-hidden="true" tabindex="-1"></a>  estimated,</span>
<span id="cb60-865"><a href="#cb60-865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`data`</span> is a data frame that contains the variables used in the formula.</span>
<span id="cb60-866"><a href="#cb60-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-867"><a href="#cb60-867" aria-hidden="true" tabindex="-1"></a>Here, our formula writes <span class="in">`sr ~ h`</span>, which means <span class="in">`sr`</span> as a function of</span>
<span id="cb60-868"><a href="#cb60-868" aria-hidden="true" tabindex="-1"></a><span class="in">`h`</span>. The data frame is <span class="in">`prtime`</span>. The result of the <span class="in">`lm`</span> function</span>
<span id="cb60-869"><a href="#cb60-869" aria-hidden="true" tabindex="-1"></a>may be directly printed (the result is then lost), or saved in an</span>
<span id="cb60-870"><a href="#cb60-870" aria-hidden="true" tabindex="-1"></a>object, which can be later printed or manipulated:</span>
<span id="cb60-871"><a href="#cb60-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-872"><a href="#cb60-872" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-873"><a href="#cb60-873" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lm_function</span></span>
<span id="cb60-874"><a href="#cb60-874" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(sr <span class="sc">~</span> h, prtime)</span>
<span id="cb60-875"><a href="#cb60-875" aria-hidden="true" tabindex="-1"></a>pxt <span class="ot">&lt;-</span> <span class="fu">lm</span>(sr <span class="sc">~</span> h, prtime)</span>
<span id="cb60-876"><a href="#cb60-876" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-877"><a href="#cb60-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-878"><a href="#cb60-878" aria-hidden="true" tabindex="-1"></a>writing directly <span class="in">`pxt`</span> is like writing <span class="in">`print(pxt)`</span>, the side effect</span>
<span id="cb60-879"><a href="#cb60-879" aria-hidden="true" tabindex="-1"></a>is to print a short description of the results, namely a remind of</span>
<span id="cb60-880"><a href="#cb60-880" aria-hidden="true" tabindex="-1"></a>the function call and the name and the values of the fitted</span>
<span id="cb60-881"><a href="#cb60-881" aria-hidden="true" tabindex="-1"></a>coefficients.</span>
<span id="cb60-882"><a href="#cb60-882" aria-hidden="true" tabindex="-1"></a><span class="in">`lm`</span> returns an object of class <span class="in">`lm`</span> which is a list of 12 elements; their names can be retrieved using the <span class="in">`names`</span> \idxfun{names}{base}function:</span>
<span id="cb60-883"><a href="#cb60-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-886"><a href="#cb60-886" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-887"><a href="#cb60-887" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: names_lm</span></span>
<span id="cb60-888"><a href="#cb60-888" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(pxt)</span>
<span id="cb60-889"><a href="#cb60-889" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-890"><a href="#cb60-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-891"><a href="#cb60-891" aria-hidden="true" tabindex="-1"></a>An element of this list can be extracted using the <span class="in">`$`</span> operator. For example:</span>
<span id="cb60-892"><a href="#cb60-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-895"><a href="#cb60-895" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-896"><a href="#cb60-896" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lm_coefficients</span></span>
<span id="cb60-897"><a href="#cb60-897" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-898"><a href="#cb60-898" aria-hidden="true" tabindex="-1"></a>pxt<span class="sc">$</span>coefficients</span>
<span id="cb60-899"><a href="#cb60-899" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-900"><a href="#cb60-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-901"><a href="#cb60-901" aria-hidden="true" tabindex="-1"></a>returns the named vector of coefficients. <span class="in">`pxt$residuals`</span> and <span class="in">`pxt$fitted.values`</span> return two vectors of length $N$ containing the residuals and the fitted values. However, it is not advised to use the <span class="in">`$`</span> operator to retrieve the elements of a fitted model. Specific functions, called extractors, should be used instead. For example, to retrieve the coefficients, the residuals and the fitted values as previously, we would use:</span>
<span id="cb60-902"><a href="#cb60-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-903"><a href="#cb60-903" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-904"><a href="#cb60-904" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: using_methods</span></span>
<span id="cb60-905"><a href="#cb60-905" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb60-906"><a href="#cb60-906" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(pxt)</span>
<span id="cb60-907"><a href="#cb60-907" aria-hidden="true" tabindex="-1"></a><span class="fu">resid</span>(pxt)</span>
<span id="cb60-908"><a href="#cb60-908" aria-hidden="true" tabindex="-1"></a><span class="fu">fitted</span>(pxt)</span>
<span id="cb60-909"><a href="#cb60-909" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-910"><a href="#cb60-910" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{resid}{stats}\idxfun{fitted}{stats}</span>
<span id="cb60-911"><a href="#cb60-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-912"><a href="#cb60-912" aria-hidden="true" tabindex="-1"></a>There are other functions that extract important information about the model, as the number of observation (<span class="in">`nobs`</span>)\idxfun{nobs}{stats} and the sum of square residuals (<span class="in">`deviance`</span>)\idxfun{deviance}{stats}:</span>
<span id="cb60-913"><a href="#cb60-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-916"><a href="#cb60-916" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-917"><a href="#cb60-917" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: other_lm_methods</span></span>
<span id="cb60-918"><a href="#cb60-918" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-919"><a href="#cb60-919" aria-hidden="true" tabindex="-1"></a><span class="fu">nobs</span>(pxt)</span>
<span id="cb60-920"><a href="#cb60-920" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(pxt)</span>
<span id="cb60-921"><a href="#cb60-921" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-922"><a href="#cb60-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-923"><a href="#cb60-923" aria-hidden="true" tabindex="-1"></a>The results of the estimation are presented in @fig-reserror;</span>
<span id="cb60-924"><a href="#cb60-924" aria-hidden="true" tabindex="-1"></a>we've added to the scatterplot:</span>
<span id="cb60-925"><a href="#cb60-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-926"><a href="#cb60-926" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the sample mean, indicated by a large circle,</span>
<span id="cb60-927"><a href="#cb60-927" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the regression line,</span>
<span id="cb60-928"><a href="#cb60-928" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the residuals, represented by arrows: upward arrows represent</span>
<span id="cb60-929"><a href="#cb60-929" aria-hidden="true" tabindex="-1"></a>  positive residuals (e.g., Brest and Toulon), and downward arrows</span>
<span id="cb60-930"><a href="#cb60-930" aria-hidden="true" tabindex="-1"></a>  represent negative residuals (e.g., Strasbourg and Nice).</span>
<span id="cb60-931"><a href="#cb60-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-932"><a href="#cb60-932" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-933"><a href="#cb60-933" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-reserror</span></span>
<span id="cb60-934"><a href="#cb60-934" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Regression line and residuals"</span></span>
<span id="cb60-935"><a href="#cb60-935" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-936"><a href="#cb60-936" aria-hidden="true" tabindex="-1"></a>Mpxtps <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">h =</span> <span class="fu">mean</span>(h), <span class="at">sr =</span> <span class="fu">mean</span>(sr))</span>
<span id="cb60-937"><a href="#cb60-937" aria-hidden="true" tabindex="-1"></a>dx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb60-938"><a href="#cb60-938" aria-hidden="true" tabindex="-1"></a><span class="co">#myarrow &lt;- arrow(length = unit(0.025, "npc"), angle = 15, type = "closed")</span></span>
<span id="cb60-939"><a href="#cb60-939" aria-hidden="true" tabindex="-1"></a>myarrow <span class="ot">&lt;-</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.03</span>, <span class="st">"npc"</span>), <span class="at">angle =</span> <span class="dv">10</span>, <span class="at">type =</span> <span class="st">"open"</span>)</span>
<span id="cb60-940"><a href="#cb60-940" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">hy =</span> <span class="fu">fitted</span>(pxt)) <span class="sc">%&gt;%</span></span>
<span id="cb60-941"><a href="#cb60-941" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb60-942"><a href="#cb60-942" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb60-943"><a href="#cb60-943" aria-hidden="true" tabindex="-1"></a>    ggrepel<span class="sc">::</span><span class="fu">geom_label_repel</span>(<span class="fu">aes</span>(<span class="at">label =</span> town)) <span class="sc">+</span> </span>
<span id="cb60-944"><a href="#cb60-944" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> h <span class="sc">-</span> dx, <span class="at">xend =</span> h <span class="sc">-</span> dx, <span class="at">yend =</span> sr, <span class="at">y =</span> hy),</span>
<span id="cb60-945"><a href="#cb60-945" aria-hidden="true" tabindex="-1"></a>                     <span class="at">arrow =</span> myarrow) <span class="sc">+</span></span>
<span id="cb60-946"><a href="#cb60-946" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">data =</span> Mpxtps, <span class="at">size =</span> <span class="dv">5</span>) </span>
<span id="cb60-947"><a href="#cb60-947" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-948"><a href="#cb60-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-949"><a href="#cb60-949" aria-hidden="true" tabindex="-1"></a>Individual coefficients can be extracted using the <span class="in">`[`</span> operator. As <span class="in">`coef`</span> returns a named vector, one can either indicate the position or the name of the coefficient to be extracted:</span>
<span id="cb60-950"><a href="#cb60-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-951"><a href="#cb60-951" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-952"><a href="#cb60-952" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: coef_extraction</span></span>
<span id="cb60-953"><a href="#cb60-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-954"><a href="#cb60-954" aria-hidden="true" tabindex="-1"></a>int <span class="ot">&lt;-</span> <span class="fu">unname</span>(<span class="fu">coef</span>(pxt)[<span class="dv">1</span>])</span>
<span id="cb60-955"><a href="#cb60-955" aria-hidden="true" tabindex="-1"></a>slope <span class="ot">&lt;-</span> <span class="fu">unname</span>(<span class="fu">coef</span>(pxt)[<span class="st">"h"</span>])</span>
<span id="cb60-956"><a href="#cb60-956" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(int, slope)</span>
<span id="cb60-957"><a href="#cb60-957" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-958"><a href="#cb60-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-959"><a href="#cb60-959" aria-hidden="true" tabindex="-1"></a>Note that we use the <span class="in">`unname`</span>\idxfun{unname}{base} function in order to remove the name of the extracted coefficient. Once the intercept (<span class="in">`int`</span>) and the slope (<span class="in">`slope`</span>) are extracted, the</span>
<span id="cb60-960"><a href="#cb60-960" aria-hidden="true" tabindex="-1"></a>structural parameters can be retrieved, as the intercept is $\alpha =</span>
<span id="cb60-961"><a href="#cb60-961" aria-hidden="true" tabindex="-1"></a>-\frac{a}{b-a}$ and the slope $\beta =\frac{1}{b-a}$. </span>
<span id="cb60-962"><a href="#cb60-962" aria-hidden="true" tabindex="-1"></a>Therefore, $a = -\frac{\alpha}{\beta}$ and $b =</span>
<span id="cb60-963"><a href="#cb60-963" aria-hidden="true" tabindex="-1"></a>\frac{1}{\beta} + a$. We finally get:</span>
<span id="cb60-964"><a href="#cb60-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-965"><a href="#cb60-965" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-966"><a href="#cb60-966" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: structural_coefficients</span></span>
<span id="cb60-967"><a href="#cb60-967" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-968"><a href="#cb60-968" aria-hidden="true" tabindex="-1"></a>ahat <span class="ot">=</span> <span class="sc">-</span> int <span class="sc">/</span> slope</span>
<span id="cb60-969"><a href="#cb60-969" aria-hidden="true" tabindex="-1"></a>bhat <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> slope <span class="sc">+</span> ahat</span>
<span id="cb60-970"><a href="#cb60-970" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(ahat, bhat)</span>
<span id="cb60-971"><a href="#cb60-971" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-972"><a href="#cb60-972" aria-hidden="true" tabindex="-1"></a>Time values therefore lie between <span class="in">`r round(ahat, 2)`</span> and </span>
<span id="cb60-973"><a href="#cb60-973" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(bhat, 2)`</span> euros per hour. The mean (and median) time value is</span>
<span id="cb60-974"><a href="#cb60-974" aria-hidden="true" tabindex="-1"></a>the mean of the two extreme values, </span>
<span id="cb60-975"><a href="#cb60-975" aria-hidden="true" tabindex="-1"></a>which is <span class="in">`r round( (ahat + bhat) / 2, 2)`</span> euros per hour.</span>
<span id="cb60-976"><a href="#cb60-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-977"><a href="#cb60-977" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data generator process and simulations {#sec-dgp_simulations}</span></span>
<span id="cb60-978"><a href="#cb60-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-979"><a href="#cb60-979" aria-hidden="true" tabindex="-1"></a>Inferential statistics rely on the notions of population and sample. The population is a large and exhaustive set of observations, and a sample is a small subset of observations drawn in this population. These notions are relevant in medical sciences and often in economic studies. For example, the first data set we used concerned smoking habits and birth weight. The population of interest is all American pregnant women in 1988, and a sample of 1388 of them was drawn from this population. The second data set concerned the relation between education and wage. The population was American labor force in 1992, and a sample of 16,481 workers was drawn from this population. On the contrary, our third data set doesn't fit with these notions of population and sample. The sample consists of major towns in France that are connected on a regular basis by rail and air to Paris. It contains 13 observations, which are not 13 observations randomly drawn from a large set of cities, but which are more or less all the relevant cities. </span>
<span id="cb60-980"><a href="#cb60-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-981"><a href="#cb60-981" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data generator process</span></span>
<span id="cb60-982"><a href="#cb60-982" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{data generator process|(}</span>
<span id="cb60-983"><a href="#cb60-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-984"><a href="#cb60-984" aria-hidden="true" tabindex="-1"></a>An interesting alternative is the notion of **data generator process**  (**DGP**). It describes how the data are assumed to have been generated. We assume in the linear regression model that: </span>
<span id="cb60-985"><a href="#cb60-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-986"><a href="#cb60-986" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mbox{E}(y|x = x_n) = \alpha + \beta x_n$: the expected value of $y$ for a given value of $x$ is a linear function of $x$,</span>
<span id="cb60-987"><a href="#cb60-987" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y_n = \mbox{E}(y|x = x_n) + \epsilon_n$: the observed value of $y_n$ is obtained by adding to the conditional expectation of $y$ for $x = x_n$ a random variable $\epsilon$ called the error.</span>
<span id="cb60-988"><a href="#cb60-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-989"><a href="#cb60-989" aria-hidden="true" tabindex="-1"></a>From @eq-slrbeta, we can write the estimator of the slope as:</span>
<span id="cb60-990"><a href="#cb60-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-991"><a href="#cb60-991" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb60-992"><a href="#cb60-992" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_{n = 1} ^ N   (x_{n} - \bar{x})(y_{n} - \bar{y})}</span>
<span id="cb60-993"><a href="#cb60-993" aria-hidden="true" tabindex="-1"></a>{\sum_{n = 1} ^ N   (x_{n} - \bar{x})^ 2}</span>
<span id="cb60-994"><a href="#cb60-994" aria-hidden="true" tabindex="-1"></a>= \sum_{n = 1} ^ N \frac{(x_{n} - \bar{x})}{\sum_{n = 1} ^ N</span>
<span id="cb60-995"><a href="#cb60-995" aria-hidden="true" tabindex="-1"></a>(x_{n} - \bar{x}) ^ 2} y_{n}</span>
<span id="cb60-996"><a href="#cb60-996" aria-hidden="true" tabindex="-1"></a>= \sum_{n} c_{n} y_{n}</span>
<span id="cb60-997"><a href="#cb60-997" aria-hidden="true" tabindex="-1"></a>$$ {#eq-linear_estimator}</span>
<span id="cb60-998"><a href="#cb60-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-999"><a href="#cb60-999" aria-hidden="true" tabindex="-1"></a>with $c_{n} = \frac{x_{n} - \bar{x}}{\sum_{n = 1} ^ N (x_{n} -</span>
<span id="cb60-1000"><a href="#cb60-1000" aria-hidden="true" tabindex="-1"></a>\bar{x}) ^ 2}$. The OLS estimator is therefore a linear estimator, i.e., a</span>
<span id="cb60-1001"><a href="#cb60-1001" aria-hidden="true" tabindex="-1"></a>linear combination of the values of $y$. The coefficients of this</span>
<span id="cb60-1002"><a href="#cb60-1002" aria-hidden="true" tabindex="-1"></a>linear combination $c_n$ are such that $\sum_{n = 1} ^ N  c_{n} = 0$ and that:</span>
<span id="cb60-1003"><a href="#cb60-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1004"><a href="#cb60-1004" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-1005"><a href="#cb60-1005" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N  c_{n} ^ 2 = \frac{\sum_{n = 1} ^ N (x_{n} -</span>
<span id="cb60-1006"><a href="#cb60-1006" aria-hidden="true" tabindex="-1"></a>\bar{x}) ^ 2}</span>
<span id="cb60-1007"><a href="#cb60-1007" aria-hidden="true" tabindex="-1"></a>{\left(\sum_{n = 1} ^ N (x_{n} -</span>
<span id="cb60-1008"><a href="#cb60-1008" aria-hidden="true" tabindex="-1"></a>\bar{x}) ^ 2\right) ^ 2}  =</span>
<span id="cb60-1009"><a href="#cb60-1009" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sum_{n = 1} ^ N (x_{n} -</span>
<span id="cb60-1010"><a href="#cb60-1010" aria-hidden="true" tabindex="-1"></a>\bar{x}) ^ 2} = \frac{1}{N\hat{\sigma}_x^2}</span>
<span id="cb60-1011"><a href="#cb60-1011" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-1012"><a href="#cb60-1012" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb60-1013"><a href="#cb60-1013" aria-hidden="true" tabindex="-1"></a>Replacing $y_{n}$ by $\alpha + \beta x_{n} + \epsilon_{n}$, we then express $\hat{\beta}$ as a function of $\epsilon_n$:</span>
<span id="cb60-1014"><a href="#cb60-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1015"><a href="#cb60-1015" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-1016"><a href="#cb60-1016" aria-hidden="true" tabindex="-1"></a>\hat{\beta}=\sum_{n = 1} ^ N c_{n} (\alpha + \beta</span>
<span id="cb60-1017"><a href="#cb60-1017" aria-hidden="true" tabindex="-1"></a>x_{n} + \epsilon_{n}) = \alpha \sum_{n = 1} ^ N c_{n}+\beta\sum_{n = 1}</span>
<span id="cb60-1018"><a href="#cb60-1018" aria-hidden="true" tabindex="-1"></a>^ N \frac{x_{n}(x_{n} - \bar{x})}{\sum_{n = 1} ^ N</span>
<span id="cb60-1019"><a href="#cb60-1019" aria-hidden="true" tabindex="-1"></a>(x_{n} - \bar{x}) ^ 2}+\sum_{n = 1} ^ N c_{n} \epsilon_{n}</span>
<span id="cb60-1020"><a href="#cb60-1020" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb60-1021"><a href="#cb60-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1022"><a href="#cb60-1022" aria-hidden="true" tabindex="-1"></a>As $\sum_{n = 1} ^ N x_{n}(x_{n} - \bar{x}) = \sum_{n = 1} ^ N</span>
<span id="cb60-1023"><a href="#cb60-1023" aria-hidden="true" tabindex="-1"></a>(x_{n} - \bar{x}) ^ 2$ </span>
<span id="cb60-1024"><a href="#cb60-1024" aria-hidden="true" tabindex="-1"></a>and $\sum_{n = 1} ^ N c_{n} = 0$, we finally get:</span>
<span id="cb60-1025"><a href="#cb60-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1026"><a href="#cb60-1026" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb60-1027"><a href="#cb60-1027" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \beta + \sum_{n = 1} ^ N c_{n} \epsilon_{n} </span>
<span id="cb60-1028"><a href="#cb60-1028" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ols_lin_comb_errors}</span>
<span id="cb60-1029"><a href="#cb60-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1030"><a href="#cb60-1030" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb60-1031"><a href="#cb60-1031" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- new page --&gt;</span></span>
<span id="cb60-1032"><a href="#cb60-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1033"><a href="#cb60-1033" aria-hidden="true" tabindex="-1"></a>The deviation of the estimator of the slope of the OLS regression line</span>
<span id="cb60-1034"><a href="#cb60-1034" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}$ from the true value $\beta$ is therefore a linear</span>
<span id="cb60-1035"><a href="#cb60-1035" aria-hidden="true" tabindex="-1"></a>combination of the $N$ errors.</span>
<span id="cb60-1036"><a href="#cb60-1036" aria-hidden="true" tabindex="-1"></a>Consider our sample used to estimate the price-time model. From a DGP perspective, this sample has been generated using the formula: $y=\alpha + \beta x + \epsilon$. Consider now that the "true" values of $\alpha$ and $\beta$ are $-0.2$ and $0.032$, we can in this case compute the vector of errors for our sample ($\epsilon = y - \alpha - \beta x$):\idxfun{pull}{dplyr}</span>
<span id="cb60-1037"><a href="#cb60-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1040"><a href="#cb60-1040" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1041"><a href="#cb60-1041" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: error_computation</span></span>
<span id="cb60-1042"><a href="#cb60-1042" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.2</span></span>
<span id="cb60-1043"><a href="#cb60-1043" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.032</span></span>
<span id="cb60-1044"><a href="#cb60-1044" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sr)</span>
<span id="cb60-1045"><a href="#cb60-1045" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(h)</span>
<span id="cb60-1046"><a href="#cb60-1046" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">=</span> y <span class="sc">-</span> alpha <span class="sc">-</span> beta <span class="sc">*</span> x</span>
<span id="cb60-1047"><a href="#cb60-1047" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1048"><a href="#cb60-1048" aria-hidden="true" tabindex="-1"></a>We then compute the OLS estimator, and we retrieve $\hat{\epsilon}$ and $\hat{y}$.\idxfun{lm}{stats}</span>
<span id="cb60-1049"><a href="#cb60-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1052"><a href="#cb60-1052" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1053"><a href="#cb60-1053" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: resid_fitted_extraction</span></span>
<span id="cb60-1054"><a href="#cb60-1054" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb60-1055"><a href="#cb60-1055" aria-hidden="true" tabindex="-1"></a>hat_eps <span class="ot">&lt;-</span> z <span class="sc">%&gt;%</span> residuals</span>
<span id="cb60-1056"><a href="#cb60-1056" aria-hidden="true" tabindex="-1"></a>hat_y <span class="ot">&lt;-</span> z <span class="sc">%&gt;%</span> fitted</span>
<span id="cb60-1057"><a href="#cb60-1057" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1058"><a href="#cb60-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1059"><a href="#cb60-1059" aria-hidden="true" tabindex="-1"></a>The observed data set, in the DGP perspective, is represented in @fig-dgp.</span>
<span id="cb60-1060"><a href="#cb60-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1061"><a href="#cb60-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-1062"><a href="#cb60-1062" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dgp</span></span>
<span id="cb60-1063"><a href="#cb60-1063" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Data generating process"</span></span>
<span id="cb60-1064"><a href="#cb60-1064" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-1065"><a href="#cb60-1065" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.2</span></span>
<span id="cb60-1066"><a href="#cb60-1066" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.032</span></span>
<span id="cb60-1067"><a href="#cb60-1067" aria-hidden="true" tabindex="-1"></a>prtime <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span></span>
<span id="cb60-1068"><a href="#cb60-1068" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">Eyx =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> h,</span>
<span id="cb60-1069"><a href="#cb60-1069" aria-hidden="true" tabindex="-1"></a>           <span class="at">hy =</span> <span class="fu">fitted</span>(pxt))</span>
<span id="cb60-1070"><a href="#cb60-1070" aria-hidden="true" tabindex="-1"></a>Mpxtps <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">h =</span> <span class="fu">mean</span>(h), <span class="at">sr =</span> <span class="fu">mean</span>(sr))</span>
<span id="cb60-1071"><a href="#cb60-1071" aria-hidden="true" tabindex="-1"></a>dx <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb60-1072"><a href="#cb60-1072" aria-hidden="true" tabindex="-1"></a>myarrow <span class="ot">&lt;-</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.025</span>, <span class="st">"npc"</span>), <span class="at">angle =</span> <span class="dv">15</span>, <span class="at">type =</span> <span class="st">"closed"</span>)</span>
<span id="cb60-1073"><a href="#cb60-1073" aria-hidden="true" tabindex="-1"></a>myarrow <span class="ot">&lt;-</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.05</span>, <span class="st">"npc"</span>), <span class="at">angle =</span> <span class="dv">10</span>, <span class="at">type =</span> <span class="st">"closed"</span>)</span>
<span id="cb60-1074"><a href="#cb60-1074" aria-hidden="true" tabindex="-1"></a>myarrow2 <span class="ot">&lt;-</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.05</span>, <span class="st">"npc"</span>), <span class="at">angle =</span> <span class="dv">10</span>, <span class="at">type =</span> <span class="st">"open"</span>)</span>
<span id="cb60-1075"><a href="#cb60-1075" aria-hidden="true" tabindex="-1"></a>prtime <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> </span>
<span id="cb60-1076"><a href="#cb60-1076" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb60-1077"><a href="#cb60-1077" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="fl">0.032</span>, <span class="at">intercept =</span> <span class="sc">-</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb60-1078"><a href="#cb60-1078" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>,</span>
<span id="cb60-1079"><a href="#cb60-1079" aria-hidden="true" tabindex="-1"></a>                <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb60-1080"><a href="#cb60-1080" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> h <span class="sc">-</span> dx, <span class="at">xend =</span> h <span class="sc">-</span> dx, <span class="at">yend =</span> sr, <span class="at">y =</span> hy),</span>
<span id="cb60-1081"><a href="#cb60-1081" aria-hidden="true" tabindex="-1"></a>                 <span class="at">arrow =</span> myarrow2, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb60-1082"><a href="#cb60-1082" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> h <span class="sc">+</span> dx, <span class="at">xend =</span> h <span class="sc">+</span> dx, <span class="at">yend =</span> sr, <span class="at">y =</span> Eyx),</span>
<span id="cb60-1083"><a href="#cb60-1083" aria-hidden="true" tabindex="-1"></a>                 <span class="at">arrow =</span> myarrow) <span class="sc">+</span></span>
<span id="cb60-1084"><a href="#cb60-1084" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> Mpxtps, <span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span> </span>
<span id="cb60-1085"><a href="#cb60-1085" aria-hidden="true" tabindex="-1"></a>    ggrepel<span class="sc">::</span><span class="fu">geom_label_repel</span>(<span class="fu">aes</span>(<span class="at">label =</span> town))</span>
<span id="cb60-1086"><a href="#cb60-1086" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1087"><a href="#cb60-1087" aria-hidden="true" tabindex="-1"></a>The "true" model is represented by the plain line. The errors are</span>
<span id="cb60-1088"><a href="#cb60-1088" aria-hidden="true" tabindex="-1"></a>represented by the plain arrows (positive errors for upward arrows,</span>
<span id="cb60-1089"><a href="#cb60-1089" aria-hidden="true" tabindex="-1"></a>negative errors for downward arrows). Each value of $y_n$ is the sum</span>
<span id="cb60-1090"><a href="#cb60-1090" aria-hidden="true" tabindex="-1"></a>of the conditional expectation of $y$ for the value of $x$: $E(y|x = x_n) =</span>
<span id="cb60-1091"><a href="#cb60-1091" aria-hidden="true" tabindex="-1"></a>\alpha + \beta x_n$ (the value returned by the plain line for the</span>
<span id="cb60-1092"><a href="#cb60-1092" aria-hidden="true" tabindex="-1"></a>given value of $x$) and the error $\epsilon_n$ represented by the plain</span>
<span id="cb60-1093"><a href="#cb60-1093" aria-hidden="true" tabindex="-1"></a>arrow. For our specific sample, we have a specific vector of</span>
<span id="cb60-1094"><a href="#cb60-1094" aria-hidden="true" tabindex="-1"></a>$\epsilon_n$, which means a specific set of points and a specific</span>
<span id="cb60-1095"><a href="#cb60-1095" aria-hidden="true" tabindex="-1"></a>regression line, the dashed line on the figure. Each value of $y_n$ is then also the sum of the fitted value (the one returned by the regression line for $x=x_n$) and the residual, represented by a dashed arrow. </span>
<span id="cb60-1096"><a href="#cb60-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1097"><a href="#cb60-1097" aria-hidden="true" tabindex="-1"></a>We can check that the residuals sum to 0 and are uncorrelated with the covariate and the fitted values:</span>
<span id="cb60-1098"><a href="#cb60-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1101"><a href="#cb60-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1102"><a href="#cb60-1102" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1103"><a href="#cb60-1103" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: zerosum_uncor_resid</span></span>
<span id="cb60-1104"><a href="#cb60-1104" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(hat_eps)</span>
<span id="cb60-1105"><a href="#cb60-1105" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(hat_eps <span class="sc">*</span> x)</span>
<span id="cb60-1106"><a href="#cb60-1106" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(hat_eps <span class="sc">*</span> hat_y)</span>
<span id="cb60-1107"><a href="#cb60-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1108"><a href="#cb60-1108" aria-hidden="true" tabindex="-1"></a>which is not the case for the errors:</span>
<span id="cb60-1109"><a href="#cb60-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1112"><a href="#cb60-1112" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1113"><a href="#cb60-1113" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1114"><a href="#cb60-1114" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: empirical_moments_errors</span></span>
<span id="cb60-1115"><a href="#cb60-1115" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(eps)</span>
<span id="cb60-1116"><a href="#cb60-1116" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(eps <span class="sc">*</span> x)</span>
<span id="cb60-1117"><a href="#cb60-1117" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(eps <span class="sc">*</span> hat_y)</span>
<span id="cb60-1118"><a href="#cb60-1118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1119"><a href="#cb60-1119" aria-hidden="true" tabindex="-1"></a>We can also check that the residuals are "smaller" than the errors by computing their standard deviations:\idxfun{sd}{stats}</span>
<span id="cb60-1120"><a href="#cb60-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1123"><a href="#cb60-1123" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1124"><a href="#cb60-1124" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: smaller_residuals</span></span>
<span id="cb60-1125"><a href="#cb60-1125" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1126"><a href="#cb60-1126" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(eps)</span>
<span id="cb60-1127"><a href="#cb60-1127" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(hat_eps)</span>
<span id="cb60-1128"><a href="#cb60-1128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1129"><a href="#cb60-1129" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{data generator process|(}</span>
<span id="cb60-1130"><a href="#cb60-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1131"><a href="#cb60-1131" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random numbers and simulations {#sec-random_numbers_simulations}</span></span>
<span id="cb60-1132"><a href="#cb60-1132" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!simple linear regression model|(}</span>
<span id="cb60-1133"><a href="#cb60-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1134"><a href="#cb60-1134" aria-hidden="true" tabindex="-1"></a>Now consider an other fictive sample for the same values of $x$, i.e., consider that the values of the threshold values of time are the same, but that the other factors that influence rail's shares (the $\epsilon$) are different. To generate such a fictive sample, we need to generate the $\epsilon$ vector, using a function that generates random numbers.^<span class="co">[</span><span class="ot">More precisely, a function that generates a sequence of numbers that looks like random numbers.</span><span class="co">]</span> With **R**, these functions have a name composed of the letter <span class="in">`r`</span> (for random) and the abbreviated name of the statistical distribution: for example <span class="in">`runif`</span>\idxfun{runif}{stats} draws numbers in a uniform distribution (by default within a $0-1$ range) and <span class="in">`rnorm`</span>\idxfun{rnorm}{stats} draws numbers in a normal distribution (by default with zero expectation and unit standard deviation). These functions have a mandatory argument which is the number of draws. For example, to get five numbers drawn from a standard normal distribution:</span>
<span id="cb60-1135"><a href="#cb60-1135" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{normal distribution}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{uniform distribution}</span>
<span id="cb60-1136"><a href="#cb60-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1139"><a href="#cb60-1139" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1140"><a href="#cb60-1140" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb60-1141"><a href="#cb60-1141" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: set_the_seed</span></span>
<span id="cb60-1142"><a href="#cb60-1142" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(10L)</span>
<span id="cb60-1143"><a href="#cb60-1143" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1144"><a href="#cb60-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1147"><a href="#cb60-1147" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1148"><a href="#cb60-1148" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: first_draw_normal</span></span>
<span id="cb60-1149"><a href="#cb60-1149" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1150"><a href="#cb60-1150" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb60-1151"><a href="#cb60-1151" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1152"><a href="#cb60-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1153"><a href="#cb60-1153" aria-hidden="true" tabindex="-1"></a>Using the same command once more, we get a completely different sequence:</span>
<span id="cb60-1156"><a href="#cb60-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1157"><a href="#cb60-1157" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: second_draw_norm</span></span>
<span id="cb60-1158"><a href="#cb60-1158" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1159"><a href="#cb60-1159" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb60-1160"><a href="#cb60-1160" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1161"><a href="#cb60-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1162"><a href="#cb60-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1163"><a href="#cb60-1163" aria-hidden="true" tabindex="-1"></a>As stated previously, the <span class="in">`rnorm`</span>\idxfun{rnorm}{stats} function doesn't draw random numbers but computes a sequence of numbers that looks like a random sequence. Imagine that <span class="in">`rnorm`</span> actually computes a sequence of thousands of numbers, what is obtained using <span class="in">`rnorm(5)`</span> is 5 consecutive numbers in this sequence, for example from the 5107th to the 5111th number. The position of the first element is called the **seed**, and it can be set to an integer using the <span class="in">`set.seed`</span>\idxfun{set.seed}{base} function. Using the same seed while starting a simulation, we would then get exactly the same pseudo-random numbers each time and therefore the same results:</span>
<span id="cb60-1164"><a href="#cb60-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1167"><a href="#cb60-1167" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1168"><a href="#cb60-1168" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setting_seed</span></span>
<span id="cb60-1169"><a href="#cb60-1169" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1170"><a href="#cb60-1170" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(7L)</span>
<span id="cb60-1171"><a href="#cb60-1171" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb60-1172"><a href="#cb60-1172" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb60-1173"><a href="#cb60-1173" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(7L)</span>
<span id="cb60-1174"><a href="#cb60-1174" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb60-1175"><a href="#cb60-1175" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb60-1176"><a href="#cb60-1176" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1177"><a href="#cb60-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1178"><a href="#cb60-1178" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{data generator process|(}</span>
<span id="cb60-1179"><a href="#cb60-1179" aria-hidden="true" tabindex="-1"></a>The DGP is completely described by specifying the distribution of $\epsilon$. We'll consider here a normal distribution with mean 0 and standard deviation $\sigma_\epsilon = 0.08$. </span>
<span id="cb60-1180"><a href="#cb60-1180" aria-hidden="true" tabindex="-1"></a>A pseudo-random sample can then be constructed as follow:\idxfun{pull}{dplyr}\idxfun{rnorm}{stats}\idxfun{tibble}{tibble}</span>
<span id="cb60-1181"><a href="#cb60-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1184"><a href="#cb60-1184" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1185"><a href="#cb60-1185" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: a_small_sample</span></span>
<span id="cb60-1186"><a href="#cb60-1186" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> nrow</span>
<span id="cb60-1187"><a href="#cb60-1187" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">pull</span>(h)</span>
<span id="cb60-1188"><a href="#cb60-1188" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.2</span> ; beta <span class="ot">&lt;-</span> <span class="fl">0.032</span> ; seps <span class="ot">&lt;-</span> <span class="fl">0.08</span></span>
<span id="cb60-1189"><a href="#cb60-1189" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">sd =</span> seps)</span>
<span id="cb60-1190"><a href="#cb60-1190" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps</span>
<span id="cb60-1191"><a href="#cb60-1191" aria-hidden="true" tabindex="-1"></a>asmpl <span class="ot">&lt;-</span> <span class="fu">tibble</span>(y, x)</span>
<span id="cb60-1192"><a href="#cb60-1192" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1193"><a href="#cb60-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1194"><a href="#cb60-1194" aria-hidden="true" tabindex="-1"></a>which gives the following OLS estimates:\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb60-1195"><a href="#cb60-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1198"><a href="#cb60-1198" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1199"><a href="#cb60-1199" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lm_on_small_random_sample</span></span>
<span id="cb60-1200"><a href="#cb60-1200" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1201"><a href="#cb60-1201" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x, asmpl) <span class="sc">%&gt;%</span> coef</span>
<span id="cb60-1202"><a href="#cb60-1202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1203"><a href="#cb60-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1204"><a href="#cb60-1204" aria-hidden="true" tabindex="-1"></a>The notion of DGP enables to perform simulations, that have two</span>
<span id="cb60-1205"><a href="#cb60-1205" aria-hidden="true" tabindex="-1"></a>purposes:</span>
<span id="cb60-1206"><a href="#cb60-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1207"><a href="#cb60-1207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the first (and the most important in practice) is that, in some</span>
<span id="cb60-1208"><a href="#cb60-1208" aria-hidden="true" tabindex="-1"></a>  situations, it is impossible to get analytical results for the</span>
<span id="cb60-1209"><a href="#cb60-1209" aria-hidden="true" tabindex="-1"></a>  properties of an estimator, and those properties can in this case be</span>
<span id="cb60-1210"><a href="#cb60-1210" aria-hidden="true" tabindex="-1"></a>  obtained using simulations,</span>
<span id="cb60-1211"><a href="#cb60-1211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the second is pedagogical, as theoretical results can be confirmed and illustrated using simulations.</span>
<span id="cb60-1212"><a href="#cb60-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1213"><a href="#cb60-1213" aria-hidden="true" tabindex="-1"></a>A simulation consists of creating a large number of samples, computing some interesting numbers for every sample and calculating some relevant statistics for these numbers. For example, we'll compute the slope of the OLS estimate for every sample, and we'll calculate the mean and the standard deviation for this slope. </span>
<span id="cb60-1214"><a href="#cb60-1214" aria-hidden="true" tabindex="-1"></a>A minimal simulation is presented in @fig-foursmpls, where we present four different samples obtained for four different sets of errors.</span>
<span id="cb60-1215"><a href="#cb60-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1216"><a href="#cb60-1216" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-1217"><a href="#cb60-1217" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-foursmpls</span></span>
<span id="cb60-1218"><a href="#cb60-1218" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "4 different samples"</span></span>
<span id="cb60-1219"><a href="#cb60-1219" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb60-1220"><a href="#cb60-1220" aria-hidden="true" tabindex="-1"></a>set.seed <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb60-1221"><a href="#cb60-1221" aria-hidden="true" tabindex="-1"></a>dx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb60-1222"><a href="#cb60-1222" aria-hidden="true" tabindex="-1"></a>smpls <span class="ot">&lt;-</span> prtime <span class="sc">%&gt;%</span> <span class="fu">select</span>(h, sr) <span class="sc">%&gt;%</span></span>
<span id="cb60-1223"><a href="#cb60-1223" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">Ey =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> h,</span>
<span id="cb60-1224"><a href="#cb60-1224" aria-hidden="true" tabindex="-1"></a>           <span class="at">e_0 =</span> sr <span class="sc">-</span> Ey,</span>
<span id="cb60-1225"><a href="#cb60-1225" aria-hidden="true" tabindex="-1"></a>           <span class="at">e_1 =</span> <span class="fu">rnorm</span>(<span class="dv">9</span>, <span class="at">sd =</span> <span class="fl">0.08</span>),</span>
<span id="cb60-1226"><a href="#cb60-1226" aria-hidden="true" tabindex="-1"></a>           <span class="at">e_2 =</span> <span class="fu">rnorm</span>(<span class="dv">9</span>, <span class="at">sd =</span> <span class="fl">0.08</span>),</span>
<span id="cb60-1227"><a href="#cb60-1227" aria-hidden="true" tabindex="-1"></a>           <span class="at">e_3 =</span> <span class="fu">rnorm</span>(<span class="dv">9</span>, <span class="at">sd =</span> <span class="fl">0.08</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1228"><a href="#cb60-1228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span> sr) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1229"><a href="#cb60-1229" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_longer</span>(<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>), <span class="at">names_to =</span> <span class="st">"sample"</span>,</span>
<span id="cb60-1230"><a href="#cb60-1230" aria-hidden="true" tabindex="-1"></a>                 <span class="at">values_to =</span> <span class="st">"error"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1231"><a href="#cb60-1231" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">sr =</span> Ey <span class="sc">+</span> error)</span>
<span id="cb60-1232"><a href="#cb60-1232" aria-hidden="true" tabindex="-1"></a>Msmpls <span class="ot">&lt;-</span> smpls <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(sample) <span class="sc">%&gt;%</span></span>
<span id="cb60-1233"><a href="#cb60-1233" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">alpha =</span> <span class="fu">coef</span>(<span class="fu">lm</span>(<span class="at">formula =</span> sr <span class="sc">~</span> h))[<span class="dv">1</span>],</span>
<span id="cb60-1234"><a href="#cb60-1234" aria-hidden="true" tabindex="-1"></a>              <span class="at">beta =</span>  <span class="fu">coef</span>(<span class="fu">lm</span>(<span class="at">formula =</span> sr <span class="sc">~</span> h))[<span class="dv">2</span>],</span>
<span id="cb60-1235"><a href="#cb60-1235" aria-hidden="true" tabindex="-1"></a>              <span class="at">h =</span> <span class="fu">mean</span>(h),</span>
<span id="cb60-1236"><a href="#cb60-1236" aria-hidden="true" tabindex="-1"></a>              <span class="at">sr =</span> <span class="fu">mean</span>(sr),</span>
<span id="cb60-1237"><a href="#cb60-1237" aria-hidden="true" tabindex="-1"></a>              <span class="at">coefs =</span> <span class="fu">paste</span>(<span class="st">"("</span>, <span class="fu">round</span>(alpha, <span class="dv">3</span>), <span class="st">") - ("</span>,</span>
<span id="cb60-1238"><a href="#cb60-1238" aria-hidden="true" tabindex="-1"></a>                            <span class="fu">round</span>(beta, <span class="dv">3</span>), <span class="st">")"</span>, <span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb60-1239"><a href="#cb60-1239" aria-hidden="true" tabindex="-1"></a>smpls <span class="ot">&lt;-</span> smpls <span class="sc">%&gt;%</span> <span class="fu">left_join</span>(<span class="fu">select</span>(Msmpls, sample, coefs))              </span>
<span id="cb60-1240"><a href="#cb60-1240" aria-hidden="true" tabindex="-1"></a>smpls <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(h, sr)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb60-1241"><a href="#cb60-1241" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> alpha, <span class="at">slope =</span> beta) <span class="sc">+</span> </span>
<span id="cb60-1242"><a href="#cb60-1242" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>,</span>
<span id="cb60-1243"><a href="#cb60-1243" aria-hidden="true" tabindex="-1"></a>                <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb60-1244"><a href="#cb60-1244" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> Msmpls, <span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb60-1245"><a href="#cb60-1245" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> h <span class="sc">+</span> dx, <span class="at">xend =</span> h <span class="sc">+</span> dx, <span class="at">yend =</span> sr, <span class="at">y =</span> Ey),</span>
<span id="cb60-1246"><a href="#cb60-1246" aria-hidden="true" tabindex="-1"></a>                 <span class="at">arrow =</span> myarrow) <span class="sc">+</span></span>
<span id="cb60-1247"><a href="#cb60-1247" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> coefs)</span>
<span id="cb60-1248"><a href="#cb60-1248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1249"><a href="#cb60-1249" aria-hidden="true" tabindex="-1"></a>For every sample, there is a specific set of errors (arrows)</span>
<span id="cb60-1250"><a href="#cb60-1250" aria-hidden="true" tabindex="-1"></a>and therefore specific data points and regression lines. The estimated</span>
<span id="cb60-1251"><a href="#cb60-1251" aria-hidden="true" tabindex="-1"></a>intercepts range from</span>
<span id="cb60-1252"><a href="#cb60-1252" aria-hidden="true" tabindex="-1"></a>$<span class="in">`r round(Msmpls %&gt;% top_n(-1, alpha) %&gt;% pull(alpha), 3)`</span>$ to</span>
<span id="cb60-1253"><a href="#cb60-1253" aria-hidden="true" tabindex="-1"></a>$<span class="in">`r round(Msmpls %&gt;% top_n(1, alpha) %&gt;% pull(alpha), 3)`</span>$ </span>
<span id="cb60-1254"><a href="#cb60-1254" aria-hidden="true" tabindex="-1"></a>and the slopes from</span>
<span id="cb60-1255"><a href="#cb60-1255" aria-hidden="true" tabindex="-1"></a>$<span class="in">`r round(Msmpls %&gt;% top_n(-1, beta) %&gt;% pull(beta), 3)`</span>$ to</span>
<span id="cb60-1256"><a href="#cb60-1256" aria-hidden="true" tabindex="-1"></a>$<span class="in">`r round(Msmpls %&gt;% top_n(1, beta) %&gt;% pull(beta), 3)`</span>$.</span>
<span id="cb60-1257"><a href="#cb60-1257" aria-hidden="true" tabindex="-1"></a>The important point is that $\beta$ is, in practice, an unknown</span>
<span id="cb60-1258"><a href="#cb60-1258" aria-hidden="true" tabindex="-1"></a>fixed parameter. $\hat{\beta}$ depends on the $N$ observations of the sample and therefore the $N$ values of the errors. Each sample is characterized by a specific</span>
<span id="cb60-1259"><a href="#cb60-1259" aria-hidden="true" tabindex="-1"></a>vector of errors and therefore by a different value of the estimator.</span>
<span id="cb60-1260"><a href="#cb60-1260" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{data generator process|(}</span>
<span id="cb60-1261"><a href="#cb60-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1262"><a href="#cb60-1262" aria-hidden="true" tabindex="-1"></a>More generally, we denote $R$ the number of replications and we construct a data frame with $R \times N$ lines. The columns are the 13 values of $x$ (fixed in this simulation), the vector $\epsilon$ drawn in a normal distribution with a standard deviation equal to 0.08 and $y = \alpha + \beta x + \epsilon$, with $\alpha = 0.2$ and $\beta = 0.032$ as previously. Finally we add a variable <span class="in">`id`</span> that is a vector containing integers from 1 to $R$ repeated 13 times that will be used to identify the sample:\idxfun{tibble}{tibble}</span>
<span id="cb60-1263"><a href="#cb60-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1264"><a href="#cb60-1264" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-1265"><a href="#cb60-1265" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: experience_setting</span></span>
<span id="cb60-1266"><a href="#cb60-1266" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.2</span> ; beta <span class="ot">&lt;-</span> <span class="fl">0.032</span> ; seps <span class="ot">&lt;-</span> <span class="fl">0.08</span></span>
<span id="cb60-1267"><a href="#cb60-1267" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fl">1E03</span> ; N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb60-1268"><a href="#cb60-1268" aria-hidden="true" tabindex="-1"></a>datas <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">id =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb60-1269"><a href="#cb60-1269" aria-hidden="true" tabindex="-1"></a>                <span class="at">x =</span> <span class="fu">rep</span>(x, R), </span>
<span id="cb60-1270"><a href="#cb60-1270" aria-hidden="true" tabindex="-1"></a>                <span class="at">eps =</span> <span class="fu">rnorm</span>(R <span class="sc">*</span> N, <span class="at">sd =</span> seps),</span>
<span id="cb60-1271"><a href="#cb60-1271" aria-hidden="true" tabindex="-1"></a>                <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps)</span>
<span id="cb60-1272"><a href="#cb60-1272" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1273"><a href="#cb60-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1274"><a href="#cb60-1274" aria-hidden="true" tabindex="-1"></a>Running regressions on every sample requires to use <span class="in">`lm(y ~ x)`</span> not on the whole data set, but on every subset defined by a value of</span>
<span id="cb60-1275"><a href="#cb60-1275" aria-hidden="true" tabindex="-1"></a><span class="in">`id`</span>. The <span class="in">`lm`</span> function\idxfun{lm}{stats} has a <span class="in">`subset`</span> argument that must be a logical expression used to select only a subset of the data frame. For example, to get the fitted model for the first sample, the logical expression is <span class="in">`id == 1`</span>  and one can use:</span>
<span id="cb60-1276"><a href="#cb60-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1277"><a href="#cb60-1277" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb60-1278"><a href="#cb60-1278" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb60-1279"><a href="#cb60-1279" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulation_with_lm</span></span>
<span id="cb60-1280"><a href="#cb60-1280" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> datas, <span class="at">subset =</span> id <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb60-1281"><a href="#cb60-1281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1282"><a href="#cb60-1282" aria-hidden="true" tabindex="-1"></a>The slope can also be calculated using @eq-linear_estimator, which shows that the OLS estimator is a linear combination of the values of the response:\idxfun{filter}{dplyr}\idxfun{mutate}{dplyr}\idxfun{summarise}{dplyr}</span>
<span id="cb60-1283"><a href="#cb60-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1286"><a href="#cb60-1286" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1287"><a href="#cb60-1287" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulation_using_manual_computation</span></span>
<span id="cb60-1288"><a href="#cb60-1288" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb60-1289"><a href="#cb60-1289" aria-hidden="true" tabindex="-1"></a>datas <span class="sc">%&gt;%</span> <span class="fu">filter</span>(id <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1290"><a href="#cb60-1290" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">cn =</span> (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">/</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb60-1291"><a href="#cb60-1291" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">slope =</span> <span class="fu">sum</span>(cn <span class="sc">*</span> y)) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1292"><a href="#cb60-1292" aria-hidden="true" tabindex="-1"></a>  pull</span>
<span id="cb60-1293"><a href="#cb60-1293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1294"><a href="#cb60-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1295"><a href="#cb60-1295" aria-hidden="true" tabindex="-1"></a>To get values of the estimator for every sample, we have to loop on this command for <span class="in">`id`</span> from 1 to <span class="in">`R`</span>. A good practice in **R** is to avoid the use of explicit loops whenever it's possible. The <span class="in">`group_by`</span> / <span class="in">`summarise`</span> couple of functions from the <span class="in">`dplyr`</span> package can be used instead. \idxfun{group<span class="sc">\_</span>by}{dplyr}\idxfun{summarise}{dplyr}</span>
<span id="cb60-1296"><a href="#cb60-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1299"><a href="#cb60-1299" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1300"><a href="#cb60-1300" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulations_group_by_summarise</span></span>
<span id="cb60-1301"><a href="#cb60-1301" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> datas  <span class="sc">%&gt;%</span> </span>
<span id="cb60-1302"><a href="#cb60-1302" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(id) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1303"><a href="#cb60-1303" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">cn =</span> (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">/</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb60-1304"><a href="#cb60-1304" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">slope =</span> <span class="fu">sum</span>(cn <span class="sc">*</span> y), <span class="at">intercept =</span> <span class="fu">mean</span>(y) <span class="sc">-</span> slope <span class="sc">*</span> <span class="fu">mean</span>(x))</span>
<span id="cb60-1305"><a href="#cb60-1305" aria-hidden="true" tabindex="-1"></a>results</span>
<span id="cb60-1306"><a href="#cb60-1306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1307"><a href="#cb60-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1308"><a href="#cb60-1308" aria-hidden="true" tabindex="-1"></a>The result is now a tibble with <span class="in">`R`</span> lines, as the computation of the</span>
<span id="cb60-1309"><a href="#cb60-1309" aria-hidden="true" tabindex="-1"></a>OLS estimator is performed for every sample. As we now have a large number of values of $\hat{\beta}$, we can analyze its statistical properties, for example its mean and its standard deviation for the 1000 random samples:</span>
<span id="cb60-1310"><a href="#cb60-1310" aria-hidden="true" tabindex="-1"></a>\idxfun{summarise}{dplyr}\idxfun{sd}{stats}</span>
<span id="cb60-1313"><a href="#cb60-1313" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1314"><a href="#cb60-1314" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulations_results</span></span>
<span id="cb60-1315"><a href="#cb60-1315" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: false</span></span>
<span id="cb60-1316"><a href="#cb60-1316" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(slope), <span class="at">sd =</span> <span class="fu">sd</span>(slope))</span>
<span id="cb60-1317"><a href="#cb60-1317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1318"><a href="#cb60-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1319"><a href="#cb60-1319" aria-hidden="true" tabindex="-1"></a>We'll see in @sec-stat_prop_slm that the expected value of $\hat{\beta}$ is $\beta=0.032$ (we'll say that the OLS estimator is unbiased) and that its standard deviation is $\sigma_\epsilon / \sigma_x / \sqrt{N} = 0.0041$. The mean and standard deviations for our 1000 values of $\hat{\beta}$ are estimates of these two theoretical values. We can see that the mean of $\hat{\beta}$ is actually equal to 0.032 and that the standard deviation equals 0.00437, which is close to the theoretical value.</span>
<span id="cb60-1320"><a href="#cb60-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1321"><a href="#cb60-1321" aria-hidden="true" tabindex="-1"></a>A more general method to perform simulation is to use list-columns of data frames. To conduct such an analysis step by step, we start with a very small example, with $R = 2$ and $N = 3$.\idxfun{tibble}{tibble}\idxfun{rnorm}{stats}\idxfun{rep}{base}</span>
<span id="cb60-1322"><a href="#cb60-1322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1325"><a href="#cb60-1325" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1326"><a href="#cb60-1326" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: creating_small_data</span></span>
<span id="cb60-1327"><a href="#cb60-1327" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb60-1328"><a href="#cb60-1328" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb60-1329"><a href="#cb60-1329" aria-hidden="true" tabindex="-1"></a>small_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">id =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N),</span>
<span id="cb60-1330"><a href="#cb60-1330" aria-hidden="true" tabindex="-1"></a>                     <span class="at">x =</span> <span class="fu">rep</span>(x[<span class="dv">1</span><span class="sc">:</span>N], R), </span>
<span id="cb60-1331"><a href="#cb60-1331" aria-hidden="true" tabindex="-1"></a>                     <span class="at">eps =</span> <span class="fu">rnorm</span>(R <span class="sc">*</span> N, <span class="at">sd =</span> seps),</span>
<span id="cb60-1332"><a href="#cb60-1332" aria-hidden="true" tabindex="-1"></a>                     <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> eps)</span>
<span id="cb60-1333"><a href="#cb60-1333" aria-hidden="true" tabindex="-1"></a>small_data</span>
<span id="cb60-1334"><a href="#cb60-1334" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1335"><a href="#cb60-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1336"><a href="#cb60-1336" aria-hidden="true" tabindex="-1"></a> Starting with <span class="in">`small_data`</span>, we use <span class="in">`tidy::nest`</span>\idxfun{nest}{tidyr} to nest the data frame using the <span class="in">`id`</span> variable (<span class="in">`.by = id`</span>); the name of the results is by default <span class="in">`data`</span>, but it can be customized using the <span class="in">`.key`</span> argument:</span>
<span id="cb60-1337"><a href="#cb60-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1340"><a href="#cb60-1340" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1341"><a href="#cb60-1341" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nesting_small_data</span></span>
<span id="cb60-1342"><a href="#cb60-1342" aria-hidden="true" tabindex="-1"></a>datalst <span class="ot">&lt;-</span> small_data <span class="sc">%&gt;%</span> <span class="fu">nest</span>(<span class="at">.by =</span> id, <span class="at">.key =</span> <span class="st">"smpl"</span>)</span>
<span id="cb60-1343"><a href="#cb60-1343" aria-hidden="true" tabindex="-1"></a>datalst</span>
<span id="cb60-1344"><a href="#cb60-1344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1345"><a href="#cb60-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1346"><a href="#cb60-1346" aria-hidden="true" tabindex="-1"></a>Each line now corresponds to a value of <span class="in">`id`</span> and <span class="in">`smpl`</span> is a list column that contains a tibble,</span>
<span id="cb60-1347"><a href="#cb60-1347" aria-hidden="true" tabindex="-1"></a>\idxfun{pull}{dplyr}</span>
<span id="cb60-1350"><a href="#cb60-1350" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1351"><a href="#cb60-1351" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: extracting_one_tibble</span></span>
<span id="cb60-1352"><a href="#cb60-1352" aria-hidden="true" tabindex="-1"></a>datalst <span class="sc">%&gt;%</span> <span class="fu">pull</span>(smpl)</span>
<span id="cb60-1353"><a href="#cb60-1353" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1354"><a href="#cb60-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1355"><a href="#cb60-1355" aria-hidden="true" tabindex="-1"></a>We now write a function that takes as argument a data frame containing the $x$ and $y$ columns and returns a one-line tibble that contains the fitted intercept and slope:</span>
<span id="cb60-1356"><a href="#cb60-1356" aria-hidden="true" tabindex="-1"></a>\idxfun{tibble}{tibble}\idxfun{function}{base}</span>
<span id="cb60-1359"><a href="#cb60-1359" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1360"><a href="#cb60-1360" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ols_function</span></span>
<span id="cb60-1361"><a href="#cb60-1361" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="cf">function</span>(s){</span>
<span id="cb60-1362"><a href="#cb60-1362" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> s<span class="sc">$</span>x ; y <span class="ot">&lt;-</span> s<span class="sc">$</span>y</span>
<span id="cb60-1363"><a href="#cb60-1363" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">slope =</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y))) <span class="sc">/</span>  </span>
<span id="cb60-1364"><a href="#cb60-1364" aria-hidden="true" tabindex="-1"></a>           <span class="fu">sum</span>( ( x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>),</span>
<span id="cb60-1365"><a href="#cb60-1365" aria-hidden="true" tabindex="-1"></a>         <span class="at">intercept =</span> <span class="fu">mean</span>(y) <span class="sc">-</span> slope <span class="sc">*</span> <span class="fu">mean</span>(x))</span>
<span id="cb60-1366"><a href="#cb60-1366" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb60-1367"><a href="#cb60-1367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1368"><a href="#cb60-1368" aria-hidden="true" tabindex="-1"></a>This function can be applied for a given sample:</span>
<span id="cb60-1369"><a href="#cb60-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1372"><a href="#cb60-1372" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1373"><a href="#cb60-1373" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ols_on_one_tibble</span></span>
<span id="cb60-1374"><a href="#cb60-1374" aria-hidden="true" tabindex="-1"></a><span class="fu">ols</span>(small_data[<span class="dv">1</span><span class="sc">:</span>N, ])</span>
<span id="cb60-1375"><a href="#cb60-1375" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1376"><a href="#cb60-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1377"><a href="#cb60-1377" aria-hidden="true" tabindex="-1"></a>But, as the <span class="in">`smpl`</span> column of <span class="in">`datalst`</span> is a list, the <span class="in">`ols`</span> function can be applied to all the elements of the list, which can be done using the <span class="in">`purrr::map`</span> function which takes as argument a list and a function:</span>
<span id="cb60-1378"><a href="#cb60-1378" aria-hidden="true" tabindex="-1"></a>\idxfun{pull}{dplyr}\idxfun{map}{purrr}</span>
<span id="cb60-1381"><a href="#cb60-1381" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1382"><a href="#cb60-1382" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ols_on_all_tibbles</span></span>
<span id="cb60-1383"><a href="#cb60-1383" aria-hidden="true" tabindex="-1"></a>datalst <span class="sc">%&gt;%</span> <span class="fu">pull</span>(smpl) <span class="sc">%&gt;%</span> <span class="fu">map</span>(ols)</span>
<span id="cb60-1384"><a href="#cb60-1384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1385"><a href="#cb60-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1386"><a href="#cb60-1386" aria-hidden="true" tabindex="-1"></a>and returns a list. But this can more easily be done using <span class="in">`mutate`</span> or <span class="in">`transmute`</span>, so that the result is a tibble:</span>
<span id="cb60-1387"><a href="#cb60-1387" aria-hidden="true" tabindex="-1"></a>\idxfun{transmute}{dplyr}\idxfun{map}{purrr}</span>
<span id="cb60-1390"><a href="#cb60-1390" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1391"><a href="#cb60-1391" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ols_on_all_tibbles_with_mutate</span></span>
<span id="cb60-1392"><a href="#cb60-1392" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> datalst <span class="sc">%&gt;%</span> <span class="fu">transmute</span>(<span class="at">smpl =</span> <span class="fu">map</span>(smpl, ols))</span>
<span id="cb60-1393"><a href="#cb60-1393" aria-hidden="true" tabindex="-1"></a>results</span>
<span id="cb60-1394"><a href="#cb60-1394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1395"><a href="#cb60-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1396"><a href="#cb60-1396" aria-hidden="true" tabindex="-1"></a>Finally, <span class="in">`tidyr::unnest`</span> expands the list column containing tibbles into rows and columns.</span>
<span id="cb60-1397"><a href="#cb60-1397" aria-hidden="true" tabindex="-1"></a>\idxfun{unnest}{tidyr}</span>
<span id="cb60-1400"><a href="#cb60-1400" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1401"><a href="#cb60-1401" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: unnesting_the_result</span></span>
<span id="cb60-1402"><a href="#cb60-1402" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> <span class="fu">unnest</span>(<span class="at">cols =</span> smpl)</span>
<span id="cb60-1403"><a href="#cb60-1403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1404"><a href="#cb60-1404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1405"><a href="#cb60-1405" aria-hidden="true" tabindex="-1"></a>which returns a standard tibble with one row for each draw and one column for each computed statistic. Going back to the full example with $R = 1000$, we get:</span>
<span id="cb60-1406"><a href="#cb60-1406" aria-hidden="true" tabindex="-1"></a>\idxfun{nest}{tidyr}\idxfun{transmute}{dplyr}\idxfun{map}{purrr}\idxfun{summarise}{dplyr}\idxfun{sd}{stats}</span>
<span id="cb60-1407"><a href="#cb60-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1410"><a href="#cb60-1410" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb60-1411"><a href="#cb60-1411" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nest_full_example</span></span>
<span id="cb60-1412"><a href="#cb60-1412" aria-hidden="true" tabindex="-1"></a>datas <span class="sc">%&gt;%</span> <span class="fu">nest</span>(<span class="at">.by =</span> id, <span class="at">.key =</span> <span class="st">"smpl"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1413"><a href="#cb60-1413" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(<span class="at">smpl =</span> <span class="fu">map</span>(smpl, ols)) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1414"><a href="#cb60-1414" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="at">cols =</span> smpl) <span class="sc">%&gt;%</span> </span>
<span id="cb60-1415"><a href="#cb60-1415" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(slope), <span class="at">sd =</span> <span class="fu">sd</span>(slope))</span>
<span id="cb60-1416"><a href="#cb60-1416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb60-1417"><a href="#cb60-1417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-1418"><a href="#cb60-1418" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!simple linear regression model|)}</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>