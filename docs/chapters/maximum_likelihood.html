<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Microeconometrics with R - 5&nbsp; Maximum likelihood estimator</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/non_spherical.html" rel="next">
<link href="../beyond_OLS.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="../site_libs/kePrint/kePrint.js"></script><link href="../site_libs/lightable/lightable.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Microeconometrics with R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../OLS.html" class="sidebar-item-text sidebar-link">Ordinary least squares estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/multiple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coefficients.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../beyond_OLS.html" class="sidebar-item-text sidebar-link">Beyond the OLS estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/maximum_likelihood.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/non_spherical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/endogeneity.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/treateff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/spatial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Spatial econometrics</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../special_responses.html" class="sidebar-item-text sidebar-link">Special responses</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/binomial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Binomial models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/tobit.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Censored and truncated models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/count.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Count data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/duration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Duration models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discrete choice models</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-ml_unique_parameter" id="toc-sec-ml_unique_parameter" class="nav-link active" data-scroll-target="#sec-ml_unique_parameter"><span class="toc-section-number">5.1</span>  ML estimation of a unique parameter</a>
  <ul class="collapse">
<li><a href="#sec-poisson_ml" id="toc-sec-poisson_ml" class="nav-link" data-scroll-target="#sec-poisson_ml">Computation of the ML estimator for the Poisson distribution</a></li>
  <li><a href="#sec-expon_distr_1par" id="toc-sec-expon_distr_1par" class="nav-link" data-scroll-target="#sec-expon_distr_1par">Computation of the ML estimator for the exponential distribution</a></li>
  <li><a href="#properties-of-the-ml-estimator" id="toc-properties-of-the-ml-estimator" class="nav-link" data-scroll-target="#properties-of-the-ml-estimator">Properties of the ML estimator</a></li>
  <li><a href="#computation-of-the-variance-for-the-poisson-and-the-exponential-distribution" id="toc-computation-of-the-variance-for-the-poisson-and-the-exponential-distribution" class="nav-link" data-scroll-target="#computation-of-the-variance-for-the-poisson-and-the-exponential-distribution">Computation of the variance for the Poisson and the exponential distribution</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ml_general" id="toc-sec-ml_general" class="nav-link" data-scroll-target="#sec-ml_general"><span class="toc-section-number">5.2</span>  ML estimation in the general case</a>
  <ul class="collapse">
<li><a href="#computation-and-properties-of-the-ml-estimator" id="toc-computation-and-properties-of-the-ml-estimator" class="nav-link" data-scroll-target="#computation-and-properties-of-the-ml-estimator">Computation and properties of the ML estimator</a></li>
  <li><a href="#sec-ml_est_expon" id="toc-sec-ml_est_expon" class="nav-link" data-scroll-target="#sec-ml_est_expon">Computation of the estimators for the exponential distribution</a></li>
  <li><a href="#sec-linear-gaussian" id="toc-sec-linear-gaussian" class="nav-link" data-scroll-target="#sec-linear-gaussian">Linear gaussian model</a></li>
  <li><a href="#transformation-of-the-response" id="toc-transformation-of-the-response" class="nav-link" data-scroll-target="#transformation-of-the-response">Transformation of the response</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ml_tests" id="toc-sec-ml_tests" class="nav-link" data-scroll-target="#sec-ml_tests"><span class="toc-section-number">5.3</span>  Tests</a>
  <ul class="collapse">
<li><a href="#sec-three_tests_ml" id="toc-sec-three_tests_ml" class="nav-link" data-scroll-target="#sec-three_tests_ml">Tests for nested models: the three classical tests</a></li>
  <li><a href="#sec-ml_cond_moment_test" id="toc-sec-ml_cond_moment_test" class="nav-link" data-scroll-target="#sec-ml_cond_moment_test">Conditional moment test</a></li>
  <li><a href="#tests-for-non-nested-models" id="toc-tests-for-non-nested-models" class="nav-link" data-scroll-target="#tests-for-non-nested-models">Tests for non-nested models</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-maximum_likelihood" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>The <strong>maximum likelihood</strong> (<strong>ML</strong>) estimator is suitable in situations where the GDP of the response is perfectly known (or supposed to be), up to some unknown parameters. It is particularly useful for models where the response is not continuous but, for example, it is a count, a binomial or a multinomial variable.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a href="#sec-ml_unique_parameter"><span>Section&nbsp;5.1</span></a> presents the ML estimator for the simplest case where there is a unique parameter to estimate. <a href="#sec-ml_general"><span>Section&nbsp;5.2</span></a> extends this basic model to the case where there is a vector of parameters to estimate. Finally <a href="#sec-ml_tests"><span>Section&nbsp;5.3</span></a> present different tests that are particularly useful in the context of ML estimation.</p>
<section id="sec-ml_unique_parameter" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="sec-ml_unique_parameter">
<span class="header-section-number">5.1</span> ML estimation of a unique parameter</h2>
<p>For a first informal view of this estimator, consider two very simple one-parameter distribution functions: </p>
<ul>
<li>the Poisson distribution, which is suitable for the distribution of count responses, i.e., responses which can take only non-negative integer values,</li>
<li>the exponential distribution, which is often used for responses that represent a time span (for example, an unemployment spell).</li>
</ul>
<section id="sec-poisson_ml" class="level3"><h3 class="anchored" data-anchor-id="sec-poisson_ml">Computation of the ML estimator for the Poisson distribution</h3>
<p></p>
<p>As an example of count data, we consider the <code>cartel</code> data set of <span class="citation" data-cites="MILL:09">Miller (<a href="#ref-MILL:09" role="doc-biblioref">2009</a>)</span> who analyzed the role of commitment to the lenient prosecution of early confessors on cartel enforcement. The response <code>ncaught</code> is a count, the number of cartel discoveries per a 6-month period, for the 1985-2005 period. A Poisson variable is defined by the following mass probability function:</p>
<p><span id="eq-poisson_probability"><span class="math display">\[
P(y; \theta) = \frac{e ^ {-\theta} \theta ^ y}{y !}
\tag{5.1}\]</span></span></p>
<p>where <span class="math inline">\(\theta\)</span> is the unique parameter of the distribution. An important characteristic of this distribution is that <span class="math inline">\(\mbox{E}(y)=\mbox{V}(y)=\theta\)</span>, which means that, for a Poisson variable, the variance equals the expectation. To have a first look at the relevance of this hypothesis for the response of our example, we compute the sample mean and variance:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cartels</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span>m <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">ncaught</span><span class="op">)</span>, v <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">ncaught</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">##       m     v</span></span>
<span><span class="co">## 1 5.175 7.789</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The two moments are of the same order of magnitude so that we can confidently rely on the Poisson distribution hypothesis. The question is now how to use the information from the sample to estimate the unique parameter of the distribution. <a href="#fig-bars_poisson">Figure&nbsp;<span>5.1</span></a> represents the empirical distribution of the response and adds several lines which represent the Poisson distribution for different values of <span class="math inline">\(\theta\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bars_poisson" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="maximum_likelihood_files/figure-html/fig-bars_poisson-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.1: Empirical distribution and theoretical Poisson probabilities</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The Poisson distribution is highly asymmetric and has a large mode for <span class="math inline">\(\theta = 2\)</span>, it gets more and more flat and less asymmetric as <span class="math inline">\(\theta\)</span> increases. From <a href="#fig-bars_poisson">Figure&nbsp;<span>5.1</span></a>, we can see that the empirical distribution is very different from the Poisson one for a value of <span class="math inline">\(\theta\)</span> as small as 2 or as large as 8. For <span class="math inline">\(\theta = 5\)</span>, there is a reasonable correspondence between the empirical and the theoretical distribution. To get unambiguously a value for our estimator, we need an objective function that is a function of <span class="math inline">\(\theta\)</span> which is, in the context of this chapter, the <strong>likelihood function</strong>. To construct it, it is much simpler to consider the sample as a random sample. For the first observation <span class="math inline">\(1\)</span>, given a value of <span class="math inline">\(\theta\)</span>, the probability of observing <span class="math inline">\(y_1\)</span> is <span class="math inline">\(P(y_1; \theta)\)</span>. For the second one, with the random sample hypothesis, the probability of observing <span class="math inline">\(y_2\)</span> is <span class="math inline">\(P(y_2;\theta)\)</span> and is independent of <span class="math inline">\(y_1\)</span>. Therefore, the joint probability of observing <span class="math inline">\((y_1, y_2)\)</span> is <span class="math inline">\(P(y_1;\theta)\times P(y_2;\theta)\)</span> and, more generally, the probability of observing the values of <span class="math inline">\(y\)</span> for the whole sample is the likelihood function for a Poisson variable in a random sample which writes:</p>
<p><span class="math display">\[
L(\theta, y) = \Pi_{n=1}^N P(y_n;\theta)
\]</span></p>
<p>and the maximum likelihood estimator of <span class="math inline">\(\theta\)</span> is the value of <span class="math inline">\(\theta\)</span> which maximizes the likelihood function. Note the change of notations: <span class="math inline">\(P(y_n; \theta)\)</span> is the probability mass function, it is a function of <span class="math inline">\(y^\top = (y_1, y_2, \ldots, y_N)\)</span> and computes a probability for a given (known) value of the parameter of the distribution. The likelihood function <span class="math inline">\(L(\theta, y)\)</span> is written as a function of the unknown value of the parameter of the distribution and its value depends on the realization of the response vector <span class="math inline">\(y\)</span> on the sample.</p>
<p>There are several good reasons to consider the logarithm of the likelihood function instead of the likelihood itself. In particular, being a product of <span class="math inline">\(N\)</span> terms, the likelihood will typically take very high or very low values for large samples. Moreover, the logarithm transforms a product in a sum, which is much more convenient. Note finally that, as the logarithm is a strictly increasing function, maximizing the likelihood or its logarithm leads to the same value of <span class="math inline">\(\theta\)</span>. Taking the log and replacing <span class="math inline">\(P(y_n; \theta)\)</span> by the expression given in <a href="#eq-poisson_probability">Equation&nbsp;<span>5.1</span></a>, we get:</p>
<p><span class="math display">\[
\ln L(\theta, y) = \sum_{n=1} ^ N \ln \frac{e ^ {-\theta} \theta ^ y}{y !}
\]</span></p>
<p>or, regrouping terms:</p>
<p><span class="math display">\[
\ln L(\theta, y) = - N \theta + \ln \theta \sum_{n=1} ^ N y_n -
\sum_{n = 1} ^ N \ln y_n !
\]</span></p>
<p>The log-likelihood is therefore the sum of three terms, and note that the last one doesn’t depend on <span class="math inline">\(\theta\)</span>. For our sample, we have: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">cartels</span><span class="op">$</span><span class="va">ncaught</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">sum_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">sum_log_fact_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html">lfactorial</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">lnl_poisson</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">-</span> <span class="va">N</span> <span class="op">*</span> <span class="va">theta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">*</span> <span class="va">sum_y</span> <span class="op">-</span></span>
<span>    <span class="va">sum_log_fact_y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The last line of code define <code>lnl_poisson</code> as a function of <code>theta</code> which returns the value of the log likelihood function for a value of <span class="math inline">\(\theta\)</span> given the value of the responses in the <code>cartel</code> data set.</p>
<p>Taking some values of <span class="math inline">\(\theta\)</span>, we compute in <a href="#tbl-poisson_probs">Table&nbsp;<span>5.1</span></a> the corresponding values of the log likelihood function. The log likelihood function is inverse U-shaped, and the maximum value for the integer values we’ve provided is reached for <span class="math inline">\(\theta = 5\)</span>. <a href="#fig-loglik_poisson">Figure&nbsp;<span>5.2</span></a> presents the log likelihood function as a smooth line in the neighborhood of <span class="math inline">\(\theta = 5\)</span>, which indicates that the maximum of the log likelihood function occurs for a value of <span class="math inline">\(\theta\)</span> between 5.0 and 5.5. The first-order condition for a maximum is that the first derivative of <span class="math inline">\(\ln L\)</span> with respect to <span class="math inline">\(\theta\)</span> is 0.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="tbl-poisson_probs" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;5.1: Poisson probabilities for different values of the parameter</caption>
<thead><tr class="header">
<th style="text-align: right;"><span class="math inline">\(\theta\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\ln L\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">-270.13</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">-166.65</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">-122.72</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: right;">-103.16</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: right;">-96.97</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: right;">-99.23</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: right;">-107.32</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: right;">-119.68</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-loglik_poisson" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="maximum_likelihood_files/figure-html/fig-loglik_poisson-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.2: Log-likelihood curve for a Poisson variable</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="math display">\[
\frac{\partial \ln L}{\partial \theta} = - N + \frac{\sum_{n=1} ^ N y_n}{\theta}=0
\]</span></p>
<p>This leads to <span class="math inline">\(\hat{\theta} = \frac{\sum_{n=1}^N y_n}{N}\)</span>. Therefore, in this simple case, we can explicitly obtain the ML estimator by solving the first-order condition, and, moreover, the ML estimator is the sample mean of the response, which is hardly surprising as <span class="math inline">\(\theta\)</span> is the expected value of <span class="math inline">\(y\)</span> for a Poisson variable. The second derivative is:</p>
<p><span class="math display">\[
\frac{\partial^2 \ln L}{\partial \theta ^ 2} = - \frac{\sum_{n=1} ^ N
y_n}{\theta ^ 2} &lt; 0
\]</span></p>
<p>and is negative for all values of <span class="math inline">\(\theta\)</span>, which indicates that the log-likelihood is globally concave and therefore that optimum we previously obtained is the global maximum. For our sample, the ML estimator is:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hat_theta</span> <span class="op">&lt;-</span> <span class="va">sum_y</span> <span class="op">/</span> <span class="va">N</span></span>
<span><span class="va">hat_theta</span></span>
<span><span class="co">## [1] 5.175</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section><section id="sec-expon_distr_1par" class="level3"><h3 class="anchored" data-anchor-id="sec-expon_distr_1par">Computation of the ML estimator for the exponential distribution</h3>
<p> The second example illustrates the computation of the ML estimator for a continuous variable. The density of a variable which follows an exponential distribution of parameter <span class="math inline">\(\theta\)</span> is:</p>
<p><span class="math display">\[
f(y; \theta) = \theta e ^{-\theta y}
\]</span></p>
<p>The expected value and the variance of <span class="math inline">\(y\)</span> are <span class="math inline">\(\mbox{E}(y) = 1 / \theta\)</span> and <span class="math inline">\(\mbox{V}(y) = 1 / \theta ^ 2\)</span> so that, for a variable which follows an exponential distribution, the mean should be equal to the standard deviation. To illustrate the use of the exponential distribution, we use the <code>oil</code> data set of <span class="citation" data-cites="FAVE:PESA:SHAR:94">Favero, Pesaran, and Sharma (<a href="#ref-FAVE:PESA:SHAR:94" role="doc-biblioref">1994</a>)</span> for which the response <code>dur</code> is the time span between the discovery of an oil field and the date of the British government’s approval to exploit this field. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">oil</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span>m <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">dur</span><span class="op">)</span>, s <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">dur</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## # A tibble: 1 × 2</span></span>
<span><span class="co">##       m     s</span></span>
<span><span class="co">##   &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">## 1  63.0  55.2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The sample mean and the standard deviation are close, in conformity with the features of the exponential distribution. <a href="#fig-histo_exponential">Figure&nbsp;<span>5.3</span></a> presents the empirical distribution of <code>dur</code> by a histogram, and we add several exponential density lines for different values of <span class="math inline">\(\theta\)</span> (0.005, 0.01 and 0.03).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-histo_exponential" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="maximum_likelihood_files/figure-html/fig-histo_exponential-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.3: Empirical distribution and density of the exponential distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<!-- !!! mettre une legende -->
<p>The adjustment between the empirical and the theoretical distribution is quite good with the dotted line which corresponds to <span class="math inline">\(\theta = 0.01\)</span>. The reasoning to construct the log-likelihood function is exactly the same as for the discrete Poisson distribution, except that the mass probability function is replaced by the density function. The log-likelihood function therefore writes:</p>
<p><span class="math display">\[
\ln L(\theta, y) = \sum_{n = 1} ^ N \ln \left(\theta e ^{-\theta y_n}\right)
\]</span></p>
<p>or, regrouping terms:</p>
<p><span class="math display">\[
\ln L(\theta, y) = N \ln \theta - \theta \sum_{n = 1} ^ N y_n
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-loglik_exponential" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="maximum_likelihood_files/figure-html/fig-loglik_exponential-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.4: Log-likelihood curve for an exponential variable</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The shape of the log-likelihood function is represented in <a href="#fig-loglik_exponential">Figure&nbsp;<span>5.4</span></a>. As in the Poisson case, the log-likelihood seems to be globally concave, with a global maximum corresponding to a value of <span class="math inline">\(\theta\)</span> approximately equal to <span class="math inline">\(0.015\)</span>. The first-order condition for a maximum is:</p>
<p><span class="math display">\[
\frac{\partial \ln L}{\partial \theta} = \frac{N}{\theta} -
\sum_{n=1}^N y_n
\]</span></p>
<p>which leads to the ML estimator: <span class="math inline">\(\hat{\theta} = \frac{N}{\sum_{n=1}^N y_n} = \frac{1}{\bar{y}}\)</span>. The second derivative is:</p>
<p><span class="math display">\[
\frac{\partial \ln^2 L}{\partial \theta^2} = -\frac{N}{\theta^2}
\]</span></p>
<p>which is negative for all values of <span class="math inline">\(\theta\)</span>, so that <span class="math inline">\(\hat{\theta}\)</span> is the global maximum of the log-likelihood function. In our example, we get:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hat_theta</span> <span class="op">&lt;-</span> <span class="va">N</span> <span class="op">/</span> <span class="va">sum_y</span></span>
<span><span class="va">hat_theta</span></span>
<span><span class="co">## [1] 0.01587</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section><section id="properties-of-the-ml-estimator" class="level3"><h3 class="anchored" data-anchor-id="properties-of-the-ml-estimator">Properties of the ML estimator</h3>
<p> We consider a variable <span class="math inline">\(y\)</span> for which we observe <span class="math inline">\(N\)</span> realizations in a random sample. We assume that <span class="math inline">\(y\)</span> follows a distribution with a unique parameter <span class="math inline">\(\theta\)</span>. The density (if <span class="math inline">\(y\)</span> is continuous) or the mass probability function (if <span class="math inline">\(y\)</span> is discrete) is denoted <span class="math inline">\(\phi(y; \theta)\)</span>. We also denote <span class="math inline">\(\lambda(y; \theta) = \ln \phi(y; \theta)\)</span> and <span class="math inline">\(\gamma(y; \theta)\)</span> and <span class="math inline">\(\psi(y; \theta)\)</span> the first two derivatives of <span class="math inline">\(\lambda\)</span> with <span class="math inline">\(\theta\)</span>. The log likelihood function is then:</p>
<p><span class="math display">\[
\ln L(\theta, y) = \sum_n \lambda(y_n; \theta) = \sum_n \ln \phi(y_n; \theta)
\]</span></p>
<p>with <span class="math inline">\(y^\top = (y_1, y_2, \ldots, y_N)\)</span> the vector containing all the values of the response in the sample. The “true value” of <span class="math inline">\(\theta\)</span> is denoted by <span class="math inline">\(\theta_0\)</span> and the maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span>. The proof of the consistency of the ML estimator is based on Jensen’s inequality, which states that, for a random variable <span class="math inline">\(X\)</span> and a concave function <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
\mbox{E}(f(X)) \leq f(\mbox{E}(X))
\]</span></p>
<p>As the logarithm is a concave function:</p>
<p><span id="eq-jensen"><span class="math display">\[
\mbox{E}\ln\frac{L(\theta)}{L(\theta_0)} &lt;  \ln \mbox{E}\frac{L(\theta)}{L(\theta_0)}
\tag{5.2}\]</span></span></p>
<p>The expectation on the right side of the equation is obtained by integrating out <span class="math inline">\(L(\theta)/L(\theta_0)\)</span> using the density of <span class="math inline">\(y\)</span>, which is <span class="math inline">\(L(\theta_0)\)</span>. Therefore:</p>
<p><span class="math display">\[
\mbox{E}\frac{L(\theta)}{L(\theta_0)} = \int
\frac{L(\theta)}{L(\theta_0)} L(\theta_0) dy = \int
L(\theta) dy = 1
\]</span></p>
<p>as any density sums to 1 for the whole support of the variable. Therefore <a href="#eq-jensen">Equation&nbsp;<span>5.2</span></a> implies that:</p>
<p><span class="math display">\[
\mbox{E}\ln L(\theta) \leq \mbox{E}\ln L(\theta_0)
\]</span></p>
<p>Dividing by <span class="math inline">\(N\)</span> and using the law of large numbers, we also have:</p>
<p><span id="eq-firstineq"><span class="math display">\[
\mbox{plim} \frac{1}{N}\ln L(\theta) \leq \mbox{plim} \frac{1}{N}\ln L(\theta_0)
\tag{5.3}\]</span></span></p>
<p>As <span class="math inline">\(\hat{\theta}\)</span> maximizes <span class="math inline">\(\ln L(\theta)\)</span>, it is also the case that <span class="math inline">\(\ln L (\hat{\theta}) \geq \ln L (\theta)\)</span>. Once again, dividing by <span class="math inline">\(N\)</span> and computing the limit, we have:</p>
<p><span id="eq-secondineq"><span class="math display">\[
\mbox{plim} \frac{1}{N}\ln L(\hat{\theta}) \geq \mbox{plim} \frac{1}{N}\ln L(\theta)
\tag{5.4}\]</span></span></p>
<p>The only solution for <a href="#eq-firstineq">Equation&nbsp;<span>5.3</span></a> and <a href="#eq-secondineq">Equation&nbsp;<span>5.4</span></a> to hold is that:</p>
<p><span id="eq-equality"><span class="math display">\[
\mbox{plim} \frac{1}{N}\ln L(\hat{\theta}) = \mbox{plim} \frac{1}{N}\ln L(\theta_0)
\tag{5.5}\]</span></span></p>
<p><a href="#eq-equality">Equation&nbsp;<span>5.5</span></a> indicates that, as <span class="math inline">\(N\)</span> tends to infinity, the average likelihood for <span class="math inline">\(\hat{\theta}\)</span> converges to the average likelihood for <span class="math inline">\(\theta_0\)</span>. Using a regularity condition, this implies that the estimator is consistent, i.e., that <span class="math inline">\(\mbox{plim} \,\hat{\theta} = \theta_0\)</span>. </p>
<p>The first two derivatives of the log-likelihood functions, which are respectively called the <strong>gradient</strong> (or the <strong>score</strong>) and the <strong>hessian</strong> of the log-likelihood are:</p>
<p><span id="eq-gradient_scalar"><span class="math display">\[
g(\theta, y) = \frac{\partial \ln L}{\partial \theta}(\theta, y) =
\sum_{n=1}^N \frac{\partial \lambda}{\partial \theta}(y_n; \theta) =
\sum_{n=1}^N \gamma(y_n; \theta)
\tag{5.6}\]</span></span></p>
<p>and</p>
<p><span id="eq-hessian_scalar"><span class="math display">\[
h(\theta, y) = \frac{\partial^2 \ln L}{\partial \theta^2}(\theta, y) =
\sum_{n=1}^N \frac{\partial^2 \lambda}{\partial \theta^2}(y_n; \theta) =
\sum_{n=1}^N \psi(y_n; \theta)
\tag{5.7}\]</span></span></p>
<p>As there is only one parameter to estimate, both functions are scalar functions. For a discrete distribution, the probabilities for every possible <span class="math inline">\(K\)</span> values of <span class="math inline">\(y\)</span> (denoted by <span class="math inline">\(y_k\)</span>) sum to unity:</p>
<p><span class="math display">\[
\sum_{k=1} ^ K \phi(y_k; \theta) = 1
\]</span></p>
<p>The same result applies with the continuous sum for a continuous random variable:</p>
<p><span class="math display">\[
\int \phi(y; \theta) dy = 1
\]</span></p>
<p>As <span class="math inline">\(\phi = e ^ \lambda\)</span>, taking the derivative with respect to <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span id="eq-gradient_0_theta0"><span class="math display">\[
\left\{
\begin{array}{rcl}
\displaystyle  \sum_{k=1} ^ K \gamma(y_k; \theta) \phi(y_k; \theta)&amp;=&amp; 0 \\
\displaystyle \int \gamma(y; \theta) \phi(y; \theta) dy&amp;=&amp; 0
\end{array}
\right.
\tag{5.8}\]</span></span></p>
<p>Evaluated for the true value <span class="math inline">\(\theta_0\)</span>, <a href="#eq-gradient_0_theta0">Equation&nbsp;<span>5.8</span></a> is the expectation of an individual contribution to the gradient. Therefore, evaluated at the true value of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\gamma(y; \theta)\)</span> is a random variable with 0 expectation. Taking now the second derivative with respect to <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span id="eq-variance_hessian"><span class="math display">\[
\left\{
\begin{array}{rcl}
\displaystyle \sum_{k=1} ^ K \gamma(y_k; \theta) ^ 2 \phi(y_k; \theta) +
\sum_{k=1} ^ K  \psi(y_k; \theta) \phi(y_k; \theta) &amp;=&amp; 0 \\
\displaystyle \int \gamma(y; \theta) ^ 2 \phi(y; \theta) dy +
\int  \psi(y; \theta) \phi(y; \theta)dy&amp;=&amp; 0
\end{array}
\right.
\tag{5.9}\]</span></span></p>
<p>We denote the first term by <span class="math inline">\(v_\gamma(\theta)\)</span>: evaluated for the true value of the parameter, this is the variance of <span class="math inline">\(\gamma(y; \theta)\)</span>:, <span class="math inline">\(v_\gamma(\theta_0) = \sigma_\gamma^2\)</span>. We denote the second term by <span class="math inline">\(m_\psi(\theta)\)</span>, which is, evaluated for the true value of the parameter, the expectation of <span class="math inline">\(\psi(y; \theta)\)</span>: <span class="math inline">\(m_\psi(\theta_0) = \mu_\psi\)</span>. Therefore, <a href="#eq-variance_hessian">Equation&nbsp;<span>5.9</span></a> indicates that <span class="math inline">\(\sigma_\gamma ^ 2 = - \mu_\psi\)</span>.</p>
<p>The gradient <span class="math inline">\(g(\theta, y)\)</span> (<a href="#eq-gradient_scalar">Equation&nbsp;<span>5.6</span></a>) is the sum of <span class="math inline">\(N\)</span> contributions <span class="math inline">\(\gamma(y_n; \theta)\)</span> which have 0 expectation. Therefore, its expectation is 0. With the random sample hypothesis, <span class="math inline">\(\gamma(y_n; \theta)\)</span> and <span class="math inline">\(\gamma(y_m; \theta)\)</span> are independent for all <span class="math inline">\(m\neq n\)</span> and the variance of the gradient is therefore the sum of the variances of its <span class="math inline">\(N\)</span> contributions, which are all equal to <span class="math inline">\(\sigma^2_\gamma\)</span>. Therefore, <span class="math inline">\(\mbox{V}(g(\theta_0, y)) = N \sigma^2_\gamma\)</span>. The variance of the gradient is called the <strong>information matrix</strong> in the general case, but it is actually in our one parameter case a scalar that we’ll denote <span class="math inline">\(\iota(\theta_0)\)</span>. The hessian being a sum of <span class="math inline">\(N\)</span> contributions <span class="math inline">\(\psi(y_n; \theta)\)</span>, which have an expectation equal to <span class="math inline">\(\mu_\psi\)</span>, its expected value is <span class="math inline">\(\mbox{E}(h(\theta_0, y)) = N \mu_\psi\)</span>. The result we previously established (<span class="math inline">\(\sigma_\gamma ^ 2 = - \mu_\psi\)</span>) implies that the variance of the gradient (the information) equals the opposite of the expectation of the hessian: </p>
<p><span id="eq-information_equality_one_par"><span class="math display">\[
\iota(\theta_0) = \mbox{V}(g(\theta_0, y)) = - \mbox{E}(h(\theta_0, y)) = N \sigma^2_\gamma = - N\mu_\psi
\tag{5.10}\]</span></span></p>
<p>This important result is called the <strong>information equality</strong>. Denoting <span class="math inline">\(\theta_0\)</span> as the true (unknown) value of the parameter, and omitting for convenience the <span class="math inline">\(y\)</span> vector, a first-order Taylor expansion of <span class="math inline">\(g(\hat{\theta})\)</span> around <span class="math inline">\(\theta_0\)</span> is:</p>
<p><span class="math display">\[
g(\hat{\theta}) \approx g(\theta_0) + h(\theta_0) (\hat{\theta} -
\theta_0)
\]</span> If we use instead an exact first-order Taylor expansion:</p>
<p><span id="eq-exact_taylor_expansion"><span class="math display">\[
g(\hat{\theta}) = g(\theta_0) + h(\bar{\theta}) (\hat{\theta} - \theta_0)
\tag{5.11}\]</span></span></p>
<p>where <span class="math inline">\(\bar{\theta}\)</span> lies between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta_0\)</span>. As <span class="math inline">\(g(\hat{\theta}) = 0\)</span>, solving for <span class="math inline">\(\hat{\theta} - \theta_0\)</span>, we get:</p>
<p><span class="math display">\[
(\hat{\theta} - \theta_0) = -
h(\bar{\theta})^{-1} g(\theta_0)
\]</span></p>
<p>or, multiplying by <span class="math inline">\(\sqrt{N}\)</span>:</p>
<p><span class="math display">\[
\sqrt{N}(\hat{\theta} - \theta_0) = -
\left(\frac{h(\bar{\theta})}{N}\right)^{-1} \frac{g(\theta_0)}{\sqrt{N}}
\]</span> </p>
<p>Assuming that the estimator is consistent, as <span class="math inline">\(N\)</span> grows, <span class="math inline">\(\hat{\theta}\)</span> converges to <span class="math inline">\(\theta_0\)</span> and so does <span class="math inline">\(\bar{\theta}\)</span>. As <span class="math inline">\(\mbox{E}(h(\theta_0)) = N \mu_\psi\)</span>, the expectation of the first term is <span class="math inline">\(\mu_\psi\)</span>, and it is also its probability limit. The second term has a 0 expected value and a variance equal to <span class="math inline">\(\sigma_\gamma ^ 2\)</span>. Therefore, <span class="math inline">\(\sqrt{N}(\hat{\theta} - \theta_0)\)</span> has a zero expectation and an asymptotic covariance equal to <span class="math inline">\(\mu_\psi ^ {-2} \sigma_\gamma^2\)</span>. Moreover, applying the central-limit theorem, its asymptotic distribution is normal. Therefore: </p>
<p><span id="eq-asdist_general"><span class="math display">\[
\sqrt{N}(\hat{\theta}-\theta_0) \overset{p}{\rightarrow} N(0, \mu_\psi ^ {-2}
\sigma_\gamma ^ 2) \; \mbox{ and } \;
\hat{\theta} \overset{a}{\sim} \mathcal{N}(\theta_0, \mu_\psi ^ {-2}
\sigma_\gamma ^ 2 / N)
\tag{5.12}\]</span></span></p>
<p> Applying the information equality, <span class="math inline">\(\iota(\theta_0) = N \sigma_\gamma ^ 2(\theta_0) = - N \mu_\psi(\theta_0)\)</span> and therefore:</p>
<p><span id="eq-asdist_info_eq"><span class="math display">\[
\hat{\theta} \overset{a}{\sim} \mathcal{N}(\theta_0, \iota(\theta_0) ^ {-1})
\tag{5.13}\]</span></span></p>
<p>We have seen that the variance of the ML estimator is the inverse of the information, which can be either obtained using the variance of the gradient or the opposite of the expectation of the hessian. In terms of the average information (<span class="math inline">\(\iota(\theta) / N\)</span>), we have:</p>
<p><span id="eq-average_info"><span class="math display">\[
\left\{
\begin{array}{rcl}
\frac{\iota(\theta)}{N} &amp;=&amp; \frac{1}{N} \mbox{V}\left(\frac{g(\theta, y)}{\sqrt{N}}\right) = \frac{1}{N}
\sum_{n= 1} ^ N \mbox{E}(\gamma(y; \theta) ^ 2) = v_\gamma (\theta) \\
\frac{\iota(\theta)}{N} &amp;=&amp; \frac{1}{N} \mbox{E}\left(- \frac{h(\theta, y)}{N}\right) = - \frac{1}{N} \sum_{n = 1} ^ N
\mbox{E}(\psi(y_n; \theta)) = - m_\psi(\theta)
\end{array}
\right.
\tag{5.14}\]</span></span></p>
<p></p>
<p>If this variance/expectation can be computed, then a natural estimator of <span class="math inline">\(\iota(\theta_0)\)</span> is <span class="math inline">\(\iota(\hat{\theta})\)</span>. This <strong>information-based estimator</strong> of the variance is obtained by inverting the information evaluated for the maximum likelihood value of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\theta}i} ^ 2 = \iota(\hat{\theta}) ^ {-1}
\]</span></p>
<p>On the contrary, if it is impossible to compute the expectations, two natural estimators of the information are based on the gradient and the hessian and are obtained by evaluating one of the two expressions in <a href="#eq-average_info">Equation&nbsp;<span>5.14</span></a> without the expectation. Denoting <span class="math inline">\(\iota_g\)</span> and <span class="math inline">\(\iota_h\)</span>, these two estimations of the information, we have from <a href="#eq-average_info">Equation&nbsp;<span>5.14</span></a>:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\frac{\iota_g(\theta)}{N} &amp;=&amp;  \frac{1}{N}
\sum_{n= 1} ^ N \gamma(y_n; \theta) ^ 2 = \hat{v}_\gamma(\theta) \\
\frac{\iota_h(\theta)}{N} &amp;=&amp; - \frac{h(\theta, y)}{N} = - \frac{1}{N} \sum_{n = 1} ^ N
\psi(\theta; y_n) = - \hat{m}_\psi(\theta)
\end{array}
\right.
\]</span> </p>
<p>Evaluated for the maximum likelihood estimator value of <span class="math inline">\(\theta\)</span>, we then get the <strong>gradient-based estimator</strong>:</p>
<p><span id="eq-gradient_est"><span class="math display">\[
\hat{\sigma}_{\hat{\theta} g} ^ 2 = \iota_g(\hat{\theta}) ^ {-1} =
\left(\sum_{n= 1} ^ N \gamma(y ; \hat{\theta}) ^ 2\right) ^ {-1} = \frac{1}{N
\hat{v}_\gamma(\hat{\theta})}
\tag{5.15}\]</span></span></p>
<p> and the <strong>hessian-based estimator</strong> of the variance of <span class="math inline">\(\theta\)</span>:</p>
<p><span id="eq-hessian_est"><span class="math display">\[
\hat{\sigma}_{\hat{\theta} h} ^ 2 = \iota_h(\hat{\theta}) ^ {-1} =
- h(\hat{\theta}, y) ^ {-1} = - \frac{1}{N \hat{m}_{\psi}(\hat{\theta})}
\tag{5.16}\]</span></span></p>
<p>A fourth estimator is based on <a href="#eq-asdist_general">Equation&nbsp;<span>5.12</span></a>, which states that, before applying the information equality,</p>
<p><span id="eq-var_general"><span class="math display">\[
\sigma_{\hat{\theta}} ^ 2 = \frac{1}{N} \frac{v_\gamma(\theta_0)}{m_\psi(\theta_0)^2}
\tag{5.17}\]</span></span></p>
<p>Removing the expectation from <a href="#eq-var_general">Equation&nbsp;<span>5.17</span></a> and evaluating for the maximum likelihood estimator of <span class="math inline">\(\theta\)</span>, we get the <strong>sandwich estimator</strong> of the variance of <span class="math inline">\(\hat{\theta}\)</span>. <span id="eq-sandwich-est"><span class="math display">\[
\hat{\sigma} ^ 2 _{\hat{\theta} s} =
\frac{1}{N}
\left(\sum_{n=1} ^ N \gamma(y_n; \hat{\theta}) ^ 2 /N\right)/
\left(\sum_{n=1} ^ N \psi(y_n; \hat{\theta}) /N\right) ^ 2
=\frac{1}{N}\frac{\hat{v}_\gamma (\hat{\theta})}{\hat{m}_\psi(\hat{\theta})}
\tag{5.18}\]</span></span></p>
<p><a href="#eq-sandwich-est">Equation&nbsp;<span>5.18</span></a> is called a sandwich estimator for a reason that will be clear when we’ll compute it in the general case where more than one parameter are estimated. It is a more general estimator than the previous three, as its consistency doesn’t rely on the information equality property, which is only valid if the distribution of <span class="math inline">\(y\)</span> is correctly specified.</p>
</section><section id="computation-of-the-variance-for-the-poisson-and-the-exponential-distribution" class="level3"><h3 class="anchored" data-anchor-id="computation-of-the-variance-for-the-poisson-and-the-exponential-distribution">Computation of the variance for the Poisson and the exponential distribution</h3>
<p> For the Poisson model, we have:</p>
<p><span class="math display">\[
g(\theta) = - N + \frac{\sum_{n=1} ^ N y_n}{\theta} \mbox{ and } h(\theta) = -
\frac{\sum_{n=1} ^ N y_n}{\theta ^ 2}
\]</span></p>
<p>The variance of the gradient is:</p>
<p><span class="math display">\[
\mbox{V}\left(g(\theta)\right) = \frac{1}{\theta ^
2}\mbox{V}\left(\sum_{n=1} ^ N y_n\right) =
\frac{1}{\theta ^
2}\sum_{n=1} ^ N \mbox{V}\left(y_n\right) =
\frac{1}{\theta ^
2} N \theta =
\frac{N}{\theta}
\]</span></p>
<p>The first equality holds because of the random sample hypothesis and the second one because <span class="math inline">\(\mbox{V}(y) = \theta\)</span>. The expected value of the hessian is:</p>
<p><span class="math display">\[
\mbox{E}\left(h(\theta)\right) = -
\frac{\sum_{n=1} ^ N \mbox{E}(y_n)}{\theta ^ 2} = - \frac{N\theta}{\theta ^ 2} = -\frac{N}{\theta}
\]</span></p>
<p>because <span class="math inline">\(\mbox{E}(y) = \theta\)</span>. Therefore, we are in the case where the information can be computed and the result illustrates the information equality. The information based estimator of <span class="math inline">\(\hat{\theta}\)</span> is:</p>
<p><span class="math display">\[
\iota(\hat{\theta}) = \frac{N}{\hat{\theta}} = \frac{N}{\bar{y}}
\]</span></p>
<p>The individual contributions to the gradient are: <span class="math inline">\(\gamma(y_n; \theta) = - 1 + y_n / \theta\)</span>, so that the gradient-based estimator of the information is:</p>
<p><span class="math display">\[
\iota_g(\theta) = \sum_{n=1}^N \gamma(y_n; \theta) ^ 2 =
\sum_{n=1}^N \frac{(y_n - \theta) ^ 2}{\theta ^ 2}
\]</span></p>
<p>Evaluating <span class="math inline">\(\iota_g\)</span> for the maximum likelihood estimator, we finally get:</p>
<p><span class="math display">\[
\iota_g(\hat{\theta}) =
\sum_{n=1}^N \frac{(y_n - \bar{y}) ^ 2}{\bar{y} ^ 2} = N \frac{\hat{\sigma}_y ^ 2}{\bar{y} ^ 2}
\]</span></p>
<p>For the hessian-based estimator of the information, we consider the opposite of the hessian evaluated for the ML estimator:</p>
<p><span class="math display">\[
\iota_h(\hat{\theta}) = \frac{\sum_n y_n}{\hat{\theta} ^ 2} =
\frac{N}{\bar{y}}
\]</span></p>
<p>Finally, from <a href="#eq-sandwich-est">Equation&nbsp;<span>5.18</span></a>, the sandwich estimator of the variance is:</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\theta}s} = \frac{1}{N}\frac{\hat{\sigma}_y ^ 2 / \bar{y} ^ 2}{(1/\bar{y}) ^ 2} = \frac{\hat{\sigma} ^ 2_y}{N}
\]</span></p>
<p>To summarize, the four estimators of the variance of <span class="math inline">\(\hat{\theta}\)</span> are:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rclrcl}
\hat{\sigma}_{\hat{\theta}i} ^ 2 &amp;=&amp; \iota(\hat{\theta}) ^ {-1} &amp;=&amp;
\bar{y} / N \\
\hat{\sigma}_{\hat{\theta}g} ^ 2 &amp;=&amp; \iota_g(\hat{\theta}) ^ {-1} &amp;=&amp;
\frac{\bar{y} ^ 2}{\hat{\sigma}_y^ 2} / N \\
\hat{\sigma}_{\hat{\theta}h} ^ 2 &amp;=&amp; \iota_h(\hat{\theta}) ^ {-1} &amp;=&amp;
\bar{y} / N \\
\hat{\sigma}_{\hat{\theta}s} ^ 2 &amp;&amp; &amp;=&amp; \hat{\sigma}^2_y / N
\end{array}
\right.
\]</span></p>
<p>Note that in this case, <span class="math inline">\(\iota(\hat{\theta}) = \iota_h(\hat{\theta})\)</span> and that, if the Poisson distribution hypothesis is correct, <span class="math inline">\(\mbox{plim} \,\bar{y} = \mbox{plim}\, \hat{\sigma}_y ^ 2 = \theta_0\)</span> so that the four estimators are consistent as <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\hat{\sigma}_y ^ 2\)</span> both converge to <span class="math inline">\(\theta_0\)</span>. Computing these four estimations of the variance of <span class="math inline">\(\hat{\theta}\)</span> for the <code>ncaught</code> variable, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">cartels</span><span class="op">)</span> ; <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">cartels</span><span class="op">$</span><span class="va">ncaught</span></span>
<span><span class="va">mean_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> ; <span class="va">var_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mean_y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>info <span class="op">=</span> <span class="va">mean_y</span> <span class="op">/</span> <span class="va">N</span>, gradient <span class="op">=</span> <span class="va">mean_y</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="va">var_y</span> <span class="op">/</span> <span class="va">N</span>,</span>
<span>  hessian <span class="op">=</span> <span class="va">mean_y</span> <span class="op">/</span> <span class="va">N</span>, sandwich <span class="op">=</span> <span class="va">var_y</span> <span class="op">/</span> <span class="va">N</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="va">sqrt</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">##     info gradient  hessian sandwich </span></span>
<span><span class="co">##    0.360    0.297    0.360    0.436</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
<p>For the exponential distribution, remember that <span class="math inline">\(\lambda(y; \theta)= \ln \theta - \theta y\)</span>, <span class="math inline">\(\gamma(y; \theta) = 1 / \theta - y\)</span> and <span class="math inline">\(\psi(y; \theta) = - 1 / \theta ^ 2\)</span>. As <span class="math inline">\(h(\theta, y) = \sum_n \psi(y; \theta)\)</span> the hessian is obviously <span class="math inline">\(h(\theta, y) = - N / \theta ^ 2\)</span> and equals its expected value, as it doesn’t depend on <span class="math inline">\(y\)</span>. Therefore, <span class="math inline">\(\iota(\theta) = N / \theta ^ 2\)</span>. Computing the variance of the gradient, we get:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mbox{V}\left(g(\theta)\right) &amp;=&amp; \mbox{E}\left(\sum_n \gamma(y_n; \theta) ^ 2 \right)
=\sum_n \mbox{E}(\gamma(y_n; \theta) ^ 2)\\
&amp;=&amp; \sum_n \mbox{E}\left((y -  1
/ \theta)^2\right) = N \mbox{V}(y) = N / \theta ^ 2
\end{array}
\]</span></p>
<p>because the expected value and the variance of <span class="math inline">\(y\)</span> are respectively equal to <span class="math inline">\(1 / \theta\)</span> and <span class="math inline">\(1 / \theta ^ 2\)</span> for an exponential distribution. Therefore <span class="math inline">\(\iota(\theta) = N / \theta ^ 2\)</span> and the information-based estimator of the information for <span class="math inline">\(\theta = \hat{\theta}\)</span> is, as <span class="math inline">\(\hat{\theta} = 1 / \bar{y}\)</span>, <span class="math inline">\(\iota(\hat{\theta}) = N \bar{y} ^ 2\)</span>. The same result obviously applies to the hessian-based approximation of the information, which is: <span class="math inline">\(\iota_h(\hat{\theta}) = \frac{N}{\hat{\theta} ^ 2} = N\bar{y} ^ 2\)</span>. Considering now the gradient-based estimate of the information, we have:</p>
<p><span class="math display">\[
\iota_g(\theta, y) = \sum_{n=1} ^ N (1 / \theta - y_n) ^ 2=\sum_{n=1} ^ N (y_n - 1 / \theta) ^ 2
\]</span></p>
<p>as <span class="math inline">\(\hat{\theta} = 1 / \bar{y}\)</span>, evaluated for the ML estimator, we have <span class="math inline">\(\iota_g(\hat{\theta}, y) = N \hat{\sigma}_y ^ 2\)</span>. Finally, the sandwich estimator of the variance of <span class="math inline">\(\hat{\theta}\)</span> is:</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\theta}s}^2 = \frac{1}{N}\frac{\hat{\sigma}_y ^ 2}{(\bar{y} ^ 2) ^ 2}=\frac{1}{N}\frac{\hat{\sigma}_y ^ 2}{\bar{y} ^ 4}
\]</span></p>
<p>The four estimators of the variance for the exponential distribution are then:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rclrcl}
\hat{\sigma}_{\hat{\theta}i} ^ 2 &amp;=&amp; \iota(\hat{\theta}) ^ {-1} &amp;=&amp;
1 / (N \bar{y} ^ 2) \\
\hat{\sigma}_{\hat{\theta}g} ^ 2 &amp;=&amp; \iota_g(\hat{\theta}) ^ {-1} &amp;=&amp;
1 / (N \hat{\sigma}_y^ 2) \\
\hat{\sigma}_{\hat{\theta}h} ^ 2 &amp;=&amp; \iota_h(\hat{\theta}) ^ {-1} &amp;=&amp;
1 / (N \bar{y} ^ 2) \\
\hat{\sigma}_{\hat{\theta}s} ^ 2 &amp;&amp; &amp;=&amp; \hat{\sigma}_y ^ 2/ (N \bar{y} ^ 4)
\end{array}
\right.
\]</span></p>
<p>Computing this four estimations of the variance of <span class="math inline">\(\hat{\theta}\)</span>, we get for the <code>oil</code> variable: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">oil</span><span class="op">$</span><span class="va">dur</span> ; <span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">mean_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> ; <span class="va">var_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mean_y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>info <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="va">mean_y</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>, gradient <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="va">var_y</span><span class="op">)</span>,</span>
<span>  hessian <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="va">mean_y</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>  sandwich <span class="op">=</span> <span class="va">var_y</span> <span class="op">/</span> <span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="va">mean_y</span> <span class="op">^</span> <span class="fl">4</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="va">sqrt</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">##     info gradient  hessian sandwich </span></span>
<span><span class="co">##   0.0022   0.0025   0.0022   0.0019</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section></section><section id="sec-ml_general" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="sec-ml_general">
<span class="header-section-number">5.2</span> ML estimation in the general case</h2>
<p>Compared to the simple case analyzed in the previous section, we consider in this section two extensions:</p>
<ul>
<li>
<span class="math inline">\(\theta\)</span> is now a vector of unknown parameters that we seek to estimate, which means that the gradient is a vector and the hessian is a matrix,</li>
<li>the density for observation <span class="math inline">\(n\)</span> not only depends on the value of the response <span class="math inline">\(y_n\)</span>, but also on the value of a vector of covariates <span class="math inline">\(x_n\)</span>.</li>
</ul>
<section id="computation-and-properties-of-the-ml-estimator" class="level3"><h3 class="anchored" data-anchor-id="computation-and-properties-of-the-ml-estimator">Computation and properties of the ML estimator</h3>
<p>The density (or the probability mass) for observation <span class="math inline">\(n\)</span> is now: <span class="math inline">\(\phi(y_n ; \theta, x_n) = \phi_n(y_n ; \theta)\)</span>; therefore, written as a function of <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> only, the density is now indexed by <span class="math inline">\(n\)</span>, as it is a function of <span class="math inline">\(x_n\)</span>. Denoting as previously <span class="math inline">\(\lambda_n(y_n; \theta) = \ln \phi_n(y_n; \theta)\)</span>, <span class="math inline">\(\gamma_n = \frac{\partial \lambda_n}{\partial \theta}\)</span> and <span class="math inline">\(\Psi_n = \frac{\partial ^2 \lambda_n}{\partial \theta \partial \theta^\top}\)</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, the log-likelihood is:</p>
<p><span class="math display">\[
\ln L(\theta, y, X) = \sum_{n=1} ^ N \ln \phi_n(y_n; \theta) =
\sum_{n=1} ^ N\lambda_n(y_n; \theta)
\]</span></p>
<p>The gradient and the hessian are:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
g(\theta, y, X) &amp;=&amp; \sum_{n=1} ^ N \gamma_n(y_n; \theta)\\
H(\theta, y, X) &amp;=&amp; \sum_{n=1} ^ N \Psi_n(y_n; \theta)
\end{array}
\right.
\]</span></p>
<p>The variance of the score is the <strong>information matrix</strong>, denoted by <span class="math inline">\(I(\theta, X)\)</span> and, by virtue of the <strong>information matrix equality</strong> demonstrated previously in the scalar case, it is equal to the opposite of the expected value of the hessian:</p>
<p><span class="math display">\[
I(\theta, X) = \mbox{V}(g(\theta, y, X)) = - \mbox{E} (H(\theta, y, X))
\]</span></p>
<p>Note that now, each individual contribution to the gradient and to the hessian depends on <span class="math inline">\(x_n\)</span>; therefore, their variance (for the gradient) and their expectation (for the hessian) are not constant as previously. In terms of the individual observations, the information matrix equality states that: </p>
<p><span class="math display">\[
\mbox{I}(\theta; X) = \sum_{n=1} ^ N
\mbox{E}\left(\gamma_n(y_n; \theta)\gamma_n(y_n; \theta)^\top\right) = -
\sum_{n=1} ^ N\mbox{E}\left(\Psi_n(y_n; \theta)\right)
\]</span></p>
<p>Define the asymptotic information and the asymptotic hessian as: </p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\mathbf{\mathcal{I}} &amp;=&amp; \frac{1}{N}\lim_{n\rightarrow +
\infty}\sum_{n=1} ^ N \gamma_n(y_n; \theta)\gamma_n(y_n; \theta)^\top \\
\mathbf{\mathcal{H}} &amp;=&amp; \frac{1}{N}\lim_{n\rightarrow +
\infty}\sum_{n=1} ^ N \Psi_n(y_n; \theta)
\end{array}
\right.
\]</span></p>
<p>The information matrix equality implies that: <span class="math inline">\(\mathbf{\mathcal{I}} = - \mathbf{\mathcal{H}}\)</span>. At the ML estimate, the gradient is 0: <span class="math inline">\(g(\hat{\theta}, y, X) = 0\)</span>. Using a first-order Taylor expansion around the true value <span class="math inline">\(\theta_0\)</span>, we have:</p>
<p><span class="math display">\[
g(\hat{\theta}, y, X) = g(\theta_0, y, X) +
H(\bar{\theta}, y, X) (\hat{\theta} - \theta_0) = 0
\]</span></p>
<p>The equivalent of <span class="math inline">\(\bar{\theta}\)</span> lying in the <span class="math inline">\(\theta_0-\hat{\theta}\)</span> interval for the scalar case (see <a href="#eq-exact_taylor_expansion">Equation&nbsp;<span>5.11</span></a>) is that <span class="math inline">\(\| \bar{\theta} - \theta_0\| \leq \| \hat{\theta}-\theta_0\|\)</span>. Solving this equation for <span class="math inline">\(\hat{\theta}-\theta_0\)</span>, we get, multiplying by <span class="math inline">\(\sqrt{N}\)</span>:</p>
<p><span class="math display">\[
\sqrt{N}(\hat{\theta}-\theta_0) = \left(- \frac{H(\bar{\theta}, y, X)}{N}\right)^{-1}\frac{g(\theta_0, y, X)}{\sqrt{N}}
\]</span></p>
<p>The probability limit of the term in brackets is <span class="math inline">\(-\mathbf{\mathcal{H}}\)</span> (as <span class="math inline">\(\bar{\theta}\)</span> converges to <span class="math inline">\(\theta_0\)</span>) and therefore:</p>
<p><span id="eq-nondeg_theta"><span class="math display">\[
\sqrt{N}(\hat{\theta}-\theta_0) \overset{a}{=} \left(- \mathcal{H}\right)^{-1}\frac{g(\theta_0, y, X)}{\sqrt{N}}
\tag{5.19}\]</span></span></p>
<p></p>
<p>where <span class="math inline">\(w\overset{a}{=}z\)</span> means that <span class="math inline">\(w\)</span> is asymptotically equal to <span class="math inline">\(z\)</span>, i.e., it tends to the same limit in probability <span class="citation" data-cites="DAVI:MACK:04">(see <a href="#ref-DAVI:MACK:04" role="doc-biblioref">Davidson and MacKinnon 2004, 205</a>)</span>.</p>
<p>The asymptotic variance of the second term is:</p>
<p><span class="math display">\[
\mbox{V}\left(\lim_{n \rightarrow \infty}\frac{g(\theta_0; y,
X}{\sqrt{N}}\right) \overset{a}{=} \lim_{n\rightarrow \infty} \frac{1}{N}\sum_{n =
1} ^ N \gamma_n(y_n; \theta_0)\gamma_n(y_n; \theta_0)^\top = \mathbf{\mathcal{I}}
\]</span> Therefore, the asymptotic variance of <span class="math inline">\(\sqrt{N}(\hat{\theta}-\theta_0)\)</span> is <span class="math inline">\(\mathbf{\mathcal{H}} ^ {-1} \mathbf{\mathcal{I}} \mathbf{\mathcal{H}} ^ {-1}\)</span>, which reduces to, applying the information matrix equality result, <span class="math inline">\(\mathbf{\mathcal{I}} ^ {-1}\)</span>. Applying the central-limit theorem, we finally get:</p>
<p><span class="math display">\[
\sqrt{N}(\hat{\theta}-\theta_0) \overset{p}{\rightarrow} \mathcal{N}(0, \mathcal{I} ^ {-1})
\]</span></p>
<p><span class="math display">\[
\hat{\theta} \overset{a}{\sim} \mathcal{N}(\theta_0,
\mathbf{\mathcal{I}} ^ {-1} / N)
\]</span></p>
<p>The asymptotic variance can be estimated using the information evaluated at <span class="math inline">\(\hat{\theta}\)</span> if the variance of the gradient or the expectation of the hessian can be computed:</p>
<p><span class="math display">\[
\hat{\mbox{V}}_I(\hat{\theta}) = \left(\sum_{n=1} ^ N
\mbox{E}\left(\gamma_n(y_n; \hat{\theta})\gamma_n(y_n; \hat{\theta})^\top\right)\right) ^ {-1} = \left(-
\sum_{n=1} ^ N\mbox{E}\left(\Psi_n(y_n; \hat{\theta})\right)\right) ^ {-1}
\]</span> </p>
<p>Two other possible estimators are obtained by evaluating the two previous expressions without the expectation. The gradient-based estimator, also called the <strong>outer product of the gradient</strong> or the <strong>BHHH</strong><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> estimator is:</p>
<p><span class="math display">\[
\hat{\mbox{V}}_g(\hat{\theta}) = \left(\sum_{n=1} ^ N
\left(\gamma_n(y_n; \hat{\theta})\gamma_n(y_n; \hat{\theta})^\top\right)\right) ^ {-1}
\]</span></p>
<p>and the hessian-based estimator is:</p>
<p><span class="math display">\[
\hat{\mbox{V}}_H(\hat{\theta}) = \left(-
\sum_{n=1} ^ N\Psi_n(y_n; \hat{\theta})\right) ^ {-1} =
\left(- H(\hat{\theta}, y, X)\right) ^ {-1}
\]</span> </p>
<p>Finally, the sandwich estimator is based on the expression of the asymptotic covariance of <span class="math inline">\(\hat{\theta}\)</span> before applying the information matrix equality theorem. Then:</p>
<p><span class="math display">\[
\hat{\mbox{V}}_s(\hat{\theta}) =
\left(- H(\hat{\theta}, y, X)\right) ^ {-1}
\left(\sum_{n=1} ^ N \left(\gamma_n(y_n; \hat{\theta})\gamma_n(y_n; \hat{\theta})^\top\right)\right)
\left(- H(\hat{\theta}, y, X)\right) ^ {-1}
\]</span> This estimator actually looks like a sandwich, the “meat” (the estimation of the variance of the gradient) being surrounded by two slices of “bread” (the inverse of the opposite of the hessian).</p>
</section><section id="sec-ml_est_expon" class="level3"><h3 class="anchored" data-anchor-id="sec-ml_est_expon">Computation of the estimators for the exponential distribution</h3>
<p> We have seen in <a href="#sec-expon_distr_1par"><span>Section&nbsp;5.1.2</span></a> that we can get an explicit solution for the maximization of the likelihood of the one parameter exponential distribution. Once covariates are introduced, there are <span class="math inline">\(K+1\)</span> parameters to estimate and there is no longer an explicit solution. Then, a numerical optimization algorithm should be used. We’ll present in this section the simplest algorithm, called the <strong>Newton-Raphson</strong> algorithm, using the <code>oil</code> data set with two covariates: </p>
<ul>
<li>
<code>p98</code> is the adaptive expectations for the real after-tax oil prices formed at the time of the approval,</li>
<li>
<code>varp98</code> is the volatility of the adaptive expectations for the real after-tax oil prices.</li>
</ul>
<p>For the exponential model, remember that <span class="math inline">\(\mbox{E}(y) = 1 / \theta\)</span>; <span class="math inline">\(\theta\)</span> should therefore be positive. The way the linear combination of the covariates <span class="math inline">\(\gamma^\top z_n\)</span> is related to the parameter of the distribution <span class="math inline">\(\theta_n\)</span> is called the <strong>link</strong>. It is customary to define <span class="math inline">\(\theta_n = e ^ {- \gamma ^ \top z_n}\)</span> (so that <span class="math inline">\(\ln \mbox{E}(y \mid x_n) = - \ln \theta_n = \gamma ^ \top z_n\)</span>).</p>
<p>Then:</p>
<p><span id="eq-exp_dist_lgh"><span class="math display">\[
\left\{
\begin{array}{rcl}
\ln L &amp;=&amp; - \sum_{n=1} ^ N \left(\gamma ^ \top z_n + e ^ {- \gamma ^  \top
z_n} y_n\right) \\
\frac{\partial \ln L}{\partial \gamma} &amp;=&amp; - \sum_{n=1} ^ N\left(1 - e ^ {-\gamma ^ \top
z_n}y_n\right)z_n \\
\frac{\partial ^ 2 \ln L}{\partial \gamma \partial \gamma ^ \top} &amp;=&amp;
-\sum_{n=1} ^ N e ^ {-\gamma ^ \top
z_n}y_nz_nz_n^\top
\end{array}
\right.
\tag{5.20}\]</span></span></p>
<p>Starting from an initial vector of parameters <span class="math inline">\(\gamma_i\)</span>, we use a first-order Taylor expansion of the gradient for <span class="math inline">\(\gamma_{i+1}\)</span> “close to” <span class="math inline">\(\gamma_i\)</span>:</p>
<p><span class="math display">\[
g(\gamma_{i+1}) \approx g(\gamma_i) + H(\gamma_i) (\gamma_{i+1} - \gamma_i)
\]</span></p>
<p>Solving the first order-conditions for a maximum, we should have <span class="math inline">\(g(\gamma_i) + H(\gamma_i) (\gamma_{i+1} - \gamma_i) = 0\)</span>, which leads to:</p>
<p><span id="eq-updating_rule"><span class="math display">\[
\gamma_{i+1} = \gamma_i - H(\gamma_i) ^ {-1} g(\gamma_i)
\tag{5.21}\]</span></span></p>
<p>Except if the gradient is a linear function of <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\gamma_{i+1}\)</span> is not the maximum, but it is closer to the maximum than <span class="math inline">\(\gamma_i\)</span> and successive iterations enable to reach a value of <span class="math inline">\(\gamma\)</span> as close as desired to the maximum. We first begin by describing the model we want to estimate using a formula and extracting the relevant components of the model, namely the vector of response and the matrix of covariates. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">form</span> <span class="op">&lt;-</span> <span class="va">dur</span> <span class="op">~</span> <span class="va">p98</span> <span class="op">+</span> <span class="va">varp98</span></span>
<span><span class="va">mf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">oil</span><span class="op">)</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">mf</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.extract.html">model.response</a></span><span class="op">(</span><span class="va">mf</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then define functions for the log-likelihood, the gradient and the hessian, as a function of the vector of parameters <span class="math inline">\(\gamma\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="va">Z</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">gamma</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">theta</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">theta</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">G</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">y</span> <span class="op">*</span> <span class="fu">theta</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">Z</span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="fu">G</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span>, <span class="fl">2</span>, <span class="va">sum</span><span class="op">)</span></span>
<span><span class="va">H</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu">theta</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="va">Z</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Starting from an initial vector of coefficients, we use the preceding formula to update the vector of coefficients. The choice of good starting values is crucial when the log-likelihood function is not concave. This is not the case here, but, anyway, the choice of good starting values limits the number of iterations. In our example, a good candidate is the ordinary least squares estimator, with <span class="math inline">\(\ln y\)</span> as the response: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gamma_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">dur</span><span class="op">)</span> <span class="op">~</span> <span class="va">p98</span> <span class="op">+</span> <span class="va">varp98</span>, <span class="va">oil</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">gamma_0</span></span>
<span><span class="co">## (Intercept)         p98      varp98 </span></span>
<span><span class="co">##      1.2007      0.9480      0.2262</span></span>
<span><span class="fu">g</span><span class="op">(</span><span class="va">gamma_0</span><span class="op">)</span></span>
<span><span class="co">## (Intercept)         p98      varp98 </span></span>
<span><span class="co">##       15.40       28.62       56.42</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then update <span class="math inline">\(\gamma\)</span> using <a href="#eq-updating_rule">Equation&nbsp;<span>5.21</span></a>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gamma_1</span> <span class="op">&lt;-</span> <span class="va">gamma_0</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu">H</span><span class="op">(</span><span class="va">gamma_0</span><span class="op">)</span>, <span class="fu">g</span><span class="op">(</span><span class="va">gamma_0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">gamma_1</span></span>
<span><span class="co">## (Intercept)         p98      varp98 </span></span>
<span><span class="co">##      1.4077      0.8615      0.2785</span></span>
<span><span class="fu">g</span><span class="op">(</span><span class="va">gamma_1</span><span class="op">)</span></span>
<span><span class="co">## (Intercept)         p98      varp98 </span></span>
<span><span class="co">##       1.687       3.198       6.646</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can see that we obtain an updated vector of coefficients which seems closer to the maximum than the initial one, as the elements of the gradient are much smaller than previously. The gradient being still quite different from 0, it is worth iterating again. We’ll stop the iterations when a scalar value obtained from the gradient is less than an arbitrary small real value. As a simple criterion, we consider the mean of the squares of the elements of the score, and we iterate as long as this scalar is greater than <span class="math inline">\(10^{-07}\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gamma_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">dur</span><span class="op">)</span> <span class="op">~</span> <span class="va">p98</span> <span class="op">+</span> <span class="va">varp98</span>, <span class="va">oil</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">i</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">gamma</span> <span class="op">&lt;-</span> <span class="va">gamma_0</span></span>
<span><span class="va">crit</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">while</span> <span class="op">(</span><span class="va">crit</span> <span class="op">&gt;</span> <span class="fl">1E-07</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">i</span> <span class="op">&lt;-</span> <span class="va">i</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>    <span class="va">gamma</span> <span class="op">&lt;-</span> <span class="va">gamma</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu">H</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span>, <span class="fu">g</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">crit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu">g</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"iteration"</span>, <span class="va">i</span>, <span class="st">"crit = "</span>, <span class="va">crit</span>, <span class="st">"\n"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>iteration 1 crit =  19.0801422479605 
iteration 2 crit =  0.00824133223009916 
iteration 3 crit =  2.88870943821704e-09 </code></pre>
</div>
</div>
<p>Only three iterations were necessary to reach the maximum (as defined by our criteria).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gamma</span></span>
<span><span class="co">## (Intercept)         p98      varp98 </span></span>
<span><span class="co">##      1.4405      0.8303      0.2952</span></span>
<span><span class="fu">g</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span></span>
<span><span class="co">## (Intercept)         p98      varp98 </span></span>
<span><span class="co">##   1.809e-05   3.445e-05   8.457e-05</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
<p>We then use <a href="#eq-exp_dist_lgh">Equation&nbsp;<span>5.20</span></a> and the functions <code>H</code> and <code>G</code> defined previously to compute the three estimations of the information matrix and the four estimators of the covariance matrix of the estimator: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Info_g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu">G</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Info_H</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fu">H</span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span></span>
<span><span class="va">Info</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span></span>
<span><span class="va">V_g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Info_g</span><span class="op">)</span></span>
<span><span class="va">V_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Info</span><span class="op">)</span></span>
<span><span class="va">V_h</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Info_H</span><span class="op">)</span></span>
<span><span class="va">V_sand</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Info_H</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Info_g</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Info_H</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then compare the resulting estimated standard errors of the estimator. To obtain a compact output, we use the <code>sapply</code> function, which is a specialized version of <code>lapply</code>. <code>lapply</code> takes as argument a list and a function, and the outcome is a list containing the result of applying the function to each element of the list. <code>sapply</code>, when possible, returns a matrix: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">V_i</span>, <span class="va">V_h</span>, <span class="va">V_g</span>, <span class="va">V_sand</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">stder</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]   [,2]   [,3]    [,4]
(Intercept) 0.5541 0.5509 0.9621 0.32300
p98         0.4858 0.4935 0.8793 0.29246
varp98      0.1584 0.1568 0.2739 0.09506</code></pre>
</div>
</div>
<p>Not that the gradient-based estimate gives fairly different results, compared to the other estimators. This is often the case, this estimator being known to perform poorly in small samples. </p>
</section><section id="sec-linear-gaussian" class="level3"><h3 class="anchored" data-anchor-id="sec-linear-gaussian">Linear gaussian model</h3>
<p> The first part of the book was devoted to the OLS estimator, suitable in situations where <span class="math inline">\(\mbox{E}(y_n \mid x_n) = \alpha + \beta^\top x_n\)</span>, with <span class="math inline">\(\mbox{V}(\epsilon_n \mid x_n) = \sigma_\epsilon ^ 2\)</span> and <span class="math inline">\(\mbox{cov}(\epsilon_n,\epsilon_m) = 0\)</span>. This model can also be estimated by maximum likelihood by specifying the conditional density of <span class="math inline">\(y\)</span>, and not only its conditional expectation. Assuming that <span class="math inline">\(y\)</span> follows a normal distribution, we have:</p>
<p><span class="math display">\[
y_n \mid x_n \sim \mathcal{N}(\alpha + \beta ^ \top x_n, \sigma)
\]</span> and the conditional density of <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[
f(y_n \mid x_n) = \frac{1}{\sqrt{2\pi}\sigma}e ^ {-\frac{1}{2}\left(\frac{y_n - \gamma ^ \top z_n}{\sigma}\right)^2} = \frac{1}{\sigma}\phi\left(\frac{y_n - \gamma ^ \top z_n}{\sigma}\right)
\]</span> The log-likelihood is then:</p>
<p><span id="eq-loglik_normal"><span class="math display">\[
\ln L = -\frac{N}{2}\ln 2\pi - N \ln \sigma - \frac{1}{2\sigma ^ 2}\sum_{n=1}^N(y_n - \gamma ^ \top z_n)^2 =
-\frac{N}{2}\ln 2\pi - \frac{N}{2} \ln \sigma ^ 2 - \frac{1}{2\sigma ^ 2} \epsilon ^ \top \epsilon
\tag{5.22}\]</span></span></p>
<p>The gradient is:</p>
<p><span id="eq-grad_ml_normal"><span class="math display">\[
g =
\left(
\begin{array}{c}
\frac{1}{\sigma ^ 2} \sum_n (y_n - \gamma ^ \top z_n)z_n \\
-\frac{N}{\sigma} + \frac{1}{\sigma ^ 3} \sum_n (y_n - \gamma ^ \top z_n) ^ 2
\end{array}
\right)
=
\left(
\begin{array}{c}
\frac{1}{\sigma ^ 2} Z ^ \top \epsilon \\
-\frac{N}{\sigma} + \frac{1}{\sigma ^ 3} \epsilon ^ \top \epsilon
\end{array}
\right)
\tag{5.23}\]</span></span></p>
<p>and the ML estimator is obtained by computing the vector of parameters <span class="math inline">\((\gamma ^ \top, \sigma)\)</span> that set this system of <span class="math inline">\(K+2\)</span> equations to 0. The hessian is:</p>
<p><span id="eq-hessian-normal"><span class="math display">\[
H =
\left(
\begin{array}{cc}
-\frac{1}{\sigma ^ 2} Z ^ \top Z &amp; 0 \\
0 &amp;  \frac{N}{\sigma ^ 2} - \frac{3}{2\sigma ^ 4}\epsilon ^ \top \epsilon
\end{array}
\right)
\tag{5.24}\]</span></span></p>
<p>Note that it is block diagonal, as the cross-derivatives are <span class="math inline">\(- 2 / \sigma ^ 3 Z ^ \top \epsilon\)</span>, which is 0 for the ML estimator. Solving the last line in <a href="#eq-grad_ml_normal">Equation&nbsp;<span>5.23</span></a> for <span class="math inline">\(\sigma\)</span>, we get:</p>
<p><span id="eq-hats2_ml"><span class="math display">\[
\sigma ^ 2 = \frac{\epsilon ^ \top \epsilon}{N}
\tag{5.25}\]</span></span></p>
<p>Replacing <span class="math inline">\(\sigma ^ 2\)</span> by this expression in <a href="#eq-loglik_normal">Equation&nbsp;<span>5.22</span></a>, we get the <strong>concentrated</strong> log-likelihood: </p>
<p><span id="eq-conc_loglik"><span class="math display">\[
\ln L =- \frac{N}{2}(1 - \ln N + \ln 2 \pi) - \frac{N}{2} \ln \epsilon ^ \top \epsilon = C - \frac{N}{2} \ln \epsilon ^ \top \epsilon
\tag{5.26}\]</span></span></p>
<p>which makes clear that maximizing the log-likelihood is equivalent to minimizing the residual sum of squares, and, therefore, that the ML and the OLS estimators of <span class="math inline">\(\gamma\)</span> are the same. Once <span class="math inline">\(\hat{\gamma}\)</span> has been computed, <span class="math inline">\(\hat{\sigma} ^ 2\)</span> can be estimated using <a href="#eq-hats2_ml">Equation&nbsp;<span>5.25</span></a>. Note that the residual sum of squares divided by the number of observations and not by the number of degrees of freedom, contrary to the OLS estimator. </p>
</section><section id="transformation-of-the-response" class="level3"><h3 class="anchored" data-anchor-id="transformation-of-the-response">Transformation of the response</h3>
<p>The ML estimator is based on the density (or probability mass) function of the response, given a set of covariates. Sometimes, it is more interesting to consider the density of a parametric transformation of the response. To illustrate this kind of model, we’ll consider the estimation of production functions, using the <code>apples</code> data set, already described in <a href="multiple_regression.html#sec-system_equation"><span>Section&nbsp;3.7</span></a>. We consider models of the form:</p>
<p><span class="math display">\[
w_n = v(y_n, \lambda) \sim \mathcal{N}(\mu_n, \sigma) \mbox{ with } \mu_n = \alpha + \beta ^ \top x_n
\]</span> The parametric transformation of <span class="math inline">\(y_n\)</span> may depends on a set of unknown parameters <span class="math inline">\(\lambda\)</span>, and it follows a normal distribution with an expectation that is a linear function of some covariates and a constant variance. As the density of <span class="math inline">\(v(y_n, \gamma)\)</span> is normal, the one for <span class="math inline">\(y_n\)</span> can be obtained using the following formula:</p>
<p><span class="math display">\[
f(y_n) = \phi(v(y_n, \gamma)) \times \left|\frac{d v}{d y} \right|
\]</span> where the last term is called the Jacobian of the transformation of <span class="math inline">\(w\)</span> on <span class="math inline">\(y\)</span>. Consider the simple case where <span class="math inline">\(v(y_n) = \ln y_n\)</span>. Then, <span class="math inline">\(v\)</span> doesn’t contain any unknown parameter, the Jacobian of the transformation is <span class="math inline">\(1/y\)</span> and the density of <span class="math inline">\(y_n\)</span>: </p>
<p><span class="math display">\[
f(y_n) = \frac{1}{y}\frac{1}{\sigma}\phi\left(\frac{\ln y - \mu_n}{\sigma}\right)
\]</span> is simply the log-normal density, which implies the following log-likelihood function:</p>
<p><span id="eq-log_lik_lognorm"><span class="math display">\[
\ln L(\gamma) = -\frac{N}{2} \ln 2\pi - N \ln \sigma - \sum_{n=1} ^ N \ln y_n + \frac{1}{2\sigma ^ 2}\sum_{n = 1} ^ {N}(\ln y_n - \gamma ^ \top z_n)^ 2
\tag{5.27}\]</span></span></p>
<p>Maximizing <a href="#eq-log_lik_lognorm">Equation&nbsp;<span>5.27</span></a> is obviously equivalent to minimizing <span class="math inline">\(\sum_{n = 1} ^ {N}(\ln y_n - \gamma ^ \top z_n)^ 2\)</span> which is the residual sum of squares of a regression with <span class="math inline">\(\ln y_n\)</span> as the response. Therefore the ML estimation of <span class="math inline">\(\gamma\)</span> can be performed using OLS. </p>
<p>We consider only one year (1986) of production for the <code>apples</code> data set, we construct a unique output variable (<code>y</code>), we rename for convenience the three factors as <code>k</code>, <code>l</code> and <code>m</code> (respectively for capital, labor and materials) and, for a reason that will be clear later, we divide all the variables by their sample mean: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">aps</span> <span class="op">&lt;-</span> <span class="va">apples</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">year</span> <span class="op">==</span> <span class="fl">1986</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">transmute</span><span class="op">(</span>y <span class="op">=</span> <span class="va">apples</span> <span class="op">+</span> <span class="va">otherprod</span>, y <span class="op">=</span> <span class="va">y</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>,</span>
<span>              k <span class="op">=</span> <span class="va">capital</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">capital</span><span class="op">)</span>,</span>
<span>              l <span class="op">=</span> <span class="va">labor</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">labor</span><span class="op">)</span>,</span>
<span>              m <span class="op">=</span> <span class="va">materials</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">materials</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first consider the Cobb-Douglas production function which is linear in logs. Denoting <span class="math inline">\(j = 1 \ldots J\)</span> the inputs, we get:</p>
<p><span id="eq-cobb_douglas_production"><span class="math display">\[
\ln y_n = \alpha + \sum_{j=1}^J \beta_j \ln q_j + \epsilon_n
\tag{5.28}\]</span></span></p>
<p>We then fit <a href="#eq-cobb_douglas_production">Equation&nbsp;<span>5.28</span></a> using <code>lm</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cobb_douglas</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span>, <span class="va">aps</span><span class="op">)</span></span>
<span><span class="va">cobb_douglas</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">## (Intercept)      log(k)      log(l)      log(m) </span></span>
<span><span class="co">##     -0.1331      0.2596      0.3537      0.5111</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The problem of using <code>lm</code> is that, although the estimates are the ML estimates, the response is <span class="math inline">\(\ln y\)</span> and not <span class="math inline">\(y\)</span>. Therefore, the log likelihood reported by <code>lm</code> is computed using the density of <span class="math inline">\(\ln y\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">cobb_douglas</span><span class="op">)</span></span>
<span><span class="co">## 'log Lik.' -120.2 (df=5)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and is incorrect as the term <span class="math inline">\(- \sum_{n=1} ^ N \ln y_n\)</span> in <a href="#eq-log_lik_lognorm">Equation&nbsp;<span>5.27</span></a> is missing. Adding this term to the log-likelihood returned by the <code>logLik</code> function, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">cobb_douglas</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">aps</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## [1] -62</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/pkg/micsr/man/loglm.html">micsr::loglm</a></code> function estimates models for which the response is in logarithm. The estimation of the parameters is performed using <code>lm</code>, but the result is a <code>micsr</code> object, from which the correct log-likelihood, gradient and hessian can be extracted: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cd_loglm</span> <span class="op">&lt;-</span> <span class="fu">loglm</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span>, <span class="va">aps</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">cd_loglm</span><span class="op">)</span></span>
<span><span class="co">## 'log Lik.' -62 (df=5)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The scale elasticity, which measures the relative growth of output for a proportional increase of all the inputs is <span class="math inline">\(\sum_{j=1} ^ J \beta_j\)</span> and therefore constant returns to scale imply that <span class="math inline">\(\sum_{j=1} ^ J \beta_j = 1\)</span>, or <span class="math inline">\(\beta_J = 1 - \sum_{j=1} ^ {J-1} \beta_j\)</span>. Replacing in <a href="#eq-cobb_douglas_production">Equation&nbsp;<span>5.28</span></a>, we get:</p>
<p><span id="eq-cobb_douglas_production_reparam"><span class="math display">\[
\ln y_n = \alpha + \sum_{j=1}^{J-1} \beta_j \ln q_j / q_J +
\left(\sum_{j=1}^{J} \beta_j - 1\right) \ln q_J +
\ln q_J + \epsilon_n
\tag{5.29}\]</span></span></p>
<p>Therefore, regressing <span class="math inline">\(\ln y_n\)</span> on <span class="math inline">\(\ln q_j / q_J, \forall j = 1\ldots J-1\)</span> and <span class="math inline">\(\ln q_J\)</span>, the hypothesis of constant returns to scale is <span class="math inline">\(\beta_J^* = \sum_{j=1}^{J} \beta_j - 1 = 0\)</span> and the constant returns to scale are imposed if <span class="math inline">\(\ln q_J\)</span> is removed from the regression. Note the presence of a constant term (<span class="math inline">\(\ln q_J\)</span>) on the right side of the equation. This is called an offset and can be introduced in a formula with the <code>offset(w)</code> syntax: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cd_repar</span> <span class="op">&lt;-</span> <span class="fu">loglm</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="op">+</span> </span>
<span>                    <span class="fu"><a href="https://rdrr.io/r/stats/offset.html">offset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span>, <span class="va">aps</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cd_repar</span><span class="op">)</span></span>
<span><span class="co">## (Intercept)    log(k/m)    log(l/m)      log(m)       sigma </span></span>
<span><span class="co">##     -0.1331      0.2596      0.3537      0.1244      0.5896</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">cd_repar</span><span class="op">)</span></span>
<span><span class="co">## 'log Lik.' -62 (df=5)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">

</div>
<p>The log-likelihood is obviously the same as previously and <span class="math inline">\(\beta_J ^ * = 0.12\)</span>, which implies a scale elasticity equal to 1.12. </p>
<p><span class="citation" data-cites="ZELL:REVA:69">Zellner and Revankar (<a href="#ref-ZELL:REVA:69" role="doc-biblioref">1969</a>)</span> proposed a generalization of the Cobb-Douglas production function of the form:</p>
<p><span class="math display">\[
\ln y + \lambda y \sim \mathcal{N}(\mu, \sigma)
\]</span></p>
<p>where <span class="math inline">\(\mu = \alpha + \sum_{j=1}^ J \beta_j \ln q_j\)</span>. The scale elasticity is:</p>
<p><span id="eq-elast"><span class="math display">\[
\psi = \frac{\sum_{j=1} ^ J \beta_j}{1 + \lambda y}
\tag{5.30}\]</span></span></p>
<p>If <span class="math inline">\(\lambda = 0\)</span>, this function reduces to the Cobb-Douglas production function with normal errors. For <span class="math inline">\(\lambda &gt; 0\)</span>, the scale elasticity is equal to <span class="math inline">\(\sum_{j=1} ^ J \beta_j\)</span> for <span class="math inline">\(y = 0\)</span> and tends to 0 as <span class="math inline">\(y\)</span> tends to <span class="math inline">\(+ \infty\)</span>. Therefore, if <span class="math inline">\(\sum_{j=1} ^ J \beta_j &gt; 1\)</span>, returns to scale are increasing for a low level of production, get constant for a level of production that is equal to <span class="math inline">\((\sum_{j=1} ^ J \beta_j - 1) / \lambda\)</span> and decreasing above this level of production. Denoting <span class="math inline">\(\epsilon_n\)</span> the difference between <span class="math inline">\(\ln y + \lambda y\)</span> and its conditional expectation, the model can be rewritten as:</p>
<p><span class="math display">\[
\ln y_n + \lambda y_n = \alpha + \sum_{j=1} ^ J \beta_j \ln q_{nj} + \epsilon_n
\]</span></p>
<p>The hypothesis of constant scale elasticity is simply <span class="math inline">\(\lambda = 0\)</span>. The hypothesis of constant return to scale adds the condition: <span class="math inline">\(\sum_j \beta_j = 1\)</span>. It can be easily tested using the same reparametrization of the model as in <a href="#eq-cobb_douglas_production_reparam">Equation&nbsp;<span>5.29</span></a>: <span class="math display">\[
\ln y_n + \lambda y_n = \alpha +
\sum_{j=1} ^ {J-1} \beta_j \ln q_{nj}^* +
\beta_J^* \ln q_{nJ} + \ln q_J +
\epsilon_n
\]</span></p>
<p>where <span class="math inline">\(q_{nj}^* = q_{nj} / q_{nJ}\)</span> and <span class="math inline">\(\beta_J^* = \sum_{j=1}^J \beta_j - 1\)</span>. The hypothesis of constant returns to scale is the joint hypothesis that <span class="math inline">\(\lambda = 0\)</span> and that the coefficient of <span class="math inline">\(\ln q_{nJ}\)</span> in this reparametrized version of the model is also 0. The Jacobian is <span class="math inline">\(\frac{1}{y} + \lambda = \frac{1 + \lambda y}{y}\)</span>, which leads to the following density:</p>
<p><span class="math display">\[
f(y_n;\theta,\beta) = \frac{1}{\sigma}\frac{1 + \lambda y_n}{y_n}
\phi\left(\frac{\ln y_n + \lambda y_n - \mu_n}{\sigma}\right)
\]</span> where <span class="math inline">\(\mu_n\)</span> is <span class="math inline">\(\alpha + \sum_{j=1}^J \beta_j \ln q_{nj}^* + \beta_J^* \ln q_{nJ} + \ln q_J\)</span>. Taking the logarithm of this density, we get the individual contribution of an observation to the log-likelihood function:</p>
<p><span class="math display">\[
l_n = - \ln \sigma - \frac{1}{2}\ln 2\pi +
\ln (1 + \lambda y_n) - \ln y_n
- \frac{1}{2\sigma ^ 2}
\left(\ln y_n + \lambda y_n - \mu_n\right) ^ 2
\]</span></p>
<p>Denoting <span class="math inline">\(\gamma^ \top = (\alpha, \beta_1, ...\beta_J^*)\)</span>, <span class="math inline">\(z_n = (1, q_{n1} ^ *, q_{n2} ^ *, \ldots q_{nJ-1}^*, q_{nJ})\)</span> and <span class="math inline">\(\epsilon_n = \ln y_n + \lambda y_n - \mu_n\)</span>, the derivatives with respect to the unknown parameters <span class="math inline">\((\gamma, \lambda, \sigma)\)</span> are:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\frac{\partial l_n}{\partial \gamma} &amp;=&amp; \frac{1}{\sigma ^ 2}\epsilon_nz_n
\\
\frac{\partial l_n}{\partial \lambda} &amp;=&amp; \frac{y_n}{1 + \lambda y_n} -
\frac{1}{\sigma ^ 2}\epsilon_n y_n \\
\frac{\partial l_n}{\partial \sigma} &amp;=&amp; - \frac{1}{\sigma} +
\frac{1}{\sigma ^ 3} \epsilon_n ^ 2\\
\end{array}\\
\right.
\]</span></p>
<p>and the second derivatives give the following individual contributions to the hessian:</p>
<p><span class="math display">\[
\left(
\begin{array}{ccc}
-\frac{1}{\sigma^2}z_n z_n^\top &amp; \frac{1}{\sigma^2} y_n z_n
&amp; - \frac{2}{\sigma^3}\epsilon_n z_n \\
\frac{1}{\sigma^2} y_n z_n^\top &amp; -\frac{y_n^2}{(1 + \lambda
y_n) ^2} - \frac{y_n^2}{\sigma^2} &amp; \frac{2}{\sigma^3} \epsilon_n y_n \\
-\frac{2}{\sigma^3}\epsilon_n z_n^\top &amp; \frac{2}{\sigma^3}\epsilon_n
y_n &amp; \frac{1}{\sigma^2} - \frac{3}{\sigma^4}\epsilon_n^2
\end{array}
\right)
\]</span></p>
<p>To estimate the generalized production function, we use the <strong>maxLik</strong> package <span class="citation" data-cites="HENN:TOOM:11">(see <a href="#ref-HENN:TOOM:11" role="doc-biblioref">Henningsen and Toomet 2011</a>)</span>, which is dedicated to ML estimation. This package provides different algorithms to compute the maximum of the likelihood, the Newton-Raphson method that we used previously being the default. A <code>maxLik</code> object is returned, and specific methods as <code>coef</code> and <code>summary</code> are provided. To use <strong>maxLik</strong>, we need to define a function of the unknown parameters which returns the value of the log-likelihood function. This function can return either a scalar (the log-likelihood) or a <span class="math inline">\(N\)</span>-length vector that contains the individual contribution to the log-likelihood. Moreover, it is advisable to provide a function that returns the gradient: it can either return a <span class="math inline">\(K+1\)</span>-length vector or a <span class="math inline">\(N \times (K + 1)\)</span> matrix on which each line is the contribution of an observation to the gradient. The analytical hessian matrix can also be provided (otherwise, a numerical approximation is computed, which is more time-consuming and less precise). Moreover, <strong>maxLik</strong> allows to provide a function that returns the maximum-likelihood and the gradient and the hessian as attributes. It is a good idea to do so, as there are often common code while writing the likelihood, the gradient and the hessian.</p>
<p>The <code><a href="https://rdrr.io/pkg/micsr/man/zellner_revankar.html">micsr::zellner_revankar</a></code> function returns the log-likelihood function for the generalized production function. Its mandatory arguments are <code>theta</code>, a vector of starting values, <code>y</code>, the vector of response and <code>Z</code>, a matrix of covariates. By default, the function computes the log-likelihood (a vector of contributions), the gradient (a matrix of contributions) and the hessian. The <code>gradient</code> and <code>hessian</code> arguments (by default <code>TRUE</code>) are booleans which enable to return only the log-likelihood if set to <code>FALSE</code>. If the <code>sum</code> argument is <code>FALSE</code> (the default), the log-likelihood and the gradient are returned as a vector and a matrix of individual contributions. If set to <code>TRUE</code>, a scalar and a vector are returned. Finally, the <code>repar</code> argument enables to write the likelihood in its “raw” form (<code>repar = FALSE</code>) or in its reparametrized form (the default).</p>
<p>We first extract the response and the covariates matrix: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">form</span> <span class="op">&lt;-</span> <span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> ;</span>
<span><span class="va">mf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">aps</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.extract.html">model.response</a></span><span class="op">(</span><span class="va">mf</span><span class="op">)</span> ;</span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">mf</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then use as starting values the coefficients of the Cobb-Douglas previously estimated, and we set the starting value of <span class="math inline">\(\lambda\)</span> to 0. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">st_val</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cd_repar</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, lambda <span class="op">=</span> <span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cd_repar</span><span class="op">)</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>&lt; We now proceed to the estimation. The two mandatory arguments of <code>maxLik</code> are the first argument <code>logLik</code> which should be a function returning the log-likelihood and <code>start</code> which is a vector of starting values. We use also here <code>y</code> and <code>Z</code> arguments that are passed to <code>zellner_revankar</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gpf</span> <span class="op">&lt;-</span> <span class="fu">maxLik</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/maxLik/man/maxLik.html">maxLik</a></span><span class="op">(</span><span class="va">zellner_revankar</span>, start <span class="op">=</span> <span class="va">st_val</span>, y <span class="op">=</span> <span class="va">y</span>, Z <span class="op">=</span> <span class="va">Z</span><span class="op">)</span></span>
<span><span class="va">gpf</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. error t value   Pr(&gt; t)
(Intercept)   0.0478    0.12869  0.3714 7.103e-01
log(k/m)      0.2574    0.11950  2.1543 3.122e-02
log(l/m)      0.4712    0.18287  2.5769 9.969e-03
log(m)        0.3245    0.16921  1.9179 5.512e-02
lambda        0.1362    0.08506  1.6011 1.093e-01
sigma         0.6565    0.05961 11.0145 3.254e-28</code></pre>
</div>
</div>
<p>The probability values for the hypothesis that <span class="math inline">\(\sum_j \beta_j =1\)</span> and that <span class="math inline">\(\lambda = 0\)</span> are respectively about 5 and 10%. The hypothesis of constant returns to scale is the joint hypothesis that <span class="math inline">\(\sum_j \beta_j =1\)</span> and <span class="math inline">\(\lambda = 0\)</span> and will be tested in the next section. Applying <a href="#eq-elast">Equation&nbsp;<span>5.30</span></a>, the level of output for which the scale elasticity is unity is 2.38, reminding that the average production is 1. We then compute the elasticity for different values of the production that are presented in <a href="#tbl-elasty">Table&nbsp;<span>5.2</span></a>. Returns to scale are increasing for more than three quarters of the sample.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="tbl-elasty" class="anchored">

<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>Table&nbsp;5.2:  Scale elasticity </caption>
 <thead><tr>
<th style="text-align:left;"> production </th>
   <th style="text-align:right;"> y </th>
   <th style="text-align:right;"> elast </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:left;"> min </td>
   <td style="text-align:right;"> 0.05 </td>
   <td style="text-align:right;"> 1.315 </td>
  </tr>
<tr>
<td style="text-align:left;"> Q1 </td>
   <td style="text-align:right;"> 0.40 </td>
   <td style="text-align:right;"> 1.257 </td>
  </tr>
<tr>
<td style="text-align:left;"> median </td>
   <td style="text-align:right;"> 0.60 </td>
   <td style="text-align:right;"> 1.225 </td>
  </tr>
<tr>
<td style="text-align:left;"> Q3 </td>
   <td style="text-align:right;"> 1.04 </td>
   <td style="text-align:right;"> 1.161 </td>
  </tr>
<tr>
<td style="text-align:left;"> max </td>
   <td style="text-align:right;"> 6.55 </td>
   <td style="text-align:right;"> 0.700 </td>
  </tr>
</tbody>
</table>
</div>
</div>
</div>
<div style="page-break-after: always;"></div>
<p>The “raw” version of the model can also be estimated by setting the <code>repar</code> argument to <code>FALSE</code>. In this case, we use the coefficients of <code>cd_loglm</code> as starting values for all the coefficients except <span class="math inline">\(\lambda\)</span> (for which the starting value is set to 0). </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">st_val2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cd_loglm</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, lambda <span class="op">=</span> <span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cd_loglm</span><span class="op">)</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">gpf2</span> <span class="op">&lt;-</span> <span class="fu">maxLik</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/maxLik/man/maxLik.html">maxLik</a></span><span class="op">(</span><span class="va">zellner_revankar</span>, y <span class="op">=</span> <span class="va">y</span>, Z <span class="op">=</span> <span class="va">Z</span>, </span>
<span>                       start <span class="op">=</span> <span class="va">st_val2</span>, repar <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
</section></section><section id="sec-ml_tests" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="sec-ml_tests">
<span class="header-section-number">5.3</span> Tests</h2>
<p>Three kinds of tests will be considered:</p>
<ul>
<li>tests for nested models: in this context, there is a “large” model that reduces to a “small” model when the hypotheses are imposed. Depending on whether H<sub>0</sub> is true or false, the small or the large model are assumed to be the “true” model,</li>
<li>conditional moment tests for which only one model is considered and moment conditions are constructed from the fitted model that should be zero if the tested hypothesis (for example, normality or homoskedasticity) are true,</li>
<li>tests for non-nested models and especially the test proposed by <span class="citation" data-cites="VUON:89">Vuong (<a href="#ref-VUON:89" role="doc-biblioref">1989</a>)</span>: in this case, whatever the values of the parameters, there is no way for one model to reduce to the other model. Moreover, the test have the interesting feature that the two models are compared without hypothesizing that one of them is the true model.</li>
</ul>
<section id="sec-three_tests_ml" class="level3"><h3 class="anchored" data-anchor-id="sec-three_tests_ml">Tests for nested models: the three classical tests</h3>
<p> We have seen in <a href="multiple_regression.html#sec-three_tests"><span>Section&nbsp;3.6.3</span></a> that a set of hypotheses defines constraints on the parameters and therefore leads to two models. The unconstrained or large model doesn’t take these constraints into account, so that the log-likelihood is maximized without constraints. The constrained or small model is obtained by maximizing the log-likelihood under the constraints corresponding to the set of hypotheses. This leads to three test principles: the likelihood ratio test (based on the comparison of both models), the Wald test (based only on the unconstrained model) and the Lagrange multiplier test (based only on the constrained model). Remember that, when applying these test principles on a model fitted by OLS, the three statistics are exactly the same.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> On the, contrary the three tests give different results for non-linear models but, if the hypothesis are true, the three test statistics follow a <span class="math inline">\(\chi ^ 2\)</span> distribution with a number of degrees of freedom equal to the number of hypotheses and converge in probability to the same value.</p>
<p>We’ll consider in this section a set of <span class="math inline">\(J\)</span> linear hypotheses, such that <span class="math inline">\(R\theta - q = 0\)</span>, and we’ll denote <span class="math inline">\(\hat{\theta}_{nc}\)</span> and <span class="math inline">\(\hat{\theta}_{c}\)</span> the unconstrained and constrained maximum likelihood estimators.</p>
<section id="three-tests-for-the-linear-gaussian-model" class="level4"><h4 class="anchored" data-anchor-id="three-tests-for-the-linear-gaussian-model">Three tests for the linear gaussian model</h4>
<p> We first compute the three tests for the linear gaussian model developed in <a href="#sec-linear-gaussian"><span>Section&nbsp;5.2.3</span></a>. We denote <span class="math inline">\(\hat{\epsilon}_{nc}\)</span> and <span class="math inline">\(\hat{\epsilon}_c\)</span> the vectors of residuals for the unconstrained and the constrained model. The F statistic for the hypothesis that <span class="math inline">\(R\gamma = q\)</span> is, denoting <span class="math inline">\(A = R(Z^\top Z)^{-1}R^\top\)</span>: <span class="math display">\[
F = \frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}} \frac{N - K - 1}{J} =
\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\dot{\sigma} ^ 2_{nc}} / J
\]</span> where <span class="math inline">\(\dot{\sigma}_{nc} ^ 2 = \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc} / (N - K -1)\)</span> is the unbiased estimator of <span class="math inline">\(\sigma ^ 2\)</span>. With the hypothesis of iid normal errors, this statistic follows a Fisher-Snedecor distribution with <span class="math inline">\(J\)</span> and <span class="math inline">\(N-K-1\)</span> degrees of freedom and the asymptotic distribution of <span class="math inline">\(J \times F\)</span> is a <span class="math inline">\(\chi ^2\)</span> with <span class="math inline">\(J\)</span> degrees of freedom. Remember from <a href="multiple_regression.html#eq-const_lm">Equation&nbsp;<span>3.27</span></a> that <span class="math inline">\(\hat{\gamma}_c=\hat{\gamma}_{nc} - (Z^\top Z)^{-1}R^\top A^{-1}(R\hat{\gamma}_{nc}-q)\)</span>, then:</p>
<p><span id="eq-resid_c_nc"><span class="math display">\[
\hat{\epsilon}_c = \hat{\epsilon}_{nc}+ Z(Z^\top Z)^{-1}R^\top A^{-1}(R\hat{\gamma}_{nc}-q)
\tag{5.31}\]</span></span></p>
<p>and, because <span class="math inline">\(Z ^ \top \hat{\epsilon}_{nc} = 0\)</span>, the relation between the two residual sum of squares is:</p>
<p><span id="eq-ssr_c_nc"><span class="math display">\[
\hat{\epsilon}_c ^ \top \hat{\epsilon}_c = \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc} + (R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)
\tag{5.32}\]</span></span></p>
<p>The likelihood ratio statistic is, using <a href="#eq-conc_loglik">Equation&nbsp;<span>5.26</span></a>: </p>
<p><span id="eq-lr_stat"><span class="math display">\[
LR = 2\left(\ln L(\hat{\gamma}_{nc})-\ln L(\hat{\gamma}_{c})\right) = N \ln \frac{\hat{\epsilon}_c ^ \top \hat{\epsilon}_c}{\hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}}
\tag{5.33}\]</span></span></p>
<p>and the Wald statistic is: </p>
<p><span id="eq-wald_stat"><span class="math display">\[
W = N\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}}=
\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\sigma} ^ 2_{nc}}
\tag{5.34}\]</span></span></p>
<p>For the Lagrange multiplier test, as the hessian is block diagonal, we can consider only the part of the gradient and the hessian that concerns <span class="math inline">\(\gamma\)</span>. For the constrained model, the subset of the gradient and of the information matrix are <span class="math inline">\(Z ^ \top \epsilon_{c} / \hat{\sigma}_{c}^2\)</span> and <span class="math inline">\(Z ^ \top Z / \hat{\sigma}_c ^ 2\)</span>, where <span class="math inline">\(\hat{\sigma}_c ^ 2 = \hat{\epsilon}_c ^ \top \hat{\epsilon}_c/N\)</span> Then the statistic is <span class="math inline">\(\hat{\epsilon}_c ^ \top Z (Z ^\top Z) ^ {-1} Z ^ \top \hat{\epsilon}_c / \hat{\sigma}_{c} ^ 2\)</span>. Using <a href="#eq-resid_c_nc">Equation&nbsp;<span>5.31</span></a>, we get: </p>
<p><span id="eq-lm_stat"><span class="math display">\[
LM = N\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\epsilon}_{c} ^ \top \hat{\epsilon}_{c}}=
\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\sigma} ^ 2_{c}}
\tag{5.35}\]</span></span></p>
<p>which is the same expression as <a href="#eq-wald_stat">Equation&nbsp;<span>5.34</span></a> except that the estimation of <span class="math inline">\(\sigma^2\)</span> is based on the constrained model. From <a href="#eq-ssr_c_nc">Equation&nbsp;<span>5.32</span></a>, <span class="math inline">\(e ^{LR / N} = 1 + W / N\)</span>. Therefore:</p>
<p><span class="math display">\[LR/N = \ln (1 + W/N)\]</span></p>
<p>Defining, <span class="math inline">\(f(x) = \ln (1 + x) - x\)</span> for <span class="math inline">\(x \geq 0\)</span>, <span class="math inline">\(f'(x) = -x / (1 + x) &lt; 0\)</span>. As <span class="math inline">\(f(0) = 0\)</span> and <span class="math inline">\(f\)</span> is a strictly decreasing function, <span class="math inline">\(f(x) &lt; 0\)</span> for <span class="math inline">\(x &gt; 0\)</span>. Therefore <span class="math inline">\(\ln (1 + W/N) - W/N = LR/N - W/N &lt; 0\)</span> and therefore <span class="math inline">\(W &gt; LR\)</span>.</p>
<p>Using <a href="#eq-wald_stat">Equation&nbsp;<span>5.34</span></a> and <a href="#eq-lm_stat">Equation&nbsp;<span>5.35</span></a>, we get: <span class="math inline">\(W / LM = \hat{\epsilon}_c ^ \top \hat{\epsilon}_c / \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}\)</span>. Finally, using <a href="#eq-ssr_c_nc">Equation&nbsp;<span>5.32</span></a>, we get: <span class="math inline">\(W/LM = 1 + W/N\)</span> or, rearranging terms:</p>
<p><span class="math display">\[
LM / N = \frac{W / N}{1 + W / N}
\]</span> Denoting <span class="math inline">\(f(x) = \ln (1+x) - x / (1 + x)\)</span>, for <span class="math inline">\(x \geq 0\)</span>, <span class="math inline">\(f'(x) = x / (1 + x) ^ 2\)</span>. Therefore, <span class="math inline">\(f(x)\)</span> is strictly increasing for <span class="math inline">\(x &gt;0\)</span> and, as <span class="math inline">\(f(0) = 0\)</span>, <span class="math inline">\(f(x) &gt; 0 \; \forall x &gt;0\)</span>. With <span class="math inline">\(x = W / N\)</span>, <span class="math inline">\(\ln (1 + W/N) - W/N / (1 + W/N) = LR/N - LM /N &gt; 0\)</span> and therefore <span class="math inline">\(LR &gt; LM\)</span>. Therefore, we have proved that:<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[
LM&lt;LR&lt;W
\]</span></p>
<p></p>
</section><section id="sec-pseudo_r2_ml" class="level4"><h4 class="anchored" data-anchor-id="sec-pseudo_r2_ml">Pseudo-R<sup>2</sup> for models estimated by maximum likelihood</h4>
<p> Consider now the case where we test the set of <span class="math inline">\(J = K\)</span> hypothesis that <span class="math inline">\(\beta = 0\)</span>. Then, <span class="math inline">\(\mbox{TSS} = \hat{\epsilon}_c ^ \top \hat{\epsilon}_c\)</span> and <span class="math inline">\(\mbox{RSS} = \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}\)</span> and, from <a href="#eq-ssr_c_nc">Equation&nbsp;<span>5.32</span></a>:</p>
<p><span class="math display">\[
\frac{\mbox{TSS}}{\mbox{RSS}} = 1 + W / N = 1 + \frac{\mbox{TSS}}{\mbox{RSS}} \times LM / N = e^{LR / N}
\]</span></p>
<p>Therefore, the three statistics are:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
W &amp;=&amp; N (TSS - RSS) / RSS\\
LM &amp;=&amp; N (TSS - RSS) / TSS\\
LR &amp;=&amp; N \ln TSS / RSS
\end{array}
\right.
\]</span></p>
<p>The <span class="math inline">\(R^2\)</span> being equal to <span class="math inline">\(1 - RSS / TSS\)</span>, it can be easily expressed, for the linear gaussian model, as a function of the three statistics:</p>
<p><span id="eq-pseudo_r2"><span class="math display">\[
R ^ 2 = \frac{W}{N + W} = 1 - e^{LR / N} = LM / N
\tag{5.36}\]</span></span></p>
<p>These pseudo-R<sup>2</sup> can be used for any model computed by maximum likelihood and will be denoted respectively by <span class="math inline">\(R^2_{W}\)</span>, <span class="math inline">\(R^2_{LR}\)</span> and <span class="math inline">\(R^2_{LM}\)</span>. They have been proposed by <span class="citation" data-cites="MAGE:90">Magee (<a href="#ref-MAGE:90" role="doc-biblioref">1990</a>)</span>. <span class="math inline">\(R^2_{LR}\)</span> is known as the <span class="math inline">\(R^2\)</span> of <span class="citation" data-cites="COX:SNEL:89">Cox and Snell (<a href="#ref-COX:SNEL:89" role="doc-biblioref">1989</a>)</span>, but it has been previously proposed by <span class="citation" data-cites="MADD:83">Maddala (<a href="#ref-MADD:83" role="doc-biblioref">1983</a>)</span>. A variant, proposed by <span class="citation" data-cites="ALDR:NELS:84">Aldrich and Nelson (<a href="#ref-ALDR:NELS:84" role="doc-biblioref">1984</a>)</span>, is obtained using the formula of <span class="math inline">\(R^2_{W}\)</span>, but using the LR statistic: <span class="math inline">\(LR / (N + LR)\)</span>. A problem with the pseudo-R<sup>2</sup> computed using the LR statistic is that, for a perfect or <strong>saturated</strong> model, it can be lower than 1. Denoting <span class="math inline">\(\ln L_0\)</span> and <span class="math inline">\(\ln L_*\)</span> the values of the log-likelihood for the null (intercept only) and the saturated model, the LR statistic for the saturated model is <span class="math inline">\(LR_* = 2(\ln L_* - \ln L_0)\)</span>. In case of discrete response (as for the Poisson model developed in the beginning of this chapter), the individual contributions to the log-likelihood are logs of probabilities (all equal to 1 for a saturated model), so that <span class="math inline">\(\ln L_* = 0\)</span> and <span class="math inline">\(LR_* = -2\ln L_0\)</span>. Therefore, <span class="math inline">\(R^2_{LR}\)</span> equals <span class="math inline">\(1 - e^{LR_*/N} \neq 1\)</span>. Scaled versions of <span class="math inline">\(R^2_{LR}\)</span> and <span class="citation" data-cites="ALDR:NELS:84">Aldrich and Nelson (<a href="#ref-ALDR:NELS:84" role="doc-biblioref">1984</a>)</span>’s <span class="math inline">\(R^2\)</span> have been proposed respectively by <span class="citation" data-cites="NAGE:91">Nagelkerke (<a href="#ref-NAGE:91" role="doc-biblioref">1991</a>)</span>:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><span class="math display">\[
R^2 = \frac{1 - e ^ {- LR / N}}{1 - e ^ {- LR _ * / N}}
\]</span></p>
<p>and by <span class="citation" data-cites="VEAL:ZIMM:96">Veall and Zimmermann (<a href="#ref-VEAL:ZIMM:96" role="doc-biblioref">1996</a>)</span>:</p>
<p><span class="math display">\[
R^2 = \frac{LR / (LR + N)}{LR _ * / (LR _ * + N)}
\]</span> </p>
</section><section id="general-formula-and-application-to-the-generalized-production-function" class="level4"><h4 class="anchored" data-anchor-id="general-formula-and-application-to-the-generalized-production-function">General formula and application to the generalized production function</h4>
<p>Although the three tests are not limited to linear constraints, we’ll consider only linear hypothesis in this section. Moreover, as seen previously, a linear model can always be reperametrized so that a set of linear hypothesis reduces to the test that a subset of the parameters (<span class="math inline">\(\theta_2\)</span>) is zero.</p>
<p>In our generalized production function example, the original set of parameters are <span class="math inline">\(\theta ^ \top = (\alpha, \beta_k, \beta_l, \beta_s, \lambda, \sigma)\)</span> and the constant returns to scale hypothesis is <span class="math inline">\(\beta_k + \beta_l + \beta_s = 1\)</span> and <span class="math inline">\(\lambda = 0\)</span>.</p>
<p><span class="math display">\[
R \theta - q = \left(
\begin{array}{cccccc}
0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
\end{array}\right)
\left(\begin{array}{c}\alpha\\ \beta_k\\ \beta_l\\ \beta_s\\ \lambda\\ \sigma\end{array}\right)
- \left(\begin{array}{c}1 \\ 0 \end{array}\right)= 0
\]</span></p>
<p>After the reparametrization, the set of parameters is <span class="math inline">\(\theta^ {*\top} = (\alpha, \beta_k, \beta_l, \beta_s^*, \lambda, \sigma)\)</span> and the constant returns to scale hypothesis is <span class="math inline">\(\lambda = \beta_s^*=0\)</span> : </p>
<p><span class="math display">\[
R^* \theta^* - q^* = \left(
\begin{array}{cccccc}
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
\end{array}\right)
\left(\begin{array}{c}\alpha\\ \beta_k\\ \beta_l\\ \beta_s ^ *\\ \lambda\\ \sigma\end{array}\right)
- \left(\begin{array}{c}0 \\ 0 \end{array}\right)= 0
\]</span></p>
<p><span class="math inline">\(R^*\)</span> is then a matrix which selects a subset of coefficients, that we’ll call <span class="math inline">\(\theta_2\)</span>.</p>
<!-- ##### Wald test -->
<p> The Wald test is based on the unconstrained model and more precisely on <span class="math inline">\(R \hat{\theta}_{nc} - q\)</span>. If H<sub>0</sub> is true, this vector of length <span class="math inline">\(J\)</span> (here <span class="math inline">\(J = 2\)</span>) should be close to 0. Using the central-limit theorem, the asymptotic distribution of <span class="math inline">\(\hat{\theta}\)</span> is normal, so that, under H<sub>0</sub>: <span class="math inline">\(R \hat{\theta}_{nc} - q \overset{a}{\sim} \mathcal{N}(0, R\hat{V}_{\hat{\theta}}R^\top)\)</span>. The Wald statistic is obtained by computing the quadratic form of this vector using the inverse of its covariance matrix:</p>
<p><span class="math display">\[
W = (R \hat{\theta}_{nc} - q)^\top(R\hat{V}_{\hat{\theta}}R^\top) ^ {-1} (R \hat{\theta}_{nc} - q)
\]</span> With the reparametrized model, denoting <span class="math inline">\(\hat{V}_{\hat{\theta}_2}\)</span> the subset of <span class="math inline">\(\hat{V}_{\hat{\theta}}\)</span> that concerns <span class="math inline">\(\theta_2\)</span>, the Wald statistic simplifies to <span class="math inline">\(W = \hat{\theta}_2 ^ \top\hat{V}_{\hat{\theta}_2}^{-1}\hat{\theta}_2\)</span>. We first extract the vector and the matrix used in the quadratic form: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">6</span><span class="op">)</span></span>
<span><span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">R</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span> <span class="op">-</span> <span class="va">q</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span> <span class="op">-</span> <span class="va">q</span></span>
<span><span class="va">V</span> <span class="op">&lt;-</span> <span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The quadratic form of the vector <code>d</code> with the inverse of the matrix <code>V</code> can be obtained using the basic tools provided by <strong>R</strong> for matrix algebra:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">d</span>, <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">V</span>, <span class="va">d</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.753</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note the use of <code>drop</code> to get a scalar and not a <span class="math inline">\(1 \times 1\)</span> matrix. The <code><a href="https://rdrr.io/pkg/micsr/man/quad_form.html">micsr::quad_form</a></code> function enables to compute more simply the quadratic form:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wald_test</span> <span class="op">&lt;-</span> <span class="fu">quad_form</span><span class="op">(</span><span class="va">d</span>, <span class="va">V</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A subset of the elements of the vector and the matrix can be used with the <code>subset</code> argument:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wald_test2</span> <span class="op">&lt;-</span> <span class="fu">quad_form</span><span class="op">(</span><span class="va">gpf</span>, subset <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"log(m)"</span>, <span class="st">"lambda"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">wald_test</span>, <span class="va">wald_test2</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.753 3.753</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which illustrates the equivalence of the two formulas for the Wald test using either the “raw” or the reparametrized model.</p>
<p>We can also use the approximate formula that indicates that the Wald statistic is close to the sum of the squares of the corresponding t statistics if the correlation between the two parameters is not too high: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">csgpf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gpf</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">t1</span> <span class="op">&lt;-</span> <span class="va">csgpf</span><span class="op">[</span><span class="st">"log(m)"</span>, <span class="fl">3</span><span class="op">]</span> ; <span class="va">t2</span> <span class="op">&lt;-</span> <span class="va">csgpf</span><span class="op">[</span><span class="st">"lambda"</span>, <span class="fl">3</span><span class="op">]</span> ; <span class="va">t1</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">t2</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="co">## [1] 6.242</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The approximation is very bad, which should be explained by a high correlation between the two coefficients: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="va">csgpf</span><span class="op">[</span><span class="st">"log(m)"</span>, <span class="fl">2</span><span class="op">]</span> ; <span class="va">s2</span> <span class="op">&lt;-</span> <span class="va">csgpf</span><span class="op">[</span><span class="st">"lambda"</span>, <span class="fl">2</span><span class="op">]</span> ; </span>
<span><span class="va">v12</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">gpf</span><span class="op">)</span><span class="op">[</span><span class="st">"log(m)"</span>, <span class="st">"lambda"</span><span class="op">]</span> ; <span class="va">r12</span> <span class="op">&lt;-</span> <span class="va">v12</span> <span class="op">/</span> <span class="op">(</span><span class="va">s1</span> <span class="op">*</span> <span class="va">s2</span><span class="op">)</span></span>
<span><span class="va">r12</span></span>
<span><span class="co">## [1] 0.7387</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>“Correcting” this correlation using <a href="multiple_regression.html#eq-ellipse">Equation&nbsp;<span>3.23</span></a>, we get the correct value of the statistic:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="op">(</span><span class="va">t1</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">t2</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">r12</span> <span class="op">*</span> <span class="va">t1</span> <span class="op">*</span> <span class="va">t2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">r12</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.753</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The statistic can also easily obtained using the <code><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">car::linearHypothesis</a></code> function described in <a href="multiple_regression.html#sec-wald_test_example"><span>Section&nbsp;3.6.4.1</span></a>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">gpf</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"log(m) = 0"</span>, <span class="st">"lambda = 0"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## Chisq = 3.753, df: 2, pval = 0.153</span></span>
<span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">gpf2</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"log(l) + log(k) + log(m) = 1"</span>, </span>
<span>                              <span class="st">"lambda = 0"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## Chisq = 3.753, df: 2, pval = 0.153</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
<!-- ##### Likelihood ratio test -->
<p> The likelihood ratio statistic is very easy to compute if the two models have been estimated as it is simply twice the difference of the log-likelihood of the two models. The coefficients of the constrained model can be obtained by least squares using the reparametrized version of the Cobb-Douglas (<a href="#eq-cobb_douglas_production_reparam">Equation&nbsp;<span>5.29</span></a>) and imposing <span class="math inline">\(\beta_m = 0\)</span>. Using <code><a href="https://rdrr.io/pkg/micsr/man/loglm.html">micsr::loglm</a></code>, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">crs</span> <span class="op">&lt;-</span> <span class="fu">loglm</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/offset.html">offset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span>, <span class="va">aps</span><span class="op">)</span></span>
<span><span class="va">lr_test</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">gpf</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">crs</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">lr_test</span></span>
<span><span class="co">## [1] 4.922</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
<!-- ##### Lagrange multiplier test -->
<p></p>
<p>The Lagrange multiplier is based on the gradient evaluated with the estimates of the constrained model: <span class="math inline">\(g(\hat{\theta}_{nc})\)</span>, which should be closed to 0. Applying the central-limit theorem, this vector is normally distributed, with, under H<sub>0</sub>, a zero expectation and a variance equal to the information matrix, which can be estimated by minus the hessian: <span class="math inline">\(g(\hat{\theta}_{nc}) \overset{a}{\sim} \mathcal{N}(0, - H_{nc})\)</span>. The statistic is then:</p>
<p><span id="eq-lm_formula"><span class="math display">\[
LM = g(\hat{\theta}_{nc}) ^ \top (- H_{nc}) ^ {-1}g(\hat{\theta}_{nc})
\tag{5.37}\]</span></span></p>
<p>For the reparametrized model, denoting <span class="math inline">\(g_2\)</span> and <span class="math inline">\(H_2\)</span> the parts of the gradient and of the hessian that concern <span class="math inline">\(\theta_2\)</span>, we get <span class="math inline">\(g_2^\top (-H_2) ^ {-1}g_2\)</span>. We consider here the case where the model is parametrized in a way that the hypothesis simply states that a subset of the coefficient <span class="math inline">\(\theta_2 ^ \top = (\beta_m, \lambda)\)</span> is zero. For the constrained model, <span class="math inline">\(\theta_1^\top = (\alpha, \beta_l, \beta_k, \sigma)\)</span> is estimated, so that elements of the gradient that are the derivatives of the log-likelihood with respect to <span class="math inline">\(\theta_1\)</span> should be 0. The other elements of the gradient are not 0, but should be close to 0 if the hypotheses are true. Using the <code>zellner_revankar</code> function, we compute all the necessary information (the log-likelihood, the gradient and the hessian) for the constrained model, which corresponds to the coefficients of the <code>crs</code> object fitted by least squares using <code>loglm</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">const_model</span> <span class="op">&lt;-</span> <span class="fu">zellner_revankar</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">crs</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span>, <span class="st">"log(m)"</span> <span class="op">=</span> <span class="fl">0</span>, </span>
<span>                                  lambda <span class="op">=</span> <span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">crs</span><span class="op">)</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, </span>
<span>                                y <span class="op">=</span> <span class="va">y</span>, Z <span class="op">=</span> <span class="va">Z</span>, sum <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">G</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="va">const_model</span>, <span class="st">"gradient"</span><span class="op">)</span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">G</span>, <span class="fl">2</span>, <span class="va">sum</span><span class="op">)</span></span>
<span><span class="va">H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="va">const_model</span>, <span class="st">"hessian"</span><span class="op">)</span></span>
<span><span class="va">g</span></span>
<span><span class="co">## (Intercept)      log(k)      log(l)      log(m)      lambda </span></span>
<span><span class="co">##   6.883e-15  -4.708e-14  -1.901e-14   1.173e+01   1.364e+01 </span></span>
<span><span class="co">##       sigma </span></span>
<span><span class="co">##  -3.297e-14</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Applying <a href="#eq-lm_formula">Equation&nbsp;<span>5.37</span></a>, or using only the part of the gradient and of the inverse of the hessian that concerns <span class="math inline">\(\theta_2\)</span>, we get:<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_test_H</span> <span class="op">&lt;-</span> <span class="fu">quad_form</span><span class="op">(</span><span class="va">g</span>, <span class="op">-</span> <span class="va">H</span><span class="op">)</span></span>
<span><span class="va">g2</span> <span class="op">&lt;-</span> <span class="va">g</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"log(m)"</span>, <span class="st">"lambda"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">H2m1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"log(m)"</span>, <span class="st">"lambda"</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"log(m)"</span>, <span class="st">"lambda"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">lm_test_H2</span> <span class="op">&lt;-</span> <span class="fu">quad_form</span><span class="op">(</span><span class="va">g2</span>, <span class="op">-</span> <span class="va">H2m1</span>, inv <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">lm_test_H</span>, <span class="va">lm_test_H2</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.923 3.923</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead of using the opposite of the hessian, we could also have used the outer product of the gradient to estimate the information matrix: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Ig</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">G</span><span class="op">)</span></span>
<span><span class="va">lm_test_G</span> <span class="op">&lt;-</span> <span class="fu">quad_form</span><span class="op">(</span><span class="va">g</span>, <span class="va">Ig</span><span class="op">)</span></span>
<span><span class="va">lm_test_G</span></span>
<span><span class="co">## [1] 13.58</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>But in this case, the statistic can be more simply computed using the results of a regression, because the statistic is:</p>
<p><span class="math display">\[
g^\top(G^\top G) ^ {-1}g
\]</span></p>
<p>The gradient <span class="math inline">\(g\)</span> being the columnwise sum of <span class="math inline">\(G\)</span>, it can be written as <span class="math inline">\(g=G^\top j\)</span>, where <span class="math inline">\(j\)</span> is a vector of 1s of length <span class="math inline">\(N\)</span>. Therefore, the test statistic is also:</p>
<p><span class="math display">\[
j^\top G (G^\top G) ^ {-1} G^ \top j = j^\top P_G j
\]</span></p>
<p>where <span class="math inline">\(P_G\)</span> is the projection matrix on the subspace defined by the columns of <span class="math inline">\(G\)</span>. Therefore, regressing <span class="math inline">\(j\)</span> on <span class="math inline">\(G\)</span>, the fitted values are <span class="math inline">\(P_G j\)</span> and the sum of squares of the fitted values are <span class="math inline">\(j ^ \top P_G j\)</span>, <span class="math inline">\(P_G\)</span> being idempotent. In a regression without intercept, this is the explained sum of squares. The total sum of squares being: <span class="math inline">\(j ^ \top j = N\)</span>, the (uncentered) R-squared is equal to <span class="math inline">\(\frac{j ^ \top P_G j}{N}\)</span> and the test statistic is therefore <span class="math inline">\(N\)</span> times the R-squared of a regression on a vector of 1 on the column of the individual contributions to the gradient. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">areg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">N</span><span class="op">)</span> <span class="op">~</span> <span class="va">G</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">lm_test_G</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">areg</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">lm_test_G2</span> <span class="op">&lt;-</span> <span class="fu">rsq</span><span class="op">(</span><span class="va">areg</span><span class="op">)</span> <span class="op">*</span> <span class="va">N</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">lm_test_G</span>, <span class="va">lm_test_G2</span><span class="op">)</span></span>
<span><span class="co">## [1] 13.58 13.58</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> To summarize the results of this section, the Wald test statistic is 3.75, the likelihood-ratio test statistic is 4.92 and the score test statistic, when computed using the hessian based estimation of the information is 3.92. The values are quite similar and the hypotheses are not rejected at the 5% level (the critical value is 5.99) and are rejected at the 10% level (the critical value is equal to 4.61) only for the likelihood ratio test. On the contrary, the score test computed using the gradient-based estimate of the information matrix has a much higher value (13.58) and leads to a rejection of the hypothesis, even at the 1% level (the critical value is 9.21).</p>
<!-- ##### Pseudo R^2^ -->
<p> To compute the pseudo R<sup>2</sup>, we should define the “null” or “intercept-only” model. In the generalized production function, it is a model with only two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma\)</span>. Therefore, the <span class="math inline">\(R\)</span> matrix selects all the coefficients except these two and, using this matrix, we can compute the Wald statistic: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span><span class="op">[</span><span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span>, <span class="op">]</span></span>
<span><span class="va">W_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>              <span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For the other two tests, the constrained model should be estimated. Actually, this is a log linear model with only an intercept, and the LM estimates of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma ^ 2\)</span> are simply the mean and the variance of <span class="math inline">\(\ln y\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">alpha_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sigma_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">alpha_c</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">null_model</span> <span class="op">&lt;-</span> <span class="fu">zellner_revankar</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha_c</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="va">sigma_c</span><span class="op">)</span>, </span>
<span>                               y <span class="op">=</span> <span class="va">y</span>, Z <span class="op">=</span> <span class="va">Z</span>, repar <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">lnL_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">null_model</span><span class="op">)</span></span>
<span><span class="va">G_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="va">null_model</span>, <span class="st">"gradient"</span><span class="op">)</span></span>
<span><span class="va">g_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">G_c</span>, <span class="fl">2</span>, <span class="va">sum</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The likelihood ratio statistic is just twice the difference between the two values of the log-likelihood: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LR_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">gpf2</span><span class="op">)</span> <span class="op">-</span> <span class="va">lnL_c</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, the Lagrange multiplier statistic can be obtained by considering <span class="math inline">\(N\)</span> times the <span class="math inline">\(R^2\)</span> of the regression of a vector of 1 on the matrix of the individual contributions to the gradient: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LM_0</span> <span class="op">&lt;-</span> <span class="va">N</span> <span class="op">*</span> <span class="fu">rsq</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">N</span><span class="op">)</span> <span class="op">~</span> <span class="va">G_c</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then compute the three pseudo-R<sup>2</sup>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R2_w</span> <span class="op">&lt;-</span> <span class="va">W_0</span> <span class="op">/</span> <span class="op">(</span><span class="va">N</span> <span class="op">+</span> <span class="va">W_0</span><span class="op">)</span></span>
<span><span class="va">R2_lr</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span> <span class="va">LR_0</span> <span class="op">/</span> <span class="va">N</span><span class="op">)</span></span>
<span><span class="va">R2_lm</span> <span class="op">&lt;-</span> <span class="va">LM_0</span> <span class="op">/</span> <span class="va">N</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>wald <span class="op">=</span> <span class="va">R2_w</span>, lik_ratio <span class="op">=</span> <span class="va">R2_lr</span>, lagr_mult <span class="op">=</span> <span class="va">R2_lm</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">print</span></span>
<span><span class="co">##      wald lik_ratio lagr_mult </span></span>
<span><span class="co">##    0.5464    0.6054    0.7134</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
</section></section><section id="sec-ml_cond_moment_test" class="level3"><h3 class="anchored" data-anchor-id="sec-ml_cond_moment_test">Conditional moment test</h3>
<p></p>
<p>Compared to the three classical tests, conditional moment tests don’t define two nested models (a “large” one and a “small” one). These tests, first presented by <span class="citation" data-cites="TAUC:85">Tauchen (<a href="#ref-TAUC:85" role="doc-biblioref">1985</a>)</span> and <span class="citation" data-cites="NEWE:85">Newey (<a href="#ref-NEWE:85" role="doc-biblioref">1985</a>)</span> are based on moment conditions that should be 0 under H<sub>0</sub>. They are particularly useful for models fitted by maximum likelihood, although they can be used for models fitted by other estimation methods. Consider the example where the distribution of the response is related to the normal distribution. This is the case for the generalized production function estimated in the previous section, and it is also the case for the probit and the tobit model that will be developed in the last part of the book. With the OLS estimator, the most important properties of the estimator, especially unbiasedness and consistency only rely on the hypothesis that the conditional expectation of the response is correctly specified. This is not the case for models fitted by ML. Therefore, if the conditional distribution of the response is not normal with a constant conditional variance, the estimator may be inconsistent. Testing the hypothesis of normality and of homoskedasticity is therefore crucial in this context.</p>
<p>Denote <span class="math inline">\(\mu_n = \mu(\theta, w_n)\)</span> a vector of length <span class="math inline">\(J\)</span> for observation <span class="math inline">\(n\)</span>, that depends on a vector of parameters (<span class="math inline">\(\theta\)</span>) and on a vector of variables (<span class="math inline">\(w_n\)</span>, which typically contains the response and a vector of covariate). The hypothesis is that <span class="math inline">\(\mbox{E}(\mu_n) = 0\)</span>. For example, to test normality, the hypothesis will be that the third moment of the errors is zero and that the fourth (standardized) moment is three and therefore: <span class="math inline">\(\mu_n^\top = (\epsilon_n ^ 3, \epsilon_n ^ 4 - 3 \sigma_\epsilon ^4)\)</span>. Denote <span class="math inline">\(m(\theta, W) = \sum_{n=1} ^ N \mu(\theta, w_n)\)</span>. The test is based on the sample equivalent of the moment conditions, which is:</p>
<p><span class="math display">\[
\hat{\tau} = m(\hat{\theta}, W) / N = \frac{1}{N} \sum_{n=1} ^ N
\mu(\hat{\theta}, w_n) = \frac{1}{N} \sum_{n=1} ^ N \hat{\mu}_n
\]</span></p>
<p>and the hypotheses won’t be rejected if <span class="math inline">\(\hat{\tau}\)</span> is sufficiently close to a vector of 0. The derivation of its variance is quite complicated because there are two sources of stochastic variations, as both <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\hat{\mu}_n\)</span> are random.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Using a first-order Taylor expansion around the true value <span class="math inline">\(\theta_0\)</span>, we have:</p>
<p><span class="math display">\[
\hat{\tau} =  \frac{1}{N}m(\theta_0, Z) + \frac{1}{N}\frac{\partial
m}{\partial \theta^\top}(\bar{\theta}, Z)
(\hat{\theta} - \theta_0)
\]</span></p>
<p>Denote: <span class="math inline">\(\mathcal{W} = \displaystyle\lim_{N\rightarrow \infty} \frac{\partial m(\theta, Z)}{\partial \theta^\top} = \displaystyle\lim_{N\rightarrow \infty} \frac{1}{N} \sum_{n=1} ^ N\frac{\partial \mu(\theta, z_n)}{\partial \theta^\top}\)</span>. As the estimator is consistent, we have:</p>
<p><span class="math display">\[
\hat{\tau} \overset{a}{=} \frac{1}{N}m(\theta_0, Z) + \mathcal{W}
(\hat{\theta} - \theta_0)
\]</span></p>
<p>Using <a href="#eq-nondeg_theta">Equation&nbsp;<span>5.19</span></a>:</p>
<p><span class="math display">\[
\hat{\tau} \overset{a}{=}
\frac{m(\theta_0, Z)}{N} - \mathcal{W}\mathcal{H}^{-1}\frac{g(\theta_0, Z)}{N}=
\frac{1}{N}\sum_{n=1} ^ N \mu(\theta_0, z_n) -
\mathcal{W}\mathcal{H}^{-1}\frac{1}{N} \sum_{n=1}^N \gamma(\theta_0, Z)
\]</span></p>
<p>Or:</p>
<p><span id="eq-tau_cmtest"><span class="math display">\[
\sqrt{N}\hat{\tau} \overset{a}{=} \left[I,
-\mathcal{W}\mathcal{H}^{-1}\right]
\left(\begin{array}{c}\frac{\sum_n \mu(\theta_0, z_n)}{\sqrt{N}} \\
\frac{\sum_n \gamma(\theta_0, z_n)}{\sqrt{N}}\end{array}\right)
\tag{5.38}\]</span></span></p>
<p>Define <span class="math inline">\(V\)</span> as the variance of the vector in parentheses in <a href="#eq-tau_cmtest">Equation&nbsp;<span>5.38</span></a>, obtained by concatenating <span class="math inline">\(g(\theta_0, Z)\)</span> (the gradient for the true value of the parameters) and <span class="math inline">\(m(\theta_0, Z)\)</span>, both divided <span class="math inline">\(\sqrt{N}\)</span>. The probability limit of <span class="math inline">\(V\)</span> is:</p>
<p><span class="math display">\[
\mathcal{V} = \lim_{N\rightarrow \infty} \frac{1}{N}
\left(
\begin{array}{rcl}
\sum_n \mu_n \mu_n ^ {\top} &amp; \sum_n \mu_n \gamma_n ^ {\top} \\
\sum_n \gamma_n \mu_n ^ {\top} &amp; \sum_n \gamma_n \gamma_n ^ {\top} \\
\end{array}
\right)
\]</span></p>
<p>The probability limit of the variance of <span class="math inline">\(\sqrt{N} \hat{\tau}\)</span> is therefore:</p>
<p><span id="eq-var_sqrtN_tau"><span class="math display">\[
\mbox{V}(\sqrt{N} \hat{\tau}) \overset{p}{\rightarrow}
\left[I, -\mathcal{W}\mathcal{H}^{-1}\right]
\mathcal{V} \left[I,
-\mathcal{W}\mathcal{H}^{-1}\right] ^ \top
\tag{5.39}\]</span></span></p>
<p>and <span class="math inline">\(\mathcal{V}\)</span> can be consistently estimated by:</p>
<p><span class="math display">\[
\frac{1}{N}
\left(
\begin{array}{rcl}
\hat{M}^\top \hat{M} &amp; \hat{M} ^ \top \hat{G} \\
\hat{G}^\top \hat{M} &amp; \hat{G}^\top \hat{G}
\end{array}
\right)
\]</span></p>
<p>with <span class="math inline">\(\hat{M}\)</span> the <span class="math inline">\(N \times J\)</span> matrix containing the individual contributions to <span class="math inline">\(\hat{m}\)</span> (with a <span class="math inline">\(n\)</span><sup>th</sup> row equal to <span class="math inline">\(\hat{\mu}_n^\top\)</span>) and <span class="math inline">\(\hat{G}\)</span> the <span class="math inline">\(N \times (K+1)\)</span> matrix containing the individual contributions to the gradient (with a <span class="math inline">\(n\)</span><sup>th</sup> row equal to <span class="math inline">\(\hat{\gamma}_n^\top\)</span>). Replacing <span class="math inline">\(\mathcal{V}\)</span> in <a href="#eq-var_sqrtN_tau">Equation&nbsp;<span>5.39</span></a>, developing the quadratic form and regrouping terms, we finally get:<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p><span class="math display">\[
\mbox{V}(\sqrt{N}\hat{\tau})
\overset{p}{\rightarrow}
\frac{1}{N}\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)^\top
\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W} ^ \top\right)
\]</span></p>
<p>and the statistic is:</p>
<p><span class="math display">\[
\hat{\tau} ^ \top \left[\frac{1}{N^2}\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)^\top
\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)\right] ^{-1}\hat{\tau}
\]</span> or, in terms of <span class="math inline">\(\hat{m} = N \hat{\tau}\)</span>:</p>
<p><span id="eq-cmtest_m"><span class="math display">\[
\hat{m} ^ \top\left[\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)^\top
\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)\right]^{-1} \hat{m}
\tag{5.40}\]</span></span></p>
<p>which is, under H<sub>0</sub>, a chi-squared with <span class="math inline">\(J\)</span> degrees of freedom. Different flavors of the test are obtained using different estimators of <span class="math inline">\(\mathcal{W}\)</span> and <span class="math inline">\(\mathcal{H}\)</span>:</p>
<ul>
<li>the first uses the expected value of the estimators of <span class="math inline">\(\mathcal{H}\)</span> and <span class="math inline">\(\mathcal{W}\)</span> which are respectively: <span class="math inline">\(\mbox{E} \frac{\partial \ln L}{\partial \theta \partial \theta ^ \top}(\hat{\theta},Z) / N\)</span> and <span class="math inline">\(\mbox{E} \frac{\partial m(\hat{\theta}, Z)}{\partial \theta} / N\)</span>,</li>
<li>the second uses the same expressions without the expectation: <span class="math inline">\(\frac{\partial \ln L}{\partial \theta \partial \theta ^ \top}(\hat{\theta},Z)/N\)</span> and <span class="math inline">\(\frac{\partial m(\hat{\theta}, Z)}{\partial \theta} / N\)</span>,</li>
<li>the third uses the information equality to estimate <span class="math inline">\(\mathcal{H}\)</span> by <span class="math inline">\(- \hat{G}^\top \hat{G} / N\)</span> and the generalized information equality to estimate <span class="math inline">\(\mathcal{W}\)</span> by <span class="math inline">\(- \hat{G}^\top \hat{M}/N\)</span>.</li>
</ul>
<p>The last one is particularly convenient, as it only requires the <span class="math inline">\(\hat{G}\)</span> matrix of the contributions to the gradient and the <span class="math inline">\(\hat{M}\)</span> matrix containing the contributions to the empirical moment vector. Rearranging terms and using the fact that <span class="math inline">\(\hat{m} = \hat{M} j\)</span> with <span class="math inline">\(j\)</span> a vector of 1s, <a href="#eq-cmtest_m">Equation&nbsp;<span>5.40</span></a> becomes:</p>
<p><span id="eq-opg_cmtest"><span class="math display">\[
j^\top \hat{M} \left[M^\top\left(I - \hat{G}(\hat{G}^\top \hat{G})^{-1}
\hat{G}^\top\right)\hat{M}\right] ^ {-1} \hat{M}^\top j
\tag{5.41}\]</span></span></p>
<p>which is just the explained sum of squares of a regression of a vector of 1s on <span class="math inline">\(\hat{G}\)</span> and <span class="math inline">\(\hat{M}\)</span>. To see that, start with the expression of the explained sum of squares which is, denoting <span class="math inline">\(y\)</span> the response and <span class="math inline">\(X\)</span> the matrix of covariates: <span class="math inline">\(y^\top X(X^\top X)^{-1}X^\top y\)</span>. In our case, the response is <span class="math inline">\(\iota\)</span> and the matrix of covariates <span class="math inline">\((\hat{G}\; \hat{M})\)</span>. Therefore, the explained sum of squares is:</p>
<p><span id="eq-ess_cmtest"><span class="math display">\[
j^\top (\hat{G}\; \hat{M})
\left(
\begin{array}{cc}
\hat{G}^\top \hat{G} &amp; \hat{G}^\top \hat{M}\\
\hat{M}^\top \hat{G} &amp; \hat{M}^\top \hat{M}
\end{array}
\right)  ^ {-1}
\left(
\begin{array}{c}
\hat{G}^\top \\
\hat{M}^\top
\end{array}
\right)
j
\tag{5.42}\]</span></span></p>
<p>But all the columns of <span class="math inline">\(\hat{G}\)</span> sum to 0 (<span class="math inline">\(\hat{G}^\top j=0\)</span>), so that <a href="#eq-ess_cmtest">Equation&nbsp;<span>5.42</span></a> reduces <span class="math inline">\(j ^ \top \hat{M} C \hat{M} ^ \top j\)</span>, where <span class="math inline">\(C\)</span> is the lower right square matrix in the formula of the partitioned inverse of the matrix in <a href="#eq-ess_cmtest">Equation&nbsp;<span>5.42</span></a> and is the matrix in bracket in <a href="#eq-opg_cmtest">Equation&nbsp;<span>5.41</span></a> (see for example <span class="citation" data-cites="GREE:03">Greene (<a href="#ref-GREE:03" role="doc-biblioref">2003</a>)</span>, equation A-74, page 824).</p>
<p>The conditional moment test can be used to test the hypothesis of normality, homoskedasticity and omitted variables. For the normality hypothesis, the theoretical moments are <span class="math inline">\(\mbox{E}(\epsilon_n ^ 3 \mid x_n) = 0\)</span> and <span class="math inline">\(\mbox{E}(\epsilon_n ^ 4 - 3 \sigma ^ 2 \mid x_n) = 0\)</span> and the empirical counterparts are:</p>
<p><span id="eq-emp_moments_normal"><span class="math display">\[
\left\{
\begin{array}{l}
\frac{1}{N}\sum_{n} \hat{\epsilon}_n ^ 3 \\
\frac{1}{N}\sum_{n} \hat{\epsilon}_n ^ 4 - 3 \hat{\sigma} ^ 2\\
\end{array}
\right.
\tag{5.43}\]</span></span></p>
<p>For the homoskedasticity hypothesis, the theoretical moments are <span class="math inline">\(\mbox{E}\left(w_n(\epsilon_n ^ 2 - \sigma ^ 2) | x_n\right)=0\)</span>, where, under the alternative hypothesis, <span class="math inline">\(w\)</span> are variables that enter the skedastic function and the empirical counterparts are:</p>
<p><span id="eq-emp_moments_homosc"><span class="math display">\[
\frac{1}{N}\sum_{n} w_n(\hat{\epsilon}_n ^2 -\hat{\sigma} ^ 2)
\tag{5.44}\]</span></span></p>
<p>For the omitted variables test, the theoretical moments are <span class="math inline">\(\mbox{E}(w_n\hat{\epsilon}_n | x_n)=0\)</span> and the empirical counterparts are:</p>
<p><span id="eq-emp_moments_omit_var"><span class="math display">\[
\frac{1}{N}\sum_{n} w_n\hat{\epsilon}_n
\tag{5.45}\]</span></span></p>
<p>As an example, we test the normality hypothesis in the context of the generalized production function previously estimated:</p>
<p><span class="math display">\[
\epsilon = \ln y + \theta \ln y - \left(\alpha + \sum_{j=1} ^ {J-1} \beta_j
\ln z_{j} ^ * + \beta_J^*\ln z_J + \ln z_J\right) \sim \mathcal{N} (0, \sigma^2)
\]</span></p>
<p>We first extract the fitted coefficients of the model <code>htheta</code>, we matrix of the individual contribution to the gradient <code>G</code> and the hessian <code>H</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">htheta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">gpf</span><span class="op">)</span></span>
<span><span class="va">lnlest</span> <span class="op">&lt;-</span> <span class="fu">zellner_revankar</span><span class="op">(</span><span class="va">htheta</span>, y <span class="op">=</span> <span class="va">y</span>, Z <span class="op">=</span> <span class="va">Z</span><span class="op">)</span></span>
<span><span class="va">G</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="va">lnlest</span>, <span class="st">"gradient"</span><span class="op">)</span></span>
<span><span class="va">H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="va">lnlest</span>, <span class="st">"hessian"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Working on the reparametrized version of the model, we then transform the matrix of covariates, compute <span class="math inline">\(\epsilon\)</span> the matrix of the individual contribution to the moments <span class="math inline">\(M\)</span> and the moment conditions <span class="math inline">\(m\)</span> which is a vector containing the sum of the columns of <span class="math inline">\(M\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Zm</span> <span class="op">&lt;-</span> <span class="va">Z</span></span>
<span><span class="va">Zm</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">Z</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">-</span> <span class="va">Z</span><span class="op">[</span>, <span class="fl">4</span><span class="op">]</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="va">Zm</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">htheta</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">Z</span><span class="op">[</span>, <span class="fl">4</span><span class="op">]</span> <span class="op">+</span> <span class="va">htheta</span><span class="op">[</span><span class="st">"lambda"</span><span class="op">]</span> <span class="op">*</span> <span class="va">y</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">^</span> <span class="fl">3</span>, <span class="va">eps</span> <span class="op">^</span> <span class="fl">4</span> <span class="op">-</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">htheta</span><span class="op">[</span><span class="st">"sigma"</span><span class="op">]</span> <span class="op">^</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="va">M</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="fl">2</span>, <span class="va">sum</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, the derivatives of <span class="math inline">\(m\)</span> with respect to the parameters of the model are computed to obtain the <span class="math inline">\(W\)</span> matrix. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">Z</span>, <span class="fl">2</span>, <span class="va">sum</span><span class="op">)</span>,</span>
<span>              <span class="fl">3</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span>, <span class="fl">0</span>,</span>
<span>              <span class="op">-</span><span class="fl">4</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">^</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">Z</span>, <span class="fl">2</span>, <span class="va">sum</span><span class="op">)</span>,</span>
<span>              <span class="fl">4</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eps</span> <span class="op">^</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span>,</span>
<span>              <span class="op">-</span> <span class="fl">12</span> <span class="op">*</span> <span class="va">htheta</span><span class="op">[</span><span class="st">"sigma"</span><span class="op">]</span> <span class="op">^</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>            ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first compute the test using minus the hessian to estimate the information and the matrix of the analytical derivatives of <span class="math inline">\(m\)</span> just computed: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">Q1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">M</span> <span class="op">-</span> <span class="va">G</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">W</span><span class="op">)</span></span>
<span><span class="fu">quad_form</span><span class="op">(</span><span class="va">m</span>, <span class="va">Q1</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.284</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The statistic is 1.28, which is far less than the critical value for a <span class="math inline">\(\chi^2\)</span> with 2 degrees of freedom, even at the 10% level. The hypothesis of normality is therefore not rejected. We then compute the version of the test based on the OPG to estimate the information matrix and on <span class="math inline">\(G^\top M\)</span> to estimate <span class="math inline">\(W\)</span>. The statistic can be computed by using matrix algebra or by taking the explained sum of squares in a regression of a column of 1 on <span class="math inline">\(G\)</span> and <span class="math inline">\(M\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Q2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">M</span> <span class="op">-</span> <span class="va">G</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">G</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span></span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">G</span>, <span class="va">M</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">quad_form</span><span class="op">(</span><span class="va">m</span>, <span class="va">Q2</span><span class="op">)</span></span>
<span><span class="co">## [1] 11.78</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">N</span><span class="op">)</span></span>
<span><span class="fu">rsq</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">j</span> <span class="op">~</span> <span class="va">G</span> <span class="op">+</span> <span class="va">M</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">N</span></span>
<span><span class="co">## [1] 11.78</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that this second version of the test leads to a much higher value of the statistic and a probability value of 0.003.</p>
<p></p>
</section><section id="tests-for-non-nested-models" class="level3"><h3 class="anchored" data-anchor-id="tests-for-non-nested-models">Tests for non-nested models</h3>
<p></p>
<section id="vuong-test" class="level4"><h4 class="anchored" data-anchor-id="vuong-test">Vuong test</h4>
<p><span class="citation" data-cites="VUON:89">Vuong (<a href="#ref-VUON:89" role="doc-biblioref">1989</a>)</span> proposed a test for non-nested models. He considered two competing models characterized by densities <span class="math inline">\(f(y|z; \beta)\)</span> and <span class="math inline">\(g(y|z; \gamma)\)</span>. Denoting <span class="math inline">\(h(y | z)\)</span> the true conditional density, the distance of the first model to the true model is measured by the minimum Kullback-Leibler information criterion (<strong>KLIC</strong>): <span class="math display">\[
D_f = \mbox{E}^0\left[\ln h(y\mid z)\right] - \mbox{E}^0\left[\ln f(y\mid z;
\beta_*)\right]
\]</span></p>
<p>where <span class="math inline">\(\mbox{E}^0\)</span> is the expected value using the true joint distribution of <span class="math inline">\((y, X)\)</span> and <span class="math inline">\(\beta_*\)</span> is the pseudo-true value of <span class="math inline">\(\beta\)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> As the true model is unobserved, denoting <span class="math inline">\(\theta^\top = (\beta ^ \top, \gamma ^ \top)\)</span>, we consider the difference of the KLIC distance to the true model of model <span class="math inline">\(G_\gamma\)</span> and model <span class="math inline">\(F_\beta\)</span>:</p>
<p><span class="math display">\[
\Lambda(\theta) = D_g - D_f = \mbox{E}^0\left[\ln f(y\mid z;
\beta_*)\right]- \mbox{E}^0\left[\ln g(y\mid z; \gamma_*)\right] =
\mbox{E}^0\left[\ln \frac{f(y\mid z; \beta_*)}{g(y\mid z;
\gamma_*)}\right]
\]</span></p>
<p>The null hypothesis is that the distances of the two models to the true models are equal or, equivalently, that: <span class="math inline">\(\Lambda=0\)</span>. The alternative hypothesis is either <span class="math inline">\(\Lambda&gt;0\)</span>, which means that the first model is better than the second or <span class="math inline">\(\Lambda&lt;0\)</span>, which means that the second model is better than the first. Denoting, for a given random sample of size <span class="math inline">\(N\)</span>, <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\gamma}\)</span> the maximum likelihood estimators of the two models and <span class="math inline">\(\ln L_f(\hat{\beta})\)</span> and <span class="math inline">\(\ln L_g(\hat{\gamma})\)</span> the corresponding values of the log-likelihood functions, <span class="math inline">\(\Lambda\)</span> can be consistently estimated by:</p>
<p><span class="math display">\[
\hat{\Lambda}_N = \frac{1}{N} \sum_{n = 1}^N \left(\ln f(y_n \mid
x_n, \hat{\beta}) - \ln g(y_n \mid x_n, \hat{\gamma})\right) =
\frac{1}{N} \left(\ln L_f(\hat{\beta}) - \ln L_g(\hat{\gamma})\right)
\]</span></p>
<p>which is the likelihood ratio divided by the sample size. Note that the statistic of the standard likelihood ratio test, suitable for nested models is <span class="math inline">\(2 \left(\ln L^f(\hat{\beta}) - \ln L^g(\hat{\gamma})\right)\)</span>, which is <span class="math inline">\(2 N \hat{\Lambda}_N\)</span>. The variance of <span class="math inline">\(\Lambda\)</span> is:</p>
<p><span class="math display">\[
\omega^2_* = \mbox{V}^o \left[\ln \frac{f(y \mid x; \beta_*)}{g(y
\mid x; \gamma_*)}\right]
\]</span></p>
<p>which can be consistently estimated by:</p>
<p><span class="math display">\[
\hat{\omega}_N^2 = \frac{1}{N} \sum_{n = 1} ^ N  \left(\ln f(y_n \mid
x_n, \hat{\beta}) - \ln g(y_n \mid x,_n \hat{\gamma})\right) ^ 2 -
\hat{\Lambda}_N ^ 2
\]</span></p>
<p>Three different cases should be considered when the two models are:</p>
<ul>
<li>nested, <span class="math inline">\(\omega^2_*\)</span> is necessarily 0,</li>
<li>overlapping (which means than the models coincide for some values of the parameters), <span class="math inline">\(\omega^2_*\)</span> <em>may be</em> equal to 0 or not,</li>
<li>strictly non-nested, <span class="math inline">\(\omega^2_*\)</span> is necessarily strictly positive.</li>
</ul>
<p>The distribution of the statistic depends on whether <span class="math inline">\(\omega^2_*\)</span> is zero or positive. If <span class="math inline">\(\omega^2_*\)</span> is positive, the statistic is <span class="math inline">\(\hat{T}_N = \sqrt{N}\frac{\hat{\Lambda}_N}{\hat{\omega}_N}\)</span> and, under the null hypothesis that the two models are equivalent, follows a standard normal distribution. This is the case for strictly non-nested models.</p>
<p>On the contrary, if <span class="math inline">\(\omega^2_* = 0\)</span>, the distribution is much more complicated. We need to define two matrices: <span class="math inline">\(A\)</span> contains the expected values of the second derivatives of <span class="math inline">\(\Lambda\)</span>:</p>
<p><span class="math display">\[
A(\theta_*) = \mbox{E}^0\left[\frac{\partial^2 \Lambda}{\partial \theta
\partial \theta ^ \top}\right] =
\mbox{E}^0\left[\begin{array}{cc}
\frac{\partial^2 \ln f}{\partial \beta \partial \beta ^
\top} &amp; 0 \\
0 &amp; -\frac{\partial^2 \ln g}{\partial \beta \partial \beta ^
\top}
\end{array}\right]
=
\left[
\begin{array}{cc}
A_f(\beta_*) &amp; 0 \\
0 &amp; - A_g(\gamma_*)
\end{array}
\right]
\]</span></p>
<p>and <span class="math inline">\(B\)</span> the variance of its first derivatives:</p>
<p><span class="math display">\[
\begin{array}{rcl}
B(\theta_*) =
\mbox{E}^0\left[\frac{\partial \Lambda}{\partial
\theta}\frac{\partial \Lambda}{\partial \theta ^ \top}\right]&amp;=&amp;
\mbox{E}^0\left[
\left(\frac{\partial \ln f}{\partial \beta},
- \frac{\partial \ln g}{\partial \gamma} \right)
\left(\frac{\partial \ln f}{\partial \beta ^ \top},
- \frac{\partial \ln g}{\partial \gamma ^ \top} \right)
\right]\\
&amp;=&amp; \mbox{E}^0\left[
\begin{array}{cc}
\frac{\partial \ln f}{\partial \beta} \frac{\partial \ln f}{\partial
\beta^\top} &amp;
- \frac{\partial \ln f}{\partial \beta} \frac{\partial \ln g}{\partial
\gamma ^ \top} \\
- \frac{\partial \ln g}{\partial \gamma} \frac{\partial \ln f}{\partial
  \beta^\top} &amp;
\frac{\partial \ln g}{\partial \gamma} \frac{\partial \ln g}{\partial \gamma^\top}
\end{array}
\right]
\end{array}
\]</span></p>
<p>or:</p>
<p><span class="math display">\[
B(\theta_*) =
\left[
\begin{array}{cc}
B_f(\beta_*) &amp; - B_{fg}(\beta_*, \gamma_*) \\
- B_{gf}(\beta_*, \gamma_*) &amp; B_g(\gamma_*)
\end{array}
\right]
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
W(\theta_*) =  B(\theta_*) \left[-A(\theta_*)\right] ^ {-1}=
\left[
\begin{array}{cc}
-B_f(\beta_*) A^{-1}_f(\beta_*) &amp; - B_{fg}(\beta_*, \gamma_*)
A^{-1}_g(\gamma_*) \\
B_{gf}(\gamma_*, \beta_*) A^{-1}_f(\beta_*) &amp; B_g(\gamma_*)
A^{-1}_g(\gamma_*)
\end{array}
\right]
\]</span></p>
<p>Denote <span class="math inline">\(\lambda_*\)</span> the eigenvalues of <span class="math inline">\(W\)</span>. When <span class="math inline">\(\omega_*^2 = 0\)</span> (which is always the case for nested models), the statistic is the one used in the standard likelihood ratio test: <span class="math inline">\(2 (\ln L_f - \ln L_g) = 2 N \hat{\Lambda}_N\)</span> which, under the null, follows a weighted <span class="math inline">\(\chi ^ 2\)</span> distribution with weights equal to <span class="math inline">\(\lambda_*\)</span>. The Vuong test can be seen in this context as a more robust version of the standard likelihood ratio test, because it doesn’t assume, under the null, that the larger model is correctly specified.</p>
<p>Note that, if the larger model is correctly specified, the information matrix equality implies that <span class="math inline">\(B_f(\theta_*)=-A_f(\theta_*)\)</span>. In this case, the two matrices on the diagonal of <span class="math inline">\(W\)</span> reduces to <span class="math inline">\(-I_{K_f}\)</span> and <span class="math inline">\(I_{K_g}\)</span>, the trace of <span class="math inline">\(W\)</span> to <span class="math inline">\(K_g - K_f\)</span> and the distribution of the statistic under the null reduce to a <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(K_g - K_f\)</span> degrees of freedom.</p>
<p>The <span class="math inline">\(W\)</span> matrix can be consistently estimated by computing the first and the second derivatives of the likelihood functions of the two models for <span class="math inline">\(\hat{\theta}\)</span>. For example,</p>
<p><span class="math display">\[
\hat{A}_f(\hat{\beta}) = \frac{1}{N}
\sum_{n= 1} ^ N \frac{\partial^2 \ln f}{\partial \beta \partial
\beta ^ \top}(\hat{\beta}, x_n, y_n)
\]</span></p>
<p><span class="math display">\[ \hat{B}_{fg}(\hat{\theta})= \frac{1}{N} \sum_{n=1}^N
\frac{\partial \ln f}{\partial \beta}(\hat{\beta}, x_n, y_n)
\frac{\partial \ln g}{\partial \gamma^\top}(\hat{\gamma}, x_n, y_n)
\]</span></p>
<p>For the overlapping case, the test should be performed in two steps:</p>
<ul>
<li>the first step consists of testing whether <span class="math inline">\(\omega_*^*\)</span> is 0 or not. This hypothesis is based on the statistic <span class="math inline">\(N \hat{\omega} ^ 2\)</span> which, under the null (<span class="math inline">\(\omega_*^2=0\)</span>) follows a weighted <span class="math inline">\(\chi ^ 2\)</span> distributions with weights equal to <span class="math inline">\(\lambda_* ^ 2\)</span>. If the null hypothesis is not rejected, the test stops at this step, and the conclusion is that the two models are equivalent;</li>
<li>if the null hypothesis is rejected, the second step consists of applying the test for non-nested models previously described.</li>
</ul>
<p> <span class="citation" data-cites="SHI:15">Shi (<a href="#ref-SHI:15" role="doc-biblioref">2015</a>)</span> provides an example of simulations of non-nested linear models that shows that the distribution of the Vuong statistic can be very different from a standard normal. The data generating process used for the simulations is:</p>
<p><span class="math display">\[
y = 1 + \sum_{k = 1} ^ {K_f} z^f_k + \sum_{k = 1} ^ {K_g} z^g_k + \epsilon
\]</span></p>
<p>where <span class="math inline">\(z^f\)</span> is the set of <span class="math inline">\(K_f\)</span> covariates that are used in the first model and <span class="math inline">\(z^g\)</span> the set of <span class="math inline">\(K_g\)</span> covariates used in the second model and <span class="math inline">\(\epsilon \sim N(0, 1 - a ^ 2)\)</span>. <span class="math inline">\(z^f_k \sim N(0, a / \sqrt{K_f})\)</span> and <span class="math inline">\(z^g_k \sim N(0, a / \sqrt{K_g})\)</span>, so that the variance explained by the two competing models is the same (equal to <span class="math inline">\(a ^ 2\)</span>) and the null hypothesis of the Vuong test is true. The <code><a href="https://rdrr.io/pkg/micsr/man/vuong_sim.html">micsr::vuong_sim</a></code> enables to simulate values of the Vuong test. As in <span class="citation" data-cites="SHI:15">Shi (<a href="#ref-SHI:15" role="doc-biblioref">2015</a>)</span>, we use a very different degree of parametrization for the two models, with <span class="math inline">\(K_f = 15\)</span> and <span class="math inline">\(K_G = 1\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Vuong</span> <span class="op">&lt;-</span> <span class="fu">vuong_sim</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">100</span>, R <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>                   Kf <span class="op">=</span> <span class="fl">15</span>, Kg <span class="op">=</span> <span class="fl">1</span>, a <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Vuong</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.838 2.950 1.672 1.377 1.527 1.922</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Vuong</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.124</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">Vuong</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1.96</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.204</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can see that the mean of the statistic for the 1000 replications is far away from 0, which means that the numerator of the Vuong statistic is seriously biased. A total of 20.4% of the values of the statistic are greater than the critical value so that the Vuong test will lead in such context to a noticeable over-rejection. The empirical density function is shown in <a href="#fig-vuong_stat_dist">Figure&nbsp;<span>5.5</span></a>, along with the normal density. </p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-vuong_stat_dist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="maximum_likelihood_files/figure-html/fig-vuong_stat_dist-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.5: Distribution of the Vuong statistic</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="citation" data-cites="SHI:15">Shi (<a href="#ref-SHI:15" role="doc-biblioref">2015</a>)</span> proposed a non-degenerate Vuong test which corrects the small sample bias of the numerator of the Vuong statistic and inflates the denominator by adding a constant. </p>
</section><section id="an-example-generalized-production-function-vs.-translog-function" class="level4"><h4 class="anchored" data-anchor-id="an-example-generalized-production-function-vs.-translog-function">An example: generalized production function vs.&nbsp;translog function</h4>
<p> A popular alternative to the generalized production function is the translog function, which is:</p>
<p><span id="eq-translog_production"><span class="math display">\[
\ln y = \alpha + \sum_{j=1} ^ J \beta_j \ln q_j + \frac{1}{2}
\sum_{j=1}^J \sum_{k=1} ^ J \beta_{jk} \ln q_j \ln q_k
\tag{5.46}\]</span></span></p>
<p>The elasticity of the production with a factor is:</p>
<p><span class="math display">\[
\frac{\partial \ln y}{\partial \ln x_j} = \beta_j + \sum_{k=1} ^ J
\beta_{jk} \ln q_k
\]</span></p>
<p>and the scale elasticity is just the sum of these <span class="math inline">\(J\)</span> elasticities:</p>
<p><span class="math display">\[
\epsilon = \sum_{j=1} ^ J \beta_j + \sum_{j=1} ^ J \sum_{k=1} ^ J
\beta_{jk} \ln q_k =
\sum_{j=1} ^ J \beta_j + \sum_{k=1} ^ J \ln q_k \sum_{j=1} ^ J\beta_{jk}
\]</span></p>
<p>The constant returns to scale hypothesis therefore implies that <span class="math inline">\(\sum_j \beta_j = 1\)</span> and <span class="math inline">\(\sum_{j=1} ^ J \beta_{jl} = 0 \;\forall\;k\)</span>. We fit <a href="#eq-translog_production">Equation&nbsp;<span>5.46</span></a> using <code>loglm</code>: <!-- With the same trick as the one used in @sec-system_equation, @eq-translog_production can be rewritten as: --></p>
<!-- $$ -->
<!-- \ln y =  -->
<!-- \sum_j ^{J-1}\beta_j q_j^* +  -->
<!-- \frac{1}{2} \sum_{j=1} ^ {J - 1} \beta_{jj} q_j ^ * +   -->
<!-- \sum_{j=1} ^ {J - 1} \sum_{k>j} ^ {J - 1} \beta_{jk}  -->
<!-- q_j ^ * q_k ^ * + \ln q_J \sum_{j=1} ^ {J-1} \beta_{jJ} ^ * q_j^* + \beta_J ^ *\ln q_J + \frac{1}{2}\beta_{JJ}^* \ln^2q_J + \ln q_j -->
<!-- $$ -->
<!-- with $q_j ^ * = \ln q_j / q_I$, $\beta_J ^ * = 1 -\sum_{n=1}^{J-1}\beta_j$, $\beta_{jJ}^* = \sum_{k=1}^{J-1}\beta_{jk}$ and $\beta_{JJ} ^ * sum_{j=1}^J \beta_{jJ} ^ *$ and the constant return to scales hypothesis is $\beta_J ^ *, \beta_{jJ}^* = \beta_{JJ} = 0$. -->
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- llcont.micsr <- function(x) x$logLik -->
<!-- llcont.maxLik <- function(x) x$objectiveFn(x$estimate, Z = Z, y = y, gradient = FALSE, hessian = FALSE) -->
<!-- ``` -->
<!-- And the constant return to scale hypothesis implies that $\beta_\lambda ^ *$ and $\beta_{jJ}^*, \forall j = 1\ldots J-1$ equal 0. -->
<div class="cell" data-layout-align="center">

</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">trsl</span> <span class="op">&lt;-</span> <span class="fu">loglm</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span> <span class="op">+</span> </span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>, <span class="va">aps</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>All that is required to compute the Vuong test for non-nested models is the contributions to the log-likelihood for both models:<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">est_gpf</span> <span class="op">&lt;-</span> <span class="fu">zellner_revankar</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">gpf</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">y</span>, Z <span class="op">=</span> <span class="va">Z</span><span class="op">)</span></span>
<span><span class="va">lnl_gpf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">est_gpf</span><span class="op">)</span></span>
<span><span class="va">lnl_trsl</span> <span class="op">&lt;-</span> <span class="va">trsl</span><span class="op">$</span><span class="va">value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then compute the average likelihood ratio statistic (<code>L</code>), its variance (<code>w2</code>) and the statistic: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lnl_gpf</span><span class="op">)</span></span>
<span><span class="va">L</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lnl_gpf</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lnl_trsl</span><span class="op">)</span></span>
<span><span class="va">w2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">lnl_gpf</span> <span class="op">-</span> <span class="va">lnl_trsl</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="va">L</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="va">vuong_stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">*</span> <span class="va">L</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">w2</span><span class="op">)</span></span>
<span><span class="va">vuong_stat</span></span>
<span><span class="co">## [1] -1.147</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The probability value is 0.126 so that the test concludes that the two models are indistinguishable, although the difference of their log-likelihood is quite high:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lnl_gpf</span><span class="op">)</span></span>
<span><span class="co">## [1] -60.28</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lnl_trsl</span><span class="op">)</span></span>
<span><span class="co">## [1] -55.86</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-ALDR:NELS:84" class="csl-entry" role="doc-biblioentry">
Aldrich, J. H., and F. D. Nelson. 1984. <em>Linear Probability, Logit, and Probit Models</em>. Sage University Press.
</div>
<div id="ref-BERN:HALL:HALL:HAUS:74" class="csl-entry" role="doc-biblioentry">
Berndt, Ernst R., Bronwyn Hall, Robert Hall, and Jerry Hausman. 1974. <span>“Estimation and Inference in Nonlinear Structural Models.”</span> In <em>Annals of Economic and Social Measurement, Volume 3, Number 4</em>, 653–65. National Bureau of Economic Research, Inc. <a href="https://EconPapers.repec.org/RePEc:nbr:nberch:10206">https://EconPapers.repec.org/RePEc:nbr:nberch:10206</a>.
</div>
<div id="ref-CAME:TRIV:05" class="csl-entry" role="doc-biblioentry">
Cameron, A. Colin, and Pravin K. Trivedi. 2005. <em>Microeconometrics</em>. Cambridge University Press. <a href="https://EconPapers.repec.org/RePEc:cup:cbooks:9780521848053">https://EconPapers.repec.org/RePEc:cup:cbooks:9780521848053</a>.
</div>
<div id="ref-COX:SNEL:89" class="csl-entry" role="doc-biblioentry">
Cox, D. R, and E. J. Snell. 1989. <em>Analysis of Binary Data</em>. 2nd ed. Monographs on statistics; applied probability.
</div>
<div id="ref-CRAG:UHLE:70" class="csl-entry" role="doc-biblioentry">
Cragg, John G., and Russell S. Uhler. 1970. <span>“<span class="nocase">The Demand for Automobiles</span>.”</span> <em>Canadian Journal of Economics</em> 3 (3): 386–406. <a href="https://ideas.repec.org/a/cje/issued/v3y1970i3p386-406.html">https://ideas.repec.org/a/cje/issued/v3y1970i3p386-406.html</a>.
</div>
<div id="ref-DAVI:MACK:04" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, and James G. MacKinnon. 2004. <em>Econometric Theory and Methods</em>. Oxford University Press.
</div>
<div id="ref-FAVE:PESA:SHAR:94" class="csl-entry" role="doc-biblioentry">
Favero, Carlo A., M. Hashem Pesaran, and Sunil Sharma. 1994. <span>“A Duration Model of Irreversible Oil Investment: Theory and Empirical Evidence.”</span> <em>Journal of Applied Econometrics</em> 9: S95–112. <a href="http://www.jstor.org/stable/2285225">http://www.jstor.org/stable/2285225</a>.
</div>
<div id="ref-GREE:03" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2003. <em>Econometrics Analysis</em>. 5th ed. Pearson.
</div>
<div id="ref-HENN:TOOM:11" class="csl-entry" role="doc-biblioentry">
Henningsen, Arne, and Ott Toomet. 2011. <span>“maxLik: A Package for Maximum Likelihood Estimation in <span>R</span>.”</span> <em>Computational Statistics</em> 26 (3): 443–58. <a href="https://doi.org/10.1007/s00180-010-0217-1">https://doi.org/10.1007/s00180-010-0217-1</a>.
</div>
<div id="ref-MADD:83" class="csl-entry" role="doc-biblioentry">
Maddala, G. S. 1983. <em>Limited-Dependent and Qualitative Variables in Econometrics</em>. Econometric Society Monographs. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511810176">https://doi.org/10.1017/CBO9780511810176</a>.
</div>
<div id="ref-MAGE:90" class="csl-entry" role="doc-biblioentry">
Magee, Lonnie. 1990. <span>“R2 Measures Based on Wald and Likelihood Ratio Joint Significance Tests.”</span> <em>The American Statistician</em> 44 (3): 250–53. <a href="http://www.jstor.org/stable/2685352">http://www.jstor.org/stable/2685352</a>.
</div>
<div id="ref-MILL:09" class="csl-entry" role="doc-biblioentry">
Miller, Nathan H. 2009. <span>“Strategic Leniency and Cartel Enforcement.”</span> <em>American Economic Review</em> 99 (3): 750–68. <a href="https://doi.org/10.1257/aer.99.3.750">https://doi.org/10.1257/aer.99.3.750</a>.
</div>
<div id="ref-NAGE:91" class="csl-entry" role="doc-biblioentry">
Nagelkerke, Nico J. D. 1991. <span>“A Note on a General Definition of the Coefficient of Determination.”</span> <em>Biometrika</em> 78: 691–92.
</div>
<div id="ref-NEWE:85" class="csl-entry" role="doc-biblioentry">
Newey, Whitney K. 1985. <span>“Maximum Likelihood Specification Testing and Conditional Moment Tests.”</span> <em>Econometrica</em> 53 (5): 1047–70. <a href="http://www.jstor.org/stable/1911011">http://www.jstor.org/stable/1911011</a>.
</div>
<div id="ref-SHI:15" class="csl-entry" role="doc-biblioentry">
Shi, Xiaoxia. 2015. <span>“A Nondegenerate Vuong Test.”</span> <em>Quantitative Economics</em>, 85–121.
</div>
<div id="ref-SKEE:VELL:99" class="csl-entry" role="doc-biblioentry">
Skeels, Christopher L., and Francis Vella. 1999. <span>“A Monte Carlo Investigation of the Sampling Behavior of Conditional Moment Tests in Tobit and Probit Models.”</span> <em>Journal of Econometrics</em> 92 (2): 275–94. <a href="https://doi.org/10.1016/S0304-4076(98)00092-X">https://doi.org/10.1016/S0304-4076(98)00092-X</a>.
</div>
<div id="ref-TAUC:85" class="csl-entry" role="doc-biblioentry">
Tauchen, George. 1985. <span>“Diagnostic Testing and Evaluation of Maximum Likelihood Models.”</span> <em>Journal of Econometrics</em> 30 (1): 415–43. <a href="https://doi.org/10.1016/0304-4076(85)90149-6">https://doi.org/10.1016/0304-4076(85)90149-6</a>.
</div>
<div id="ref-VAND:81" class="csl-entry" role="doc-biblioentry">
Vandaele, Walter. 1981. <span>“Wald, Likelihood Ratio, and Lagrange Multiplier Tests as an f Test.”</span> <em>Economics Letters</em> 8 (4): 361–65. <a href="https://doi.org/10.1016/0165-1765(81)90026-4">https://doi.org/10.1016/0165-1765(81)90026-4</a>.
</div>
<div id="ref-VEAL:ZIMM:96" class="csl-entry" role="doc-biblioentry">
Veall, Michael R., and Klaus F. Zimmermann. 1996. <span>“Pseudo-R2 Measures for Some Common Limited Dependent Variable Models.”</span> <em>Journal of Economic Surveys</em> 10 (3): 241–59. <a href="https://doi.org/10.1111/j.1467-6419.1996.tb00013.x">https://doi.org/10.1111/j.1467-6419.1996.tb00013.x</a>.
</div>
<div id="ref-VUON:89" class="csl-entry" role="doc-biblioentry">
Vuong, Quang H. 1989. <span>“Likelihood Ratio Tests for Selection and Non-Nested Hypotheses.”</span> <em>Econometrica</em> 57 (2): 397–33.
</div>
<div id="ref-ZELL:REVA:69" class="csl-entry" role="doc-biblioentry">
Zellner, Arnold, and N. S. Revankar. 1969. <span>“<span>Generalized Production Functions</span>.”</span> <em>Review of Economic Studies</em> 36 (2): 241–50. <a href="https://ideas.repec.org/a/oup/restud/v36y1969i2p241-250..html">https://ideas.repec.org/a/oup/restud/v36y1969i2p241-250..html</a>.
</div>
</div>
</section></section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>These models will be presented in details in the third part of the book.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We now have a matrix of second derivatives, denoted by <span class="math inline">\(\Psi\)</span>, which replaces the scalar second derivative <span class="math inline">\(\psi\)</span> in the previous section.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The initials of the authors of <span class="citation" data-cites="BERN:HALL:HALL:HAUS:74">Berndt et al. (<a href="#ref-BERN:HALL:HALL:HAUS:74" role="doc-biblioref">1974</a>)</span> who first proposed this estimator.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Actually, the Lagrange multiplier test is numerically different, only because the estimation of <span class="math inline">\(\sigma^2_\epsilon\)</span> is based on the constrained model.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See <span class="citation" data-cites="VAND:81">Vandaele (<a href="#ref-VAND:81" role="doc-biblioref">1981</a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>And previously used by <span class="citation" data-cites="CRAG:UHLE:70">Cragg and Uhler (<a href="#ref-CRAG:UHLE:70" role="doc-biblioref">1970</a>)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Note the use of the <code>inv</code> argument set to <code>FALSE</code> in the second call to <code>quad_form</code> as the matrix provided (<code>H2m1</code>) is a subset of the inverse of the covariance matrix of the estimator.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><span class="citation" data-cites="CAME:TRIV:05">Cameron and Trivedi (<a href="#ref-CAME:TRIV:05" role="doc-biblioref">2005</a>)</span>, page 260.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><span class="citation" data-cites="SKEE:VELL:99">Skeels and Vella (<a href="#ref-SKEE:VELL:99" role="doc-biblioref">1999</a>)</span>, eq. 2.13.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><span class="math inline">\(\beta_*\)</span> is called the pseudo-true value because <span class="math inline">\(f\)</span> may be an incorrect model.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>The object returned by <code>loglm</code> contains an element called <code>values</code> that is a vector of individual contributions to the log-likelihood.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../beyond_OLS.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Beyond the OLS estimator</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/non_spherical.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb57" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Maximum likelihood estimator {#sec-maximum_likelihood}</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: maximum_likelihood</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../_commonR.R"</span>)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>The **maximum likelihood** (**ML**) estimator is suitable in situations where the GDP</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>of the response is perfectly known (or supposed to be), up to some</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>unknown parameters. It is particularly useful for models where the response is not continuous but, for example, it is a count, a binomial or a multinomial variable.^<span class="co">[</span><span class="ot">These models will be presented in details in the third part of the book.</span><span class="co">]</span> @sec-ml_unique_parameter presents the ML estimator for the simplest case where there is a unique parameter to estimate. @sec-ml_general extends this basic model to the case where there is a vector of parameters to estimate. Finally @sec-ml_tests present different tests that are particularly useful in the context of ML estimation.</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## ML estimation of a unique parameter {#sec-ml_unique_parameter}</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>For a first informal view of this estimator, consider two very simple</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>one-parameter distribution functions:</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Poisson distribution}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution}</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the Poisson distribution, which is suitable for the distribution</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>    of count responses, i.e., responses which can take only non-negative</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>    integer values,</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the exponential distribution, which is often used for responses that</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>    represent a time span (for example, an unemployment spell).</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the ML estimator for the Poisson distribution {#sec-poisson_ml}</span></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Poisson distribution|(}</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>As an example of count data, we consider the <span class="in">`cartel`</span> data set of @MILL:09\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Miller} who analyzed the role of commitment to the lenient prosecution of early</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>confessors on cartel enforcement. The response <span class="in">`ncaught`</span> is a count, the</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>number of cartel discoveries per a 6-month period, for the 1985-2005</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>period. </span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>A Poisson variable is defined by the following mass probability</span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a>function:</span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a>P(y; \theta) = \frac{e ^ {-\theta} \theta ^ y}{y !}</span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a>$$ {#eq-poisson_probability}</span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a>where $\theta$ is the unique parameter of the distribution. An important</span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a>characteristic of this distribution is that $\mbox{E}(y)=\mbox{V}(y)=\theta$, which means</span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>that, for a Poisson variable, the variance equals the expectation. To</span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>have a first look at the relevance of this hypothesis for the response</span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a>of our example, we compute the sample mean and variance:</span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a>cartels <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">m =</span> <span class="fu">mean</span>(ncaught), <span class="at">v =</span> <span class="fu">var</span>(ncaught))</span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a>The two moments are of the same order of magnitude so that we can</span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a>confidently rely on the Poisson distribution hypothesis. The question is</span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a>now how to use the information from the sample to estimate the unique</span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a>parameter of the distribution. @fig-bars_poisson</span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a>represents the empirical distribution of the response and adds several</span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a>lines which represent the Poisson distribution for different values of</span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a>$\theta$.</span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-59"><a href="#cb57-59" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-60"><a href="#cb57-60" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-61"><a href="#cb57-61" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bars_poisson</span></span>
<span id="cb57-62"><a href="#cb57-62" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Empirical distribution and theoretical Poisson probabilities"</span></span>
<span id="cb57-63"><a href="#cb57-63" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">12</span></span>
<span id="cb57-64"><a href="#cb57-64" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb57-65"><a href="#cb57-65" aria-hidden="true" tabindex="-1"></a>Poisson <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rep</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">12</span>, R), <span class="at">lambda =</span> <span class="fu">rep</span>(<span class="fu">c</span>(2L, 5L, 8L), <span class="at">each =</span> <span class="dv">13</span>), <span class="at">P =</span> <span class="fu">dpois</span>(x, lambda)) <span class="sc">%&gt;%</span></span>
<span id="cb57-66"><a href="#cb57-66" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">lambda =</span> <span class="fu">factor</span>(lambda))</span>
<span id="cb57-67"><a href="#cb57-67" aria-hidden="true" tabindex="-1"></a>cartels <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(ncaught)) <span class="sc">+</span> <span class="fu">geom_bar</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(prop)), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>) <span class="sc">+</span></span>
<span id="cb57-68"><a href="#cb57-68" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">data =</span> Poisson, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> P, <span class="at">linetype =</span> lambda)) <span class="sc">+</span> </span>
<span id="cb57-69"><a href="#cb57-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> Poisson, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> P)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">linetype =</span> <span class="cn">NULL</span>)</span>
<span id="cb57-70"><a href="#cb57-70" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-71"><a href="#cb57-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-72"><a href="#cb57-72" aria-hidden="true" tabindex="-1"></a>The Poisson distribution is highly asymmetric and has a large mode for</span>
<span id="cb57-73"><a href="#cb57-73" aria-hidden="true" tabindex="-1"></a>$\theta = 2$, it gets more and more flat and less asymmetric as $\theta$</span>
<span id="cb57-74"><a href="#cb57-74" aria-hidden="true" tabindex="-1"></a>increases. From @fig-bars_poisson, we can see that the empirical distribution</span>
<span id="cb57-75"><a href="#cb57-75" aria-hidden="true" tabindex="-1"></a>is very different from the Poisson one for a value of $\theta$ as small</span>
<span id="cb57-76"><a href="#cb57-76" aria-hidden="true" tabindex="-1"></a>as 2 or as large as 8. For $\theta = 5$, there is a reasonable</span>
<span id="cb57-77"><a href="#cb57-77" aria-hidden="true" tabindex="-1"></a>correspondence between the empirical and the theoretical distribution.</span>
<span id="cb57-78"><a href="#cb57-78" aria-hidden="true" tabindex="-1"></a>To get unambiguously a value for our estimator, we need an objective</span>
<span id="cb57-79"><a href="#cb57-79" aria-hidden="true" tabindex="-1"></a>function that is a function of $\theta$ which is, in the context of this chapter, the</span>
<span id="cb57-80"><a href="#cb57-80" aria-hidden="true" tabindex="-1"></a>**likelihood function**. To construct it, it is much simpler to consider the sample as a</span>
<span id="cb57-81"><a href="#cb57-81" aria-hidden="true" tabindex="-1"></a>random sample. For the first observation $1$, given a value of $\theta$,</span>
<span id="cb57-82"><a href="#cb57-82" aria-hidden="true" tabindex="-1"></a>the probability of observing $y_1$ is $P(y_1; \theta)$. For the</span>
<span id="cb57-83"><a href="#cb57-83" aria-hidden="true" tabindex="-1"></a>second one, with the random sample hypothesis, the probability of</span>
<span id="cb57-84"><a href="#cb57-84" aria-hidden="true" tabindex="-1"></a>observing $y_2$ is $P(y_2;\theta)$ and is independent of $y_1$.</span>
<span id="cb57-85"><a href="#cb57-85" aria-hidden="true" tabindex="-1"></a>Therefore, the joint probability of observing $(y_1, y_2)$ is</span>
<span id="cb57-86"><a href="#cb57-86" aria-hidden="true" tabindex="-1"></a>$P(y_1;\theta)\times P(y_2;\theta)$ and, more generally, the probability</span>
<span id="cb57-87"><a href="#cb57-87" aria-hidden="true" tabindex="-1"></a>of observing the values of $y$ for the whole sample is the likelihood</span>
<span id="cb57-88"><a href="#cb57-88" aria-hidden="true" tabindex="-1"></a>function for a Poisson variable in a random sample which writes:</span>
<span id="cb57-89"><a href="#cb57-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-90"><a href="#cb57-90" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-91"><a href="#cb57-91" aria-hidden="true" tabindex="-1"></a>L(\theta, y) = \Pi_{n=1}^N P(y_n;\theta)</span>
<span id="cb57-92"><a href="#cb57-92" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-93"><a href="#cb57-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-94"><a href="#cb57-94" aria-hidden="true" tabindex="-1"></a>and the maximum likelihood estimator of $\theta$ is the value of $\theta$ which maximizes</span>
<span id="cb57-95"><a href="#cb57-95" aria-hidden="true" tabindex="-1"></a>the likelihood function. Note the change of notations:</span>
<span id="cb57-96"><a href="#cb57-96" aria-hidden="true" tabindex="-1"></a>$P(y_n; \theta)$ is the probability mass function, it is a function of</span>
<span id="cb57-97"><a href="#cb57-97" aria-hidden="true" tabindex="-1"></a>$y^\top = (y_1, y_2, \ldots, y_N)$ and computes a probability for a given (known) value of the</span>
<span id="cb57-98"><a href="#cb57-98" aria-hidden="true" tabindex="-1"></a>parameter of the distribution. The likelihood function $L(\theta, y)$</span>
<span id="cb57-99"><a href="#cb57-99" aria-hidden="true" tabindex="-1"></a>is written as a function of the unknown value of the parameter of the</span>
<span id="cb57-100"><a href="#cb57-100" aria-hidden="true" tabindex="-1"></a>distribution and its value depends on the realization of the response</span>
<span id="cb57-101"><a href="#cb57-101" aria-hidden="true" tabindex="-1"></a>vector $y$ on the sample.</span>
<span id="cb57-102"><a href="#cb57-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-103"><a href="#cb57-103" aria-hidden="true" tabindex="-1"></a>There are several good reasons to consider the logarithm of the</span>
<span id="cb57-104"><a href="#cb57-104" aria-hidden="true" tabindex="-1"></a>likelihood function instead of the likelihood itself. In particular,</span>
<span id="cb57-105"><a href="#cb57-105" aria-hidden="true" tabindex="-1"></a>being a product of $N$ terms, the likelihood will typically take very</span>
<span id="cb57-106"><a href="#cb57-106" aria-hidden="true" tabindex="-1"></a>high or very low values for large samples. Moreover, the logarithm</span>
<span id="cb57-107"><a href="#cb57-107" aria-hidden="true" tabindex="-1"></a>transforms a product in a sum, which is much more convenient. Note</span>
<span id="cb57-108"><a href="#cb57-108" aria-hidden="true" tabindex="-1"></a>finally that, as the logarithm is a strictly increasing function,</span>
<span id="cb57-109"><a href="#cb57-109" aria-hidden="true" tabindex="-1"></a>maximizing the likelihood or its logarithm leads to the same value of</span>
<span id="cb57-110"><a href="#cb57-110" aria-hidden="true" tabindex="-1"></a>$\theta$. Taking the log and replacing $P(y_n; \theta)$ by the expression given in @eq-poisson_probability,  we get:</span>
<span id="cb57-111"><a href="#cb57-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-112"><a href="#cb57-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-113"><a href="#cb57-113" aria-hidden="true" tabindex="-1"></a>\ln L(\theta, y) = \sum_{n=1} ^ N \ln \frac{e ^ {-\theta} \theta ^ y}{y !}</span>
<span id="cb57-114"><a href="#cb57-114" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-115"><a href="#cb57-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-116"><a href="#cb57-116" aria-hidden="true" tabindex="-1"></a>or, regrouping terms:</span>
<span id="cb57-117"><a href="#cb57-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-118"><a href="#cb57-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-119"><a href="#cb57-119" aria-hidden="true" tabindex="-1"></a>\ln L(\theta, y) = - N \theta + \ln \theta \sum_{n=1} ^ N y_n -</span>
<span id="cb57-120"><a href="#cb57-120" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N \ln y_n !</span>
<span id="cb57-121"><a href="#cb57-121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-122"><a href="#cb57-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-123"><a href="#cb57-123" aria-hidden="true" tabindex="-1"></a>The log-likelihood is therefore the sum of three terms, and note that the</span>
<span id="cb57-124"><a href="#cb57-124" aria-hidden="true" tabindex="-1"></a>last one doesn't depend on $\theta$.</span>
<span id="cb57-125"><a href="#cb57-125" aria-hidden="true" tabindex="-1"></a>For our sample, we have:</span>
<span id="cb57-126"><a href="#cb57-126" aria-hidden="true" tabindex="-1"></a>\idxfun{length}{base}\idxfun{lfactorial}{base}</span>
<span id="cb57-127"><a href="#cb57-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-128"><a href="#cb57-128" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-129"><a href="#cb57-129" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> cartels<span class="sc">$</span>ncaught</span>
<span id="cb57-130"><a href="#cb57-130" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb57-131"><a href="#cb57-131" aria-hidden="true" tabindex="-1"></a>sum_y <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb57-132"><a href="#cb57-132" aria-hidden="true" tabindex="-1"></a>sum_log_fact_y <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">lfactorial</span>(y))</span>
<span id="cb57-133"><a href="#cb57-133" aria-hidden="true" tabindex="-1"></a>lnl_poisson <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) <span class="sc">-</span> N <span class="sc">*</span> theta <span class="sc">+</span> <span class="fu">log</span>(theta) <span class="sc">*</span> sum_y <span class="sc">-</span></span>
<span id="cb57-134"><a href="#cb57-134" aria-hidden="true" tabindex="-1"></a>    sum_log_fact_y</span>
<span id="cb57-135"><a href="#cb57-135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-136"><a href="#cb57-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-137"><a href="#cb57-137" aria-hidden="true" tabindex="-1"></a>The last line of code define <span class="in">`lnl_poisson`</span> as a function of <span class="in">`theta`</span> which returns the value of the log likelihood function for a value of $\theta$ given the value of the responses in the <span class="in">`cartel`</span> data set. </span>
<span id="cb57-138"><a href="#cb57-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-139"><a href="#cb57-139" aria-hidden="true" tabindex="-1"></a>Taking some values of $\theta$, we compute in @tbl-poisson_probs the</span>
<span id="cb57-140"><a href="#cb57-140" aria-hidden="true" tabindex="-1"></a>corresponding values of the log likelihood function. The log likelihood function is inverse U-shaped, and the</span>
<span id="cb57-141"><a href="#cb57-141" aria-hidden="true" tabindex="-1"></a>maximum value for the integer values we've provided is reached for</span>
<span id="cb57-142"><a href="#cb57-142" aria-hidden="true" tabindex="-1"></a>$\theta = 5$. @fig-loglik_poisson presents the log likelihood function</span>
<span id="cb57-143"><a href="#cb57-143" aria-hidden="true" tabindex="-1"></a>as a smooth line in the neighborhood of $\theta = 5$, which indicates that the maximum of the log likelihood function occurs</span>
<span id="cb57-144"><a href="#cb57-144" aria-hidden="true" tabindex="-1"></a>for a value of $\theta$ between 5.0 and 5.5.</span>
<span id="cb57-145"><a href="#cb57-145" aria-hidden="true" tabindex="-1"></a>The first-order condition for a maximum is that the first derivative of</span>
<span id="cb57-146"><a href="#cb57-146" aria-hidden="true" tabindex="-1"></a>$\ln L$ with respect to $\theta$ is 0.</span>
<span id="cb57-147"><a href="#cb57-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-148"><a href="#cb57-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-149"><a href="#cb57-149" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-150"><a href="#cb57-150" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: 'asis'</span></span>
<span id="cb57-151"><a href="#cb57-151" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-152"><a href="#cb57-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-poisson_probs</span></span>
<span id="cb57-153"><a href="#cb57-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Poisson probabilities for different values of the parameter"</span></span>
<span id="cb57-154"><a href="#cb57-154" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span></span>
<span id="cb57-155"><a href="#cb57-155" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">tibble</span>(<span class="st">`</span><span class="at">$</span><span class="sc">\\</span><span class="at">theta$</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>, <span class="st">`</span><span class="at">$</span><span class="sc">\\</span><span class="at">ln L$</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">lnl_poisson</span>(theta)), <span class="at">digits =</span> <span class="dv">2</span>, <span class="at">booktabs =</span> <span class="cn">TRUE</span>, <span class="at">linesep =</span> <span class="st">""</span>, <span class="at">escape =</span> <span class="cn">FALSE</span>) <span class="co">#%&gt;%</span></span>
<span id="cb57-156"><a href="#cb57-156" aria-hidden="true" tabindex="-1"></a><span class="co">#  kableExtra::kable_styling(protect_latex = FALSE)</span></span>
<span id="cb57-157"><a href="#cb57-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-158"><a href="#cb57-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-159"><a href="#cb57-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-160"><a href="#cb57-160" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-161"><a href="#cb57-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Log-likelihood curve for a Poisson variable"</span></span>
<span id="cb57-162"><a href="#cb57-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-loglik_poisson</span></span>
<span id="cb57-163"><a href="#cb57-163" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_function</span>(<span class="at">fun =</span> lnl_poisson) <span class="sc">+</span></span>
<span id="cb57-164"><a href="#cb57-164" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb57-165"><a href="#cb57-165" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"theta"</span>, <span class="at">y =</span> <span class="st">"log-likelihood"</span>)</span>
<span id="cb57-166"><a href="#cb57-166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-167"><a href="#cb57-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-168"><a href="#cb57-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-169"><a href="#cb57-169" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln L}{\partial \theta} = - N + \frac{\sum_{n=1} ^ N y_n}{\theta}=0</span>
<span id="cb57-170"><a href="#cb57-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-171"><a href="#cb57-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-172"><a href="#cb57-172" aria-hidden="true" tabindex="-1"></a>This leads to $\hat{\theta} = \frac{\sum_{n=1}^N y_n}{N}$. Therefore,</span>
<span id="cb57-173"><a href="#cb57-173" aria-hidden="true" tabindex="-1"></a>in this simple case, we can explicitly obtain the ML estimator by</span>
<span id="cb57-174"><a href="#cb57-174" aria-hidden="true" tabindex="-1"></a>solving the first-order condition, and, moreover, the ML estimator is</span>
<span id="cb57-175"><a href="#cb57-175" aria-hidden="true" tabindex="-1"></a>the sample mean of the response, which is hardly surprising as</span>
<span id="cb57-176"><a href="#cb57-176" aria-hidden="true" tabindex="-1"></a>$\theta$ is the expected value of $y$ for a Poisson variable. The</span>
<span id="cb57-177"><a href="#cb57-177" aria-hidden="true" tabindex="-1"></a>second derivative is:</span>
<span id="cb57-178"><a href="#cb57-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-179"><a href="#cb57-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-180"><a href="#cb57-180" aria-hidden="true" tabindex="-1"></a>\frac{\partial^2 \ln L}{\partial \theta ^ 2} = - \frac{\sum_{n=1} ^ N</span>
<span id="cb57-181"><a href="#cb57-181" aria-hidden="true" tabindex="-1"></a>y_n}{\theta ^ 2} &lt; 0</span>
<span id="cb57-182"><a href="#cb57-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-183"><a href="#cb57-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-184"><a href="#cb57-184" aria-hidden="true" tabindex="-1"></a>and is negative for all values of $\theta$, which indicates that the</span>
<span id="cb57-185"><a href="#cb57-185" aria-hidden="true" tabindex="-1"></a>log-likelihood is globally concave and therefore that optimum we</span>
<span id="cb57-186"><a href="#cb57-186" aria-hidden="true" tabindex="-1"></a>previously obtained is the global maximum. For our sample, the ML estimator is:</span>
<span id="cb57-187"><a href="#cb57-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-188"><a href="#cb57-188" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-189"><a href="#cb57-189" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-190"><a href="#cb57-190" aria-hidden="true" tabindex="-1"></a>hat_theta <span class="ot">&lt;-</span> sum_y <span class="sc">/</span> N</span>
<span id="cb57-191"><a href="#cb57-191" aria-hidden="true" tabindex="-1"></a>hat_theta</span>
<span id="cb57-192"><a href="#cb57-192" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-193"><a href="#cb57-193" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Poisson distribution|(}</span>
<span id="cb57-194"><a href="#cb57-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-195"><a href="#cb57-195" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the ML estimator for the exponential distribution {#sec-expon_distr_1par}</span></span>
<span id="cb57-196"><a href="#cb57-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-197"><a href="#cb57-197" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution|(}</span>
<span id="cb57-198"><a href="#cb57-198" aria-hidden="true" tabindex="-1"></a>The second example illustrates the computation of the ML estimator for</span>
<span id="cb57-199"><a href="#cb57-199" aria-hidden="true" tabindex="-1"></a>a continuous variable.  The density of a variable which follows an</span>
<span id="cb57-200"><a href="#cb57-200" aria-hidden="true" tabindex="-1"></a>exponential distribution of parameter $\theta$ is:</span>
<span id="cb57-201"><a href="#cb57-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-202"><a href="#cb57-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-203"><a href="#cb57-203" aria-hidden="true" tabindex="-1"></a>f(y; \theta) = \theta e ^{-\theta y}</span>
<span id="cb57-204"><a href="#cb57-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-205"><a href="#cb57-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-206"><a href="#cb57-206" aria-hidden="true" tabindex="-1"></a>The expected value and the variance of $y$ are $\mbox{E}(y) = 1 /</span>
<span id="cb57-207"><a href="#cb57-207" aria-hidden="true" tabindex="-1"></a>\theta$ and $\mbox{V}(y) = 1 / \theta ^ 2$ so that, for a variable</span>
<span id="cb57-208"><a href="#cb57-208" aria-hidden="true" tabindex="-1"></a>which follows an exponential distribution, the mean should be equal to</span>
<span id="cb57-209"><a href="#cb57-209" aria-hidden="true" tabindex="-1"></a>the standard deviation.  To illustrate the use of the exponential</span>
<span id="cb57-210"><a href="#cb57-210" aria-hidden="true" tabindex="-1"></a>distribution, we use the <span class="in">`oil`</span> data set of</span>
<span id="cb57-211"><a href="#cb57-211" aria-hidden="true" tabindex="-1"></a>@FAVE:PESA:SHAR:94\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Favero}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Pesaran}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Sharma}</span>
<span id="cb57-212"><a href="#cb57-212" aria-hidden="true" tabindex="-1"></a>for which the response <span class="in">`dur`</span> is the time span between the discovery of</span>
<span id="cb57-213"><a href="#cb57-213" aria-hidden="true" tabindex="-1"></a>an oil field and the date of the British government's approval to</span>
<span id="cb57-214"><a href="#cb57-214" aria-hidden="true" tabindex="-1"></a>exploit this field.  \idxfun{summarise}{dplyr}\idxfun{sd}{stats}</span>
<span id="cb57-215"><a href="#cb57-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-216"><a href="#cb57-216" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-217"><a href="#cb57-217" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-218"><a href="#cb57-218" aria-hidden="true" tabindex="-1"></a>oil <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">m =</span> <span class="fu">mean</span>(dur), <span class="at">s =</span> <span class="fu">sd</span>(dur))</span>
<span id="cb57-219"><a href="#cb57-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-220"><a href="#cb57-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-221"><a href="#cb57-221" aria-hidden="true" tabindex="-1"></a>The sample mean and the standard deviation are close, in conformity with</span>
<span id="cb57-222"><a href="#cb57-222" aria-hidden="true" tabindex="-1"></a>the features of the exponential distribution. @fig-histo_exponential</span>
<span id="cb57-223"><a href="#cb57-223" aria-hidden="true" tabindex="-1"></a>presents the empirical distribution of <span class="in">`dur`</span> by a histogram, and we</span>
<span id="cb57-224"><a href="#cb57-224" aria-hidden="true" tabindex="-1"></a>add several exponential density lines for different values of $\theta$</span>
<span id="cb57-225"><a href="#cb57-225" aria-hidden="true" tabindex="-1"></a>(0.005, 0.01 and 0.03).</span>
<span id="cb57-226"><a href="#cb57-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-227"><a href="#cb57-227" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-228"><a href="#cb57-228" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-229"><a href="#cb57-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Empirical distribution and density of the exponential distribution"</span></span>
<span id="cb57-230"><a href="#cb57-230" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-histo_exponential</span></span>
<span id="cb57-231"><a href="#cb57-231" aria-hidden="true" tabindex="-1"></a>lns <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">227</span>, <span class="dv">3</span>), </span>
<span id="cb57-232"><a href="#cb57-232" aria-hidden="true" tabindex="-1"></a>              <span class="at">rate =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.03</span>), <span class="at">each =</span> <span class="dv">227</span>),</span>
<span id="cb57-233"><a href="#cb57-233" aria-hidden="true" tabindex="-1"></a>              <span class="at">ratef =</span> <span class="fu">factor</span>(rate)) <span class="sc">%&gt;%</span></span>
<span id="cb57-234"><a href="#cb57-234" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">dens =</span> <span class="fu">dexp</span>(x, rate))</span>
<span id="cb57-235"><a href="#cb57-235" aria-hidden="true" tabindex="-1"></a>oil <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(dur)) <span class="sc">+</span></span>
<span id="cb57-236"><a href="#cb57-236" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">200</span>, <span class="dv">250</span>),</span>
<span id="cb57-237"><a href="#cb57-237" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)),</span>
<span id="cb57-238"><a href="#cb57-238" aria-hidden="true" tabindex="-1"></a>                   <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>) <span class="sc">+</span></span>
<span id="cb57-239"><a href="#cb57-239" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">data =</span> lns, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> dens, <span class="at">linetype =</span> ratef)) <span class="sc">+</span> </span>
<span id="cb57-240"><a href="#cb57-240" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>)) <span class="sc">+</span> </span>
<span id="cb57-241"><a href="#cb57-241" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">linetype =</span> <span class="cn">NULL</span>)</span>
<span id="cb57-242"><a href="#cb57-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-243"><a href="#cb57-243" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- !!! mettre une legende --&gt;</span></span>
<span id="cb57-244"><a href="#cb57-244" aria-hidden="true" tabindex="-1"></a>The adjustment between the empirical and the theoretical distribution is</span>
<span id="cb57-245"><a href="#cb57-245" aria-hidden="true" tabindex="-1"></a>quite good with the dotted line which corresponds to $\theta = 0.01$.</span>
<span id="cb57-246"><a href="#cb57-246" aria-hidden="true" tabindex="-1"></a>The reasoning to construct the log-likelihood function is exactly the</span>
<span id="cb57-247"><a href="#cb57-247" aria-hidden="true" tabindex="-1"></a>same as for the discrete Poisson distribution, except that the mass</span>
<span id="cb57-248"><a href="#cb57-248" aria-hidden="true" tabindex="-1"></a>probability function is replaced by the density function. The</span>
<span id="cb57-249"><a href="#cb57-249" aria-hidden="true" tabindex="-1"></a>log-likelihood function therefore writes:</span>
<span id="cb57-250"><a href="#cb57-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-251"><a href="#cb57-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-252"><a href="#cb57-252" aria-hidden="true" tabindex="-1"></a>\ln L(\theta, y) = \sum_{n = 1} ^ N \ln \left(\theta e ^{-\theta y_n}\right)</span>
<span id="cb57-253"><a href="#cb57-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-254"><a href="#cb57-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-255"><a href="#cb57-255" aria-hidden="true" tabindex="-1"></a>or, regrouping terms:</span>
<span id="cb57-256"><a href="#cb57-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-257"><a href="#cb57-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-258"><a href="#cb57-258" aria-hidden="true" tabindex="-1"></a>\ln L(\theta, y) = N \ln \theta - \theta \sum_{n = 1} ^ N y_n</span>
<span id="cb57-259"><a href="#cb57-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-260"><a href="#cb57-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-261"><a href="#cb57-261" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-262"><a href="#cb57-262" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-263"><a href="#cb57-263" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Log-likelihood curve for an exponential  variable"</span></span>
<span id="cb57-264"><a href="#cb57-264" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-loglik_exponential</span></span>
<span id="cb57-265"><a href="#cb57-265" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> oil<span class="sc">$</span>dur</span>
<span id="cb57-266"><a href="#cb57-266" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb57-267"><a href="#cb57-267" aria-hidden="true" tabindex="-1"></a>sum_y <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb57-268"><a href="#cb57-268" aria-hidden="true" tabindex="-1"></a>lnl_exp <span class="ot">&lt;-</span> <span class="cf">function</span>(x) N <span class="sc">*</span> <span class="fu">log</span>(x) <span class="sc">-</span> x <span class="sc">*</span> sum_y</span>
<span id="cb57-269"><a href="#cb57-269" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_function</span>(<span class="at">fun =</span> lnl_exp) <span class="sc">+</span></span>
<span id="cb57-270"><a href="#cb57-270" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">5E-02</span>)) <span class="sc">+</span></span>
<span id="cb57-271"><a href="#cb57-271" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">350</span>, <span class="sc">-</span><span class="dv">270</span>)) <span class="sc">+</span></span>
<span id="cb57-272"><a href="#cb57-272" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"theta"</span>, <span class="at">y =</span> <span class="st">"log-likelihood"</span>)</span>
<span id="cb57-273"><a href="#cb57-273" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-274"><a href="#cb57-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-275"><a href="#cb57-275" aria-hidden="true" tabindex="-1"></a>The shape of the log-likelihood function is represented in</span>
<span id="cb57-276"><a href="#cb57-276" aria-hidden="true" tabindex="-1"></a>@fig-loglik_exponential.</span>
<span id="cb57-277"><a href="#cb57-277" aria-hidden="true" tabindex="-1"></a>As in the Poisson case, the log-likelihood seems to be globally concave,</span>
<span id="cb57-278"><a href="#cb57-278" aria-hidden="true" tabindex="-1"></a>with a global maximum corresponding to a value of $\theta$</span>
<span id="cb57-279"><a href="#cb57-279" aria-hidden="true" tabindex="-1"></a>approximately equal to $0.015$.</span>
<span id="cb57-280"><a href="#cb57-280" aria-hidden="true" tabindex="-1"></a>The first-order condition for a maximum is:</span>
<span id="cb57-281"><a href="#cb57-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-282"><a href="#cb57-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-283"><a href="#cb57-283" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln L}{\partial \theta} = \frac{N}{\theta} -</span>
<span id="cb57-284"><a href="#cb57-284" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N y_n</span>
<span id="cb57-285"><a href="#cb57-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-286"><a href="#cb57-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-287"><a href="#cb57-287" aria-hidden="true" tabindex="-1"></a>which leads to the ML estimator: $\hat{\theta} =</span>
<span id="cb57-288"><a href="#cb57-288" aria-hidden="true" tabindex="-1"></a>\frac{N}{\sum_{n=1}^N y_n} = \frac{1}{\bar{y}}$. The second derivative</span>
<span id="cb57-289"><a href="#cb57-289" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb57-290"><a href="#cb57-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-291"><a href="#cb57-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-292"><a href="#cb57-292" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln^2 L}{\partial \theta^2} = -\frac{N}{\theta^2}</span>
<span id="cb57-293"><a href="#cb57-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-294"><a href="#cb57-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-295"><a href="#cb57-295" aria-hidden="true" tabindex="-1"></a>which is negative for all values of $\theta$, so that</span>
<span id="cb57-296"><a href="#cb57-296" aria-hidden="true" tabindex="-1"></a>$\hat{\theta}$ is the global maximum of the log-likelihood function.</span>
<span id="cb57-297"><a href="#cb57-297" aria-hidden="true" tabindex="-1"></a>In our example, we get:</span>
<span id="cb57-298"><a href="#cb57-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-299"><a href="#cb57-299" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-300"><a href="#cb57-300" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-301"><a href="#cb57-301" aria-hidden="true" tabindex="-1"></a>hat_theta <span class="ot">&lt;-</span> N <span class="sc">/</span> sum_y</span>
<span id="cb57-302"><a href="#cb57-302" aria-hidden="true" tabindex="-1"></a>hat_theta</span>
<span id="cb57-303"><a href="#cb57-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-304"><a href="#cb57-304" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution|)}</span>
<span id="cb57-305"><a href="#cb57-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-306"><a href="#cb57-306" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of the ML estimator</span></span>
<span id="cb57-307"><a href="#cb57-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-308"><a href="#cb57-308" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{consistency!maximum likelihood estimator|(}</span>
<span id="cb57-309"><a href="#cb57-309" aria-hidden="true" tabindex="-1"></a>We consider a variable $y$ for which we observe $N$ realizations in a</span>
<span id="cb57-310"><a href="#cb57-310" aria-hidden="true" tabindex="-1"></a>random sample. We assume that $y$ follows a distribution with a unique</span>
<span id="cb57-311"><a href="#cb57-311" aria-hidden="true" tabindex="-1"></a>parameter $\theta$. The density (if $y$ is continuous) or the mass</span>
<span id="cb57-312"><a href="#cb57-312" aria-hidden="true" tabindex="-1"></a>probability function (if $y$ is discrete) is denoted $\phi(y;</span>
<span id="cb57-313"><a href="#cb57-313" aria-hidden="true" tabindex="-1"></a>\theta)$. We also denote $\lambda(y; \theta) = \ln \phi(y; \theta)$ and</span>
<span id="cb57-314"><a href="#cb57-314" aria-hidden="true" tabindex="-1"></a>$\gamma(y; \theta)$ and $\psi(y; \theta)$ the first two derivatives of</span>
<span id="cb57-315"><a href="#cb57-315" aria-hidden="true" tabindex="-1"></a>$\lambda$ with $\theta$. The log likelihood function is then:</span>
<span id="cb57-316"><a href="#cb57-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-317"><a href="#cb57-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-318"><a href="#cb57-318" aria-hidden="true" tabindex="-1"></a>\ln L(\theta, y) = \sum_n \lambda(y_n; \theta) = \sum_n \ln \phi(y_n; \theta)</span>
<span id="cb57-319"><a href="#cb57-319" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-320"><a href="#cb57-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-321"><a href="#cb57-321" aria-hidden="true" tabindex="-1"></a>with $y^\top = (y_1, y_2, \ldots, y_N)$ the vector containing all the</span>
<span id="cb57-322"><a href="#cb57-322" aria-hidden="true" tabindex="-1"></a>values of the response in the sample. </span>
<span id="cb57-323"><a href="#cb57-323" aria-hidden="true" tabindex="-1"></a>The "true value" of $\theta$ is denoted by $\theta_0$ and the maximum</span>
<span id="cb57-324"><a href="#cb57-324" aria-hidden="true" tabindex="-1"></a>likelihood estimator $\hat{\theta}$. The proof of the consistency of</span>
<span id="cb57-325"><a href="#cb57-325" aria-hidden="true" tabindex="-1"></a>the ML estimator is based on Jensen's inequality, which states that,</span>
<span id="cb57-326"><a href="#cb57-326" aria-hidden="true" tabindex="-1"></a>for a random variable $X$ and a  concave function $f$:</span>
<span id="cb57-327"><a href="#cb57-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-328"><a href="#cb57-328" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-329"><a href="#cb57-329" aria-hidden="true" tabindex="-1"></a>\mbox{E}(f(X)) \leq f(\mbox{E}(X))</span>
<span id="cb57-330"><a href="#cb57-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-331"><a href="#cb57-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-332"><a href="#cb57-332" aria-hidden="true" tabindex="-1"></a>As the logarithm is a  concave function:</span>
<span id="cb57-333"><a href="#cb57-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-334"><a href="#cb57-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-335"><a href="#cb57-335" aria-hidden="true" tabindex="-1"></a>\mbox{E}\ln\frac{L(\theta)}{L(\theta_0)} &lt;  \ln \mbox{E}\frac{L(\theta)}{L(\theta_0)}</span>
<span id="cb57-336"><a href="#cb57-336" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-jensen}</span>
<span id="cb57-337"><a href="#cb57-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-338"><a href="#cb57-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-339"><a href="#cb57-339" aria-hidden="true" tabindex="-1"></a>The expectation on the right side of the equation is obtained by</span>
<span id="cb57-340"><a href="#cb57-340" aria-hidden="true" tabindex="-1"></a>integrating out $L(\theta)/L(\theta_0)$ using the density of $y$,</span>
<span id="cb57-341"><a href="#cb57-341" aria-hidden="true" tabindex="-1"></a>which is $L(\theta_0)$. Therefore:</span>
<span id="cb57-342"><a href="#cb57-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-343"><a href="#cb57-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-344"><a href="#cb57-344" aria-hidden="true" tabindex="-1"></a>\mbox{E}\frac{L(\theta)}{L(\theta_0)} = \int</span>
<span id="cb57-345"><a href="#cb57-345" aria-hidden="true" tabindex="-1"></a>\frac{L(\theta)}{L(\theta_0)} L(\theta_0) dy = \int</span>
<span id="cb57-346"><a href="#cb57-346" aria-hidden="true" tabindex="-1"></a>L(\theta) dy = 1</span>
<span id="cb57-347"><a href="#cb57-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-348"><a href="#cb57-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-349"><a href="#cb57-349" aria-hidden="true" tabindex="-1"></a>as any density sums to 1 for the whole support of the</span>
<span id="cb57-350"><a href="#cb57-350" aria-hidden="true" tabindex="-1"></a>variable. Therefore @eq-jensen implies that:</span>
<span id="cb57-351"><a href="#cb57-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-352"><a href="#cb57-352" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-353"><a href="#cb57-353" aria-hidden="true" tabindex="-1"></a>\mbox{E}\ln L(\theta) \leq \mbox{E}\ln L(\theta_0)</span>
<span id="cb57-354"><a href="#cb57-354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-355"><a href="#cb57-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-356"><a href="#cb57-356" aria-hidden="true" tabindex="-1"></a>Dividing by $N$ and using the law of large numbers, we also have:</span>
<span id="cb57-357"><a href="#cb57-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-358"><a href="#cb57-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-359"><a href="#cb57-359" aria-hidden="true" tabindex="-1"></a>\mbox{plim} \frac{1}{N}\ln L(\theta) \leq \mbox{plim} \frac{1}{N}\ln L(\theta_0)</span>
<span id="cb57-360"><a href="#cb57-360" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-firstineq}</span>
<span id="cb57-361"><a href="#cb57-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-362"><a href="#cb57-362" aria-hidden="true" tabindex="-1"></a>As $\hat{\theta}$ maximizes $\ln L(\theta)$, it is also the case that</span>
<span id="cb57-363"><a href="#cb57-363" aria-hidden="true" tabindex="-1"></a>$\ln L (\hat{\theta}) \geq \ln L (\theta)$. Once again, dividing</span>
<span id="cb57-364"><a href="#cb57-364" aria-hidden="true" tabindex="-1"></a>by $N$ and computing the limit, we have:</span>
<span id="cb57-365"><a href="#cb57-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-366"><a href="#cb57-366" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-367"><a href="#cb57-367" aria-hidden="true" tabindex="-1"></a>\mbox{plim} \frac{1}{N}\ln L(\hat{\theta}) \geq \mbox{plim} \frac{1}{N}\ln L(\theta)</span>
<span id="cb57-368"><a href="#cb57-368" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-secondineq}</span>
<span id="cb57-369"><a href="#cb57-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-370"><a href="#cb57-370" aria-hidden="true" tabindex="-1"></a>The only solution for @eq-firstineq and @eq-secondineq to hold is</span>
<span id="cb57-371"><a href="#cb57-371" aria-hidden="true" tabindex="-1"></a>that:</span>
<span id="cb57-372"><a href="#cb57-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-373"><a href="#cb57-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-374"><a href="#cb57-374" aria-hidden="true" tabindex="-1"></a>\mbox{plim} \frac{1}{N}\ln L(\hat{\theta}) = \mbox{plim} \frac{1}{N}\ln L(\theta_0)</span>
<span id="cb57-375"><a href="#cb57-375" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-equality}</span>
<span id="cb57-376"><a href="#cb57-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-377"><a href="#cb57-377" aria-hidden="true" tabindex="-1"></a>@eq-equality indicates that, as $N$ tends to infinity, the average</span>
<span id="cb57-378"><a href="#cb57-378" aria-hidden="true" tabindex="-1"></a>likelihood for $\hat{\theta}$ converges to the average likelihood for</span>
<span id="cb57-379"><a href="#cb57-379" aria-hidden="true" tabindex="-1"></a>$\theta_0$. Using a regularity condition, this implies that the</span>
<span id="cb57-380"><a href="#cb57-380" aria-hidden="true" tabindex="-1"></a>estimator is consistent, i.e., that $\mbox{plim} \,\hat{\theta} = \theta_0$.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{consistency!maximum likelihood estimator|)}</span>
<span id="cb57-381"><a href="#cb57-381" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{information matrix equality|(}</span>
<span id="cb57-382"><a href="#cb57-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-383"><a href="#cb57-383" aria-hidden="true" tabindex="-1"></a>The first two derivatives of the log-likelihood functions, which are</span>
<span id="cb57-384"><a href="#cb57-384" aria-hidden="true" tabindex="-1"></a>respectively called the **gradient** (or the **score**) and the **hessian** of the</span>
<span id="cb57-385"><a href="#cb57-385" aria-hidden="true" tabindex="-1"></a>log-likelihood are:</span>
<span id="cb57-386"><a href="#cb57-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-387"><a href="#cb57-387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-388"><a href="#cb57-388" aria-hidden="true" tabindex="-1"></a>g(\theta, y) = \frac{\partial \ln L}{\partial \theta}(\theta, y) =</span>
<span id="cb57-389"><a href="#cb57-389" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N \frac{\partial \lambda}{\partial \theta}(y_n; \theta) = </span>
<span id="cb57-390"><a href="#cb57-390" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N \gamma(y_n; \theta)</span>
<span id="cb57-391"><a href="#cb57-391" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-gradient_scalar}</span>
<span id="cb57-392"><a href="#cb57-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-393"><a href="#cb57-393" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb57-394"><a href="#cb57-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-395"><a href="#cb57-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-396"><a href="#cb57-396" aria-hidden="true" tabindex="-1"></a>h(\theta, y) = \frac{\partial^2 \ln L}{\partial \theta^2}(\theta, y) =</span>
<span id="cb57-397"><a href="#cb57-397" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N \frac{\partial^2 \lambda}{\partial \theta^2}(y_n; \theta) = </span>
<span id="cb57-398"><a href="#cb57-398" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N \psi(y_n; \theta)</span>
<span id="cb57-399"><a href="#cb57-399" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-hessian_scalar}</span>
<span id="cb57-400"><a href="#cb57-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-401"><a href="#cb57-401" aria-hidden="true" tabindex="-1"></a>As there is only one parameter to estimate, both functions are scalar functions.</span>
<span id="cb57-402"><a href="#cb57-402" aria-hidden="true" tabindex="-1"></a>For a discrete distribution, the probabilities for every possible $K$ values</span>
<span id="cb57-403"><a href="#cb57-403" aria-hidden="true" tabindex="-1"></a>of $y$ (denoted by $y_k$) sum to unity:</span>
<span id="cb57-404"><a href="#cb57-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-405"><a href="#cb57-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-406"><a href="#cb57-406" aria-hidden="true" tabindex="-1"></a>\sum_{k=1} ^ K \phi(y_k; \theta) = 1</span>
<span id="cb57-407"><a href="#cb57-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-408"><a href="#cb57-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-409"><a href="#cb57-409" aria-hidden="true" tabindex="-1"></a>The same result applies with the continuous sum for a continuous random</span>
<span id="cb57-410"><a href="#cb57-410" aria-hidden="true" tabindex="-1"></a>variable:</span>
<span id="cb57-411"><a href="#cb57-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-412"><a href="#cb57-412" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-413"><a href="#cb57-413" aria-hidden="true" tabindex="-1"></a>\int \phi(y; \theta) dy = 1</span>
<span id="cb57-414"><a href="#cb57-414" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-415"><a href="#cb57-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-416"><a href="#cb57-416" aria-hidden="true" tabindex="-1"></a>As $\phi = e ^ \lambda$, taking the derivative with respect to</span>
<span id="cb57-417"><a href="#cb57-417" aria-hidden="true" tabindex="-1"></a>$\theta$, we get:</span>
<span id="cb57-418"><a href="#cb57-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-419"><a href="#cb57-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-420"><a href="#cb57-420" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-421"><a href="#cb57-421" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-422"><a href="#cb57-422" aria-hidden="true" tabindex="-1"></a>\displaystyle  \sum_{k=1} ^ K \gamma(y_k; \theta) \phi(y_k; \theta)&amp;=&amp; 0 <span class="sc">\\</span></span>
<span id="cb57-423"><a href="#cb57-423" aria-hidden="true" tabindex="-1"></a>\displaystyle \int \gamma(y; \theta) \phi(y; \theta) dy&amp;=&amp; 0</span>
<span id="cb57-424"><a href="#cb57-424" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-425"><a href="#cb57-425" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-426"><a href="#cb57-426" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gradient_0_theta0}</span>
<span id="cb57-427"><a href="#cb57-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-428"><a href="#cb57-428" aria-hidden="true" tabindex="-1"></a>Evaluated for the true value $\theta_0$, @eq-gradient_0_theta0 is the expectation of an individual contribution to the  gradient. Therefore, evaluated at the true value of $\theta$, $\gamma(y; \theta)$ is a random variable with 0 expectation. </span>
<span id="cb57-429"><a href="#cb57-429" aria-hidden="true" tabindex="-1"></a>Taking now the second derivative with respect to $\theta$, we get:</span>
<span id="cb57-430"><a href="#cb57-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-431"><a href="#cb57-431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-432"><a href="#cb57-432" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-433"><a href="#cb57-433" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-434"><a href="#cb57-434" aria-hidden="true" tabindex="-1"></a>\displaystyle \sum_{k=1} ^ K \gamma(y_k; \theta) ^ 2 \phi(y_k; \theta) + </span>
<span id="cb57-435"><a href="#cb57-435" aria-hidden="true" tabindex="-1"></a>\sum_{k=1} ^ K  \psi(y_k; \theta) \phi(y_k; \theta) &amp;=&amp; 0 <span class="sc">\\</span></span>
<span id="cb57-436"><a href="#cb57-436" aria-hidden="true" tabindex="-1"></a>\displaystyle \int \gamma(y; \theta) ^ 2 \phi(y; \theta) dy +</span>
<span id="cb57-437"><a href="#cb57-437" aria-hidden="true" tabindex="-1"></a>\int  \psi(y; \theta) \phi(y; \theta)dy&amp;=&amp; 0</span>
<span id="cb57-438"><a href="#cb57-438" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-439"><a href="#cb57-439" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-440"><a href="#cb57-440" aria-hidden="true" tabindex="-1"></a>$$ {#eq-variance_hessian}</span>
<span id="cb57-441"><a href="#cb57-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-442"><a href="#cb57-442" aria-hidden="true" tabindex="-1"></a>We denote the first term by $v_\gamma(\theta)$: evaluated for the true</span>
<span id="cb57-443"><a href="#cb57-443" aria-hidden="true" tabindex="-1"></a>value of the parameter, this is the variance of $\gamma(y; \theta)$:,</span>
<span id="cb57-444"><a href="#cb57-444" aria-hidden="true" tabindex="-1"></a>$v_\gamma(\theta_0) = \sigma_\gamma^2$. We denote the second term by</span>
<span id="cb57-445"><a href="#cb57-445" aria-hidden="true" tabindex="-1"></a>$m_\psi(\theta)$, which is, evaluated for the true value of the</span>
<span id="cb57-446"><a href="#cb57-446" aria-hidden="true" tabindex="-1"></a>parameter, the expectation of $\psi(y; \theta)$: $m_\psi(\theta_0) =</span>
<span id="cb57-447"><a href="#cb57-447" aria-hidden="true" tabindex="-1"></a>\mu_\psi$. Therefore, @eq-variance_hessian indicates that</span>
<span id="cb57-448"><a href="#cb57-448" aria-hidden="true" tabindex="-1"></a>$\sigma_\gamma ^ 2 = - \mu_\psi$.</span>
<span id="cb57-449"><a href="#cb57-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-450"><a href="#cb57-450" aria-hidden="true" tabindex="-1"></a>The gradient $g(\theta, y)$ (@eq-gradient_scalar) is the sum of $N$</span>
<span id="cb57-451"><a href="#cb57-451" aria-hidden="true" tabindex="-1"></a>contributions $\gamma(y_n; \theta)$ which have 0</span>
<span id="cb57-452"><a href="#cb57-452" aria-hidden="true" tabindex="-1"></a>expectation. Therefore, its expectation is 0. With the random sample</span>
<span id="cb57-453"><a href="#cb57-453" aria-hidden="true" tabindex="-1"></a>hypothesis, $\gamma(y_n; \theta)$ and $\gamma(y_m; \theta)$ are</span>
<span id="cb57-454"><a href="#cb57-454" aria-hidden="true" tabindex="-1"></a>independent for all $m\neq n$ and the variance of the gradient is</span>
<span id="cb57-455"><a href="#cb57-455" aria-hidden="true" tabindex="-1"></a>therefore the sum of the variances of its $N$ contributions, which are</span>
<span id="cb57-456"><a href="#cb57-456" aria-hidden="true" tabindex="-1"></a>all equal to $\sigma^2_\gamma$. Therefore, $\mbox{V}(g(\theta_0, y)) = N</span>
<span id="cb57-457"><a href="#cb57-457" aria-hidden="true" tabindex="-1"></a>\sigma^2_\gamma$. The variance of the gradient is called the</span>
<span id="cb57-458"><a href="#cb57-458" aria-hidden="true" tabindex="-1"></a>**information matrix** in the general case, but it is actually in our one</span>
<span id="cb57-459"><a href="#cb57-459" aria-hidden="true" tabindex="-1"></a>parameter case a scalar that we'll denote $\iota(\theta_0)$.</span>
<span id="cb57-460"><a href="#cb57-460" aria-hidden="true" tabindex="-1"></a>The hessian being a sum of $N$ contributions $\psi(y_n; \theta)$,</span>
<span id="cb57-461"><a href="#cb57-461" aria-hidden="true" tabindex="-1"></a>which have an expectation equal to $\mu_\psi$, its expected value is</span>
<span id="cb57-462"><a href="#cb57-462" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(h(\theta_0, y)) = N \mu_\psi$.  The result we previously</span>
<span id="cb57-463"><a href="#cb57-463" aria-hidden="true" tabindex="-1"></a>established ($\sigma_\gamma ^ 2 = - \mu_\psi$) implies that the</span>
<span id="cb57-464"><a href="#cb57-464" aria-hidden="true" tabindex="-1"></a>variance of the gradient (the information) equals the opposite of the</span>
<span id="cb57-465"><a href="#cb57-465" aria-hidden="true" tabindex="-1"></a>expectation of the hessian:</span>
<span id="cb57-466"><a href="#cb57-466" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{information matrix equality|)}</span>
<span id="cb57-467"><a href="#cb57-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-468"><a href="#cb57-468" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-469"><a href="#cb57-469" aria-hidden="true" tabindex="-1"></a>\iota(\theta_0) = \mbox{V}(g(\theta_0, y)) = - \mbox{E}(h(\theta_0, y)) = N \sigma^2_\gamma = - N\mu_\psi</span>
<span id="cb57-470"><a href="#cb57-470" aria-hidden="true" tabindex="-1"></a>$$ {#eq-information_equality_one_par}</span>
<span id="cb57-471"><a href="#cb57-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-472"><a href="#cb57-472" aria-hidden="true" tabindex="-1"></a>This important result is called the **information equality**.</span>
<span id="cb57-473"><a href="#cb57-473" aria-hidden="true" tabindex="-1"></a>Denoting $\theta_0$ as the true (unknown) value of the parameter, and</span>
<span id="cb57-474"><a href="#cb57-474" aria-hidden="true" tabindex="-1"></a>omitting for convenience the $y$ vector, a first-order Taylor</span>
<span id="cb57-475"><a href="#cb57-475" aria-hidden="true" tabindex="-1"></a>expansion of $g(\hat{\theta})$ around $\theta_0$ is:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Taylor expansion|(}</span>
<span id="cb57-476"><a href="#cb57-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-477"><a href="#cb57-477" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-478"><a href="#cb57-478" aria-hidden="true" tabindex="-1"></a>g(\hat{\theta}) \approx g(\theta_0) + h(\theta_0) (\hat{\theta} -</span>
<span id="cb57-479"><a href="#cb57-479" aria-hidden="true" tabindex="-1"></a>\theta_0)</span>
<span id="cb57-480"><a href="#cb57-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-481"><a href="#cb57-481" aria-hidden="true" tabindex="-1"></a>If we use instead an exact first-order Taylor expansion:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Taylor expansion!exact}</span>
<span id="cb57-482"><a href="#cb57-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-483"><a href="#cb57-483" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-484"><a href="#cb57-484" aria-hidden="true" tabindex="-1"></a>g(\hat{\theta}) = g(\theta_0) + h(\bar{\theta}) (\hat{\theta} - \theta_0)</span>
<span id="cb57-485"><a href="#cb57-485" aria-hidden="true" tabindex="-1"></a>$$ {#eq-exact_taylor_expansion}</span>
<span id="cb57-486"><a href="#cb57-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-487"><a href="#cb57-487" aria-hidden="true" tabindex="-1"></a>where $\bar{\theta}$ lies between $\hat{\theta}$ and $\theta_0$. As</span>
<span id="cb57-488"><a href="#cb57-488" aria-hidden="true" tabindex="-1"></a>$g(\hat{\theta}) = 0$, solving for $\hat{\theta} - \theta_0$, we get:</span>
<span id="cb57-489"><a href="#cb57-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-490"><a href="#cb57-490" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-491"><a href="#cb57-491" aria-hidden="true" tabindex="-1"></a>(\hat{\theta} - \theta_0) = -</span>
<span id="cb57-492"><a href="#cb57-492" aria-hidden="true" tabindex="-1"></a>h(\bar{\theta})^{-1} g(\theta_0)</span>
<span id="cb57-493"><a href="#cb57-493" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-494"><a href="#cb57-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-495"><a href="#cb57-495" aria-hidden="true" tabindex="-1"></a>or, multiplying by $\sqrt{N}$:</span>
<span id="cb57-496"><a href="#cb57-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-497"><a href="#cb57-497" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-498"><a href="#cb57-498" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\theta} - \theta_0) = -</span>
<span id="cb57-499"><a href="#cb57-499" aria-hidden="true" tabindex="-1"></a>\left(\frac{h(\bar{\theta})}{N}\right)^{-1} \frac{g(\theta_0)}{\sqrt{N}}</span>
<span id="cb57-500"><a href="#cb57-500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-501"><a href="#cb57-501" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Taylor expansion|)}</span>
<span id="cb57-502"><a href="#cb57-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-503"><a href="#cb57-503" aria-hidden="true" tabindex="-1"></a>Assuming that the estimator is consistent, as $N$ grows,</span>
<span id="cb57-504"><a href="#cb57-504" aria-hidden="true" tabindex="-1"></a>$\hat{\theta}$ converges to $\theta_0$ and so does $\bar{\theta}$. As</span>
<span id="cb57-505"><a href="#cb57-505" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(h(\theta_0)) = N \mu_\psi$, the expectation of the first</span>
<span id="cb57-506"><a href="#cb57-506" aria-hidden="true" tabindex="-1"></a>term is $\mu_\psi$, and it is also its probability limit. The second</span>
<span id="cb57-507"><a href="#cb57-507" aria-hidden="true" tabindex="-1"></a>term has a 0 expected value and a variance equal to $\sigma_\gamma ^</span>
<span id="cb57-508"><a href="#cb57-508" aria-hidden="true" tabindex="-1"></a>2$. Therefore, $\sqrt{N}(\hat{\theta} - \theta_0)$ has a zero</span>
<span id="cb57-509"><a href="#cb57-509" aria-hidden="true" tabindex="-1"></a>expectation and an asymptotic covariance equal to $\mu_\psi ^ {-2}</span>
<span id="cb57-510"><a href="#cb57-510" aria-hidden="true" tabindex="-1"></a>\sigma_\gamma^2$. Moreover, applying the central-limit theorem, its</span>
<span id="cb57-511"><a href="#cb57-511" aria-hidden="true" tabindex="-1"></a>asymptotic distribution is normal. Therefore:</span>
<span id="cb57-512"><a href="#cb57-512" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{central-limit theorem}</span>
<span id="cb57-513"><a href="#cb57-513" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic distribution!maximum likelihood estimator}</span>
<span id="cb57-514"><a href="#cb57-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-515"><a href="#cb57-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-516"><a href="#cb57-516" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\theta}-\theta_0) \overset{p}{\rightarrow} N(0, \mu_\psi ^ {-2}</span>
<span id="cb57-517"><a href="#cb57-517" aria-hidden="true" tabindex="-1"></a>\sigma_\gamma ^ 2) \; \mbox{ and } \;</span>
<span id="cb57-518"><a href="#cb57-518" aria-hidden="true" tabindex="-1"></a>\hat{\theta} \overset{a}{\sim} \mathcal{N}(\theta_0, \mu_\psi ^ {-2}</span>
<span id="cb57-519"><a href="#cb57-519" aria-hidden="true" tabindex="-1"></a>\sigma_\gamma ^ 2 / N)</span>
<span id="cb57-520"><a href="#cb57-520" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-asdist_general}</span>
<span id="cb57-521"><a href="#cb57-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-522"><a href="#cb57-522" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{information matrix equality}</span>
<span id="cb57-523"><a href="#cb57-523" aria-hidden="true" tabindex="-1"></a>Applying the information equality, $\iota(\theta_0) = N</span>
<span id="cb57-524"><a href="#cb57-524" aria-hidden="true" tabindex="-1"></a>\sigma_\gamma ^ 2(\theta_0) = - N \mu_\psi(\theta_0)$ and therefore:</span>
<span id="cb57-525"><a href="#cb57-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-526"><a href="#cb57-526" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-527"><a href="#cb57-527" aria-hidden="true" tabindex="-1"></a>\hat{\theta} \overset{a}{\sim} \mathcal{N}(\theta_0, \iota(\theta_0) ^ {-1})</span>
<span id="cb57-528"><a href="#cb57-528" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-asdist_info_eq}</span>
<span id="cb57-529"><a href="#cb57-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-530"><a href="#cb57-530" aria-hidden="true" tabindex="-1"></a>We have seen that the variance of the ML estimator is the inverse</span>
<span id="cb57-531"><a href="#cb57-531" aria-hidden="true" tabindex="-1"></a>of the information, which can be either obtained using the</span>
<span id="cb57-532"><a href="#cb57-532" aria-hidden="true" tabindex="-1"></a>variance of the gradient or the opposite of the expectation of the</span>
<span id="cb57-533"><a href="#cb57-533" aria-hidden="true" tabindex="-1"></a>hessian. In terms of</span>
<span id="cb57-534"><a href="#cb57-534" aria-hidden="true" tabindex="-1"></a>the average information ($\iota(\theta) / N$), we have:</span>
<span id="cb57-535"><a href="#cb57-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-536"><a href="#cb57-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-537"><a href="#cb57-537" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-538"><a href="#cb57-538" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-539"><a href="#cb57-539" aria-hidden="true" tabindex="-1"></a>\frac{\iota(\theta)}{N} &amp;=&amp; \frac{1}{N} \mbox{V}\left(\frac{g(\theta, y)}{\sqrt{N}}\right) = \frac{1}{N}</span>
<span id="cb57-540"><a href="#cb57-540" aria-hidden="true" tabindex="-1"></a>\sum_{n= 1} ^ N \mbox{E}(\gamma(y; \theta) ^ 2) = v_\gamma (\theta) <span class="sc">\\</span></span>
<span id="cb57-541"><a href="#cb57-541" aria-hidden="true" tabindex="-1"></a>\frac{\iota(\theta)}{N} &amp;=&amp; \frac{1}{N} \mbox{E}\left(- \frac{h(\theta, y)}{N}\right) = - \frac{1}{N} \sum_{n = 1} ^ N</span>
<span id="cb57-542"><a href="#cb57-542" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\psi(y_n; \theta)) = - m_\psi(\theta)</span>
<span id="cb57-543"><a href="#cb57-543" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-544"><a href="#cb57-544" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-545"><a href="#cb57-545" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-average_info}</span>
<span id="cb57-546"><a href="#cb57-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-547"><a href="#cb57-547" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!information based}</span>
<span id="cb57-548"><a href="#cb57-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-549"><a href="#cb57-549" aria-hidden="true" tabindex="-1"></a>If this variance/expectation can be computed, then a natural</span>
<span id="cb57-550"><a href="#cb57-550" aria-hidden="true" tabindex="-1"></a>estimator of $\iota(\theta_0)$ is $\iota(\hat{\theta})$.</span>
<span id="cb57-551"><a href="#cb57-551" aria-hidden="true" tabindex="-1"></a>This **information-based estimator** of the variance is obtained by</span>
<span id="cb57-552"><a href="#cb57-552" aria-hidden="true" tabindex="-1"></a>inverting the information evaluated for the maximum likelihood value</span>
<span id="cb57-553"><a href="#cb57-553" aria-hidden="true" tabindex="-1"></a>of $\theta$:</span>
<span id="cb57-554"><a href="#cb57-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-555"><a href="#cb57-555" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-556"><a href="#cb57-556" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}i} ^ 2 = \iota(\hat{\theta}) ^ {-1}</span>
<span id="cb57-557"><a href="#cb57-557" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-558"><a href="#cb57-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-559"><a href="#cb57-559" aria-hidden="true" tabindex="-1"></a>On the contrary, if it is impossible to compute the expectations,</span>
<span id="cb57-560"><a href="#cb57-560" aria-hidden="true" tabindex="-1"></a>two natural estimators of the information are based on the</span>
<span id="cb57-561"><a href="#cb57-561" aria-hidden="true" tabindex="-1"></a>gradient and the hessian and are obtained by evaluating one of the two</span>
<span id="cb57-562"><a href="#cb57-562" aria-hidden="true" tabindex="-1"></a>expressions in @eq-average_info without the expectation. Denoting</span>
<span id="cb57-563"><a href="#cb57-563" aria-hidden="true" tabindex="-1"></a>$\iota_g$ and $\iota_h$, these two estimations of the information, we</span>
<span id="cb57-564"><a href="#cb57-564" aria-hidden="true" tabindex="-1"></a>have from @eq-average_info:</span>
<span id="cb57-565"><a href="#cb57-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-566"><a href="#cb57-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-567"><a href="#cb57-567" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-568"><a href="#cb57-568" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-569"><a href="#cb57-569" aria-hidden="true" tabindex="-1"></a>\frac{\iota_g(\theta)}{N} &amp;=&amp;  \frac{1}{N}</span>
<span id="cb57-570"><a href="#cb57-570" aria-hidden="true" tabindex="-1"></a>\sum_{n= 1} ^ N \gamma(y_n; \theta) ^ 2 = \hat{v}_\gamma(\theta) <span class="sc">\\</span></span>
<span id="cb57-571"><a href="#cb57-571" aria-hidden="true" tabindex="-1"></a>\frac{\iota_h(\theta)}{N} &amp;=&amp; - \frac{h(\theta, y)}{N} = - \frac{1}{N} \sum_{n = 1} ^ N</span>
<span id="cb57-572"><a href="#cb57-572" aria-hidden="true" tabindex="-1"></a>\psi(\theta; y_n) = - \hat{m}_\psi(\theta)</span>
<span id="cb57-573"><a href="#cb57-573" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-574"><a href="#cb57-574" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-575"><a href="#cb57-575" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-576"><a href="#cb57-576" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!gradient based}</span>
<span id="cb57-577"><a href="#cb57-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-578"><a href="#cb57-578" aria-hidden="true" tabindex="-1"></a>Evaluated for the maximum likelihood estimator value of $\theta$, we</span>
<span id="cb57-579"><a href="#cb57-579" aria-hidden="true" tabindex="-1"></a>then get the **gradient-based estimator**:</span>
<span id="cb57-580"><a href="#cb57-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-581"><a href="#cb57-581" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-582"><a href="#cb57-582" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta} g} ^ 2 = \iota_g(\hat{\theta}) ^ {-1} =</span>
<span id="cb57-583"><a href="#cb57-583" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n= 1} ^ N \gamma(y ; \hat{\theta}) ^ 2\right) ^ {-1} = \frac{1}{N</span>
<span id="cb57-584"><a href="#cb57-584" aria-hidden="true" tabindex="-1"></a>\hat{v}_\gamma(\hat{\theta})}</span>
<span id="cb57-585"><a href="#cb57-585" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-gradient_est}</span>
<span id="cb57-586"><a href="#cb57-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-587"><a href="#cb57-587" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!hessian based}</span>
<span id="cb57-588"><a href="#cb57-588" aria-hidden="true" tabindex="-1"></a>and the **hessian-based estimator** of the variance of $\theta$:</span>
<span id="cb57-589"><a href="#cb57-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-590"><a href="#cb57-590" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-591"><a href="#cb57-591" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta} h} ^ 2 = \iota_h(\hat{\theta}) ^ {-1} =</span>
<span id="cb57-592"><a href="#cb57-592" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>h(\hat{\theta}, y) ^ {-1} = - \frac{1}{N \hat{m}_{\psi}(\hat{\theta})} </span>
<span id="cb57-593"><a href="#cb57-593" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-hessian_est}</span>
<span id="cb57-594"><a href="#cb57-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-595"><a href="#cb57-595" aria-hidden="true" tabindex="-1"></a>A fourth estimator is based on @eq-asdist_general, which states that,</span>
<span id="cb57-596"><a href="#cb57-596" aria-hidden="true" tabindex="-1"></a>before applying the information equality, </span>
<span id="cb57-597"><a href="#cb57-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-598"><a href="#cb57-598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-599"><a href="#cb57-599" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\theta}} ^ 2 = \frac{1}{N} \frac{v_\gamma(\theta_0)}{m_\psi(\theta_0)^2}</span>
<span id="cb57-600"><a href="#cb57-600" aria-hidden="true" tabindex="-1"></a>$$ {#eq-var_general}</span>
<span id="cb57-601"><a href="#cb57-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-602"><a href="#cb57-602" aria-hidden="true" tabindex="-1"></a>Removing the expectation from @eq-var_general and evaluating for the</span>
<span id="cb57-603"><a href="#cb57-603" aria-hidden="true" tabindex="-1"></a>maximum likelihood estimator of $\theta$, we get the **sandwich</span>
<span id="cb57-604"><a href="#cb57-604" aria-hidden="true" tabindex="-1"></a>estimator** of the variance of $\hat{\theta}$.</span>
<span id="cb57-605"><a href="#cb57-605" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!sandwich}</span>
<span id="cb57-606"><a href="#cb57-606" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!maximum likelihood estimator}</span>
<span id="cb57-607"><a href="#cb57-607" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-608"><a href="#cb57-608" aria-hidden="true" tabindex="-1"></a>\hat{\sigma} ^ 2 _{\hat{\theta} s} = </span>
<span id="cb57-609"><a href="#cb57-609" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}</span>
<span id="cb57-610"><a href="#cb57-610" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N \gamma(y_n; \hat{\theta}) ^ 2 /N\right)/</span>
<span id="cb57-611"><a href="#cb57-611" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N \psi(y_n; \hat{\theta}) /N\right) ^ 2</span>
<span id="cb57-612"><a href="#cb57-612" aria-hidden="true" tabindex="-1"></a> =\frac{1}{N}\frac{\hat{v}_\gamma (\hat{\theta})}{\hat{m}_\psi(\hat{\theta})} </span>
<span id="cb57-613"><a href="#cb57-613" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-sandwich-est}</span>
<span id="cb57-614"><a href="#cb57-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-615"><a href="#cb57-615" aria-hidden="true" tabindex="-1"></a>@eq-sandwich-est is called a sandwich estimator for a reason that will</span>
<span id="cb57-616"><a href="#cb57-616" aria-hidden="true" tabindex="-1"></a>be clear when we'll compute it in the general case where more than one</span>
<span id="cb57-617"><a href="#cb57-617" aria-hidden="true" tabindex="-1"></a>parameter are estimated. It is a more general estimator than the</span>
<span id="cb57-618"><a href="#cb57-618" aria-hidden="true" tabindex="-1"></a>previous three, as its consistency doesn't rely on the information</span>
<span id="cb57-619"><a href="#cb57-619" aria-hidden="true" tabindex="-1"></a>equality property, which is only valid if the distribution of $y$ is</span>
<span id="cb57-620"><a href="#cb57-620" aria-hidden="true" tabindex="-1"></a>correctly specified. </span>
<span id="cb57-621"><a href="#cb57-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-622"><a href="#cb57-622" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the variance for the Poisson and the exponential distribution</span></span>
<span id="cb57-623"><a href="#cb57-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-624"><a href="#cb57-624" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Poisson distribution|(}</span>
<span id="cb57-625"><a href="#cb57-625" aria-hidden="true" tabindex="-1"></a>For the Poisson model, we have:</span>
<span id="cb57-626"><a href="#cb57-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-627"><a href="#cb57-627" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-628"><a href="#cb57-628" aria-hidden="true" tabindex="-1"></a>g(\theta) = - N + \frac{\sum_{n=1} ^ N y_n}{\theta} \mbox{ and } h(\theta) = -</span>
<span id="cb57-629"><a href="#cb57-629" aria-hidden="true" tabindex="-1"></a>\frac{\sum_{n=1} ^ N y_n}{\theta ^ 2}</span>
<span id="cb57-630"><a href="#cb57-630" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-631"><a href="#cb57-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-632"><a href="#cb57-632" aria-hidden="true" tabindex="-1"></a>The variance of the gradient is:</span>
<span id="cb57-633"><a href="#cb57-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-634"><a href="#cb57-634" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-635"><a href="#cb57-635" aria-hidden="true" tabindex="-1"></a>\mbox{V}\left(g(\theta)\right) = \frac{1}{\theta ^</span>
<span id="cb57-636"><a href="#cb57-636" aria-hidden="true" tabindex="-1"></a>2}\mbox{V}\left(\sum_{n=1} ^ N y_n\right) = </span>
<span id="cb57-637"><a href="#cb57-637" aria-hidden="true" tabindex="-1"></a>\frac{1}{\theta ^</span>
<span id="cb57-638"><a href="#cb57-638" aria-hidden="true" tabindex="-1"></a>2}\sum_{n=1} ^ N \mbox{V}\left(y_n\right) = </span>
<span id="cb57-639"><a href="#cb57-639" aria-hidden="true" tabindex="-1"></a>\frac{1}{\theta ^</span>
<span id="cb57-640"><a href="#cb57-640" aria-hidden="true" tabindex="-1"></a>2} N \theta = </span>
<span id="cb57-641"><a href="#cb57-641" aria-hidden="true" tabindex="-1"></a>\frac{N}{\theta}</span>
<span id="cb57-642"><a href="#cb57-642" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-643"><a href="#cb57-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-644"><a href="#cb57-644" aria-hidden="true" tabindex="-1"></a>The first equality holds because of the random sample hypothesis and</span>
<span id="cb57-645"><a href="#cb57-645" aria-hidden="true" tabindex="-1"></a>the second one because $\mbox{V}(y) = \theta$. The expected value</span>
<span id="cb57-646"><a href="#cb57-646" aria-hidden="true" tabindex="-1"></a>of the hessian is:</span>
<span id="cb57-647"><a href="#cb57-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-648"><a href="#cb57-648" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-649"><a href="#cb57-649" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left(h(\theta)\right) = -</span>
<span id="cb57-650"><a href="#cb57-650" aria-hidden="true" tabindex="-1"></a>\frac{\sum_{n=1} ^ N \mbox{E}(y_n)}{\theta ^ 2} = - \frac{N\theta}{\theta ^ 2} = -\frac{N}{\theta}</span>
<span id="cb57-651"><a href="#cb57-651" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-652"><a href="#cb57-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-653"><a href="#cb57-653" aria-hidden="true" tabindex="-1"></a>because $\mbox{E}(y) = \theta$. Therefore, we are in the case where</span>
<span id="cb57-654"><a href="#cb57-654" aria-hidden="true" tabindex="-1"></a>the information can be computed and the result illustrates</span>
<span id="cb57-655"><a href="#cb57-655" aria-hidden="true" tabindex="-1"></a>the information equality. The information based estimator of</span>
<span id="cb57-656"><a href="#cb57-656" aria-hidden="true" tabindex="-1"></a>$\hat{\theta}$ is:</span>
<span id="cb57-657"><a href="#cb57-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-658"><a href="#cb57-658" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-659"><a href="#cb57-659" aria-hidden="true" tabindex="-1"></a>\iota(\hat{\theta}) = \frac{N}{\hat{\theta}} = \frac{N}{\bar{y}}</span>
<span id="cb57-660"><a href="#cb57-660" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-661"><a href="#cb57-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-662"><a href="#cb57-662" aria-hidden="true" tabindex="-1"></a>The individual contributions to the gradient are: $\gamma(y_n; \theta)</span>
<span id="cb57-663"><a href="#cb57-663" aria-hidden="true" tabindex="-1"></a>= - 1 + y_n / \theta$, so that the gradient-based estimator of the</span>
<span id="cb57-664"><a href="#cb57-664" aria-hidden="true" tabindex="-1"></a>information is:</span>
<span id="cb57-665"><a href="#cb57-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-666"><a href="#cb57-666" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-667"><a href="#cb57-667" aria-hidden="true" tabindex="-1"></a>\iota_g(\theta) = \sum_{n=1}^N \gamma(y_n; \theta) ^ 2 = </span>
<span id="cb57-668"><a href="#cb57-668" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N \frac{(y_n - \theta) ^ 2}{\theta ^ 2}</span>
<span id="cb57-669"><a href="#cb57-669" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-670"><a href="#cb57-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-671"><a href="#cb57-671" aria-hidden="true" tabindex="-1"></a>Evaluating $\iota_g$ for the maximum likelihood estimator, we finally</span>
<span id="cb57-672"><a href="#cb57-672" aria-hidden="true" tabindex="-1"></a>get:</span>
<span id="cb57-673"><a href="#cb57-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-674"><a href="#cb57-674" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-675"><a href="#cb57-675" aria-hidden="true" tabindex="-1"></a>\iota_g(\hat{\theta}) = </span>
<span id="cb57-676"><a href="#cb57-676" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^N \frac{(y_n - \bar{y}) ^ 2}{\bar{y} ^ 2} = N \frac{\hat{\sigma}_y ^ 2}{\bar{y} ^ 2}</span>
<span id="cb57-677"><a href="#cb57-677" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-678"><a href="#cb57-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-679"><a href="#cb57-679" aria-hidden="true" tabindex="-1"></a>For the hessian-based estimator of the information, we consider the</span>
<span id="cb57-680"><a href="#cb57-680" aria-hidden="true" tabindex="-1"></a>opposite of the hessian evaluated for the ML estimator:</span>
<span id="cb57-681"><a href="#cb57-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-682"><a href="#cb57-682" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-683"><a href="#cb57-683" aria-hidden="true" tabindex="-1"></a>\iota_h(\hat{\theta}) = \frac{\sum_n y_n}{\hat{\theta} ^ 2} =</span>
<span id="cb57-684"><a href="#cb57-684" aria-hidden="true" tabindex="-1"></a>\frac{N}{\bar{y}}</span>
<span id="cb57-685"><a href="#cb57-685" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-686"><a href="#cb57-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-687"><a href="#cb57-687" aria-hidden="true" tabindex="-1"></a>Finally, from @eq-sandwich-est, the sandwich estimator of the variance</span>
<span id="cb57-688"><a href="#cb57-688" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb57-689"><a href="#cb57-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-690"><a href="#cb57-690" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-691"><a href="#cb57-691" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}s} = \frac{1}{N}\frac{\hat{\sigma}_y ^ 2 / \bar{y} ^ 2}{(1/\bar{y}) ^ 2} = \frac{\hat{\sigma} ^ 2_y}{N}</span>
<span id="cb57-692"><a href="#cb57-692" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-693"><a href="#cb57-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-694"><a href="#cb57-694" aria-hidden="true" tabindex="-1"></a>To summarize, the four estimators of the variance of $\hat{\theta}$ are:</span>
<span id="cb57-695"><a href="#cb57-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-696"><a href="#cb57-696" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-697"><a href="#cb57-697" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-698"><a href="#cb57-698" aria-hidden="true" tabindex="-1"></a>\begin{array}{rclrcl}</span>
<span id="cb57-699"><a href="#cb57-699" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}i} ^ 2 &amp;=&amp; \iota(\hat{\theta}) ^ {-1} &amp;=&amp;</span>
<span id="cb57-700"><a href="#cb57-700" aria-hidden="true" tabindex="-1"></a>\bar{y} / N <span class="sc">\\</span></span>
<span id="cb57-701"><a href="#cb57-701" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}g} ^ 2 &amp;=&amp; \iota_g(\hat{\theta}) ^ {-1} &amp;=&amp;</span>
<span id="cb57-702"><a href="#cb57-702" aria-hidden="true" tabindex="-1"></a>\frac{\bar{y} ^ 2}{\hat{\sigma}_y^ 2} / N <span class="sc">\\</span></span>
<span id="cb57-703"><a href="#cb57-703" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}h} ^ 2 &amp;=&amp; \iota_h(\hat{\theta}) ^ {-1} &amp;=&amp;</span>
<span id="cb57-704"><a href="#cb57-704" aria-hidden="true" tabindex="-1"></a>\bar{y} / N <span class="sc">\\</span></span>
<span id="cb57-705"><a href="#cb57-705" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}s} ^ 2 &amp;&amp; &amp;=&amp; \hat{\sigma}^2_y / N</span>
<span id="cb57-706"><a href="#cb57-706" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-707"><a href="#cb57-707" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-708"><a href="#cb57-708" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-709"><a href="#cb57-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-710"><a href="#cb57-710" aria-hidden="true" tabindex="-1"></a>Note that in this case, $\iota(\hat{\theta}) =</span>
<span id="cb57-711"><a href="#cb57-711" aria-hidden="true" tabindex="-1"></a>\iota_h(\hat{\theta})$ and that, if the Poisson distribution</span>
<span id="cb57-712"><a href="#cb57-712" aria-hidden="true" tabindex="-1"></a>hypothesis is correct, $\mbox{plim} \,\bar{y} = \mbox{plim}\,</span>
<span id="cb57-713"><a href="#cb57-713" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_y ^ 2 = \theta_0$ so that the four estimators are consistent as</span>
<span id="cb57-714"><a href="#cb57-714" aria-hidden="true" tabindex="-1"></a>$\bar{y}$ and $\hat{\sigma}_y ^ 2$ both converge to $\theta_0$.</span>
<span id="cb57-715"><a href="#cb57-715" aria-hidden="true" tabindex="-1"></a>Computing these four estimations of the variance of $\hat{\theta}$ for the <span class="in">`ncaught`</span> variable, we get:</span>
<span id="cb57-716"><a href="#cb57-716" aria-hidden="true" tabindex="-1"></a>\idxfun{nrow}{base}</span>
<span id="cb57-717"><a href="#cb57-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-718"><a href="#cb57-718" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-719"><a href="#cb57-719" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-720"><a href="#cb57-720" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(cartels) ; y <span class="ot">&lt;-</span> cartels<span class="sc">$</span>ncaught</span>
<span id="cb57-721"><a href="#cb57-721" aria-hidden="true" tabindex="-1"></a>mean_y <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) ; var_y <span class="ot">&lt;-</span> <span class="fu">mean</span>( (y <span class="sc">-</span> mean_y) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb57-722"><a href="#cb57-722" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">info =</span> mean_y <span class="sc">/</span> N, <span class="at">gradient =</span> mean_y <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> var_y <span class="sc">/</span> N,</span>
<span id="cb57-723"><a href="#cb57-723" aria-hidden="true" tabindex="-1"></a>  <span class="at">hessian =</span> mean_y <span class="sc">/</span> N, <span class="at">sandwich =</span> var_y <span class="sc">/</span> N) <span class="sc">%&gt;%</span></span>
<span id="cb57-724"><a href="#cb57-724" aria-hidden="true" tabindex="-1"></a>    sqrt <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">3</span>)</span>
<span id="cb57-725"><a href="#cb57-725" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-726"><a href="#cb57-726" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Poisson distribution|)}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution|(}</span>
<span id="cb57-727"><a href="#cb57-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-728"><a href="#cb57-728" aria-hidden="true" tabindex="-1"></a>For the exponential distribution, remember that $\lambda(y; \theta)=</span>
<span id="cb57-729"><a href="#cb57-729" aria-hidden="true" tabindex="-1"></a>\ln \theta - \theta y$, $\gamma(y; \theta) = 1 / \theta - y$ and</span>
<span id="cb57-730"><a href="#cb57-730" aria-hidden="true" tabindex="-1"></a>$\psi(y; \theta) = - 1 / \theta ^ 2$. As $h(\theta, y) = \sum_n</span>
<span id="cb57-731"><a href="#cb57-731" aria-hidden="true" tabindex="-1"></a>\psi(y; \theta)$ the hessian is obviously $h(\theta, y) = - N / \theta</span>
<span id="cb57-732"><a href="#cb57-732" aria-hidden="true" tabindex="-1"></a>^ 2$ and equals its expected value, as it doesn't depend on</span>
<span id="cb57-733"><a href="#cb57-733" aria-hidden="true" tabindex="-1"></a>$y$. Therefore, $\iota(\theta) = N / \theta ^ 2$. Computing the variance</span>
<span id="cb57-734"><a href="#cb57-734" aria-hidden="true" tabindex="-1"></a>of the gradient, we get:</span>
<span id="cb57-735"><a href="#cb57-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-736"><a href="#cb57-736" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-737"><a href="#cb57-737" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-738"><a href="#cb57-738" aria-hidden="true" tabindex="-1"></a>\mbox{V}\left(g(\theta)\right) &amp;=&amp; \mbox{E}\left(\sum_n \gamma(y_n; \theta) ^ 2 \right)</span>
<span id="cb57-739"><a href="#cb57-739" aria-hidden="true" tabindex="-1"></a>=\sum_n \mbox{E}(\gamma(y_n; \theta) ^ 2)<span class="sc">\\</span></span>
<span id="cb57-740"><a href="#cb57-740" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \sum_n \mbox{E}\left((y -  1</span>
<span id="cb57-741"><a href="#cb57-741" aria-hidden="true" tabindex="-1"></a>/ \theta)^2\right) = N \mbox{V}(y) = N / \theta ^ 2</span>
<span id="cb57-742"><a href="#cb57-742" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-743"><a href="#cb57-743" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-744"><a href="#cb57-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-745"><a href="#cb57-745" aria-hidden="true" tabindex="-1"></a>because the expected value and the variance of $y$ are respectively</span>
<span id="cb57-746"><a href="#cb57-746" aria-hidden="true" tabindex="-1"></a>equal to $1 / \theta$ and $1 / \theta ^ 2$ for an exponential</span>
<span id="cb57-747"><a href="#cb57-747" aria-hidden="true" tabindex="-1"></a>distribution. Therefore $\iota(\theta) = N / \theta ^ 2$ and the</span>
<span id="cb57-748"><a href="#cb57-748" aria-hidden="true" tabindex="-1"></a>information-based estimator of the information for $\theta =</span>
<span id="cb57-749"><a href="#cb57-749" aria-hidden="true" tabindex="-1"></a>\hat{\theta}$ is, as $\hat{\theta} = 1 / \bar{y}$,</span>
<span id="cb57-750"><a href="#cb57-750" aria-hidden="true" tabindex="-1"></a>$\iota(\hat{\theta}) = N \bar{y} ^ 2$. The same result obviously</span>
<span id="cb57-751"><a href="#cb57-751" aria-hidden="true" tabindex="-1"></a>applies to the hessian-based approximation of the information, which</span>
<span id="cb57-752"><a href="#cb57-752" aria-hidden="true" tabindex="-1"></a>is: $\iota_h(\hat{\theta}) = \frac{N}{\hat{\theta} ^ 2} = N\bar{y} ^</span>
<span id="cb57-753"><a href="#cb57-753" aria-hidden="true" tabindex="-1"></a>2$. Considering now the gradient-based estimate of the information, we</span>
<span id="cb57-754"><a href="#cb57-754" aria-hidden="true" tabindex="-1"></a>have:</span>
<span id="cb57-755"><a href="#cb57-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-756"><a href="#cb57-756" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-757"><a href="#cb57-757" aria-hidden="true" tabindex="-1"></a>\iota_g(\theta, y) = \sum_{n=1} ^ N (1 / \theta - y_n) ^ 2=\sum_{n=1} ^ N (y_n - 1 / \theta) ^ 2</span>
<span id="cb57-758"><a href="#cb57-758" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-759"><a href="#cb57-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-760"><a href="#cb57-760" aria-hidden="true" tabindex="-1"></a>as $\hat{\theta} = 1 / \bar{y}$, evaluated for the ML estimator, we</span>
<span id="cb57-761"><a href="#cb57-761" aria-hidden="true" tabindex="-1"></a>have $\iota_g(\hat{\theta}, y) = N \hat{\sigma}_y ^ 2$.</span>
<span id="cb57-762"><a href="#cb57-762" aria-hidden="true" tabindex="-1"></a>Finally, the sandwich estimator of the variance of $\hat{\theta}$ is:</span>
<span id="cb57-763"><a href="#cb57-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-764"><a href="#cb57-764" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-765"><a href="#cb57-765" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}s}^2 = \frac{1}{N}\frac{\hat{\sigma}_y ^ 2}{(\bar{y} ^ 2) ^ 2}=\frac{1}{N}\frac{\hat{\sigma}_y ^ 2}{\bar{y} ^ 4}</span>
<span id="cb57-766"><a href="#cb57-766" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-767"><a href="#cb57-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-768"><a href="#cb57-768" aria-hidden="true" tabindex="-1"></a>The four estimators of the variance for the exponential</span>
<span id="cb57-769"><a href="#cb57-769" aria-hidden="true" tabindex="-1"></a>distribution are then:</span>
<span id="cb57-770"><a href="#cb57-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-771"><a href="#cb57-771" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-772"><a href="#cb57-772" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-773"><a href="#cb57-773" aria-hidden="true" tabindex="-1"></a>\begin{array}{rclrcl}</span>
<span id="cb57-774"><a href="#cb57-774" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}i} ^ 2 &amp;=&amp; \iota(\hat{\theta}) ^ {-1} &amp;=&amp;</span>
<span id="cb57-775"><a href="#cb57-775" aria-hidden="true" tabindex="-1"></a>1 / (N \bar{y} ^ 2) <span class="sc">\\</span></span>
<span id="cb57-776"><a href="#cb57-776" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}g} ^ 2 &amp;=&amp; \iota_g(\hat{\theta}) ^ {-1} &amp;=&amp;</span>
<span id="cb57-777"><a href="#cb57-777" aria-hidden="true" tabindex="-1"></a> 1 / (N \hat{\sigma}_y^ 2) <span class="sc">\\</span></span>
<span id="cb57-778"><a href="#cb57-778" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}h} ^ 2 &amp;=&amp; \iota_h(\hat{\theta}) ^ {-1} &amp;=&amp;</span>
<span id="cb57-779"><a href="#cb57-779" aria-hidden="true" tabindex="-1"></a>1 / (N \bar{y} ^ 2) <span class="sc">\\</span></span>
<span id="cb57-780"><a href="#cb57-780" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\theta}s} ^ 2 &amp;&amp; &amp;=&amp; \hat{\sigma}_y ^ 2/ (N \bar{y} ^ 4)</span>
<span id="cb57-781"><a href="#cb57-781" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-782"><a href="#cb57-782" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-783"><a href="#cb57-783" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-784"><a href="#cb57-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-785"><a href="#cb57-785" aria-hidden="true" tabindex="-1"></a>Computing this four estimations of the variance of $\hat{\theta}$, we get for the <span class="in">`oil`</span> variable:</span>
<span id="cb57-786"><a href="#cb57-786" aria-hidden="true" tabindex="-1"></a>\idxfun{length}{base}\idxfun{round}{base}</span>
<span id="cb57-787"><a href="#cb57-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-788"><a href="#cb57-788" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-789"><a href="#cb57-789" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-790"><a href="#cb57-790" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> oil<span class="sc">$</span>dur ; N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb57-791"><a href="#cb57-791" aria-hidden="true" tabindex="-1"></a>mean_y <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) ; var_y <span class="ot">&lt;-</span> <span class="fu">mean</span>( (y <span class="sc">-</span> mean_y) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb57-792"><a href="#cb57-792" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">info =</span> <span class="dv">1</span> <span class="sc">/</span> (N <span class="sc">*</span> mean_y <span class="sc">^</span> <span class="dv">2</span>), <span class="at">gradient =</span> <span class="dv">1</span> <span class="sc">/</span> (N <span class="sc">*</span> var_y),</span>
<span id="cb57-793"><a href="#cb57-793" aria-hidden="true" tabindex="-1"></a>  <span class="at">hessian =</span> <span class="dv">1</span> <span class="sc">/</span> (N <span class="sc">*</span> mean_y <span class="sc">^</span> <span class="dv">2</span>),</span>
<span id="cb57-794"><a href="#cb57-794" aria-hidden="true" tabindex="-1"></a>  <span class="at">sandwich =</span> var_y <span class="sc">/</span> (N <span class="sc">*</span> mean_y <span class="sc">^</span> <span class="dv">4</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb57-795"><a href="#cb57-795" aria-hidden="true" tabindex="-1"></a>    sqrt <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">4</span>)</span>
<span id="cb57-796"><a href="#cb57-796" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-797"><a href="#cb57-797" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution|)}</span>
<span id="cb57-798"><a href="#cb57-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-799"><a href="#cb57-799" aria-hidden="true" tabindex="-1"></a><span class="fu">## ML estimation in the general case {#sec-ml_general}</span></span>
<span id="cb57-800"><a href="#cb57-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-801"><a href="#cb57-801" aria-hidden="true" tabindex="-1"></a>Compared to the simple case analyzed in the previous section, we</span>
<span id="cb57-802"><a href="#cb57-802" aria-hidden="true" tabindex="-1"></a>consider in this section two extensions:</span>
<span id="cb57-803"><a href="#cb57-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-804"><a href="#cb57-804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta$ is now a vector of unknown parameters</span>
<span id="cb57-805"><a href="#cb57-805" aria-hidden="true" tabindex="-1"></a>  that we seek to estimate, which means that the gradient is a vector</span>
<span id="cb57-806"><a href="#cb57-806" aria-hidden="true" tabindex="-1"></a>  and the hessian is a matrix,</span>
<span id="cb57-807"><a href="#cb57-807" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the density for observation $n$ not only depends on the value of the response $y_n$, but also on the   value of a vector of covariates $x_n$. </span>
<span id="cb57-808"><a href="#cb57-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-809"><a href="#cb57-809" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation and properties of the ML estimator</span></span>
<span id="cb57-810"><a href="#cb57-810" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-811"><a href="#cb57-811" aria-hidden="true" tabindex="-1"></a>The density (or the probability mass) for observation $n$ is now:</span>
<span id="cb57-812"><a href="#cb57-812" aria-hidden="true" tabindex="-1"></a>$\phi(y_n ; \theta, x_n) = \phi_n(y_n ; \theta)$; therefore, written as</span>
<span id="cb57-813"><a href="#cb57-813" aria-hidden="true" tabindex="-1"></a>a function of $y$ and $\theta$ only, the density is now indexed by $n$,</span>
<span id="cb57-814"><a href="#cb57-814" aria-hidden="true" tabindex="-1"></a>as it is a function of $x_n$. Denoting as previously $\lambda_n(y_n;</span>
<span id="cb57-815"><a href="#cb57-815" aria-hidden="true" tabindex="-1"></a>\theta) = \ln \phi_n(y_n; \theta)$, $\gamma_n = \frac{\partial</span>
<span id="cb57-816"><a href="#cb57-816" aria-hidden="true" tabindex="-1"></a>\lambda_n}{\partial \theta}$ and $\Psi_n = \frac{\partial ^2</span>
<span id="cb57-817"><a href="#cb57-817" aria-hidden="true" tabindex="-1"></a>\lambda_n}{\partial \theta \partial \theta^\top}$^[We now have a</span>
<span id="cb57-818"><a href="#cb57-818" aria-hidden="true" tabindex="-1"></a>matrix of second derivatives, denoted by $\Psi$, which replaces the scalar</span>
<span id="cb57-819"><a href="#cb57-819" aria-hidden="true" tabindex="-1"></a>second derivative $\psi$ in the previous section.], the log-likelihood</span>
<span id="cb57-820"><a href="#cb57-820" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb57-821"><a href="#cb57-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-822"><a href="#cb57-822" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-823"><a href="#cb57-823" aria-hidden="true" tabindex="-1"></a>\ln L(\theta, y, X) = \sum_{n=1} ^ N \ln \phi_n(y_n; \theta) =</span>
<span id="cb57-824"><a href="#cb57-824" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N\lambda_n(y_n; \theta)</span>
<span id="cb57-825"><a href="#cb57-825" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-826"><a href="#cb57-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-827"><a href="#cb57-827" aria-hidden="true" tabindex="-1"></a>The gradient and the hessian are:</span>
<span id="cb57-828"><a href="#cb57-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-829"><a href="#cb57-829" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-830"><a href="#cb57-830" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-831"><a href="#cb57-831" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-832"><a href="#cb57-832" aria-hidden="true" tabindex="-1"></a>g(\theta, y, X) &amp;=&amp; \sum_{n=1} ^ N \gamma_n(y_n; \theta)<span class="sc">\\</span></span>
<span id="cb57-833"><a href="#cb57-833" aria-hidden="true" tabindex="-1"></a>H(\theta, y, X) &amp;=&amp; \sum_{n=1} ^ N \Psi_n(y_n; \theta)</span>
<span id="cb57-834"><a href="#cb57-834" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-835"><a href="#cb57-835" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-836"><a href="#cb57-836" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-837"><a href="#cb57-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-838"><a href="#cb57-838" aria-hidden="true" tabindex="-1"></a>The variance of the score is the **information matrix**, denoted by</span>
<span id="cb57-839"><a href="#cb57-839" aria-hidden="true" tabindex="-1"></a>$I(\theta, X)$ and, by virtue of the **information matrix equality**</span>
<span id="cb57-840"><a href="#cb57-840" aria-hidden="true" tabindex="-1"></a>demonstrated previously in the scalar case, it is equal to the</span>
<span id="cb57-841"><a href="#cb57-841" aria-hidden="true" tabindex="-1"></a>opposite of the expected value of the hessian:</span>
<span id="cb57-842"><a href="#cb57-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-843"><a href="#cb57-843" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-844"><a href="#cb57-844" aria-hidden="true" tabindex="-1"></a>I(\theta, X) = \mbox{V}(g(\theta, y, X)) = - \mbox{E} (H(\theta, y, X))</span>
<span id="cb57-845"><a href="#cb57-845" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-846"><a href="#cb57-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-847"><a href="#cb57-847" aria-hidden="true" tabindex="-1"></a>Note that now, each individual contribution to the gradient and to the</span>
<span id="cb57-848"><a href="#cb57-848" aria-hidden="true" tabindex="-1"></a>hessian depends on $x_n$; therefore, their variance (for the gradient)</span>
<span id="cb57-849"><a href="#cb57-849" aria-hidden="true" tabindex="-1"></a>and their expectation (for the hessian) are not constant as previously. In terms of</span>
<span id="cb57-850"><a href="#cb57-850" aria-hidden="true" tabindex="-1"></a>the individual observations, the information matrix equality states that:</span>
<span id="cb57-851"><a href="#cb57-851" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{information matrix equality|(}</span>
<span id="cb57-852"><a href="#cb57-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-853"><a href="#cb57-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-854"><a href="#cb57-854" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-855"><a href="#cb57-855" aria-hidden="true" tabindex="-1"></a>\mbox{I}(\theta; X) = \sum_{n=1} ^ N</span>
<span id="cb57-856"><a href="#cb57-856" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left(\gamma_n(y_n; \theta)\gamma_n(y_n; \theta)^\top\right) = -</span>
<span id="cb57-857"><a href="#cb57-857" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N\mbox{E}\left(\Psi_n(y_n; \theta)\right)</span>
<span id="cb57-858"><a href="#cb57-858" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-859"><a href="#cb57-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-860"><a href="#cb57-860" aria-hidden="true" tabindex="-1"></a>Define the asymptotic information and the asymptotic hessian as:</span>
<span id="cb57-861"><a href="#cb57-861" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic information matrix}</span>
<span id="cb57-862"><a href="#cb57-862" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic hessian}</span>
<span id="cb57-863"><a href="#cb57-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-864"><a href="#cb57-864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-865"><a href="#cb57-865" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-866"><a href="#cb57-866" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-867"><a href="#cb57-867" aria-hidden="true" tabindex="-1"></a>\mathbf{\mathcal{I}} &amp;=&amp; \frac{1}{N}\lim_{n\rightarrow +</span>
<span id="cb57-868"><a href="#cb57-868" aria-hidden="true" tabindex="-1"></a>\infty}\sum_{n=1} ^ N \gamma_n(y_n; \theta)\gamma_n(y_n; \theta)^\top <span class="sc">\\</span></span>
<span id="cb57-869"><a href="#cb57-869" aria-hidden="true" tabindex="-1"></a>\mathbf{\mathcal{H}} &amp;=&amp; \frac{1}{N}\lim_{n\rightarrow +</span>
<span id="cb57-870"><a href="#cb57-870" aria-hidden="true" tabindex="-1"></a>\infty}\sum_{n=1} ^ N \Psi_n(y_n; \theta)</span>
<span id="cb57-871"><a href="#cb57-871" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-872"><a href="#cb57-872" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-873"><a href="#cb57-873" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-874"><a href="#cb57-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-875"><a href="#cb57-875" aria-hidden="true" tabindex="-1"></a>The information matrix equality implies that:</span>
<span id="cb57-876"><a href="#cb57-876" aria-hidden="true" tabindex="-1"></a>$\mathbf{\mathcal{I}} = - \mathbf{\mathcal{H}}$.</span>
<span id="cb57-877"><a href="#cb57-877" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{information matrix equality|)}</span>
<span id="cb57-878"><a href="#cb57-878" aria-hidden="true" tabindex="-1"></a>At the ML estimate, the gradient is 0:</span>
<span id="cb57-879"><a href="#cb57-879" aria-hidden="true" tabindex="-1"></a>$g(\hat{\theta}, y, X) = 0$. Using a first-order Taylor expansion</span>
<span id="cb57-880"><a href="#cb57-880" aria-hidden="true" tabindex="-1"></a>around the true value $\theta_0$, we have:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Taylor expansion!exact}</span>
<span id="cb57-881"><a href="#cb57-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-882"><a href="#cb57-882" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-883"><a href="#cb57-883" aria-hidden="true" tabindex="-1"></a>g(\hat{\theta}, y, X) = g(\theta_0, y, X) + </span>
<span id="cb57-884"><a href="#cb57-884" aria-hidden="true" tabindex="-1"></a>H(\bar{\theta}, y, X) (\hat{\theta} - \theta_0) = 0</span>
<span id="cb57-885"><a href="#cb57-885" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-886"><a href="#cb57-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-887"><a href="#cb57-887" aria-hidden="true" tabindex="-1"></a>The equivalent of $\bar{\theta}$ lying in the $\theta_0-\hat{\theta}$ interval for the scalar case (see @eq-exact_taylor_expansion) is that $\| \bar{\theta} - \theta_0\| \leq</span>
<span id="cb57-888"><a href="#cb57-888" aria-hidden="true" tabindex="-1"></a>\| \hat{\theta}-\theta_0\|$. Solving this equation for</span>
<span id="cb57-889"><a href="#cb57-889" aria-hidden="true" tabindex="-1"></a>$\hat{\theta}-\theta_0$, we get, multiplying by $\sqrt{N}$:</span>
<span id="cb57-890"><a href="#cb57-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-891"><a href="#cb57-891" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-892"><a href="#cb57-892" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\theta}-\theta_0) = \left(- \frac{H(\bar{\theta}, y, X)}{N}\right)^{-1}\frac{g(\theta_0, y, X)}{\sqrt{N}}</span>
<span id="cb57-893"><a href="#cb57-893" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-894"><a href="#cb57-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-895"><a href="#cb57-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-896"><a href="#cb57-896" aria-hidden="true" tabindex="-1"></a>The probability limit of the term in brackets is $-\mathbf{\mathcal{H}}$</span>
<span id="cb57-897"><a href="#cb57-897" aria-hidden="true" tabindex="-1"></a>(as $\bar{\theta}$ converges to $\theta_0$)</span>
<span id="cb57-898"><a href="#cb57-898" aria-hidden="true" tabindex="-1"></a>and therefore:</span>
<span id="cb57-899"><a href="#cb57-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-900"><a href="#cb57-900" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-901"><a href="#cb57-901" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\theta}-\theta_0) \overset{a}{=} \left(- \mathcal{H}\right)^{-1}\frac{g(\theta_0, y, X)}{\sqrt{N}}</span>
<span id="cb57-902"><a href="#cb57-902" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-nondeg_theta}</span>
<span id="cb57-903"><a href="#cb57-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-904"><a href="#cb57-904" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic equality}</span>
<span id="cb57-905"><a href="#cb57-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-906"><a href="#cb57-906" aria-hidden="true" tabindex="-1"></a>where $w\overset{a}{=}z$ means that $w$ is asymptotically equal to $z$, i.e., it tends to the same limit in probability <span class="co">[</span><span class="ot">see @DAVI:MACK:04, p. 205</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Davidson}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{McKinnon}.</span>
<span id="cb57-907"><a href="#cb57-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-908"><a href="#cb57-908" aria-hidden="true" tabindex="-1"></a>The asymptotic variance of the second term is:</span>
<span id="cb57-909"><a href="#cb57-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-910"><a href="#cb57-910" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-911"><a href="#cb57-911" aria-hidden="true" tabindex="-1"></a>\mbox{V}\left(\lim_{n \rightarrow \infty}\frac{g(\theta_0; y,</span>
<span id="cb57-912"><a href="#cb57-912" aria-hidden="true" tabindex="-1"></a>X}{\sqrt{N}}\right) \overset{a}{=} \lim_{n\rightarrow \infty} \frac{1}{N}\sum_{n =</span>
<span id="cb57-913"><a href="#cb57-913" aria-hidden="true" tabindex="-1"></a>1} ^ N \gamma_n(y_n; \theta_0)\gamma_n(y_n; \theta_0)^\top = \mathbf{\mathcal{I}}</span>
<span id="cb57-914"><a href="#cb57-914" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-915"><a href="#cb57-915" aria-hidden="true" tabindex="-1"></a>Therefore, the asymptotic variance of $\sqrt{N}(\hat{\theta}-\theta_0)$ is</span>
<span id="cb57-916"><a href="#cb57-916" aria-hidden="true" tabindex="-1"></a>$\mathbf{\mathcal{H}} ^ {-1} \mathbf{\mathcal{I}} \mathbf{\mathcal{H}}</span>
<span id="cb57-917"><a href="#cb57-917" aria-hidden="true" tabindex="-1"></a>^ {-1}$, which reduces to, applying the information matrix equality</span>
<span id="cb57-918"><a href="#cb57-918" aria-hidden="true" tabindex="-1"></a>result, $\mathbf{\mathcal{I}} ^ {-1}$. Applying the central-limit</span>
<span id="cb57-919"><a href="#cb57-919" aria-hidden="true" tabindex="-1"></a>theorem, we finally get:</span>
<span id="cb57-920"><a href="#cb57-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-921"><a href="#cb57-921" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-922"><a href="#cb57-922" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\theta}-\theta_0) \overset{p}{\rightarrow} \mathcal{N}(0, \mathcal{I} ^ {-1})</span>
<span id="cb57-923"><a href="#cb57-923" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-924"><a href="#cb57-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-925"><a href="#cb57-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-926"><a href="#cb57-926" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-927"><a href="#cb57-927" aria-hidden="true" tabindex="-1"></a>\hat{\theta} \overset{a}{\sim} \mathcal{N}(\theta_0,</span>
<span id="cb57-928"><a href="#cb57-928" aria-hidden="true" tabindex="-1"></a>\mathbf{\mathcal{I}} ^ {-1} / N)</span>
<span id="cb57-929"><a href="#cb57-929" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-930"><a href="#cb57-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-931"><a href="#cb57-931" aria-hidden="true" tabindex="-1"></a>The asymptotic variance can be estimated using the information</span>
<span id="cb57-932"><a href="#cb57-932" aria-hidden="true" tabindex="-1"></a>evaluated at $\hat{\theta}$ if the variance of the gradient or the expectation of the hessian can be computed:</span>
<span id="cb57-933"><a href="#cb57-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-934"><a href="#cb57-934" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-935"><a href="#cb57-935" aria-hidden="true" tabindex="-1"></a>\hat{\mbox{V}}_I(\hat{\theta}) = \left(\sum_{n=1} ^ N</span>
<span id="cb57-936"><a href="#cb57-936" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left(\gamma_n(y_n; \hat{\theta})\gamma_n(y_n; \hat{\theta})^\top\right)\right) ^ {-1} = \left(-</span>
<span id="cb57-937"><a href="#cb57-937" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N\mbox{E}\left(\Psi_n(y_n; \hat{\theta})\right)\right) ^ {-1}</span>
<span id="cb57-938"><a href="#cb57-938" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-939"><a href="#cb57-939" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!information based}</span>
<span id="cb57-940"><a href="#cb57-940" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!gradient based}</span>
<span id="cb57-941"><a href="#cb57-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-942"><a href="#cb57-942" aria-hidden="true" tabindex="-1"></a>Two other possible estimators are obtained by evaluating</span>
<span id="cb57-943"><a href="#cb57-943" aria-hidden="true" tabindex="-1"></a>the two previous expressions without the expectation. The gradient-based estimator, also called the **outer product of the gradient** or the **BHHH**^<span class="co">[</span><span class="ot">The initials of the authors of @BERN:HALL:HALL:HAUS:74\index[author]{Berndt}\index[author]{Hall, Browny}\index[author]{Hall, Robert}\index[author]{Hausman} who first proposed this estimator.</span><span class="co">]</span></span>
<span id="cb57-944"><a href="#cb57-944" aria-hidden="true" tabindex="-1"></a>estimator is:</span>
<span id="cb57-945"><a href="#cb57-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-946"><a href="#cb57-946" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-947"><a href="#cb57-947" aria-hidden="true" tabindex="-1"></a>\hat{\mbox{V}}_g(\hat{\theta}) = \left(\sum_{n=1} ^ N</span>
<span id="cb57-948"><a href="#cb57-948" aria-hidden="true" tabindex="-1"></a>\left(\gamma_n(y_n; \hat{\theta})\gamma_n(y_n; \hat{\theta})^\top\right)\right) ^ {-1}</span>
<span id="cb57-949"><a href="#cb57-949" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-950"><a href="#cb57-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-951"><a href="#cb57-951" aria-hidden="true" tabindex="-1"></a>and the hessian-based estimator is:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!hessian based}</span>
<span id="cb57-952"><a href="#cb57-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-953"><a href="#cb57-953" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-954"><a href="#cb57-954" aria-hidden="true" tabindex="-1"></a>\hat{\mbox{V}}_H(\hat{\theta}) = \left(-</span>
<span id="cb57-955"><a href="#cb57-955" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N\Psi_n(y_n; \hat{\theta})\right) ^ {-1} = </span>
<span id="cb57-956"><a href="#cb57-956" aria-hidden="true" tabindex="-1"></a>\left(- H(\hat{\theta}, y, X)\right) ^ {-1}</span>
<span id="cb57-957"><a href="#cb57-957" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-958"><a href="#cb57-958" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!maximum likelihood estimator!sandwich}</span>
<span id="cb57-959"><a href="#cb57-959" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!maximum likelihood estimator}</span>
<span id="cb57-960"><a href="#cb57-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-961"><a href="#cb57-961" aria-hidden="true" tabindex="-1"></a>Finally, the sandwich estimator is based on the expression of the</span>
<span id="cb57-962"><a href="#cb57-962" aria-hidden="true" tabindex="-1"></a>asymptotic covariance of $\hat{\theta}$ before applying the information matrix</span>
<span id="cb57-963"><a href="#cb57-963" aria-hidden="true" tabindex="-1"></a>equality theorem. Then:</span>
<span id="cb57-964"><a href="#cb57-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-965"><a href="#cb57-965" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-966"><a href="#cb57-966" aria-hidden="true" tabindex="-1"></a>\hat{\mbox{V}}_s(\hat{\theta}) =</span>
<span id="cb57-967"><a href="#cb57-967" aria-hidden="true" tabindex="-1"></a>\left(- H(\hat{\theta}, y, X)\right) ^ {-1}</span>
<span id="cb57-968"><a href="#cb57-968" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N \left(\gamma_n(y_n; \hat{\theta})\gamma_n(y_n; \hat{\theta})^\top\right)\right)</span>
<span id="cb57-969"><a href="#cb57-969" aria-hidden="true" tabindex="-1"></a>\left(- H(\hat{\theta}, y, X)\right) ^ {-1}</span>
<span id="cb57-970"><a href="#cb57-970" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-971"><a href="#cb57-971" aria-hidden="true" tabindex="-1"></a>This estimator actually looks like a sandwich, the "meat" (the estimation of the variance of the gradient) being surrounded by two slices of "bread" (the inverse of the opposite of the hessian). </span>
<span id="cb57-972"><a href="#cb57-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-973"><a href="#cb57-973" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the estimators for the exponential distribution {#sec-ml_est_expon}</span></span>
<span id="cb57-974"><a href="#cb57-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-975"><a href="#cb57-975" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution|(}</span>
<span id="cb57-976"><a href="#cb57-976" aria-hidden="true" tabindex="-1"></a>We have seen in @sec-expon_distr_1par that we can get an explicit solution for the maximization of the likelihood of the one parameter exponential distribution. Once covariates are introduced, there are $K+1$ parameters to estimate and there is no longer an explicit solution. Then, a numerical optimization algorithm should be used. We'll present in this section the simplest algorithm, called the **Newton-Raphson** algorithm, using the <span class="in">`oil`</span> data set with two covariates:</span>
<span id="cb57-977"><a href="#cb57-977" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Newton-Raphson algorithm|(}</span>
<span id="cb57-978"><a href="#cb57-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-979"><a href="#cb57-979" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`p98`</span> is the adaptive expectations for the real after-tax oil prices formed at the time of the approval,</span>
<span id="cb57-980"><a href="#cb57-980" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`varp98`</span> is the volatility of the adaptive expectations for the real after-tax oil prices.</span>
<span id="cb57-981"><a href="#cb57-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-982"><a href="#cb57-982" aria-hidden="true" tabindex="-1"></a>For the exponential model, remember that $\mbox{E}(y) = 1 /</span>
<span id="cb57-983"><a href="#cb57-983" aria-hidden="true" tabindex="-1"></a>\theta$; $\theta$ should therefore be positive. The way the linear combination of the covariates $\gamma^\top z_n$ is related to the parameter of the distribution $\theta_n$</span>
<span id="cb57-984"><a href="#cb57-984" aria-hidden="true" tabindex="-1"></a> is called the **link**. It is customary to define</span>
<span id="cb57-985"><a href="#cb57-985" aria-hidden="true" tabindex="-1"></a>$\theta_n = e ^ {- \gamma ^ \top z_n}$ (so that $\ln \mbox{E}(y \mid</span>
<span id="cb57-986"><a href="#cb57-986" aria-hidden="true" tabindex="-1"></a>x_n) = - \ln \theta_n = \gamma ^ \top z_n$).</span>
<span id="cb57-987"><a href="#cb57-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-988"><a href="#cb57-988" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb57-989"><a href="#cb57-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-990"><a href="#cb57-990" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-991"><a href="#cb57-991" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-992"><a href="#cb57-992" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-993"><a href="#cb57-993" aria-hidden="true" tabindex="-1"></a>\ln L &amp;=&amp; - \sum_{n=1} ^ N \left(\gamma ^ \top z_n + e ^ {- \gamma ^  \top</span>
<span id="cb57-994"><a href="#cb57-994" aria-hidden="true" tabindex="-1"></a>z_n} y_n\right) <span class="sc">\\</span></span>
<span id="cb57-995"><a href="#cb57-995" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln L}{\partial \gamma} &amp;=&amp; - \sum_{n=1} ^ N\left(1 - e ^ {-\gamma ^ \top</span>
<span id="cb57-996"><a href="#cb57-996" aria-hidden="true" tabindex="-1"></a>z_n}y_n\right)z_n <span class="sc">\\</span></span>
<span id="cb57-997"><a href="#cb57-997" aria-hidden="true" tabindex="-1"></a>\frac{\partial ^ 2 \ln L}{\partial \gamma \partial \gamma ^ \top} &amp;=&amp;</span>
<span id="cb57-998"><a href="#cb57-998" aria-hidden="true" tabindex="-1"></a>-\sum_{n=1} ^ N e ^ {-\gamma ^ \top</span>
<span id="cb57-999"><a href="#cb57-999" aria-hidden="true" tabindex="-1"></a>z_n}y_nz_nz_n^\top</span>
<span id="cb57-1000"><a href="#cb57-1000" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-1001"><a href="#cb57-1001" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-1002"><a href="#cb57-1002" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-exp_dist_lgh}</span>
<span id="cb57-1003"><a href="#cb57-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1004"><a href="#cb57-1004" aria-hidden="true" tabindex="-1"></a>Starting from an initial vector of parameters $\gamma_i$, we use a first-order Taylor expansion of the gradient for $\gamma_{i+1}$ "close to"</span>
<span id="cb57-1005"><a href="#cb57-1005" aria-hidden="true" tabindex="-1"></a>$\gamma_i$:</span>
<span id="cb57-1006"><a href="#cb57-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1007"><a href="#cb57-1007" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1008"><a href="#cb57-1008" aria-hidden="true" tabindex="-1"></a>g(\gamma_{i+1}) \approx g(\gamma_i) + H(\gamma_i) (\gamma_{i+1} - \gamma_i)</span>
<span id="cb57-1009"><a href="#cb57-1009" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1010"><a href="#cb57-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1011"><a href="#cb57-1011" aria-hidden="true" tabindex="-1"></a>Solving the first order-conditions for a maximum, we</span>
<span id="cb57-1012"><a href="#cb57-1012" aria-hidden="true" tabindex="-1"></a>should have $g(\gamma_i) + H(\gamma_i) (\gamma_{i+1} - \gamma_i) = 0$,</span>
<span id="cb57-1013"><a href="#cb57-1013" aria-hidden="true" tabindex="-1"></a>which leads to:</span>
<span id="cb57-1014"><a href="#cb57-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1015"><a href="#cb57-1015" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1016"><a href="#cb57-1016" aria-hidden="true" tabindex="-1"></a>\gamma_{i+1} = \gamma_i - H(\gamma_i) ^ {-1} g(\gamma_i)</span>
<span id="cb57-1017"><a href="#cb57-1017" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-updating_rule}</span>
<span id="cb57-1018"><a href="#cb57-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1019"><a href="#cb57-1019" aria-hidden="true" tabindex="-1"></a>Except if the gradient is a linear function of $\gamma$, $\gamma_{i+1}$ is</span>
<span id="cb57-1020"><a href="#cb57-1020" aria-hidden="true" tabindex="-1"></a>not the maximum, but it is closer to the maximum than $\gamma_i$ and</span>
<span id="cb57-1021"><a href="#cb57-1021" aria-hidden="true" tabindex="-1"></a>successive iterations enable to reach a value of $\gamma$ as close as</span>
<span id="cb57-1022"><a href="#cb57-1022" aria-hidden="true" tabindex="-1"></a>desired to the maximum.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Newton-Raphson algorithm|)}</span>
<span id="cb57-1023"><a href="#cb57-1023" aria-hidden="true" tabindex="-1"></a>We first begin by describing the model we want to estimate using a</span>
<span id="cb57-1024"><a href="#cb57-1024" aria-hidden="true" tabindex="-1"></a>formula and extracting the relevant components of the model, namely the</span>
<span id="cb57-1025"><a href="#cb57-1025" aria-hidden="true" tabindex="-1"></a>vector of response and the matrix of covariates.</span>
<span id="cb57-1026"><a href="#cb57-1026" aria-hidden="true" tabindex="-1"></a>\idxfun{length}{base}\idxfun{model.frame}{stats}\idxfun{model.matrix}{stats}\idxfun{model.response}{stats}</span>
<span id="cb57-1027"><a href="#cb57-1027" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1028"><a href="#cb57-1028" aria-hidden="true" tabindex="-1"></a>form <span class="ot">&lt;-</span> dur <span class="sc">~</span> p98 <span class="sc">+</span> varp98</span>
<span id="cb57-1029"><a href="#cb57-1029" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(form, oil)</span>
<span id="cb57-1030"><a href="#cb57-1030" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(form, mf)</span>
<span id="cb57-1031"><a href="#cb57-1031" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">model.response</span>(mf)</span>
<span id="cb57-1032"><a href="#cb57-1032" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb57-1033"><a href="#cb57-1033" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1034"><a href="#cb57-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1035"><a href="#cb57-1035" aria-hidden="true" tabindex="-1"></a>We then define functions for the log-likelihood, the gradient and</span>
<span id="cb57-1036"><a href="#cb57-1036" aria-hidden="true" tabindex="-1"></a>the hessian, as a function of the vector of parameters $\gamma$.</span>
<span id="cb57-1037"><a href="#cb57-1037" aria-hidden="true" tabindex="-1"></a>\idxfun{function}{base}\idxfun{crossprod}{base}\idxfun{apply}{base}\idxfun{drop}{base}</span>
<span id="cb57-1038"><a href="#cb57-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1039"><a href="#cb57-1039" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1040"><a href="#cb57-1040" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma) theta <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span> <span class="fu">drop</span>(Z <span class="sc">%*%</span> gamma))</span>
<span id="cb57-1041"><a href="#cb57-1041" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma) <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">theta</span>(gamma)) <span class="sc">+</span> <span class="fu">theta</span>(gamma) <span class="sc">*</span> y)</span>
<span id="cb57-1042"><a href="#cb57-1042" aria-hidden="true" tabindex="-1"></a>G <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma) <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> y <span class="sc">*</span> <span class="fu">theta</span>(gamma)) <span class="sc">*</span> Z</span>
<span id="cb57-1043"><a href="#cb57-1043" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma) <span class="fu">apply</span>(<span class="fu">G</span>(gamma), <span class="dv">2</span>, sum)</span>
<span id="cb57-1044"><a href="#cb57-1044" aria-hidden="true" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma) <span class="sc">-</span> <span class="fu">crossprod</span>(<span class="fu">sqrt</span>(<span class="fu">theta</span>(gamma) <span class="sc">*</span> y) <span class="sc">*</span> Z)</span>
<span id="cb57-1045"><a href="#cb57-1045" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1046"><a href="#cb57-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1047"><a href="#cb57-1047" aria-hidden="true" tabindex="-1"></a>Starting from an initial vector of coefficients, we use the preceding</span>
<span id="cb57-1048"><a href="#cb57-1048" aria-hidden="true" tabindex="-1"></a>formula to update the vector of coefficients. The choice of good</span>
<span id="cb57-1049"><a href="#cb57-1049" aria-hidden="true" tabindex="-1"></a>starting values is crucial when the log-likelihood function is not</span>
<span id="cb57-1050"><a href="#cb57-1050" aria-hidden="true" tabindex="-1"></a>concave. This is not the case here, but, anyway, the choice of good</span>
<span id="cb57-1051"><a href="#cb57-1051" aria-hidden="true" tabindex="-1"></a>starting values limits the number of iterations. In our example, a</span>
<span id="cb57-1052"><a href="#cb57-1052" aria-hidden="true" tabindex="-1"></a>good candidate is the ordinary least squares estimator, with $\ln y$</span>
<span id="cb57-1053"><a href="#cb57-1053" aria-hidden="true" tabindex="-1"></a>as the response:</span>
<span id="cb57-1054"><a href="#cb57-1054" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb57-1055"><a href="#cb57-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1056"><a href="#cb57-1056" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1057"><a href="#cb57-1057" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1058"><a href="#cb57-1058" aria-hidden="true" tabindex="-1"></a>gamma_0 <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(<span class="fu">log</span>(dur) <span class="sc">~</span> p98 <span class="sc">+</span> varp98, oil))</span>
<span id="cb57-1059"><a href="#cb57-1059" aria-hidden="true" tabindex="-1"></a>gamma_0</span>
<span id="cb57-1060"><a href="#cb57-1060" aria-hidden="true" tabindex="-1"></a><span class="fu">g</span>(gamma_0)</span>
<span id="cb57-1061"><a href="#cb57-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1062"><a href="#cb57-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1063"><a href="#cb57-1063" aria-hidden="true" tabindex="-1"></a>We then update $\gamma$ using @eq-updating_rule:</span>
<span id="cb57-1064"><a href="#cb57-1064" aria-hidden="true" tabindex="-1"></a>\idxfun{solve}{base}</span>
<span id="cb57-1065"><a href="#cb57-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1066"><a href="#cb57-1066" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1067"><a href="#cb57-1067" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1068"><a href="#cb57-1068" aria-hidden="true" tabindex="-1"></a>gamma_1 <span class="ot">&lt;-</span> gamma_0 <span class="sc">-</span> <span class="fu">solve</span>(<span class="fu">H</span>(gamma_0), <span class="fu">g</span>(gamma_0))</span>
<span id="cb57-1069"><a href="#cb57-1069" aria-hidden="true" tabindex="-1"></a>gamma_1</span>
<span id="cb57-1070"><a href="#cb57-1070" aria-hidden="true" tabindex="-1"></a><span class="fu">g</span>(gamma_1)</span>
<span id="cb57-1071"><a href="#cb57-1071" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1072"><a href="#cb57-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1073"><a href="#cb57-1073" aria-hidden="true" tabindex="-1"></a>We can see that we obtain an updated vector of coefficients</span>
<span id="cb57-1074"><a href="#cb57-1074" aria-hidden="true" tabindex="-1"></a>which seems closer to the maximum than the initial one, as the elements</span>
<span id="cb57-1075"><a href="#cb57-1075" aria-hidden="true" tabindex="-1"></a>of the gradient are much smaller than previously.</span>
<span id="cb57-1076"><a href="#cb57-1076" aria-hidden="true" tabindex="-1"></a>The gradient being still quite different from 0, it is worth iterating</span>
<span id="cb57-1077"><a href="#cb57-1077" aria-hidden="true" tabindex="-1"></a>again. We'll stop the iterations when a scalar value obtained from the</span>
<span id="cb57-1078"><a href="#cb57-1078" aria-hidden="true" tabindex="-1"></a>gradient is less than an arbitrary small real value. As a simple</span>
<span id="cb57-1079"><a href="#cb57-1079" aria-hidden="true" tabindex="-1"></a>criterion, we consider the mean of the squares of the elements of the</span>
<span id="cb57-1080"><a href="#cb57-1080" aria-hidden="true" tabindex="-1"></a>score, and we iterate as long as this scalar is greater than</span>
<span id="cb57-1081"><a href="#cb57-1081" aria-hidden="true" tabindex="-1"></a>$10^{-07}$:</span>
<span id="cb57-1082"><a href="#cb57-1082" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}\idxfun{paste}{base}\idxfun{cat}{base}\idxfun{solve}{base}</span>
<span id="cb57-1083"><a href="#cb57-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1084"><a href="#cb57-1084" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1085"><a href="#cb57-1085" aria-hidden="true" tabindex="-1"></a>gamma_0 <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(<span class="fu">log</span>(dur) <span class="sc">~</span> p98 <span class="sc">+</span> varp98, oil))</span>
<span id="cb57-1086"><a href="#cb57-1086" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb57-1087"><a href="#cb57-1087" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> gamma_0</span>
<span id="cb57-1088"><a href="#cb57-1088" aria-hidden="true" tabindex="-1"></a>crit <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb57-1089"><a href="#cb57-1089" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (crit <span class="sc">&gt;</span> <span class="fl">1E-07</span>){</span>
<span id="cb57-1090"><a href="#cb57-1090" aria-hidden="true" tabindex="-1"></a>    i <span class="ot">&lt;-</span> i <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb57-1091"><a href="#cb57-1091" aria-hidden="true" tabindex="-1"></a>    gamma <span class="ot">&lt;-</span> gamma <span class="sc">-</span> <span class="fu">solve</span>(<span class="fu">H</span>(gamma), <span class="fu">g</span>(gamma))</span>
<span id="cb57-1092"><a href="#cb57-1092" aria-hidden="true" tabindex="-1"></a>    crit <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">g</span>(gamma) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb57-1093"><a href="#cb57-1093" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">paste</span>(<span class="st">"iteration"</span>, i, <span class="st">"crit = "</span>, crit, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>))</span>
<span id="cb57-1094"><a href="#cb57-1094" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-1095"><a href="#cb57-1095" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1096"><a href="#cb57-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1097"><a href="#cb57-1097" aria-hidden="true" tabindex="-1"></a>Only three iterations were necessary to reach the maximum (as defined by</span>
<span id="cb57-1098"><a href="#cb57-1098" aria-hidden="true" tabindex="-1"></a>our criteria). </span>
<span id="cb57-1099"><a href="#cb57-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1100"><a href="#cb57-1100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1101"><a href="#cb57-1101" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1102"><a href="#cb57-1102" aria-hidden="true" tabindex="-1"></a>gamma</span>
<span id="cb57-1103"><a href="#cb57-1103" aria-hidden="true" tabindex="-1"></a><span class="fu">g</span>(gamma)</span>
<span id="cb57-1104"><a href="#cb57-1104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1105"><a href="#cb57-1105" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Taylor expansion}</span>
<span id="cb57-1106"><a href="#cb57-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1107"><a href="#cb57-1107" aria-hidden="true" tabindex="-1"></a>We then use @eq-exp_dist_lgh and the functions <span class="in">`H`</span> and <span class="in">`G`</span> defined</span>
<span id="cb57-1108"><a href="#cb57-1108" aria-hidden="true" tabindex="-1"></a>previously to compute the three estimations of the information matrix</span>
<span id="cb57-1109"><a href="#cb57-1109" aria-hidden="true" tabindex="-1"></a>and the four estimators of the covariance matrix of the estimator:</span>
<span id="cb57-1110"><a href="#cb57-1110" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}\idxfun{solve}{base}</span>
<span id="cb57-1111"><a href="#cb57-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1112"><a href="#cb57-1112" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1113"><a href="#cb57-1113" aria-hidden="true" tabindex="-1"></a>Info_g <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">G</span>(gamma))</span>
<span id="cb57-1114"><a href="#cb57-1114" aria-hidden="true" tabindex="-1"></a>Info_H <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fu">H</span>(gamma)</span>
<span id="cb57-1115"><a href="#cb57-1115" aria-hidden="true" tabindex="-1"></a>Info <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(Z)</span>
<span id="cb57-1116"><a href="#cb57-1116" aria-hidden="true" tabindex="-1"></a>V_g <span class="ot">&lt;-</span> <span class="fu">solve</span>(Info_g)</span>
<span id="cb57-1117"><a href="#cb57-1117" aria-hidden="true" tabindex="-1"></a>V_i <span class="ot">&lt;-</span> <span class="fu">solve</span>(Info)</span>
<span id="cb57-1118"><a href="#cb57-1118" aria-hidden="true" tabindex="-1"></a>V_h <span class="ot">&lt;-</span> <span class="fu">solve</span>(Info_H)</span>
<span id="cb57-1119"><a href="#cb57-1119" aria-hidden="true" tabindex="-1"></a>V_sand <span class="ot">&lt;-</span> <span class="fu">solve</span>(Info_H) <span class="sc">%*%</span> Info_g <span class="sc">%*%</span> <span class="fu">solve</span>(Info_H)</span>
<span id="cb57-1120"><a href="#cb57-1120" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1121"><a href="#cb57-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1122"><a href="#cb57-1122" aria-hidden="true" tabindex="-1"></a>We then compare the resulting estimated standard errors of the estimator. To obtain a compact output, we use the <span class="in">`sapply`</span> function, which is a specialized version of <span class="in">`lapply`</span>. <span class="in">`lapply`</span> takes as argument a list and a function, and the outcome is a list containing the result of applying the function to each element of the list. <span class="in">`sapply`</span>, when possible, returns a matrix:</span>
<span id="cb57-1123"><a href="#cb57-1123" aria-hidden="true" tabindex="-1"></a>\idxfun{sapply}{base}\idxfun{list}{base}\idxfun{stder}{micsr}\idxfun{function}{base}</span>
<span id="cb57-1124"><a href="#cb57-1124" aria-hidden="true" tabindex="-1"></a>                                                                   </span>
<span id="cb57-1125"><a href="#cb57-1125" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1126"><a href="#cb57-1126" aria-hidden="true" tabindex="-1"></a><span class="fu">sapply</span>(<span class="fu">list</span>(V_i, V_h, V_g, V_sand), <span class="cf">function</span>(x) <span class="fu">stder</span>(x))</span>
<span id="cb57-1127"><a href="#cb57-1127" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1128"><a href="#cb57-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1129"><a href="#cb57-1129" aria-hidden="true" tabindex="-1"></a>Not that the gradient-based estimate gives fairly different results,</span>
<span id="cb57-1130"><a href="#cb57-1130" aria-hidden="true" tabindex="-1"></a>compared to the other estimators. This is often the case, this</span>
<span id="cb57-1131"><a href="#cb57-1131" aria-hidden="true" tabindex="-1"></a>estimator being known to perform poorly in small samples.</span>
<span id="cb57-1132"><a href="#cb57-1132" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exponential distribution|(}</span>
<span id="cb57-1133"><a href="#cb57-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1134"><a href="#cb57-1134" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear gaussian model {#sec-linear-gaussian}</span></span>
<span id="cb57-1135"><a href="#cb57-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1136"><a href="#cb57-1136" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{linear gaussian model|(}</span>
<span id="cb57-1137"><a href="#cb57-1137" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{normal distribution|(}</span>
<span id="cb57-1138"><a href="#cb57-1138" aria-hidden="true" tabindex="-1"></a>The first part of the book was devoted to the OLS estimator, suitable in situations where $\mbox{E}(y_n \mid x_n) = \alpha + \beta^\top x_n$, with $\mbox{V}(\epsilon_n \mid x_n) = \sigma_\epsilon ^ 2$ and $\mbox{cov}(\epsilon_n,\epsilon_m) = 0$. This model can also be estimated by maximum likelihood by specifying the conditional density of $y$, and not only its conditional expectation. Assuming that $y$ follows a normal distribution, we have:</span>
<span id="cb57-1139"><a href="#cb57-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1140"><a href="#cb57-1140" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1141"><a href="#cb57-1141" aria-hidden="true" tabindex="-1"></a>y_n \mid x_n \sim \mathcal{N}(\alpha + \beta ^ \top x_n, \sigma)</span>
<span id="cb57-1142"><a href="#cb57-1142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1143"><a href="#cb57-1143" aria-hidden="true" tabindex="-1"></a>and the conditional density of $y$ is:</span>
<span id="cb57-1144"><a href="#cb57-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1145"><a href="#cb57-1145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1146"><a href="#cb57-1146" aria-hidden="true" tabindex="-1"></a>f(y_n \mid x_n) = \frac{1}{\sqrt{2\pi}\sigma}e ^ {-\frac{1}{2}\left(\frac{y_n - \gamma ^ \top z_n}{\sigma}\right)^2} = \frac{1}{\sigma}\phi\left(\frac{y_n - \gamma ^ \top z_n}{\sigma}\right)</span>
<span id="cb57-1147"><a href="#cb57-1147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1148"><a href="#cb57-1148" aria-hidden="true" tabindex="-1"></a>The log-likelihood is then:</span>
<span id="cb57-1149"><a href="#cb57-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1150"><a href="#cb57-1150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1151"><a href="#cb57-1151" aria-hidden="true" tabindex="-1"></a>\ln L = -\frac{N}{2}\ln 2\pi - N \ln \sigma - \frac{1}{2\sigma ^ 2}\sum_{n=1}^N(y_n - \gamma ^ \top z_n)^2 = </span>
<span id="cb57-1152"><a href="#cb57-1152" aria-hidden="true" tabindex="-1"></a>-\frac{N}{2}\ln 2\pi - \frac{N}{2} \ln \sigma ^ 2 - \frac{1}{2\sigma ^ 2} \epsilon ^ \top \epsilon</span>
<span id="cb57-1153"><a href="#cb57-1153" aria-hidden="true" tabindex="-1"></a>$$ {#eq-loglik_normal}</span>
<span id="cb57-1154"><a href="#cb57-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1155"><a href="#cb57-1155" aria-hidden="true" tabindex="-1"></a>The gradient is:</span>
<span id="cb57-1156"><a href="#cb57-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1157"><a href="#cb57-1157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1158"><a href="#cb57-1158" aria-hidden="true" tabindex="-1"></a>g = </span>
<span id="cb57-1159"><a href="#cb57-1159" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-1160"><a href="#cb57-1160" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb57-1161"><a href="#cb57-1161" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma ^ 2} \sum_n (y_n - \gamma ^ \top z_n)z_n <span class="sc">\\</span></span>
<span id="cb57-1162"><a href="#cb57-1162" aria-hidden="true" tabindex="-1"></a>-\frac{N}{\sigma} + \frac{1}{\sigma ^ 3} \sum_n (y_n - \gamma ^ \top z_n) ^ 2</span>
<span id="cb57-1163"><a href="#cb57-1163" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-1164"><a href="#cb57-1164" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb57-1165"><a href="#cb57-1165" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb57-1166"><a href="#cb57-1166" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-1167"><a href="#cb57-1167" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb57-1168"><a href="#cb57-1168" aria-hidden="true" tabindex="-1"></a> \frac{1}{\sigma ^ 2} Z ^ \top \epsilon <span class="sc">\\</span></span>
<span id="cb57-1169"><a href="#cb57-1169" aria-hidden="true" tabindex="-1"></a> -\frac{N}{\sigma} + \frac{1}{\sigma ^ 3} \epsilon ^ \top \epsilon</span>
<span id="cb57-1170"><a href="#cb57-1170" aria-hidden="true" tabindex="-1"></a> \end{array}</span>
<span id="cb57-1171"><a href="#cb57-1171" aria-hidden="true" tabindex="-1"></a> \right)</span>
<span id="cb57-1172"><a href="#cb57-1172" aria-hidden="true" tabindex="-1"></a>$$ {#eq-grad_ml_normal}</span>
<span id="cb57-1173"><a href="#cb57-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1174"><a href="#cb57-1174" aria-hidden="true" tabindex="-1"></a>and the ML estimator is obtained by computing the vector of parameters $(\gamma ^ \top, \sigma)$ that set this system of $K+2$ equations to 0. The hessian is:</span>
<span id="cb57-1175"><a href="#cb57-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1176"><a href="#cb57-1176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1177"><a href="#cb57-1177" aria-hidden="true" tabindex="-1"></a>H = </span>
<span id="cb57-1178"><a href="#cb57-1178" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-1179"><a href="#cb57-1179" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb57-1180"><a href="#cb57-1180" aria-hidden="true" tabindex="-1"></a>-\frac{1}{\sigma ^ 2} Z ^ \top Z &amp; 0 <span class="sc">\\</span></span>
<span id="cb57-1181"><a href="#cb57-1181" aria-hidden="true" tabindex="-1"></a>0 &amp;  \frac{N}{\sigma ^ 2} - \frac{3}{2\sigma ^ 4}\epsilon ^ \top \epsilon</span>
<span id="cb57-1182"><a href="#cb57-1182" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-1183"><a href="#cb57-1183" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb57-1184"><a href="#cb57-1184" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hessian-normal}</span>
<span id="cb57-1185"><a href="#cb57-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1186"><a href="#cb57-1186" aria-hidden="true" tabindex="-1"></a>Note that it is block diagonal, as the cross-derivatives are $- 2 / \sigma ^ 3 Z ^ \top \epsilon$, which is 0 for the ML estimator.</span>
<span id="cb57-1187"><a href="#cb57-1187" aria-hidden="true" tabindex="-1"></a>Solving the last line in @eq-grad_ml_normal for $\sigma$, we get:</span>
<span id="cb57-1188"><a href="#cb57-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1189"><a href="#cb57-1189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1190"><a href="#cb57-1190" aria-hidden="true" tabindex="-1"></a>\sigma ^ 2 = \frac{\epsilon ^ \top \epsilon}{N}</span>
<span id="cb57-1191"><a href="#cb57-1191" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hats2_ml}</span>
<span id="cb57-1192"><a href="#cb57-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1193"><a href="#cb57-1193" aria-hidden="true" tabindex="-1"></a>Replacing $\sigma ^ 2$ by this expression in @eq-loglik_normal, we get the **concentrated** log-likelihood:</span>
<span id="cb57-1194"><a href="#cb57-1194" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{concentrated log-likelihood function}</span>
<span id="cb57-1195"><a href="#cb57-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1196"><a href="#cb57-1196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1197"><a href="#cb57-1197" aria-hidden="true" tabindex="-1"></a>\ln L =- \frac{N}{2}(1 - \ln N + \ln 2 \pi) - \frac{N}{2} \ln \epsilon ^ \top \epsilon = C - \frac{N}{2} \ln \epsilon ^ \top \epsilon</span>
<span id="cb57-1198"><a href="#cb57-1198" aria-hidden="true" tabindex="-1"></a>$$ {#eq-conc_loglik}</span>
<span id="cb57-1199"><a href="#cb57-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1200"><a href="#cb57-1200" aria-hidden="true" tabindex="-1"></a>which makes clear that maximizing the log-likelihood is equivalent to minimizing the residual sum of squares, and, therefore, that the ML and the OLS estimators of $\gamma$ are the same. Once $\hat{\gamma}$ has been computed, $\hat{\sigma} ^ 2$ can be estimated using @eq-hats2_ml. Note that the residual sum of squares divided by the number of observations and not by the number of degrees of freedom, contrary to the OLS estimator. </span>
<span id="cb57-1201"><a href="#cb57-1201" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{normal distribution|)}</span>
<span id="cb57-1202"><a href="#cb57-1202" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{linear gaussian model|)}</span>
<span id="cb57-1203"><a href="#cb57-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1204"><a href="#cb57-1204" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transformation of the response</span></span>
<span id="cb57-1205"><a href="#cb57-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1206"><a href="#cb57-1206" aria-hidden="true" tabindex="-1"></a>The ML estimator is based on the density (or probability mass) function of the response,  given a set of covariates. Sometimes, it is more interesting to consider the density of a parametric transformation of the response. To illustrate this kind of model, we'll consider the estimation of production functions, using the <span class="in">`apples`</span> data set, already described in @sec-system_equation.</span>
<span id="cb57-1207"><a href="#cb57-1207" aria-hidden="true" tabindex="-1"></a>We consider models of the form:</span>
<span id="cb57-1208"><a href="#cb57-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1209"><a href="#cb57-1209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1210"><a href="#cb57-1210" aria-hidden="true" tabindex="-1"></a>w_n = v(y_n, \lambda) \sim \mathcal{N}(\mu_n, \sigma) \mbox{ with } \mu_n = \alpha + \beta ^ \top x_n</span>
<span id="cb57-1211"><a href="#cb57-1211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1212"><a href="#cb57-1212" aria-hidden="true" tabindex="-1"></a>The parametric transformation of $y_n$ may depends on a set of unknown parameters $\lambda$, and it follows a normal distribution with an expectation that is a linear function of some covariates and a constant variance.</span>
<span id="cb57-1213"><a href="#cb57-1213" aria-hidden="true" tabindex="-1"></a>As the density of $v(y_n, \gamma)$ is normal, the one for $y_n$</span>
<span id="cb57-1214"><a href="#cb57-1214" aria-hidden="true" tabindex="-1"></a>can be obtained using the following formula:</span>
<span id="cb57-1215"><a href="#cb57-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1216"><a href="#cb57-1216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1217"><a href="#cb57-1217" aria-hidden="true" tabindex="-1"></a>f(y_n) = \phi(v(y_n, \gamma)) \times \left|\frac{d v}{d y} \right|</span>
<span id="cb57-1218"><a href="#cb57-1218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1219"><a href="#cb57-1219" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{log-normal model|(}</span>
<span id="cb57-1220"><a href="#cb57-1220" aria-hidden="true" tabindex="-1"></a>where the last term is called the Jacobian of the transformation of</span>
<span id="cb57-1221"><a href="#cb57-1221" aria-hidden="true" tabindex="-1"></a>$w$ on $y$. Consider the simple case where  $v(y_n) = \ln y_n$. Then, $v$ doesn't contain any unknown parameter, the Jacobian of the transformation is $1/y$ and the density of $y_n$:</span>
<span id="cb57-1222"><a href="#cb57-1222" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Jacobian}</span>
<span id="cb57-1223"><a href="#cb57-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1224"><a href="#cb57-1224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1225"><a href="#cb57-1225" aria-hidden="true" tabindex="-1"></a>f(y_n) = \frac{1}{y}\frac{1}{\sigma}\phi\left(\frac{\ln y - \mu_n}{\sigma}\right)</span>
<span id="cb57-1226"><a href="#cb57-1226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1227"><a href="#cb57-1227" aria-hidden="true" tabindex="-1"></a>is simply the log-normal density, which implies the following log-likelihood function:</span>
<span id="cb57-1228"><a href="#cb57-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1229"><a href="#cb57-1229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1230"><a href="#cb57-1230" aria-hidden="true" tabindex="-1"></a>\ln L(\gamma) = -\frac{N}{2} \ln 2\pi - N \ln \sigma - \sum_{n=1} ^ N \ln y_n + \frac{1}{2\sigma ^ 2}\sum_{n = 1} ^ {N}(\ln y_n - \gamma ^ \top z_n)^ 2</span>
<span id="cb57-1231"><a href="#cb57-1231" aria-hidden="true" tabindex="-1"></a>$$ {#eq-log_lik_lognorm}</span>
<span id="cb57-1232"><a href="#cb57-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1233"><a href="#cb57-1233" aria-hidden="true" tabindex="-1"></a>Maximizing @eq-log_lik_lognorm is obviously equivalent to minimizing $\sum_{n = 1} ^ {N}(\ln y_n - \gamma ^ \top z_n)^ 2$ which is the residual sum of squares of a regression with $\ln y_n$ as the response. Therefore the ML estimation of $\gamma$ can be performed using OLS. </span>
<span id="cb57-1234"><a href="#cb57-1234" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-1235"><a href="#cb57-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1236"><a href="#cb57-1236" aria-hidden="true" tabindex="-1"></a>We consider only one year (1986) of production for the <span class="in">`apples`</span> data set, we construct a unique output variable (<span class="in">`y`</span>), we rename for convenience the three factors as <span class="in">`k`</span>, <span class="in">`l`</span> and <span class="in">`m`</span> (respectively for capital, labor and materials) and,</span>
<span id="cb57-1237"><a href="#cb57-1237" aria-hidden="true" tabindex="-1"></a>for a reason that will be clear later, we divide all the variables</span>
<span id="cb57-1238"><a href="#cb57-1238" aria-hidden="true" tabindex="-1"></a>by their sample mean:</span>
<span id="cb57-1239"><a href="#cb57-1239" aria-hidden="true" tabindex="-1"></a>\idxfun{filter}{dplyr}\idxfun{transmute}{dplyr}</span>
<span id="cb57-1240"><a href="#cb57-1240" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1241"><a href="#cb57-1241" aria-hidden="true" tabindex="-1"></a>aps <span class="ot">&lt;-</span> apples <span class="sc">%&gt;%</span></span>
<span id="cb57-1242"><a href="#cb57-1242" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">1986</span>) <span class="sc">%&gt;%</span></span>
<span id="cb57-1243"><a href="#cb57-1243" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(<span class="at">y =</span> apples <span class="sc">+</span> otherprod, <span class="at">y =</span> y <span class="sc">/</span> <span class="fu">mean</span>(y),</span>
<span id="cb57-1244"><a href="#cb57-1244" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> capital <span class="sc">/</span> <span class="fu">mean</span>(capital),</span>
<span id="cb57-1245"><a href="#cb57-1245" aria-hidden="true" tabindex="-1"></a>              <span class="at">l =</span> labor <span class="sc">/</span> <span class="fu">mean</span>(labor),</span>
<span id="cb57-1246"><a href="#cb57-1246" aria-hidden="true" tabindex="-1"></a>              <span class="at">m =</span> materials <span class="sc">/</span> <span class="fu">mean</span>(materials))</span>
<span id="cb57-1247"><a href="#cb57-1247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1248"><a href="#cb57-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1249"><a href="#cb57-1249" aria-hidden="true" tabindex="-1"></a>We first consider the Cobb-Douglas production function which is linear in logs. Denoting $j = 1 \ldots J$ the inputs, we get:</span>
<span id="cb57-1250"><a href="#cb57-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1251"><a href="#cb57-1251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1252"><a href="#cb57-1252" aria-hidden="true" tabindex="-1"></a>\ln y_n = \alpha + \sum_{j=1}^J \beta_j \ln q_j + \epsilon_n</span>
<span id="cb57-1253"><a href="#cb57-1253" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cobb_douglas_production}</span>
<span id="cb57-1254"><a href="#cb57-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1255"><a href="#cb57-1255" aria-hidden="true" tabindex="-1"></a>We then fit @eq-cobb_douglas_production using <span class="in">`lm`</span>:</span>
<span id="cb57-1256"><a href="#cb57-1256" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb57-1257"><a href="#cb57-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1260"><a href="#cb57-1260" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1261"><a href="#cb57-1261" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1262"><a href="#cb57-1262" aria-hidden="true" tabindex="-1"></a>cobb_douglas <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> <span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">log</span>(m), aps)</span>
<span id="cb57-1263"><a href="#cb57-1263" aria-hidden="true" tabindex="-1"></a>cobb_douglas <span class="sc">%&gt;%</span> coef</span>
<span id="cb57-1264"><a href="#cb57-1264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span> </span>
<span id="cb57-1265"><a href="#cb57-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1266"><a href="#cb57-1266" aria-hidden="true" tabindex="-1"></a>The problem of using <span class="in">`lm`</span> is that, although the estimates are the ML estimates, the response is $\ln y$ and not $y$. Therefore, the log likelihood reported by <span class="in">`lm`</span> is computed using the density of $\ln y$:</span>
<span id="cb57-1267"><a href="#cb57-1267" aria-hidden="true" tabindex="-1"></a>\idxfun{logLik}{stats}</span>
<span id="cb57-1268"><a href="#cb57-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1271"><a href="#cb57-1271" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1272"><a href="#cb57-1272" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: uncorrect_logLik_lognormal</span></span>
<span id="cb57-1273"><a href="#cb57-1273" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1274"><a href="#cb57-1274" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(cobb_douglas)</span>
<span id="cb57-1275"><a href="#cb57-1275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1276"><a href="#cb57-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1277"><a href="#cb57-1277" aria-hidden="true" tabindex="-1"></a>and is incorrect as the term $- \sum_{n=1} ^ N \ln y_n$ in @eq-log_lik_lognorm is missing. Adding this term to the log-likelihood returned by the <span class="in">`logLik`</span> function, we get:</span>
<span id="cb57-1278"><a href="#cb57-1278" aria-hidden="true" tabindex="-1"></a>\idxfun{as.numeric}{base}\idxfun{logLik}{stats}</span>
<span id="cb57-1279"><a href="#cb57-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1282"><a href="#cb57-1282" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1283"><a href="#cb57-1283" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: correct_logLik_lognormal</span></span>
<span id="cb57-1284"><a href="#cb57-1284" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1285"><a href="#cb57-1285" aria-hidden="true" tabindex="-1"></a><span class="fu">as.numeric</span>(<span class="fu">logLik</span>(cobb_douglas)) <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">log</span>(aps<span class="sc">$</span>y))</span>
<span id="cb57-1286"><a href="#cb57-1286" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1287"><a href="#cb57-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1288"><a href="#cb57-1288" aria-hidden="true" tabindex="-1"></a>The <span class="in">`micsr::loglm`</span> function estimates models for which the response is in logarithm. The estimation of the parameters is performed using <span class="in">`lm`</span>, but the result is a <span class="in">`micsr`</span> object, from which the correct log-likelihood, gradient and hessian can be extracted:</span>
<span id="cb57-1289"><a href="#cb57-1289" aria-hidden="true" tabindex="-1"></a>\idxfun{loglm}{micsr}\idxfun{logLik}{stats}</span>
<span id="cb57-1290"><a href="#cb57-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1293"><a href="#cb57-1293" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1294"><a href="#cb57-1294" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: loglm_logLik_lognormal</span></span>
<span id="cb57-1295"><a href="#cb57-1295" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1296"><a href="#cb57-1296" aria-hidden="true" tabindex="-1"></a>cd_loglm <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> <span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">log</span>(m), aps)</span>
<span id="cb57-1297"><a href="#cb57-1297" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(cd_loglm)</span>
<span id="cb57-1298"><a href="#cb57-1298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1299"><a href="#cb57-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1300"><a href="#cb57-1300" aria-hidden="true" tabindex="-1"></a>The scale elasticity, which measures the relative growth of output for a</span>
<span id="cb57-1301"><a href="#cb57-1301" aria-hidden="true" tabindex="-1"></a>proportional increase of all the inputs is $\sum_{j=1} ^ J \beta_j$ and therefore constant returns to scale imply that $\sum_{j=1} ^ J \beta_j = 1$, or $\beta_J = 1 - \sum_{j=1} ^ {J-1} \beta_j$. Replacing in @eq-cobb_douglas_production, we get:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{reparametrization!Cobb-Douglas}</span>
<span id="cb57-1302"><a href="#cb57-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1303"><a href="#cb57-1303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1304"><a href="#cb57-1304" aria-hidden="true" tabindex="-1"></a>\ln y_n = \alpha + \sum_{j=1}^{J-1} \beta_j \ln q_j / q_J + </span>
<span id="cb57-1305"><a href="#cb57-1305" aria-hidden="true" tabindex="-1"></a>\left(\sum_{j=1}^{J} \beta_j - 1\right) \ln q_J + </span>
<span id="cb57-1306"><a href="#cb57-1306" aria-hidden="true" tabindex="-1"></a>\ln q_J + \epsilon_n</span>
<span id="cb57-1307"><a href="#cb57-1307" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cobb_douglas_production_reparam}</span>
<span id="cb57-1308"><a href="#cb57-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1309"><a href="#cb57-1309" aria-hidden="true" tabindex="-1"></a>Therefore, regressing  $\ln y_n$ on $\ln q_j / q_J, \forall j = 1\ldots J-1$ and $\ln q_J$, the hypothesis of constant returns to scale is $\beta_J^* = \sum_{j=1}^{J} \beta_j - 1 = 0$ and the constant returns to scale are imposed if $\ln q_J$ is removed from the regression. Note the presence of a constant term ($\ln q_J$) on the right side of the equation. This is called an offset and can be introduced in a formula with the <span class="in">`offset(w)`</span> syntax:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{offset}</span>
<span id="cb57-1310"><a href="#cb57-1310" aria-hidden="true" tabindex="-1"></a>\idxfun{loglm}{micsr}\idxfun{coef}{stats}\idxfun{logLik}{stats}</span>
<span id="cb57-1311"><a href="#cb57-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1314"><a href="#cb57-1314" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1315"><a href="#cb57-1315" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: cobb_douglas_reparam</span></span>
<span id="cb57-1316"><a href="#cb57-1316" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1317"><a href="#cb57-1317" aria-hidden="true" tabindex="-1"></a>cd_repar <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> <span class="fu">log</span>(k <span class="sc">/</span> m) <span class="sc">+</span> <span class="fu">log</span>(l <span class="sc">/</span> m) <span class="sc">+</span> <span class="fu">log</span>(m) <span class="sc">+</span> </span>
<span id="cb57-1318"><a href="#cb57-1318" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">offset</span>(<span class="fu">log</span>(m)), aps)</span>
<span id="cb57-1319"><a href="#cb57-1319" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cd_repar)</span>
<span id="cb57-1320"><a href="#cb57-1320" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(cd_repar)</span>
<span id="cb57-1321"><a href="#cb57-1321" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1322"><a href="#cb57-1322" aria-hidden="true" tabindex="-1"></a>\idxfun{unname}{base}\idxfun{coef}{stats}</span>
<span id="cb57-1323"><a href="#cb57-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1326"><a href="#cb57-1326" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1327"><a href="#cb57-1327" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: coef_cd_repar_hide</span></span>
<span id="cb57-1328"><a href="#cb57-1328" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-1329"><a href="#cb57-1329" aria-hidden="true" tabindex="-1"></a>bm <span class="ot">&lt;-</span> <span class="fu">unname</span>(<span class="fu">coef</span>(cd_repar)[<span class="st">"log(m)"</span>])</span>
<span id="cb57-1330"><a href="#cb57-1330" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1331"><a href="#cb57-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1332"><a href="#cb57-1332" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-1333"><a href="#cb57-1333" aria-hidden="true" tabindex="-1"></a>The log-likelihood is obviously the same as previously and $\beta_J ^ * = <span class="in">`r round(bm, 2)`</span>$, which implies a scale elasticity equal to <span class="in">`r round(1 + bm, 2)`</span>.</span>
<span id="cb57-1334"><a href="#cb57-1334" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{log-normal model|)}</span>
<span id="cb57-1335"><a href="#cb57-1335" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized production function|(}</span>
<span id="cb57-1336"><a href="#cb57-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1337"><a href="#cb57-1337" aria-hidden="true" tabindex="-1"></a>@ZELL:REVA:69\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Zellner}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Revankar} proposed a generalization of the Cobb-Douglas production function of the form:</span>
<span id="cb57-1338"><a href="#cb57-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1339"><a href="#cb57-1339" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1340"><a href="#cb57-1340" aria-hidden="true" tabindex="-1"></a>\ln y + \lambda y \sim \mathcal{N}(\mu, \sigma)</span>
<span id="cb57-1341"><a href="#cb57-1341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1342"><a href="#cb57-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1343"><a href="#cb57-1343" aria-hidden="true" tabindex="-1"></a>where $\mu = \alpha + \sum_{j=1}^ J \beta_j \ln q_j$. The scale elasticity is:</span>
<span id="cb57-1344"><a href="#cb57-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1345"><a href="#cb57-1345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1346"><a href="#cb57-1346" aria-hidden="true" tabindex="-1"></a>\psi = \frac{\sum_{j=1} ^ J \beta_j}{1 + \lambda y}</span>
<span id="cb57-1347"><a href="#cb57-1347" aria-hidden="true" tabindex="-1"></a>$$ {#eq-elast}</span>
<span id="cb57-1348"><a href="#cb57-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1349"><a href="#cb57-1349" aria-hidden="true" tabindex="-1"></a>If $\lambda = 0$, this function reduces to the</span>
<span id="cb57-1350"><a href="#cb57-1350" aria-hidden="true" tabindex="-1"></a>Cobb-Douglas production function with normal errors. For $\lambda &gt; 0$, the scale elasticity is equal to $\sum_{j=1} ^ J \beta_j$ for $y = 0$ and tends to 0 as $y$ tends to $+</span>
<span id="cb57-1351"><a href="#cb57-1351" aria-hidden="true" tabindex="-1"></a>\infty$. Therefore, if $\sum_{j=1} ^ J \beta_j &gt; 1$, returns to scale</span>
<span id="cb57-1352"><a href="#cb57-1352" aria-hidden="true" tabindex="-1"></a>are increasing for a low level of production, get constant for a level</span>
<span id="cb57-1353"><a href="#cb57-1353" aria-hidden="true" tabindex="-1"></a>of production that is equal to $(\sum_{j=1} ^ J \beta_j - 1) / \lambda$ and</span>
<span id="cb57-1354"><a href="#cb57-1354" aria-hidden="true" tabindex="-1"></a>decreasing above this level of production.</span>
<span id="cb57-1355"><a href="#cb57-1355" aria-hidden="true" tabindex="-1"></a>Denoting $\epsilon_n$ the difference between $\ln y + \lambda y$ and</span>
<span id="cb57-1356"><a href="#cb57-1356" aria-hidden="true" tabindex="-1"></a>its conditional expectation, the model can be rewritten as:</span>
<span id="cb57-1357"><a href="#cb57-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1358"><a href="#cb57-1358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1359"><a href="#cb57-1359" aria-hidden="true" tabindex="-1"></a>\ln y_n + \lambda y_n = \alpha + \sum_{j=1} ^ J \beta_j \ln q_{nj} + \epsilon_n</span>
<span id="cb57-1360"><a href="#cb57-1360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1361"><a href="#cb57-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1362"><a href="#cb57-1362" aria-hidden="true" tabindex="-1"></a>The hypothesis of constant scale elasticity is simply $\lambda =</span>
<span id="cb57-1363"><a href="#cb57-1363" aria-hidden="true" tabindex="-1"></a>0$. The hypothesis of constant return to scale adds the condition:</span>
<span id="cb57-1364"><a href="#cb57-1364" aria-hidden="true" tabindex="-1"></a>$\sum_j \beta_j = 1$. It can be easily tested using the same </span>
<span id="cb57-1365"><a href="#cb57-1365" aria-hidden="true" tabindex="-1"></a>reparametrization of the model as in @eq-cobb_douglas_production_reparam:</span>
<span id="cb57-1366"><a href="#cb57-1366" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{reparametrization!generalized production function}</span>
<span id="cb57-1367"><a href="#cb57-1367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1368"><a href="#cb57-1368" aria-hidden="true" tabindex="-1"></a>\ln y_n + \lambda y_n = \alpha + </span>
<span id="cb57-1369"><a href="#cb57-1369" aria-hidden="true" tabindex="-1"></a>\sum_{j=1} ^ {J-1} \beta_j \ln q_{nj}^* + </span>
<span id="cb57-1370"><a href="#cb57-1370" aria-hidden="true" tabindex="-1"></a>\beta_J^* \ln q_{nJ} + \ln q_J + </span>
<span id="cb57-1371"><a href="#cb57-1371" aria-hidden="true" tabindex="-1"></a>\epsilon_n</span>
<span id="cb57-1372"><a href="#cb57-1372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1373"><a href="#cb57-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1374"><a href="#cb57-1374" aria-hidden="true" tabindex="-1"></a>where $q_{nj}^* = q_{nj} / q_{nJ}$ and</span>
<span id="cb57-1375"><a href="#cb57-1375" aria-hidden="true" tabindex="-1"></a>$\beta_J^* = \sum_{j=1}^J \beta_j - 1$.</span>
<span id="cb57-1376"><a href="#cb57-1376" aria-hidden="true" tabindex="-1"></a>The hypothesis of constant</span>
<span id="cb57-1377"><a href="#cb57-1377" aria-hidden="true" tabindex="-1"></a>returns to scale is the joint hypothesis that $\lambda = 0$ and that</span>
<span id="cb57-1378"><a href="#cb57-1378" aria-hidden="true" tabindex="-1"></a>the coefficient of $\ln q_{nJ}$ in this reparametrized version of the model</span>
<span id="cb57-1379"><a href="#cb57-1379" aria-hidden="true" tabindex="-1"></a>is also 0.</span>
<span id="cb57-1380"><a href="#cb57-1380" aria-hidden="true" tabindex="-1"></a>The Jacobian is $\frac{1}{y} + \lambda = \frac{1 + \lambda y}{y}$, which</span>
<span id="cb57-1381"><a href="#cb57-1381" aria-hidden="true" tabindex="-1"></a>leads to the following density:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Jacobian!generalized production function}</span>
<span id="cb57-1382"><a href="#cb57-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1383"><a href="#cb57-1383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1384"><a href="#cb57-1384" aria-hidden="true" tabindex="-1"></a>f(y_n;\theta,\beta) = \frac{1}{\sigma}\frac{1 + \lambda y_n}{y_n}</span>
<span id="cb57-1385"><a href="#cb57-1385" aria-hidden="true" tabindex="-1"></a>\phi\left(\frac{\ln y_n + \lambda y_n - \mu_n}{\sigma}\right)</span>
<span id="cb57-1386"><a href="#cb57-1386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1387"><a href="#cb57-1387" aria-hidden="true" tabindex="-1"></a>where $\mu_n$ is $\alpha + \sum_{j=1}^J \beta_j \ln q_{nj}^* + \beta_J^* \ln q_{nJ} + \ln q_J$.</span>
<span id="cb57-1388"><a href="#cb57-1388" aria-hidden="true" tabindex="-1"></a>Taking the logarithm of this density, we get the individual</span>
<span id="cb57-1389"><a href="#cb57-1389" aria-hidden="true" tabindex="-1"></a>contribution of an observation to the log-likelihood function:</span>
<span id="cb57-1390"><a href="#cb57-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1391"><a href="#cb57-1391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1392"><a href="#cb57-1392" aria-hidden="true" tabindex="-1"></a>l_n = - \ln \sigma - \frac{1}{2}\ln 2\pi + </span>
<span id="cb57-1393"><a href="#cb57-1393" aria-hidden="true" tabindex="-1"></a>\ln (1 + \lambda y_n) - \ln y_n</span>
<span id="cb57-1394"><a href="#cb57-1394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\frac{1}{2\sigma ^ 2} </span>
<span id="cb57-1395"><a href="#cb57-1395" aria-hidden="true" tabindex="-1"></a>\left(\ln y_n + \lambda y_n - \mu_n\right) ^ 2</span>
<span id="cb57-1396"><a href="#cb57-1396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1397"><a href="#cb57-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1398"><a href="#cb57-1398" aria-hidden="true" tabindex="-1"></a>Denoting $\gamma^ \top = (\alpha, \beta_1, ...\beta_J^*)$, $z_n = (1, q_{n1} ^ *, q_{n2} ^ *, \ldots q_{nJ-1}^*, q_{nJ})$ and $\epsilon_n = \ln y_n + \lambda y_n - \mu_n$, the derivatives with respect to the unknown parameters $(\gamma, \lambda, \sigma)$ are:</span>
<span id="cb57-1399"><a href="#cb57-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1400"><a href="#cb57-1400" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1401"><a href="#cb57-1401" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-1402"><a href="#cb57-1402" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-1403"><a href="#cb57-1403" aria-hidden="true" tabindex="-1"></a>\frac{\partial l_n}{\partial \gamma} &amp;=&amp; \frac{1}{\sigma ^ 2}\epsilon_nz_n</span>
<span id="cb57-1404"><a href="#cb57-1404" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb57-1405"><a href="#cb57-1405" aria-hidden="true" tabindex="-1"></a>\frac{\partial l_n}{\partial \lambda} &amp;=&amp; \frac{y_n}{1 + \lambda y_n} -</span>
<span id="cb57-1406"><a href="#cb57-1406" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma ^ 2}\epsilon_n y_n <span class="sc">\\</span></span>
<span id="cb57-1407"><a href="#cb57-1407" aria-hidden="true" tabindex="-1"></a>\frac{\partial l_n}{\partial \sigma} &amp;=&amp; - \frac{1}{\sigma} +</span>
<span id="cb57-1408"><a href="#cb57-1408" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma ^ 3} \epsilon_n ^ 2<span class="sc">\\</span></span>
<span id="cb57-1409"><a href="#cb57-1409" aria-hidden="true" tabindex="-1"></a>\end{array}<span class="sc">\\</span></span>
<span id="cb57-1410"><a href="#cb57-1410" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-1411"><a href="#cb57-1411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1412"><a href="#cb57-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1413"><a href="#cb57-1413" aria-hidden="true" tabindex="-1"></a>and the second derivatives give the following individual</span>
<span id="cb57-1414"><a href="#cb57-1414" aria-hidden="true" tabindex="-1"></a>contributions to the hessian:</span>
<span id="cb57-1415"><a href="#cb57-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1416"><a href="#cb57-1416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1417"><a href="#cb57-1417" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-1418"><a href="#cb57-1418" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccc}</span>
<span id="cb57-1419"><a href="#cb57-1419" aria-hidden="true" tabindex="-1"></a>-\frac{1}{\sigma^2}z_n z_n^\top &amp; \frac{1}{\sigma^2} y_n z_n</span>
<span id="cb57-1420"><a href="#cb57-1420" aria-hidden="true" tabindex="-1"></a>&amp; - \frac{2}{\sigma^3}\epsilon_n z_n <span class="sc">\\</span></span>
<span id="cb57-1421"><a href="#cb57-1421" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma^2} y_n z_n^\top &amp; -\frac{y_n^2}{(1 + \lambda</span>
<span id="cb57-1422"><a href="#cb57-1422" aria-hidden="true" tabindex="-1"></a>y_n) ^2} - \frac{y_n^2}{\sigma^2} &amp; \frac{2}{\sigma^3} \epsilon_n y_n <span class="sc">\\</span></span>
<span id="cb57-1423"><a href="#cb57-1423" aria-hidden="true" tabindex="-1"></a>-\frac{2}{\sigma^3}\epsilon_n z_n^\top &amp; \frac{2}{\sigma^3}\epsilon_n</span>
<span id="cb57-1424"><a href="#cb57-1424" aria-hidden="true" tabindex="-1"></a>y_n &amp; \frac{1}{\sigma^2} - \frac{3}{\sigma^4}\epsilon_n^2</span>
<span id="cb57-1425"><a href="#cb57-1425" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-1426"><a href="#cb57-1426" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb57-1427"><a href="#cb57-1427" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1428"><a href="#cb57-1428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1429"><a href="#cb57-1429" aria-hidden="true" tabindex="-1"></a>To estimate the generalized production function, we use the **maxLik** package <span class="co">[</span><span class="ot">see @HENN:TOOM:11</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Henningsen}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Hamann}, which is dedicated to  ML estimation. This package provides different algorithms to</span>
<span id="cb57-1430"><a href="#cb57-1430" aria-hidden="true" tabindex="-1"></a>compute the maximum of the likelihood, the Newton-Raphson method that</span>
<span id="cb57-1431"><a href="#cb57-1431" aria-hidden="true" tabindex="-1"></a>we used previously being the default. A <span class="in">`maxLik`</span> object is returned, and specific methods as <span class="in">`coef`</span> and <span class="in">`summary`</span> are provided. To use **maxLik**, we need to define a function of the</span>
<span id="cb57-1432"><a href="#cb57-1432" aria-hidden="true" tabindex="-1"></a>unknown parameters which returns the value of the log-likelihood</span>
<span id="cb57-1433"><a href="#cb57-1433" aria-hidden="true" tabindex="-1"></a>function. This function can return either a scalar (the</span>
<span id="cb57-1434"><a href="#cb57-1434" aria-hidden="true" tabindex="-1"></a>log-likelihood) or a $N$-length vector that contains the individual</span>
<span id="cb57-1435"><a href="#cb57-1435" aria-hidden="true" tabindex="-1"></a>contribution to the log-likelihood. Moreover, it is advisable to</span>
<span id="cb57-1436"><a href="#cb57-1436" aria-hidden="true" tabindex="-1"></a>provide a function that returns the gradient: it can either return a</span>
<span id="cb57-1437"><a href="#cb57-1437" aria-hidden="true" tabindex="-1"></a>$K+1$-length vector or a $N \times (K + 1)$ matrix on which each line is the</span>
<span id="cb57-1438"><a href="#cb57-1438" aria-hidden="true" tabindex="-1"></a>contribution of an observation to the gradient. The analytical hessian</span>
<span id="cb57-1439"><a href="#cb57-1439" aria-hidden="true" tabindex="-1"></a>matrix can also be provided (otherwise, a numerical approximation is</span>
<span id="cb57-1440"><a href="#cb57-1440" aria-hidden="true" tabindex="-1"></a>computed, which is more time-consuming and less precise).  Moreover,</span>
<span id="cb57-1441"><a href="#cb57-1441" aria-hidden="true" tabindex="-1"></a>**maxLik** allows to provide a function that returns the</span>
<span id="cb57-1442"><a href="#cb57-1442" aria-hidden="true" tabindex="-1"></a>maximum-likelihood and the gradient and the hessian as attributes. It</span>
<span id="cb57-1443"><a href="#cb57-1443" aria-hidden="true" tabindex="-1"></a>is a good idea to do so, as there are often common code while writing the</span>
<span id="cb57-1444"><a href="#cb57-1444" aria-hidden="true" tabindex="-1"></a>likelihood, the gradient and the hessian. </span>
<span id="cb57-1445"><a href="#cb57-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1446"><a href="#cb57-1446" aria-hidden="true" tabindex="-1"></a>The <span class="in">`micsr::zellner_revankar`</span> function returns the log-likelihood function for the generalized production function. Its mandatory arguments are <span class="in">`theta`</span>, a vector of starting values, <span class="in">`y`</span>, the vector of response and <span class="in">`Z`</span>, a matrix of covariates. By default, the function </span>
<span id="cb57-1447"><a href="#cb57-1447" aria-hidden="true" tabindex="-1"></a>computes the log-likelihood (a vector of contributions),</span>
<span id="cb57-1448"><a href="#cb57-1448" aria-hidden="true" tabindex="-1"></a>the gradient (a matrix of contributions) and the hessian. The</span>
<span id="cb57-1449"><a href="#cb57-1449" aria-hidden="true" tabindex="-1"></a><span class="in">`gradient`</span> and <span class="in">`hessian`</span> arguments (by default <span class="in">`TRUE`</span>) are booleans</span>
<span id="cb57-1450"><a href="#cb57-1450" aria-hidden="true" tabindex="-1"></a>which enable to return only the log-likelihood if set to <span class="in">`FALSE`</span>. If</span>
<span id="cb57-1451"><a href="#cb57-1451" aria-hidden="true" tabindex="-1"></a>the <span class="in">`sum`</span> argument is <span class="in">`FALSE`</span> (the default), the log-likelihood and</span>
<span id="cb57-1452"><a href="#cb57-1452" aria-hidden="true" tabindex="-1"></a>the gradient are returned as a vector and a matrix of individual</span>
<span id="cb57-1453"><a href="#cb57-1453" aria-hidden="true" tabindex="-1"></a>contributions. If set to <span class="in">`TRUE`</span>, a scalar and a vector are returned.</span>
<span id="cb57-1454"><a href="#cb57-1454" aria-hidden="true" tabindex="-1"></a>Finally, the <span class="in">`repar`</span> argument enables to write the likelihood in its</span>
<span id="cb57-1455"><a href="#cb57-1455" aria-hidden="true" tabindex="-1"></a>"raw" form (<span class="in">`repar = FALSE`</span>) or in its reparametrized form (the</span>
<span id="cb57-1456"><a href="#cb57-1456" aria-hidden="true" tabindex="-1"></a>default). </span>
<span id="cb57-1457"><a href="#cb57-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1458"><a href="#cb57-1458" aria-hidden="true" tabindex="-1"></a>We first extract the response and the covariates matrix:</span>
<span id="cb57-1459"><a href="#cb57-1459" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-1460"><a href="#cb57-1460" aria-hidden="true" tabindex="-1"></a>\idxfun{model.frame}{stats}\idxfun{model.matrix}{stats}\idxfun{model.response}{stats}</span>
<span id="cb57-1461"><a href="#cb57-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1462"><a href="#cb57-1462" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1463"><a href="#cb57-1463" aria-hidden="true" tabindex="-1"></a>form <span class="ot">&lt;-</span> y <span class="sc">~</span> <span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">log</span>(m) ;</span>
<span id="cb57-1464"><a href="#cb57-1464" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(form, aps)</span>
<span id="cb57-1465"><a href="#cb57-1465" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">model.response</span>(mf) ;</span>
<span id="cb57-1466"><a href="#cb57-1466" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(form, mf)</span>
<span id="cb57-1467"><a href="#cb57-1467" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1468"><a href="#cb57-1468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1469"><a href="#cb57-1469" aria-hidden="true" tabindex="-1"></a>We then use as starting values the coefficients of the Cobb-Douglas previously estimated, and we set the starting value of $\lambda$ to 0.</span>
<span id="cb57-1470"><a href="#cb57-1470" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}</span>
<span id="cb57-1471"><a href="#cb57-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1472"><a href="#cb57-1472" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1473"><a href="#cb57-1473" aria-hidden="true" tabindex="-1"></a>st_val <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(cd_repar)[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">lambda =</span> <span class="dv">0</span>, <span class="fu">coef</span>(cd_repar)[<span class="dv">5</span>])</span>
<span id="cb57-1474"><a href="#cb57-1474" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1475"><a href="#cb57-1475" aria-hidden="true" tabindex="-1"></a>&lt;</span>
<span id="cb57-1476"><a href="#cb57-1476" aria-hidden="true" tabindex="-1"></a>We now proceed to the estimation. The two mandatory arguments of <span class="in">`maxLik`</span> are the first argument <span class="in">`logLik`</span> which should be a function returning the log-likelihood and <span class="in">`start`</span> which is a vector of starting values. We use also here <span class="in">`y`</span> and <span class="in">`Z`</span> arguments that are passed to <span class="in">`zellner_revankar`</span>:</span>
<span id="cb57-1477"><a href="#cb57-1477" aria-hidden="true" tabindex="-1"></a>\idxfun{maxLik}{maxLik}\idxfun{coef}{stats}</span>
<span id="cb57-1478"><a href="#cb57-1478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1479"><a href="#cb57-1479" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1480"><a href="#cb57-1480" aria-hidden="true" tabindex="-1"></a>gpf <span class="ot">&lt;-</span> maxLik<span class="sc">::</span><span class="fu">maxLik</span>(zellner_revankar, <span class="at">start =</span> st_val, <span class="at">y =</span> y, <span class="at">Z =</span> Z)</span>
<span id="cb57-1481"><a href="#cb57-1481" aria-hidden="true" tabindex="-1"></a>gpf <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb57-1482"><a href="#cb57-1482" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1483"><a href="#cb57-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1484"><a href="#cb57-1484" aria-hidden="true" tabindex="-1"></a>The probability values for the hypothesis that $\sum_j \beta_j =1$ and that $\lambda = 0$  are respectively about 5 and 10%. The hypothesis of constant returns to scale is the joint hypothesis</span>
<span id="cb57-1485"><a href="#cb57-1485" aria-hidden="true" tabindex="-1"></a>that $\sum_j \beta_j =1$ and $\lambda = 0$ and will be tested in the</span>
<span id="cb57-1486"><a href="#cb57-1486" aria-hidden="true" tabindex="-1"></a>next section.</span>
<span id="cb57-1487"><a href="#cb57-1487" aria-hidden="true" tabindex="-1"></a>Applying @eq-elast, the level of output for which the scale</span>
<span id="cb57-1488"><a href="#cb57-1488" aria-hidden="true" tabindex="-1"></a>elasticity is unity is</span>
<span id="cb57-1489"><a href="#cb57-1489" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(coef(gpf)["log(m)"] / coef(gpf)["lambda"], 2)`</span>,</span>
<span id="cb57-1490"><a href="#cb57-1490" aria-hidden="true" tabindex="-1"></a>reminding that the average production is 1.  We then compute the</span>
<span id="cb57-1491"><a href="#cb57-1491" aria-hidden="true" tabindex="-1"></a>elasticity for different values of the production that are presented in @tbl-elasty. Returns to scale are increasing for more than three quarters of the sample.</span>
<span id="cb57-1492"><a href="#cb57-1492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1493"><a href="#cb57-1493" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1494"><a href="#cb57-1494" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-elasty</span></span>
<span id="cb57-1495"><a href="#cb57-1495" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-1496"><a href="#cb57-1496" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Scale elasticity</span></span>
<span id="cb57-1497"><a href="#cb57-1497" aria-hidden="true" tabindex="-1"></a>elast <span class="ot">&lt;-</span> <span class="cf">function</span>(x) (<span class="fu">coef</span>(gpf)[<span class="st">"log(m)"</span>] <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">coef</span>(gpf)[<span class="st">"lambda"</span>] <span class="sc">*</span> x)</span>
<span id="cb57-1498"><a href="#cb57-1498" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">production =</span> <span class="fu">c</span>(<span class="st">"min"</span>, <span class="st">"Q1"</span>, <span class="st">"median"</span>, <span class="st">"Q3"</span>, <span class="st">"max"</span>),</span>
<span id="cb57-1499"><a href="#cb57-1499" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">fivenum</span>(y),</span>
<span id="cb57-1500"><a href="#cb57-1500" aria-hidden="true" tabindex="-1"></a>       <span class="at">elast =</span> <span class="fu">elast</span>(y)) <span class="sc">%&gt;%</span> </span>
<span id="cb57-1501"><a href="#cb57-1501" aria-hidden="true" tabindex="-1"></a>    knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="at">digits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="at">booktabs =</span> <span class="cn">TRUE</span>, <span class="at">linesep =</span> <span class="st">""</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb57-1502"><a href="#cb57-1502" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kable_styling</span>()</span>
<span id="cb57-1503"><a href="#cb57-1503" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1504"><a href="#cb57-1504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1505"><a href="#cb57-1505" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb57-1506"><a href="#cb57-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1507"><a href="#cb57-1507" aria-hidden="true" tabindex="-1"></a>The "raw" version of the model can also be estimated by setting the <span class="in">`repar`</span> argument to <span class="in">`FALSE`</span>. In this case, we use the coefficients of <span class="in">`cd_loglm`</span> as starting values for all the coefficients except $\lambda$ (for which the starting value is set to 0).</span>
<span id="cb57-1508"><a href="#cb57-1508" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{maxLik}{maxLik}</span>
<span id="cb57-1509"><a href="#cb57-1509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1512"><a href="#cb57-1512" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1513"><a href="#cb57-1513" aria-hidden="true" tabindex="-1"></a>st_val2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(cd_loglm)[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">lambda =</span> <span class="dv">0</span>, <span class="fu">coef</span>(cd_loglm)[<span class="dv">5</span>])</span>
<span id="cb57-1514"><a href="#cb57-1514" aria-hidden="true" tabindex="-1"></a>gpf2 <span class="ot">&lt;-</span> maxLik<span class="sc">::</span><span class="fu">maxLik</span>(zellner_revankar, <span class="at">y =</span> y, <span class="at">Z =</span> Z, </span>
<span id="cb57-1515"><a href="#cb57-1515" aria-hidden="true" tabindex="-1"></a>                       <span class="at">start =</span> st_val2, <span class="at">repar =</span> <span class="cn">FALSE</span>)</span>
<span id="cb57-1516"><a href="#cb57-1516" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1517"><a href="#cb57-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1518"><a href="#cb57-1518" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized production function|)}</span>
<span id="cb57-1519"><a href="#cb57-1519" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-1520"><a href="#cb57-1520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1521"><a href="#cb57-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1522"><a href="#cb57-1522" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tests {#sec-ml_tests}</span></span>
<span id="cb57-1523"><a href="#cb57-1523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1524"><a href="#cb57-1524" aria-hidden="true" tabindex="-1"></a>Three kinds of tests will be considered:</span>
<span id="cb57-1525"><a href="#cb57-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1526"><a href="#cb57-1526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>tests for nested models: in this context, there is a "large" model</span>
<span id="cb57-1527"><a href="#cb57-1527" aria-hidden="true" tabindex="-1"></a>that reduces to a "small" model when the hypotheses are</span>
<span id="cb57-1528"><a href="#cb57-1528" aria-hidden="true" tabindex="-1"></a>imposed. Depending on whether H~0~ is true or false, the small or the</span>
<span id="cb57-1529"><a href="#cb57-1529" aria-hidden="true" tabindex="-1"></a>large model are assumed to be the "true" model,</span>
<span id="cb57-1530"><a href="#cb57-1530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>conditional moment tests for which only one model is considered and</span>
<span id="cb57-1531"><a href="#cb57-1531" aria-hidden="true" tabindex="-1"></a>  moment conditions are constructed from the fitted model that should</span>
<span id="cb57-1532"><a href="#cb57-1532" aria-hidden="true" tabindex="-1"></a>  be zero if the tested hypothesis (for example, normality or</span>
<span id="cb57-1533"><a href="#cb57-1533" aria-hidden="true" tabindex="-1"></a>  homoskedasticity) are true,</span>
<span id="cb57-1534"><a href="#cb57-1534" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>tests for non-nested models and especially the test proposed by</span>
<span id="cb57-1535"><a href="#cb57-1535" aria-hidden="true" tabindex="-1"></a>  @VUON:89\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Vuong}: in this case, whatever the values of the parameters,</span>
<span id="cb57-1536"><a href="#cb57-1536" aria-hidden="true" tabindex="-1"></a>  there is no way for one model to reduce to the other model. Moreover, the</span>
<span id="cb57-1537"><a href="#cb57-1537" aria-hidden="true" tabindex="-1"></a>  test have the interesting feature that the two models are compared</span>
<span id="cb57-1538"><a href="#cb57-1538" aria-hidden="true" tabindex="-1"></a>  without hypothesizing that one of them is the true model.</span>
<span id="cb57-1539"><a href="#cb57-1539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1540"><a href="#cb57-1540" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tests for nested models: the three classical tests {#sec-three_tests_ml}</span></span>
<span id="cb57-1541"><a href="#cb57-1541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1542"><a href="#cb57-1542" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!three test principles!maximum likelihood estimator|(}</span>
<span id="cb57-1543"><a href="#cb57-1543" aria-hidden="true" tabindex="-1"></a>We have seen in @sec-three_tests that a set of hypotheses defines constraints on the parameters and therefore leads to two models. The unconstrained or large model doesn't take these constraints into account, so that the log-likelihood is maximized without constraints. The constrained or small model is obtained by maximizing the log-likelihood under the constraints corresponding to the set of hypotheses. This leads to three test principles: the likelihood ratio test (based on the comparison of both models), the Wald test (based only on the unconstrained model) and the Lagrange multiplier test (based only on the constrained model). Remember that, when applying these test principles on a model fitted by OLS, the three statistics are exactly</span>
<span id="cb57-1544"><a href="#cb57-1544" aria-hidden="true" tabindex="-1"></a>the same.^[Actually, the Lagrange multiplier test is</span>
<span id="cb57-1545"><a href="#cb57-1545" aria-hidden="true" tabindex="-1"></a>numerically different, only because the estimation of</span>
<span id="cb57-1546"><a href="#cb57-1546" aria-hidden="true" tabindex="-1"></a>$\sigma^2_\epsilon$ is based on the constrained model.] On the,</span>
<span id="cb57-1547"><a href="#cb57-1547" aria-hidden="true" tabindex="-1"></a>contrary the three tests give different results for non-linear models</span>
<span id="cb57-1548"><a href="#cb57-1548" aria-hidden="true" tabindex="-1"></a>but, if the hypothesis are true, the three test statistics follow a $\chi ^ 2$ distribution with a number of degrees of freedom equal to the number of hypotheses and converge in</span>
<span id="cb57-1549"><a href="#cb57-1549" aria-hidden="true" tabindex="-1"></a>probability to the same value.</span>
<span id="cb57-1550"><a href="#cb57-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1551"><a href="#cb57-1551" aria-hidden="true" tabindex="-1"></a>We'll consider in this section a set of $J$ linear hypotheses, such that $R\theta - q = 0$, and we'll denote $\hat{\theta}_{nc}$ and $\hat{\theta}_{c}$ the unconstrained and constrained maximum likelihood estimators.</span>
<span id="cb57-1552"><a href="#cb57-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1553"><a href="#cb57-1553" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Three tests for the linear gaussian model</span></span>
<span id="cb57-1554"><a href="#cb57-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1555"><a href="#cb57-1555" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!three test principles!linear gaussian model|(}</span>
<span id="cb57-1556"><a href="#cb57-1556" aria-hidden="true" tabindex="-1"></a>We first compute the three tests for the linear gaussian model developed in @sec-linear-gaussian. We denote</span>
<span id="cb57-1557"><a href="#cb57-1557" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon}_{nc}$ and $\hat{\epsilon}_c$ the vectors of residuals for the unconstrained and the constrained model. </span>
<span id="cb57-1558"><a href="#cb57-1558" aria-hidden="true" tabindex="-1"></a>The F statistic for the hypothesis that $R\gamma = q$ is, denoting $A = R(Z^\top Z)^{-1}R^\top$:</span>
<span id="cb57-1559"><a href="#cb57-1559" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{F statistic}</span>
<span id="cb57-1560"><a href="#cb57-1560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1561"><a href="#cb57-1561" aria-hidden="true" tabindex="-1"></a>F = \frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}} \frac{N - K - 1}{J} =</span>
<span id="cb57-1562"><a href="#cb57-1562" aria-hidden="true" tabindex="-1"></a>\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\dot{\sigma} ^ 2_{nc}} / J</span>
<span id="cb57-1563"><a href="#cb57-1563" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1564"><a href="#cb57-1564" aria-hidden="true" tabindex="-1"></a>where $\dot{\sigma}_{nc} ^ 2 = \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc} / (N - K -1)$ is the unbiased estimator of $\sigma ^ 2$. With the hypothesis of iid normal errors, this statistic follows a Fisher-Snedecor distribution with $J$ and $N-K-1$ degrees of freedom and the asymptotic distribution of $J \times F$ is a $\chi ^2$ with $J$ degrees of freedom.</span>
<span id="cb57-1565"><a href="#cb57-1565" aria-hidden="true" tabindex="-1"></a>Remember from @eq-const_lm that $\hat{\gamma}_c=\hat{\gamma}_{nc} - (Z^\top Z)^{-1}R^\top A^{-1}(R\hat{\gamma}_{nc}-q)$, then:</span>
<span id="cb57-1566"><a href="#cb57-1566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1567"><a href="#cb57-1567" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1568"><a href="#cb57-1568" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_c = \hat{\epsilon}_{nc}+ Z(Z^\top Z)^{-1}R^\top A^{-1}(R\hat{\gamma}_{nc}-q)</span>
<span id="cb57-1569"><a href="#cb57-1569" aria-hidden="true" tabindex="-1"></a>$$ {#eq-resid_c_nc}</span>
<span id="cb57-1570"><a href="#cb57-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1571"><a href="#cb57-1571" aria-hidden="true" tabindex="-1"></a>and, because $Z ^ \top \hat{\epsilon}_{nc} = 0$, the relation between the two residual sum of squares is:</span>
<span id="cb57-1572"><a href="#cb57-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1573"><a href="#cb57-1573" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1574"><a href="#cb57-1574" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_c ^ \top \hat{\epsilon}_c = \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc} + (R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)</span>
<span id="cb57-1575"><a href="#cb57-1575" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ssr_c_nc}</span>
<span id="cb57-1576"><a href="#cb57-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1577"><a href="#cb57-1577" aria-hidden="true" tabindex="-1"></a>The likelihood ratio statistic is, using @eq-conc_loglik:</span>
<span id="cb57-1578"><a href="#cb57-1578" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test!linear gaussian model}</span>
<span id="cb57-1579"><a href="#cb57-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1580"><a href="#cb57-1580" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1581"><a href="#cb57-1581" aria-hidden="true" tabindex="-1"></a>LR = 2\left(\ln L(\hat{\gamma}_{nc})-\ln L(\hat{\gamma}_{c})\right) = N \ln \frac{\hat{\epsilon}_c ^ \top \hat{\epsilon}_c}{\hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}}</span>
<span id="cb57-1582"><a href="#cb57-1582" aria-hidden="true" tabindex="-1"></a>$$ {#eq-lr_stat}</span>
<span id="cb57-1583"><a href="#cb57-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1584"><a href="#cb57-1584" aria-hidden="true" tabindex="-1"></a>and the Wald statistic is:</span>
<span id="cb57-1585"><a href="#cb57-1585" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test!linear gaussian model}</span>
<span id="cb57-1586"><a href="#cb57-1586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1587"><a href="#cb57-1587" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1588"><a href="#cb57-1588" aria-hidden="true" tabindex="-1"></a>W = N\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}}=</span>
<span id="cb57-1589"><a href="#cb57-1589" aria-hidden="true" tabindex="-1"></a>\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\sigma} ^ 2_{nc}}</span>
<span id="cb57-1590"><a href="#cb57-1590" aria-hidden="true" tabindex="-1"></a>$$ {#eq-wald_stat}</span>
<span id="cb57-1591"><a href="#cb57-1591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1592"><a href="#cb57-1592" aria-hidden="true" tabindex="-1"></a>For the Lagrange multiplier test, as the hessian is block diagonal, we can consider only the part of the gradient and the hessian that concerns $\gamma$. For the constrained model, the subset of the gradient and of the information matrix are $Z ^ \top \epsilon_{c} / \hat{\sigma}_{c}^2$ and $Z ^ \top Z / \hat{\sigma}_c ^ 2$, where $\hat{\sigma}_c ^ 2 = \hat{\epsilon}_c ^ \top \hat{\epsilon}_c/N$ Then the statistic is $\hat{\epsilon}_c ^ \top Z (Z ^\top Z) ^ {-1} Z ^ \top \hat{\epsilon}_c / \hat{\sigma}_{c} ^ 2$. Using @eq-resid_c_nc, we get:</span>
<span id="cb57-1593"><a href="#cb57-1593" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test!linear gaussian model}</span>
<span id="cb57-1594"><a href="#cb57-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1595"><a href="#cb57-1595" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1596"><a href="#cb57-1596" aria-hidden="true" tabindex="-1"></a>LM = N\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\epsilon}_{c} ^ \top \hat{\epsilon}_{c}}=</span>
<span id="cb57-1597"><a href="#cb57-1597" aria-hidden="true" tabindex="-1"></a>\frac{(R\hat{\gamma}_{nc} - q) ^ \top A ^ {-1} (R\hat{\gamma}_{nc} - q)}{\hat{\sigma} ^ 2_{c}}</span>
<span id="cb57-1598"><a href="#cb57-1598" aria-hidden="true" tabindex="-1"></a>$$ {#eq-lm_stat}</span>
<span id="cb57-1599"><a href="#cb57-1599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1600"><a href="#cb57-1600" aria-hidden="true" tabindex="-1"></a>which is the same expression as @eq-wald_stat except that the estimation of $\sigma^2$ is based on the constrained model.</span>
<span id="cb57-1601"><a href="#cb57-1601" aria-hidden="true" tabindex="-1"></a>From @eq-ssr_c_nc, $e ^{LR / N} = 1 + W / N$. Therefore:</span>
<span id="cb57-1602"><a href="#cb57-1602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1603"><a href="#cb57-1603" aria-hidden="true" tabindex="-1"></a>$$LR/N = \ln (1 + W/N)$$</span>
<span id="cb57-1604"><a href="#cb57-1604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1605"><a href="#cb57-1605" aria-hidden="true" tabindex="-1"></a>Defining, $f(x) = \ln (1 + x) - x$ for $x \geq 0$, $f'(x) = -x / (1 + x) &lt; 0$. As $f(0) = 0$ and $f$ is a strictly decreasing function, $f(x) &lt; 0$ for $x &gt; 0$. Therefore $\ln (1 + W/N) - W/N = LR/N - W/N &lt; 0$ and therefore $W &gt; LR$.</span>
<span id="cb57-1606"><a href="#cb57-1606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1607"><a href="#cb57-1607" aria-hidden="true" tabindex="-1"></a>Using @eq-wald_stat and @eq-lm_stat, we get: $W / LM = \hat{\epsilon}_c ^ \top \hat{\epsilon}_c / \hat{\epsilon}_{nc} ^ \top \hat{\epsilon}_{nc}$. Finally, using @eq-ssr_c_nc, we get: $W/LM = 1 + W/N$ or, rearranging terms:</span>
<span id="cb57-1608"><a href="#cb57-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1609"><a href="#cb57-1609" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1610"><a href="#cb57-1610" aria-hidden="true" tabindex="-1"></a>LM / N = \frac{W / N}{1 + W / N}</span>
<span id="cb57-1611"><a href="#cb57-1611" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1612"><a href="#cb57-1612" aria-hidden="true" tabindex="-1"></a>Denoting $f(x) = \ln (1+x) - x / (1 + x)$, for $x \geq 0$, $f'(x) = x / (1 + x) ^ 2$. Therefore, $f(x)$ is strictly increasing for $x &gt;0$ and, as $f(0) = 0$, $f(x) &gt; 0 \; \forall x &gt;0$. With $x = W / N$, $\ln (1 + W/N) - W/N / (1 + W/N) = LR/N - LM /N &gt; 0$ and therefore $LR &gt; LM$. Therefore, we have proved that:^<span class="co">[</span><span class="ot">See @VAND:81.</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Vandaele}</span>
<span id="cb57-1613"><a href="#cb57-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1614"><a href="#cb57-1614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1615"><a href="#cb57-1615" aria-hidden="true" tabindex="-1"></a>LM&lt;LR<span class="kw">&lt;W</span></span>
<span id="cb57-1616"><a href="#cb57-1616" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1617"><a href="#cb57-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1618"><a href="#cb57-1618" aria-hidden="true" tabindex="-1"></a><span class="er">\index[general]{test!three</span> <span class="er">test</span> <span class="er">principles!linear</span> <span class="er">gaussian</span> <span class="er">model|)}</span></span>
<span id="cb57-1619"><a href="#cb57-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1620"><a href="#cb57-1620" aria-hidden="true" tabindex="-1"></a><span class="ot">#</span><span class="er">###</span> <span class="er">Pseudo-R^2^</span> <span class="er">for</span> <span class="er">models</span> <span class="er">estimated</span> <span class="er">by</span> <span class="er">maximum</span> <span class="er">likelihood</span> <span class="er">{#sec-pseudo_r2_ml}</span></span>
<span id="cb57-1621"><a href="#cb57-1621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1622"><a href="#cb57-1622" aria-hidden="true" tabindex="-1"></a><span class="er">\index[general]{coefficient</span> <span class="er">of</span> <span class="er">determination!pseudo-three</span> <span class="er">tests</span> <span class="er">principles|(}</span></span>
<span id="cb57-1623"><a href="#cb57-1623" aria-hidden="true" tabindex="-1"></a><span class="ot">Consider</span> <span class="er">now</span> <span class="er">the</span> <span class="er">case</span> <span class="er">where</span> <span class="er">we</span> <span class="er">test</span> <span class="er">the</span> <span class="er">set</span> <span class="er">of</span> <span class="er">$J</span> <span class="ot">=</span> <span class="st">K$</span> <span class="er">hypothesis</span> <span class="er">that</span> <span class="er">$\beta</span> <span class="ot">=</span> <span class="st">0$.</span> <span class="er">Then,</span> <span class="er">$\mbox{TSS}</span> <span class="ot">=</span> <span class="st">\hat{\epsilon}_c</span> <span class="er">^</span> <span class="er">\top</span> <span class="er">\hat{\epsilon}_c$</span> <span class="er">and</span> <span class="er">$\mbox{RSS}</span> <span class="ot">=</span> <span class="st">\hat{\epsilon}_{nc}</span> <span class="er">^</span> <span class="er">\top</span> <span class="er">\hat{\epsilon}_{nc}$</span> <span class="er">and,</span> <span class="er">from</span> <span class="er">@eq-ssr_c_nc:</span></span>
<span id="cb57-1624"><a href="#cb57-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1625"><a href="#cb57-1625" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1626"><a href="#cb57-1626" aria-hidden="true" tabindex="-1"></a><span class="er">\frac{\mbox{TSS}}{\mbox{RSS}}</span> <span class="ot">=</span> <span class="st">1</span> <span class="er">+</span> <span class="er">W</span> <span class="er">/</span> <span class="er">N</span> <span class="ot">=</span> <span class="st">1</span> <span class="er">+</span> <span class="er">\frac{\mbox{TSS}}{\mbox{RSS}}</span> <span class="er">\times</span> <span class="er">LM</span> <span class="er">/</span> <span class="er">N</span> <span class="ot">=</span> <span class="st">e^{LR</span> <span class="er">/</span> <span class="er">N}</span></span>
<span id="cb57-1627"><a href="#cb57-1627" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1628"><a href="#cb57-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1629"><a href="#cb57-1629" aria-hidden="true" tabindex="-1"></a><span class="ot">Therefore</span><span class="er">,</span> <span class="er">the</span> <span class="er">three</span> <span class="er">statistics</span> <span class="er">are:</span></span>
<span id="cb57-1630"><a href="#cb57-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1631"><a href="#cb57-1631" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1632"><a href="#cb57-1632" aria-hidden="true" tabindex="-1"></a><span class="er">\left\{</span></span>
<span id="cb57-1633"><a href="#cb57-1633" aria-hidden="true" tabindex="-1"></a><span class="er">\begin{array}{rcl}</span></span>
<span id="cb57-1634"><a href="#cb57-1634" aria-hidden="true" tabindex="-1"></a><span class="ot">W</span> <span class="er">&amp;</span><span class="ot">=</span><span class="er">&amp;</span> <span class="er">N</span> <span class="er">(TSS</span> <span class="er">-</span> <span class="er">RSS)</span> <span class="er">/</span> <span class="er">RSS\\</span></span>
<span id="cb57-1635"><a href="#cb57-1635" aria-hidden="true" tabindex="-1"></a><span class="ot">LM</span> <span class="er">&amp;</span><span class="ot">=</span><span class="er">&amp;</span> <span class="er">N</span> <span class="er">(TSS</span> <span class="er">-</span> <span class="er">RSS)</span> <span class="er">/</span> <span class="er">TSS\\</span></span>
<span id="cb57-1636"><a href="#cb57-1636" aria-hidden="true" tabindex="-1"></a><span class="ot">LR</span> <span class="er">&amp;</span><span class="ot">=</span><span class="er">&amp;</span> <span class="er">N</span> <span class="er">\ln</span> <span class="er">TSS</span> <span class="er">/</span> <span class="er">RSS</span></span>
<span id="cb57-1637"><a href="#cb57-1637" aria-hidden="true" tabindex="-1"></a><span class="er">\end{array}</span></span>
<span id="cb57-1638"><a href="#cb57-1638" aria-hidden="true" tabindex="-1"></a><span class="er">\right.</span></span>
<span id="cb57-1639"><a href="#cb57-1639" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1640"><a href="#cb57-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1641"><a href="#cb57-1641" aria-hidden="true" tabindex="-1"></a><span class="ot">The</span> <span class="er">$R^2$</span> <span class="er">being</span> <span class="er">equal</span> <span class="er">to</span> <span class="er">$1</span> <span class="er">-</span> <span class="er">RSS</span> <span class="er">/</span> <span class="er">TSS$,</span> <span class="er">it</span> <span class="er">can</span> <span class="er">be</span> <span class="er">easily</span> <span class="er">expressed,</span> <span class="er">for</span> <span class="er">the</span> <span class="er">linear</span> <span class="er">gaussian</span> <span class="er">model,</span> <span class="er">as</span> <span class="er">a</span> <span class="er">function</span> <span class="er">of</span> <span class="er">the</span> <span class="er">three</span> <span class="er">statistics:</span></span>
<span id="cb57-1642"><a href="#cb57-1642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1643"><a href="#cb57-1643" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1644"><a href="#cb57-1644" aria-hidden="true" tabindex="-1"></a><span class="ot">R</span> <span class="er">^</span> <span class="er">2</span> <span class="ot">=</span> <span class="st">\frac{W}{N</span> <span class="er">+</span> <span class="er">W}</span> <span class="ot">=</span> <span class="st">1</span> <span class="er">-</span> <span class="er">e^{LR</span> <span class="er">/</span> <span class="er">N}</span> <span class="ot">=</span> <span class="st">LM</span> <span class="er">/</span> <span class="er">N</span></span>
<span id="cb57-1645"><a href="#cb57-1645" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span> <span class="er">{#eq-pseudo_r2}</span></span>
<span id="cb57-1646"><a href="#cb57-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1647"><a href="#cb57-1647" aria-hidden="true" tabindex="-1"></a><span class="ot">These</span> <span class="er">pseudo-R^2^</span> <span class="er">can</span> <span class="er">be</span> <span class="er">used</span> <span class="er">for</span> <span class="er">any</span> <span class="er">model</span> <span class="er">computed</span> <span class="er">by</span> <span class="er">maximum</span> <span class="er">likelihood</span> <span class="er">and</span> <span class="er">will</span> <span class="er">be</span> <span class="er">denoted</span> <span class="er">respectively</span> <span class="er">by</span> <span class="er">$R^2_{W}$,</span> <span class="er">$R^2_{LR}$</span> <span class="er">and</span> <span class="er">$R^2_{LM}$.</span> <span class="er">They</span> <span class="er">have</span> <span class="er">been</span> <span class="er">proposed</span> <span class="er">by</span> <span class="er">@MAGE:90\index[author]{Magee}.</span> <span class="er">$R^2_{LR}$</span> <span class="er">is</span> <span class="er">known</span> <span class="er">as</span> <span class="er">the</span> <span class="er">$R^2$</span> <span class="er">of</span> <span class="er">@COX:SNEL:89\index[author]{Cox}\index[author]{Snell},</span> <span class="er">but</span> <span class="er">it</span> <span class="er">has</span> <span class="er">been</span></span>
<span id="cb57-1648"><a href="#cb57-1648" aria-hidden="true" tabindex="-1"></a><span class="ot">previously</span> <span class="er">proposed</span> <span class="er">by</span> <span class="er">@MADD:83\index[author]{Maddala}.</span> <span class="er">A</span> <span class="er">variant,</span> <span class="er">proposed</span> <span class="er">by</span> <span class="er">@ALDR:NELS:84\index[author]{Aldrich}\index[author]{Nelson},</span>  <span class="er">is</span> <span class="er">obtained</span> <span class="er">using</span> <span class="er">the</span> <span class="er">formula</span> <span class="er">of</span> <span class="er">$R^2_{W}$,</span> <span class="er">but</span> <span class="er">using</span> <span class="er">the</span> <span class="er">LR</span> <span class="er">statistic:</span> <span class="er">$LR</span> <span class="er">/</span> <span class="er">(N</span> <span class="er">+</span> <span class="er">LR)$.</span> <span class="er">A</span> <span class="er">problem</span> <span class="er">with</span> <span class="er">the</span> <span class="er">pseudo-R^2^</span> <span class="er">computed</span> <span class="er">using</span> <span class="er">the</span> <span class="er">LR</span> <span class="er">statistic</span> <span class="er">is</span> <span class="er">that,</span> <span class="er">for</span> <span class="er">a</span> <span class="er">perfect</span> <span class="er">or</span> <span class="er">**saturated**</span> <span class="er">model,</span> <span class="er">it</span> <span class="er">can</span> <span class="er">be</span> <span class="er">lower</span> <span class="er">than</span> <span class="er">1.</span> <span class="er">Denoting</span> <span class="er">$\ln</span> <span class="er">L_0$</span> <span class="er">and</span> <span class="er">$\ln</span> <span class="er">L_*$</span> <span class="er">the</span> <span class="er">values</span> <span class="er">of</span> <span class="er">the</span> <span class="er">log-likelihood</span> <span class="er">for</span> <span class="er">the</span> <span class="er">null</span> <span class="er">(intercept</span> <span class="er">only)</span> <span class="er">and</span> <span class="er">the</span> <span class="er">saturated</span> <span class="er">model,</span> <span class="er">the</span> <span class="er">LR</span> <span class="er">statistic</span> <span class="er">for</span> <span class="er">the</span> <span class="er">saturated</span> <span class="er">model</span> <span class="er">is</span> <span class="er">$LR_*</span> <span class="ot">=</span> <span class="st">2(\ln</span> <span class="er">L_*</span> <span class="er">-</span> <span class="er">\ln</span> <span class="er">L_0)$.</span> <span class="er">In</span> <span class="er">case</span> <span class="er">of</span> <span class="er">discrete</span> <span class="er">response</span> <span class="er">(as</span> <span class="er">for</span> <span class="er">the</span> <span class="er">Poisson</span> <span class="er">model</span> <span class="er">developed</span> <span class="er">in</span> <span class="er">the</span> <span class="er">beginning</span> <span class="er">of</span> <span class="er">this</span> <span class="er">chapter),</span> <span class="er">the</span> <span class="er">individual</span> <span class="er">contributions</span> <span class="er">to</span> <span class="er">the</span> <span class="er">log-likelihood</span> <span class="er">are</span> <span class="er">logs</span> <span class="er">of</span> <span class="er">probabilities</span> <span class="er">(all</span> <span class="er">equal</span> <span class="er">to</span> <span class="er">1</span> <span class="er">for</span> <span class="er">a</span> <span class="er">saturated</span> <span class="er">model),</span> <span class="er">so</span> <span class="er">that</span> <span class="er">$\ln</span> <span class="er">L_*</span> <span class="ot">=</span> <span class="st">0$</span> <span class="er">and</span> <span class="er">$LR_*</span> <span class="ot">=</span> <span class="st">-2\ln</span> <span class="er">L_0$.</span> <span class="er">Therefore,</span> <span class="er">$R^2_{LR}$</span> <span class="er">equals</span> <span class="er">$1</span> <span class="er">-</span> <span class="er">e^{LR_*/N}</span> <span class="er">\neq</span> <span class="er">1$.</span> <span class="er">Scaled</span> <span class="er">versions</span> <span class="er">of</span> <span class="er">$R^2_{LR}$</span> <span class="er">and</span> <span class="er">@ALDR:NELS:84's\index[author]{Aldrich}\index[author]{Nelson}</span> <span class="er">$R^2$</span> <span class="er">have</span> <span class="er">been</span> <span class="er">proposed</span> <span class="er">respectively</span> <span class="er">by</span> <span class="er">@NAGE:91\index[author]{Nagelkerke}:^[And</span> <span class="er">previously</span> <span class="er">used</span> <span class="er">by</span> <span class="er">@CRAG:UHLE:70\index[author]{Cragg}\index[author]{Uhler}.]</span></span>
<span id="cb57-1649"><a href="#cb57-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1650"><a href="#cb57-1650" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1651"><a href="#cb57-1651" aria-hidden="true" tabindex="-1"></a><span class="ot">R</span><span class="er">^2</span> <span class="ot">=</span> <span class="st">\frac{1</span> <span class="er">-</span> <span class="er">e</span> <span class="er">^</span> <span class="er">{-</span> <span class="er">LR</span> <span class="er">/</span> <span class="er">N}}{1</span> <span class="er">-</span> <span class="er">e</span> <span class="er">^</span> <span class="er">{-</span> <span class="er">LR</span> <span class="er">_</span> <span class="er">*</span> <span class="er">/</span> <span class="er">N}}</span></span>
<span id="cb57-1652"><a href="#cb57-1652" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1653"><a href="#cb57-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1654"><a href="#cb57-1654" aria-hidden="true" tabindex="-1"></a><span class="ot">and</span> <span class="er">by</span> <span class="er">@VEAL:ZIMM:96:\index[author]{Veall}\index[author]{Zimmermann}</span></span>
<span id="cb57-1655"><a href="#cb57-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1656"><a href="#cb57-1656" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1657"><a href="#cb57-1657" aria-hidden="true" tabindex="-1"></a><span class="ot">R</span><span class="er">^2</span> <span class="ot">=</span> <span class="st">\frac{LR</span> <span class="er">/</span> <span class="er">(LR</span> <span class="er">+</span> <span class="er">N)}{LR</span> <span class="er">_</span> <span class="er">*</span> <span class="er">/</span> <span class="er">(LR</span> <span class="er">_</span> <span class="er">*</span> <span class="er">+</span> <span class="er">N)}</span></span>
<span id="cb57-1658"><a href="#cb57-1658" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1659"><a href="#cb57-1659" aria-hidden="true" tabindex="-1"></a><span class="er">\index[general]{coefficient</span> <span class="er">of</span> <span class="er">determination!pseudo-three</span> <span class="er">tests</span> <span class="er">principles|)}</span></span>
<span id="cb57-1660"><a href="#cb57-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1661"><a href="#cb57-1661" aria-hidden="true" tabindex="-1"></a><span class="ot">#</span><span class="er">###</span> <span class="er">General</span> <span class="er">formula</span> <span class="er">and</span> <span class="er">application</span> <span class="er">to</span> <span class="er">the</span> <span class="er">generalized</span> <span class="er">production</span> <span class="er">function</span></span>
<span id="cb57-1662"><a href="#cb57-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1663"><a href="#cb57-1663" aria-hidden="true" tabindex="-1"></a><span class="ot">Although</span> <span class="er">the</span> <span class="er">three</span> <span class="er">tests</span> <span class="er">are</span> <span class="er">not</span> <span class="er">limited</span> <span class="er">to</span> <span class="er">linear</span> <span class="er">constraints,</span> <span class="er">we'll</span></span>
<span id="cb57-1664"><a href="#cb57-1664" aria-hidden="true" tabindex="-1"></a><span class="ot">consider</span> <span class="er">only</span> <span class="er">linear</span> <span class="er">hypothesis</span> <span class="er">in</span> <span class="er">this</span> <span class="er">section.</span> <span class="er">Moreover,</span> <span class="er">as</span> <span class="er">seen</span></span>
<span id="cb57-1665"><a href="#cb57-1665" aria-hidden="true" tabindex="-1"></a><span class="ot">previously</span><span class="er">,</span> <span class="er">a</span> <span class="er">linear</span> <span class="er">model</span> <span class="er">can</span> <span class="er">always</span> <span class="er">be</span> <span class="er">reperametrized</span> <span class="er">so</span> <span class="er">that</span> <span class="er">a</span> <span class="er">set</span> <span class="er">of</span></span>
<span id="cb57-1666"><a href="#cb57-1666" aria-hidden="true" tabindex="-1"></a><span class="ot">linear</span> <span class="er">hypothesis</span> <span class="er">reduces</span> <span class="er">to</span> <span class="er">the</span> <span class="er">test</span> <span class="er">that</span> <span class="er">a</span> <span class="er">subset</span> <span class="er">of</span> <span class="er">the</span> <span class="er">parameters</span></span>
<span id="cb57-1667"><a href="#cb57-1667" aria-hidden="true" tabindex="-1"></a><span class="ot">(</span><span class="er">$\theta_2$)</span> <span class="er">is</span> <span class="er">zero.</span></span>
<span id="cb57-1668"><a href="#cb57-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1669"><a href="#cb57-1669" aria-hidden="true" tabindex="-1"></a><span class="ot">In</span> <span class="er">our</span> <span class="er">generalized</span> <span class="er">production</span> <span class="er">function</span> <span class="er">example,</span> <span class="er">the</span> <span class="er">original</span> <span class="er">set</span> <span class="er">of</span></span>
<span id="cb57-1670"><a href="#cb57-1670" aria-hidden="true" tabindex="-1"></a><span class="ot">parameters</span> <span class="er">are</span> <span class="er">$\theta</span> <span class="er">^</span> <span class="er">\top</span> <span class="ot">=</span> <span class="st">(\alpha,</span> <span class="er">\beta_k,</span> <span class="er">\beta_l,</span> <span class="er">\beta_s,</span> <span class="er">\lambda,</span> <span class="er">\sigma)$</span></span>
<span id="cb57-1671"><a href="#cb57-1671" aria-hidden="true" tabindex="-1"></a><span class="ot">and</span> <span class="er">the</span> <span class="er">constant</span> <span class="er">returns</span> <span class="er">to</span> <span class="er">scale</span> <span class="er">hypothesis</span> <span class="er">is</span> </span>
<span id="cb57-1672"><a href="#cb57-1672" aria-hidden="true" tabindex="-1"></a><span class="er">$\beta_k</span> <span class="er">+</span> <span class="er">\beta_l</span> <span class="er">+</span> <span class="er">\beta_s</span> <span class="ot">=</span> <span class="st">1$</span> <span class="er">and</span> <span class="er">$\lambda</span> <span class="ot">=</span> <span class="st">0$.</span></span>
<span id="cb57-1673"><a href="#cb57-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1674"><a href="#cb57-1674" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1675"><a href="#cb57-1675" aria-hidden="true" tabindex="-1"></a><span class="ot">R</span> <span class="er">\theta</span> <span class="er">-</span> <span class="er">q</span> <span class="ot">=</span> <span class="st">\left(</span></span>
<span id="cb57-1676"><a href="#cb57-1676" aria-hidden="true" tabindex="-1"></a><span class="er">\begin{array}{cccccc}</span></span>
<span id="cb57-1677"><a href="#cb57-1677" aria-hidden="true" tabindex="-1"></a><span class="er">0</span> <span class="er">&amp;</span> <span class="er">1</span> <span class="er">&amp;</span> <span class="er">1</span> <span class="er">&amp;</span> <span class="er">1</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0\\</span></span>
<span id="cb57-1678"><a href="#cb57-1678" aria-hidden="true" tabindex="-1"></a><span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">1</span> <span class="er">&amp;</span> <span class="er">0\\</span></span>
<span id="cb57-1679"><a href="#cb57-1679" aria-hidden="true" tabindex="-1"></a><span class="er">\end{array}\right)</span></span>
<span id="cb57-1680"><a href="#cb57-1680" aria-hidden="true" tabindex="-1"></a><span class="er">\left(\begin{array}{c}\alpha\\</span> <span class="er">\beta_k\\</span> <span class="er">\beta_l\\</span> <span class="er">\beta_s\\</span> <span class="er">\lambda\\</span> <span class="er">\sigma\end{array}\right)</span></span>
<span id="cb57-1681"><a href="#cb57-1681" aria-hidden="true" tabindex="-1"></a><span class="er">-</span> <span class="er">\left(\begin{array}{c}1</span> <span class="er">\\</span> <span class="er">0</span> <span class="er">\end{array}\right)</span><span class="ot">=</span> <span class="st">0</span></span>
<span id="cb57-1682"><a href="#cb57-1682" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1683"><a href="#cb57-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1684"><a href="#cb57-1684" aria-hidden="true" tabindex="-1"></a><span class="ot">After</span> <span class="er">the</span> <span class="er">reparametrization,</span> <span class="er">the</span></span>
<span id="cb57-1685"><a href="#cb57-1685" aria-hidden="true" tabindex="-1"></a><span class="ot">set</span> <span class="er">of</span> <span class="er">parameters</span> <span class="er">is</span> <span class="er">$\theta^</span> <span class="er">{*\top}</span> <span class="ot">=</span> <span class="st">(\alpha,</span> <span class="er">\beta_k,</span> <span class="er">\beta_l,</span> <span class="er">\beta_s^*,</span> <span class="er">\lambda,</span> <span class="er">\sigma)$</span> <span class="er">and</span> <span class="er">the</span> <span class="er">constant</span> <span class="er">returns</span> <span class="er">to</span> <span class="er">scale</span> <span class="er">hypothesis</span> <span class="er">is</span> <span class="er">$\lambda</span> <span class="ot">=</span> <span class="st">\beta_s^*=0$</span> <span class="er">:</span></span>
<span id="cb57-1686"><a href="#cb57-1686" aria-hidden="true" tabindex="-1"></a><span class="er">\index[general]{reparametrization!generalized</span> <span class="er">production</span> <span class="er">function}</span></span>
<span id="cb57-1687"><a href="#cb57-1687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1688"><a href="#cb57-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1689"><a href="#cb57-1689" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1690"><a href="#cb57-1690" aria-hidden="true" tabindex="-1"></a><span class="ot">R</span><span class="er">^*</span> <span class="er">\theta^*</span> <span class="er">-</span> <span class="er">q^*</span> <span class="ot">=</span> <span class="st">\left(</span></span>
<span id="cb57-1691"><a href="#cb57-1691" aria-hidden="true" tabindex="-1"></a><span class="er">\begin{array}{cccccc}</span></span>
<span id="cb57-1692"><a href="#cb57-1692" aria-hidden="true" tabindex="-1"></a><span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">1</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0\\</span></span>
<span id="cb57-1693"><a href="#cb57-1693" aria-hidden="true" tabindex="-1"></a><span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">0</span> <span class="er">&amp;</span> <span class="er">1</span> <span class="er">&amp;</span> <span class="er">0\\</span></span>
<span id="cb57-1694"><a href="#cb57-1694" aria-hidden="true" tabindex="-1"></a><span class="er">\end{array}\right)</span></span>
<span id="cb57-1695"><a href="#cb57-1695" aria-hidden="true" tabindex="-1"></a><span class="er">\left(\begin{array}{c}\alpha\\</span> <span class="er">\beta_k\\</span> <span class="er">\beta_l\\</span> <span class="er">\beta_s</span> <span class="er">^</span> <span class="er">*\\</span> <span class="er">\lambda\\</span> <span class="er">\sigma\end{array}\right)</span></span>
<span id="cb57-1696"><a href="#cb57-1696" aria-hidden="true" tabindex="-1"></a><span class="er">-</span> <span class="er">\left(\begin{array}{c}0</span> <span class="er">\\</span> <span class="er">0</span> <span class="er">\end{array}\right)</span><span class="ot">=</span> <span class="st">0</span></span>
<span id="cb57-1697"><a href="#cb57-1697" aria-hidden="true" tabindex="-1"></a><span class="er">$$</span></span>
<span id="cb57-1698"><a href="#cb57-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1699"><a href="#cb57-1699" aria-hidden="true" tabindex="-1"></a><span class="er">$R^*$</span> <span class="er">is</span> <span class="er">then</span> <span class="er">a</span> <span class="er">matrix</span> <span class="er">which</span> <span class="er">selects</span> <span class="er">a</span> <span class="er">subset</span> <span class="er">of</span> <span class="er">coefficients,</span> <span class="er">that</span> <span class="er">we'll</span> <span class="er">call</span> <span class="er">$\theta_2$.</span></span>
<span id="cb57-1700"><a href="#cb57-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1701"><a href="#cb57-1701" aria-hidden="true" tabindex="-1"></a><span class="er">&lt;!--</span> <span class="er">#####</span> <span class="er">Wald</span> <span class="er">test</span> <span class="er">--</span><span class="kw">&gt;</span></span>
<span id="cb57-1702"><a href="#cb57-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1703"><a href="#cb57-1703" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test!generalized production function|(}</span>
<span id="cb57-1704"><a href="#cb57-1704" aria-hidden="true" tabindex="-1"></a>The Wald test is based on the unconstrained model and more precisely on $R \hat{\theta}_{nc} - q$. If H~0~ is true, this vector of length $J$ (here $J = 2$) should be close to 0. Using the central-limit theorem, the asymptotic distribution of $\hat{\theta}$ is normal, so that, under H~0~: $R \hat{\theta}_{nc} - q \overset{a}{\sim} \mathcal{N}(0, R\hat{V}_{\hat{\theta}}R^\top)$. The Wald statistic is obtained by computing the quadratic form of this vector using the inverse of its covariance matrix:</span>
<span id="cb57-1705"><a href="#cb57-1705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1706"><a href="#cb57-1706" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1707"><a href="#cb57-1707" aria-hidden="true" tabindex="-1"></a>W = (R \hat{\theta}_{nc} - q)^\top(R\hat{V}_{\hat{\theta}}R^\top) ^ {-1} (R \hat{\theta}_{nc} - q)</span>
<span id="cb57-1708"><a href="#cb57-1708" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1709"><a href="#cb57-1709" aria-hidden="true" tabindex="-1"></a>With the reparametrized model, denoting $\hat{V}_{\hat{\theta}_2}$ the subset of $\hat{V}_{\hat{\theta}}$ that concerns $\theta_2$, the Wald statistic simplifies to $W = \hat{\theta}_2 ^ \top\hat{V}_{\hat{\theta}_2}^{-1}\hat{\theta}_2$. We first extract the vector and the matrix used in the quadratic form:</span>
<span id="cb57-1710"><a href="#cb57-1710" aria-hidden="true" tabindex="-1"></a>\idxfun{matrix}{base}\idxfun{coef}{stats}\idxfun{vcov}{stats}\idxfun{drop}{base}</span>
<span id="cb57-1711"><a href="#cb57-1711" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}\idxfun{quad<span class="sc">\_</span>form}{micsr}</span>
<span id="cb57-1712"><a href="#cb57-1712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1715"><a href="#cb57-1715" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1716"><a href="#cb57-1716" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1717"><a href="#cb57-1717" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">6</span>)</span>
<span id="cb57-1718"><a href="#cb57-1718" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">1</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">&lt;-</span> R[<span class="dv">2</span>, <span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb57-1719"><a href="#cb57-1719" aria-hidden="true" tabindex="-1"></a>q <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb57-1720"><a href="#cb57-1720" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">drop</span>(R <span class="sc">%*%</span> <span class="fu">coef</span>(gpf2) <span class="sc">-</span> q)</span>
<span id="cb57-1721"><a href="#cb57-1721" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> R <span class="sc">%*%</span> <span class="fu">coef</span>(gpf2) <span class="sc">-</span> q</span>
<span id="cb57-1722"><a href="#cb57-1722" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> R <span class="sc">%*%</span> <span class="fu">vcov</span>(gpf2) <span class="sc">%*%</span> <span class="fu">t</span>(R)</span>
<span id="cb57-1723"><a href="#cb57-1723" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1724"><a href="#cb57-1724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1725"><a href="#cb57-1725" aria-hidden="true" tabindex="-1"></a>The quadratic form of the vector <span class="in">`d`</span> with the inverse of the matrix <span class="in">`V`</span> can be obtained using the basic tools provided by **R** for matrix algebra:</span>
<span id="cb57-1726"><a href="#cb57-1726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1729"><a href="#cb57-1729" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1730"><a href="#cb57-1730" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1731"><a href="#cb57-1731" aria-hidden="true" tabindex="-1"></a><span class="fu">drop</span>(<span class="fu">crossprod</span>(d, <span class="fu">solve</span>(V, d)))</span>
<span id="cb57-1732"><a href="#cb57-1732" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1733"><a href="#cb57-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1734"><a href="#cb57-1734" aria-hidden="true" tabindex="-1"></a>Note the use of <span class="in">`drop`</span>\idxfun{drop}{base} to get a scalar and not a $1 \times 1$ matrix. The <span class="in">`micsr::quad_form`</span>\idxfun{quad<span class="sc">\_</span>form}{micsr} function enables to compute more simply the quadratic form:</span>
<span id="cb57-1735"><a href="#cb57-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1738"><a href="#cb57-1738" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1739"><a href="#cb57-1739" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1740"><a href="#cb57-1740" aria-hidden="true" tabindex="-1"></a>wald_test <span class="ot">&lt;-</span> <span class="fu">quad_form</span>(d, V)</span>
<span id="cb57-1741"><a href="#cb57-1741" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1742"><a href="#cb57-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1743"><a href="#cb57-1743" aria-hidden="true" tabindex="-1"></a>A subset of the elements of the vector and the matrix can be used with the <span class="in">`subset`</span> argument:</span>
<span id="cb57-1744"><a href="#cb57-1744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1747"><a href="#cb57-1747" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1748"><a href="#cb57-1748" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1749"><a href="#cb57-1749" aria-hidden="true" tabindex="-1"></a>wald_test2 <span class="ot">&lt;-</span> <span class="fu">quad_form</span>(gpf, <span class="at">subset =</span> <span class="fu">c</span>(<span class="st">"log(m)"</span>, <span class="st">"lambda"</span>))</span>
<span id="cb57-1750"><a href="#cb57-1750" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(wald_test, wald_test2)</span>
<span id="cb57-1751"><a href="#cb57-1751" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1752"><a href="#cb57-1752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1753"><a href="#cb57-1753" aria-hidden="true" tabindex="-1"></a>which illustrates the equivalence of the two formulas for the Wald test using either the "raw" or the reparametrized model.</span>
<span id="cb57-1754"><a href="#cb57-1754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1755"><a href="#cb57-1755" aria-hidden="true" tabindex="-1"></a>We can also use the approximate formula that indicates that the Wald statistic is close to the sum of the squares of the corresponding t statistics if the correlation between the two parameters is not too high:</span>
<span id="cb57-1756"><a href="#cb57-1756" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}</span>
<span id="cb57-1757"><a href="#cb57-1757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1760"><a href="#cb57-1760" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1761"><a href="#cb57-1761" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1762"><a href="#cb57-1762" aria-hidden="true" tabindex="-1"></a>csgpf <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">summary</span>(gpf))</span>
<span id="cb57-1763"><a href="#cb57-1763" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> csgpf[<span class="st">"log(m)"</span>, <span class="dv">3</span>] ; t2 <span class="ot">&lt;-</span> csgpf[<span class="st">"lambda"</span>, <span class="dv">3</span>] ; t1 <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> t2 <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb57-1764"><a href="#cb57-1764" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1765"><a href="#cb57-1765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1766"><a href="#cb57-1766" aria-hidden="true" tabindex="-1"></a>The approximation is very bad, which should be explained by a high correlation between the two coefficients:</span>
<span id="cb57-1767"><a href="#cb57-1767" aria-hidden="true" tabindex="-1"></a>\idxfun{vcov}{stats}</span>
<span id="cb57-1768"><a href="#cb57-1768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1771"><a href="#cb57-1771" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1772"><a href="#cb57-1772" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1773"><a href="#cb57-1773" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> csgpf[<span class="st">"log(m)"</span>, <span class="dv">2</span>] ; s2 <span class="ot">&lt;-</span> csgpf[<span class="st">"lambda"</span>, <span class="dv">2</span>] ; </span>
<span id="cb57-1774"><a href="#cb57-1774" aria-hidden="true" tabindex="-1"></a>v12 <span class="ot">&lt;-</span> <span class="fu">vcov</span>(gpf)[<span class="st">"log(m)"</span>, <span class="st">"lambda"</span>] ; r12 <span class="ot">&lt;-</span> v12 <span class="sc">/</span> (s1 <span class="sc">*</span> s2)</span>
<span id="cb57-1775"><a href="#cb57-1775" aria-hidden="true" tabindex="-1"></a>r12</span>
<span id="cb57-1776"><a href="#cb57-1776" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1777"><a href="#cb57-1777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1778"><a href="#cb57-1778" aria-hidden="true" tabindex="-1"></a>"Correcting" this correlation using @eq-ellipse, we get the correct value of the statistic:</span>
<span id="cb57-1779"><a href="#cb57-1779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1782"><a href="#cb57-1782" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1783"><a href="#cb57-1783" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1784"><a href="#cb57-1784" aria-hidden="true" tabindex="-1"></a>(t1 <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> t2 <span class="sc">^</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> r12 <span class="sc">*</span> t1 <span class="sc">*</span> t2) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> r12 <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb57-1785"><a href="#cb57-1785" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1786"><a href="#cb57-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1787"><a href="#cb57-1787" aria-hidden="true" tabindex="-1"></a>The statistic can also easily obtained using the <span class="in">`car::linearHypothesis`</span> function described in @sec-wald_test_example:</span>
<span id="cb57-1788"><a href="#cb57-1788" aria-hidden="true" tabindex="-1"></a>\idxfun{linearHypothesis}{car}\idxfun{gaze}{micsr}</span>
<span id="cb57-1789"><a href="#cb57-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1792"><a href="#cb57-1792" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1793"><a href="#cb57-1793" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1794"><a href="#cb57-1794" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">linearHypothesis</span>(gpf, <span class="fu">c</span>(<span class="st">"log(m) = 0"</span>, <span class="st">"lambda = 0"</span>)) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb57-1795"><a href="#cb57-1795" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">linearHypothesis</span>(gpf2, <span class="fu">c</span>(<span class="st">"log(l) + log(k) + log(m) = 1"</span>, </span>
<span id="cb57-1796"><a href="#cb57-1796" aria-hidden="true" tabindex="-1"></a>                              <span class="st">"lambda = 0"</span>)) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb57-1797"><a href="#cb57-1797" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1798"><a href="#cb57-1798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1799"><a href="#cb57-1799" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test!generalized production function|)}</span>
<span id="cb57-1800"><a href="#cb57-1800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1801"><a href="#cb57-1801" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ##### Likelihood ratio test --&gt;</span></span>
<span id="cb57-1802"><a href="#cb57-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1803"><a href="#cb57-1803" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test!generalized production function|(}</span>
<span id="cb57-1804"><a href="#cb57-1804" aria-hidden="true" tabindex="-1"></a>The likelihood ratio statistic is very easy to compute if the two</span>
<span id="cb57-1805"><a href="#cb57-1805" aria-hidden="true" tabindex="-1"></a>models have been estimated as it is simply twice the difference of the</span>
<span id="cb57-1806"><a href="#cb57-1806" aria-hidden="true" tabindex="-1"></a>log-likelihood of the two models. The coefficients of the constrained</span>
<span id="cb57-1807"><a href="#cb57-1807" aria-hidden="true" tabindex="-1"></a>model can be obtained by least squares using the reparametrized version of the Cobb-Douglas (@eq-cobb_douglas_production_reparam) and imposing $\beta_m = 0$. Using <span class="in">`micsr::loglm`</span>, we get:</span>
<span id="cb57-1808"><a href="#cb57-1808" aria-hidden="true" tabindex="-1"></a>\idxfun{offset}{stats}\idxfun{logLik}{stats}\idxfun{loglm}{micsr}\idxfun{as.numeric}{base}</span>
<span id="cb57-1809"><a href="#cb57-1809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1810"><a href="#cb57-1810" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1811"><a href="#cb57-1811" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1812"><a href="#cb57-1812" aria-hidden="true" tabindex="-1"></a>crs <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> <span class="fu">log</span>(k <span class="sc">/</span> m) <span class="sc">+</span> <span class="fu">log</span>(l <span class="sc">/</span> m) <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(m)), aps)</span>
<span id="cb57-1813"><a href="#cb57-1813" aria-hidden="true" tabindex="-1"></a>lr_test <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">as.numeric</span>(<span class="fu">logLik</span>(gpf) <span class="sc">-</span> <span class="fu">logLik</span>(crs))</span>
<span id="cb57-1814"><a href="#cb57-1814" aria-hidden="true" tabindex="-1"></a>lr_test</span>
<span id="cb57-1815"><a href="#cb57-1815" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1816"><a href="#cb57-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1817"><a href="#cb57-1817" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test!generalized production function|)}</span>
<span id="cb57-1818"><a href="#cb57-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1819"><a href="#cb57-1819" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ##### Lagrange multiplier test --&gt;</span></span>
<span id="cb57-1820"><a href="#cb57-1820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1821"><a href="#cb57-1821" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test!generalized production function|(}</span>
<span id="cb57-1822"><a href="#cb57-1822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1823"><a href="#cb57-1823" aria-hidden="true" tabindex="-1"></a>The Lagrange multiplier is based on the gradient evaluated with the estimates of the constrained model: $g(\hat{\theta}_{nc})$, which should be closed to 0. Applying the central-limit theorem, this vector is normally distributed, with, under H~0~, a zero expectation and a variance equal to the information matrix, which can be estimated by minus the hessian: $g(\hat{\theta}_{nc}) \overset{a}{\sim} \mathcal{N}(0, - H_{nc})$. The statistic is then:</span>
<span id="cb57-1824"><a href="#cb57-1824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1825"><a href="#cb57-1825" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1826"><a href="#cb57-1826" aria-hidden="true" tabindex="-1"></a>LM = g(\hat{\theta}_{nc}) ^ \top (- H_{nc}) ^ {-1}g(\hat{\theta}_{nc})</span>
<span id="cb57-1827"><a href="#cb57-1827" aria-hidden="true" tabindex="-1"></a>$$ {#eq-lm_formula}</span>
<span id="cb57-1828"><a href="#cb57-1828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1829"><a href="#cb57-1829" aria-hidden="true" tabindex="-1"></a>For the reparametrized model, denoting $g_2$ and $H_2$ the parts of the gradient and of the hessian that concern $\theta_2$, we get $g_2^\top (-H_2) ^ {-1}g_2$.</span>
<span id="cb57-1830"><a href="#cb57-1830" aria-hidden="true" tabindex="-1"></a>We consider here the case where the model is  parametrized</span>
<span id="cb57-1831"><a href="#cb57-1831" aria-hidden="true" tabindex="-1"></a>in a way that the hypothesis simply states that a subset of the</span>
<span id="cb57-1832"><a href="#cb57-1832" aria-hidden="true" tabindex="-1"></a>coefficient $\theta_2 ^ \top  = (\beta_m, \lambda)$ is zero. For the constrained model,</span>
<span id="cb57-1833"><a href="#cb57-1833" aria-hidden="true" tabindex="-1"></a>$\theta_1^\top = (\alpha, \beta_l, \beta_k, \sigma)$ is estimated, so that elements of the gradient that are the</span>
<span id="cb57-1834"><a href="#cb57-1834" aria-hidden="true" tabindex="-1"></a>derivatives of the log-likelihood with respect to $\theta_1$ should be 0. The other</span>
<span id="cb57-1835"><a href="#cb57-1835" aria-hidden="true" tabindex="-1"></a>elements of the gradient are not 0, but should be close to 0 if the</span>
<span id="cb57-1836"><a href="#cb57-1836" aria-hidden="true" tabindex="-1"></a>hypotheses are true.  Using the <span class="in">`zellner_revankar`</span> function, we compute all the necessary information (the log-likelihood, the gradient and the hessian) for the constrained model, which corresponds to the coefficients of the <span class="in">`crs`</span> object fitted by least squares using <span class="in">`loglm`</span>:</span>
<span id="cb57-1837"><a href="#cb57-1837" aria-hidden="true" tabindex="-1"></a>\idxfun{zellner<span class="sc">\_</span>revankar}{micsr}\idxfun{coef}{stats}\idxfun{attr}{base}\idxfun{apply}{base}</span>
<span id="cb57-1838"><a href="#cb57-1838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1841"><a href="#cb57-1841" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1842"><a href="#cb57-1842" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1843"><a href="#cb57-1843" aria-hidden="true" tabindex="-1"></a>const_model <span class="ot">&lt;-</span> <span class="fu">zellner_revankar</span>(<span class="fu">c</span>(<span class="fu">coef</span>(crs)[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="st">"log(m)"</span> <span class="ot">=</span> <span class="dv">0</span>, </span>
<span id="cb57-1844"><a href="#cb57-1844" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lambda =</span> <span class="dv">0</span>, <span class="fu">coef</span>(crs)[<span class="dv">4</span>]), </span>
<span id="cb57-1845"><a href="#cb57-1845" aria-hidden="true" tabindex="-1"></a>                                <span class="at">y =</span> y, <span class="at">Z =</span> Z, <span class="at">sum =</span> <span class="cn">FALSE</span>)</span>
<span id="cb57-1846"><a href="#cb57-1846" aria-hidden="true" tabindex="-1"></a>G <span class="ot">&lt;-</span> <span class="fu">attr</span>(const_model, <span class="st">"gradient"</span>)</span>
<span id="cb57-1847"><a href="#cb57-1847" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">apply</span>(G, <span class="dv">2</span>, sum)</span>
<span id="cb57-1848"><a href="#cb57-1848" aria-hidden="true" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="fu">attr</span>(const_model, <span class="st">"hessian"</span>)</span>
<span id="cb57-1849"><a href="#cb57-1849" aria-hidden="true" tabindex="-1"></a>g</span>
<span id="cb57-1850"><a href="#cb57-1850" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1851"><a href="#cb57-1851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1852"><a href="#cb57-1852" aria-hidden="true" tabindex="-1"></a>Applying @eq-lm_formula, or using only the part of the gradient and of the inverse of the hessian that concerns $\theta_2$, we get:^<span class="co">[</span><span class="ot">Note the use of the `inv` argument set to `FALSE` in the second call to `quad_form` as the matrix provided (`H2m1`) is a subset of the inverse of the covariance matrix of the estimator.</span><span class="co">]</span></span>
<span id="cb57-1853"><a href="#cb57-1853" aria-hidden="true" tabindex="-1"></a>\idxfun{drop}{base}\idxfun{solve}{base}\idxfun{quad<span class="sc">\_</span>form}{micsr}</span>
<span id="cb57-1854"><a href="#cb57-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1857"><a href="#cb57-1857" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1858"><a href="#cb57-1858" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1859"><a href="#cb57-1859" aria-hidden="true" tabindex="-1"></a>lm_test_H <span class="ot">&lt;-</span> <span class="fu">quad_form</span>(g, <span class="sc">-</span> H)</span>
<span id="cb57-1860"><a href="#cb57-1860" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">&lt;-</span> g[<span class="fu">c</span>(<span class="st">"log(m)"</span>, <span class="st">"lambda"</span>)]</span>
<span id="cb57-1861"><a href="#cb57-1861" aria-hidden="true" tabindex="-1"></a>H2m1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(H)[<span class="fu">c</span>(<span class="st">"log(m)"</span>, <span class="st">"lambda"</span>), <span class="fu">c</span>(<span class="st">"log(m)"</span>, <span class="st">"lambda"</span>)]</span>
<span id="cb57-1862"><a href="#cb57-1862" aria-hidden="true" tabindex="-1"></a>lm_test_H2 <span class="ot">&lt;-</span> <span class="fu">quad_form</span>(g2, <span class="sc">-</span> H2m1, <span class="at">inv =</span> <span class="cn">FALSE</span>)</span>
<span id="cb57-1863"><a href="#cb57-1863" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(lm_test_H, lm_test_H2)</span>
<span id="cb57-1864"><a href="#cb57-1864" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1865"><a href="#cb57-1865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1866"><a href="#cb57-1866" aria-hidden="true" tabindex="-1"></a>Instead of using the opposite of the hessian, we could also have used the outer product of the gradient to estimate the information matrix:</span>
<span id="cb57-1867"><a href="#cb57-1867" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}\idxfun{solve}{base}\idxfun{drop}{base}\idxfun{quad<span class="sc">\_</span>form}{micsr}</span>
<span id="cb57-1868"><a href="#cb57-1868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1869"><a href="#cb57-1869" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1870"><a href="#cb57-1870" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1871"><a href="#cb57-1871" aria-hidden="true" tabindex="-1"></a>Ig <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(G)</span>
<span id="cb57-1872"><a href="#cb57-1872" aria-hidden="true" tabindex="-1"></a>lm_test_G <span class="ot">&lt;-</span> <span class="fu">quad_form</span>(g, Ig)</span>
<span id="cb57-1873"><a href="#cb57-1873" aria-hidden="true" tabindex="-1"></a>lm_test_G</span>
<span id="cb57-1874"><a href="#cb57-1874" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1875"><a href="#cb57-1875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1876"><a href="#cb57-1876" aria-hidden="true" tabindex="-1"></a>But in this case, the statistic can be more simply computed using the</span>
<span id="cb57-1877"><a href="#cb57-1877" aria-hidden="true" tabindex="-1"></a>results of a regression, because the statistic is:</span>
<span id="cb57-1878"><a href="#cb57-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1879"><a href="#cb57-1879" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1880"><a href="#cb57-1880" aria-hidden="true" tabindex="-1"></a>g^\top(G^\top G) ^ {-1}g</span>
<span id="cb57-1881"><a href="#cb57-1881" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1882"><a href="#cb57-1882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1883"><a href="#cb57-1883" aria-hidden="true" tabindex="-1"></a>The gradient $g$ being the columnwise sum of $G$, it can be written</span>
<span id="cb57-1884"><a href="#cb57-1884" aria-hidden="true" tabindex="-1"></a>as $g=G^\top j$, where $j$ is a vector of 1s of length $N$.</span>
<span id="cb57-1885"><a href="#cb57-1885" aria-hidden="true" tabindex="-1"></a>Therefore, the test statistic is also:</span>
<span id="cb57-1886"><a href="#cb57-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1887"><a href="#cb57-1887" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1888"><a href="#cb57-1888" aria-hidden="true" tabindex="-1"></a>j^\top G (G^\top G) ^ {-1} G^ \top j = j^\top P_G j</span>
<span id="cb57-1889"><a href="#cb57-1889" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-1890"><a href="#cb57-1890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1891"><a href="#cb57-1891" aria-hidden="true" tabindex="-1"></a>where $P_G$ is the projection matrix on the subspace defined by the columns of $G$.</span>
<span id="cb57-1892"><a href="#cb57-1892" aria-hidden="true" tabindex="-1"></a>Therefore, regressing $j$ on $G$, the fitted values  are $P_G j$ and the sum of squares of the fitted</span>
<span id="cb57-1893"><a href="#cb57-1893" aria-hidden="true" tabindex="-1"></a>values are $j ^ \top P_G j$, $P_G$ being idempotent. In a</span>
<span id="cb57-1894"><a href="#cb57-1894" aria-hidden="true" tabindex="-1"></a>regression without intercept, this is the explained sum of</span>
<span id="cb57-1895"><a href="#cb57-1895" aria-hidden="true" tabindex="-1"></a>squares. The total sum of squares being: $j ^ \top j = N$, the</span>
<span id="cb57-1896"><a href="#cb57-1896" aria-hidden="true" tabindex="-1"></a>(uncentered) R-squared is equal to $\frac{j ^ \top P_G j}{N}$</span>
<span id="cb57-1897"><a href="#cb57-1897" aria-hidden="true" tabindex="-1"></a>and the test statistic is therefore $N$ times the R-squared of a</span>
<span id="cb57-1898"><a href="#cb57-1898" aria-hidden="true" tabindex="-1"></a>regression on a vector of 1 on the column of the individual</span>
<span id="cb57-1899"><a href="#cb57-1899" aria-hidden="true" tabindex="-1"></a>contributions to the gradient.</span>
<span id="cb57-1900"><a href="#cb57-1900" aria-hidden="true" tabindex="-1"></a>\idxfun{length}{base}\idxfun{lm}{stats}\idxfun{fitted}{stats}\idxfun{rsq}{micsr}\idxfun{rep}{base}</span>
<span id="cb57-1901"><a href="#cb57-1901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1902"><a href="#cb57-1902" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-1903"><a href="#cb57-1903" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1904"><a href="#cb57-1904" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb57-1905"><a href="#cb57-1905" aria-hidden="true" tabindex="-1"></a>areg <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">rep</span>(<span class="dv">1</span>, N) <span class="sc">~</span> G <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb57-1906"><a href="#cb57-1906" aria-hidden="true" tabindex="-1"></a>lm_test_G <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">fitted</span>(areg) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb57-1907"><a href="#cb57-1907" aria-hidden="true" tabindex="-1"></a>lm_test_G2 <span class="ot">&lt;-</span> <span class="fu">rsq</span>(areg) <span class="sc">*</span> N</span>
<span id="cb57-1908"><a href="#cb57-1908" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(lm_test_G, lm_test_G2)</span>
<span id="cb57-1909"><a href="#cb57-1909" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1910"><a href="#cb57-1910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1911"><a href="#cb57-1911" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test!generalized production function|)}</span>
<span id="cb57-1912"><a href="#cb57-1912" aria-hidden="true" tabindex="-1"></a>To summarize the results of this section, the Wald test statistic is </span>
<span id="cb57-1913"><a href="#cb57-1913" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(wald_test, 2)`</span>, the likelihood-ratio test statistic is </span>
<span id="cb57-1914"><a href="#cb57-1914" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(lr_test, 2)`</span> and the score test statistic, when computed </span>
<span id="cb57-1915"><a href="#cb57-1915" aria-hidden="true" tabindex="-1"></a>using the hessian based</span>
<span id="cb57-1916"><a href="#cb57-1916" aria-hidden="true" tabindex="-1"></a>estimation of the information is <span class="in">`r round(lm_test_H, 2)`</span>. The values are</span>
<span id="cb57-1917"><a href="#cb57-1917" aria-hidden="true" tabindex="-1"></a>quite similar and the hypotheses are not rejected at the 5% level</span>
<span id="cb57-1918"><a href="#cb57-1918" aria-hidden="true" tabindex="-1"></a>(the critical value is <span class="in">`r round(qchisq(.95, df = 2), 2)`</span>) and are rejected at</span>
<span id="cb57-1919"><a href="#cb57-1919" aria-hidden="true" tabindex="-1"></a>the 10% level (the critical value is equal to <span class="in">`r round(qchisq(.90, df = 2), 2)`</span>) only</span>
<span id="cb57-1920"><a href="#cb57-1920" aria-hidden="true" tabindex="-1"></a>for the likelihood ratio test. On the contrary, the score test computed</span>
<span id="cb57-1921"><a href="#cb57-1921" aria-hidden="true" tabindex="-1"></a>using the gradient-based estimate of the information matrix has a much higher</span>
<span id="cb57-1922"><a href="#cb57-1922" aria-hidden="true" tabindex="-1"></a>value (<span class="in">`r round(lm_test_G, 2)`</span>) and leads to a rejection of the</span>
<span id="cb57-1923"><a href="#cb57-1923" aria-hidden="true" tabindex="-1"></a>hypothesis, even at the 1% level (the critical value is</span>
<span id="cb57-1924"><a href="#cb57-1924" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(qchisq(.99, df = 2), 2)`</span>).</span>
<span id="cb57-1925"><a href="#cb57-1925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1926"><a href="#cb57-1926" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ##### Pseudo R^2^ --&gt;</span></span>
<span id="cb57-1927"><a href="#cb57-1927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1928"><a href="#cb57-1928" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{coefficient of determination!pseudo-three tests principles|(}</span>
<span id="cb57-1929"><a href="#cb57-1929" aria-hidden="true" tabindex="-1"></a>To compute the pseudo R^2^, we should define the "null" or "intercept-only" model. In the generalized production function, it is a model with only two parameters, $\alpha$ and $\sigma$. Therefore, the $R$ matrix selects all the coefficients except these two and, using this matrix, we can compute the Wald statistic:</span>
<span id="cb57-1930"><a href="#cb57-1930" aria-hidden="true" tabindex="-1"></a>\idxfun{diag}{base}\idxfun{coef}{stats}\idxfun{vcov}{stats}\idxfun{drop}{base}</span>
<span id="cb57-1931"><a href="#cb57-1931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1934"><a href="#cb57-1934" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1935"><a href="#cb57-1935" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">6</span>)[<span class="sc">-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>), ]</span>
<span id="cb57-1936"><a href="#cb57-1936" aria-hidden="true" tabindex="-1"></a>W_0 <span class="ot">&lt;-</span> <span class="fu">drop</span>(<span class="fu">t</span>(R <span class="sc">%*%</span> <span class="fu">coef</span>(gpf2)) <span class="sc">%*%</span> <span class="fu">solve</span>(R <span class="sc">%*%</span> <span class="fu">vcov</span>(gpf2) <span class="sc">%*%</span> <span class="fu">t</span>(R)) <span class="sc">%*%</span> </span>
<span id="cb57-1937"><a href="#cb57-1937" aria-hidden="true" tabindex="-1"></a>              (R <span class="sc">%*%</span> <span class="fu">coef</span>(gpf2)))</span>
<span id="cb57-1938"><a href="#cb57-1938" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1939"><a href="#cb57-1939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1940"><a href="#cb57-1940" aria-hidden="true" tabindex="-1"></a>For the other two tests, the constrained model should be estimated. Actually, this is a log linear model with only an intercept, and the LM estimates of $\alpha$ and $\sigma ^ 2$ are simply the mean and the variance of $\ln y$. </span>
<span id="cb57-1941"><a href="#cb57-1941" aria-hidden="true" tabindex="-1"></a>\idxfun{zellner<span class="sc">\_</span>revankar}{micsr}\idxfun{apply}{base}\idxfun{attr}{base}</span>
<span id="cb57-1942"><a href="#cb57-1942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1945"><a href="#cb57-1945" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1946"><a href="#cb57-1946" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1947"><a href="#cb57-1947" aria-hidden="true" tabindex="-1"></a>alpha_c <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">log</span>(y))</span>
<span id="cb57-1948"><a href="#cb57-1948" aria-hidden="true" tabindex="-1"></a>sigma_c <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">log</span>(y) <span class="sc">-</span> alpha_c) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb57-1949"><a href="#cb57-1949" aria-hidden="true" tabindex="-1"></a>null_model <span class="ot">&lt;-</span> <span class="fu">zellner_revankar</span>(<span class="fu">c</span>(alpha_c, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, sigma_c), </span>
<span id="cb57-1950"><a href="#cb57-1950" aria-hidden="true" tabindex="-1"></a>                               <span class="at">y =</span> y, <span class="at">Z =</span> Z, <span class="at">repar =</span> <span class="cn">FALSE</span>)</span>
<span id="cb57-1951"><a href="#cb57-1951" aria-hidden="true" tabindex="-1"></a>lnL_c <span class="ot">&lt;-</span> <span class="fu">sum</span>(null_model)</span>
<span id="cb57-1952"><a href="#cb57-1952" aria-hidden="true" tabindex="-1"></a>G_c <span class="ot">&lt;-</span> <span class="fu">attr</span>(null_model, <span class="st">"gradient"</span>)</span>
<span id="cb57-1953"><a href="#cb57-1953" aria-hidden="true" tabindex="-1"></a>g_c <span class="ot">&lt;-</span> <span class="fu">apply</span>(G_c, <span class="dv">2</span>, sum)</span>
<span id="cb57-1954"><a href="#cb57-1954" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1955"><a href="#cb57-1955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1956"><a href="#cb57-1956" aria-hidden="true" tabindex="-1"></a>The likelihood ratio statistic is just twice the difference between the two values of the log-likelihood:</span>
<span id="cb57-1957"><a href="#cb57-1957" aria-hidden="true" tabindex="-1"></a>\idxfun{as.numeric}{base}\idxfun{logLik}{stats}</span>
<span id="cb57-1958"><a href="#cb57-1958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1961"><a href="#cb57-1961" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1962"><a href="#cb57-1962" aria-hidden="true" tabindex="-1"></a>LR_0 <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="dv">2</span> <span class="sc">*</span> (<span class="fu">logLik</span>(gpf2) <span class="sc">-</span> lnL_c))</span>
<span id="cb57-1963"><a href="#cb57-1963" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1964"><a href="#cb57-1964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1965"><a href="#cb57-1965" aria-hidden="true" tabindex="-1"></a>Finally, the Lagrange multiplier statistic can be obtained by considering $N$ times the $R^2$ of the regression of a vector of 1 on the matrix of the individual contributions to the gradient:</span>
<span id="cb57-1966"><a href="#cb57-1966" aria-hidden="true" tabindex="-1"></a>\idxfun{rsq}{micsr}\idxfun{lm}{stats}\idxfun{rep}{base}</span>
<span id="cb57-1967"><a href="#cb57-1967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1970"><a href="#cb57-1970" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1971"><a href="#cb57-1971" aria-hidden="true" tabindex="-1"></a>LM_0 <span class="ot">&lt;-</span> N <span class="sc">*</span> <span class="fu">rsq</span>(<span class="fu">lm</span>(<span class="fu">rep</span>(<span class="dv">1</span>, N) <span class="sc">~</span> G_c <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb57-1972"><a href="#cb57-1972" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1973"><a href="#cb57-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1974"><a href="#cb57-1974" aria-hidden="true" tabindex="-1"></a>We can then compute the three pseudo-R^2^:</span>
<span id="cb57-1975"><a href="#cb57-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1976"><a href="#cb57-1976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1979"><a href="#cb57-1979" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-1980"><a href="#cb57-1980" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-1981"><a href="#cb57-1981" aria-hidden="true" tabindex="-1"></a>R2_w <span class="ot">&lt;-</span> W_0 <span class="sc">/</span> (N <span class="sc">+</span> W_0)</span>
<span id="cb57-1982"><a href="#cb57-1982" aria-hidden="true" tabindex="-1"></a>R2_lr <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span> LR_0 <span class="sc">/</span> N)</span>
<span id="cb57-1983"><a href="#cb57-1983" aria-hidden="true" tabindex="-1"></a>R2_lm <span class="ot">&lt;-</span> LM_0 <span class="sc">/</span> N</span>
<span id="cb57-1984"><a href="#cb57-1984" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">wald =</span> R2_w, <span class="at">lik_ratio =</span> R2_lr, <span class="at">lagr_mult =</span> R2_lm) <span class="sc">%&gt;%</span> print</span>
<span id="cb57-1985"><a href="#cb57-1985" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-1986"><a href="#cb57-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1987"><a href="#cb57-1987" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-1988"><a href="#cb57-1988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1989"><a href="#cb57-1989" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{coefficient of determination!pseudo-three tests principles|)}</span>
<span id="cb57-1990"><a href="#cb57-1990" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!three test principles!maximum likelihood estimator|)}</span>
<span id="cb57-1991"><a href="#cb57-1991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1992"><a href="#cb57-1992" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditional moment test {#sec-ml_cond_moment_test}</span></span>
<span id="cb57-1993"><a href="#cb57-1993" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{conditional moment test|(}</span>
<span id="cb57-1994"><a href="#cb57-1994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-1995"><a href="#cb57-1995" aria-hidden="true" tabindex="-1"></a>Compared to the three classical tests, conditional moment tests don't</span>
<span id="cb57-1996"><a href="#cb57-1996" aria-hidden="true" tabindex="-1"></a>define two nested models (a "large" one and a "small" one). These</span>
<span id="cb57-1997"><a href="#cb57-1997" aria-hidden="true" tabindex="-1"></a>tests, first presented by @TAUC:85\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Tauchen} and @NEWE:85\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Newey} are based on moment conditions that should be 0 under H~0~. They</span>
<span id="cb57-1998"><a href="#cb57-1998" aria-hidden="true" tabindex="-1"></a>are particularly useful for models fitted by maximum likelihood,</span>
<span id="cb57-1999"><a href="#cb57-1999" aria-hidden="true" tabindex="-1"></a>although they can be used for models fitted by other estimation</span>
<span id="cb57-2000"><a href="#cb57-2000" aria-hidden="true" tabindex="-1"></a>methods. Consider the example where the distribution of the response</span>
<span id="cb57-2001"><a href="#cb57-2001" aria-hidden="true" tabindex="-1"></a>is related to the normal distribution. This is the case for the</span>
<span id="cb57-2002"><a href="#cb57-2002" aria-hidden="true" tabindex="-1"></a>generalized production function estimated in the previous section, and</span>
<span id="cb57-2003"><a href="#cb57-2003" aria-hidden="true" tabindex="-1"></a>it is also the case for the probit and the tobit model that will be</span>
<span id="cb57-2004"><a href="#cb57-2004" aria-hidden="true" tabindex="-1"></a>developed in the last part of the book. With the OLS estimator, the most important properties of the estimator,</span>
<span id="cb57-2005"><a href="#cb57-2005" aria-hidden="true" tabindex="-1"></a>especially unbiasedness and consistency only rely on the hypothesis</span>
<span id="cb57-2006"><a href="#cb57-2006" aria-hidden="true" tabindex="-1"></a>that the conditional expectation of the response is correctly</span>
<span id="cb57-2007"><a href="#cb57-2007" aria-hidden="true" tabindex="-1"></a>specified. This is not the case for models fitted by ML. Therefore, if the conditional distribution of the response</span>
<span id="cb57-2008"><a href="#cb57-2008" aria-hidden="true" tabindex="-1"></a>is not normal with a constant conditional variance, the estimator may</span>
<span id="cb57-2009"><a href="#cb57-2009" aria-hidden="true" tabindex="-1"></a>be inconsistent. Testing the hypothesis of normality and of</span>
<span id="cb57-2010"><a href="#cb57-2010" aria-hidden="true" tabindex="-1"></a>homoskedasticity is therefore crucial in this context.</span>
<span id="cb57-2011"><a href="#cb57-2011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2012"><a href="#cb57-2012" aria-hidden="true" tabindex="-1"></a>Denote $\mu_n = \mu(\theta, w_n)$ a vector of length $J$ for observation $n$, that</span>
<span id="cb57-2013"><a href="#cb57-2013" aria-hidden="true" tabindex="-1"></a>depends on a vector of parameters ($\theta$) and on a vector of</span>
<span id="cb57-2014"><a href="#cb57-2014" aria-hidden="true" tabindex="-1"></a>variables ($w_n$, which typically contains the response and a vector</span>
<span id="cb57-2015"><a href="#cb57-2015" aria-hidden="true" tabindex="-1"></a>of covariate). The hypothesis is that $\mbox{E}(\mu_n) = 0$.</span>
<span id="cb57-2016"><a href="#cb57-2016" aria-hidden="true" tabindex="-1"></a>For example, to test normality, the hypothesis will be that the third</span>
<span id="cb57-2017"><a href="#cb57-2017" aria-hidden="true" tabindex="-1"></a>moment of the errors is zero and that the fourth (standardized) moment</span>
<span id="cb57-2018"><a href="#cb57-2018" aria-hidden="true" tabindex="-1"></a>is three and therefore: $\mu_n^\top = (\epsilon_n ^ 3, \epsilon_n ^</span>
<span id="cb57-2019"><a href="#cb57-2019" aria-hidden="true" tabindex="-1"></a>4 - 3 \sigma_\epsilon ^4)$.</span>
<span id="cb57-2020"><a href="#cb57-2020" aria-hidden="true" tabindex="-1"></a>Denote $m(\theta, W) = \sum_{n=1} ^ N \mu(\theta, w_n)$.  The test is</span>
<span id="cb57-2021"><a href="#cb57-2021" aria-hidden="true" tabindex="-1"></a>based on the sample equivalent of the moment conditions, which is:</span>
<span id="cb57-2022"><a href="#cb57-2022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2023"><a href="#cb57-2023" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2024"><a href="#cb57-2024" aria-hidden="true" tabindex="-1"></a>\hat{\tau} = m(\hat{\theta}, W) / N = \frac{1}{N} \sum_{n=1} ^ N </span>
<span id="cb57-2025"><a href="#cb57-2025" aria-hidden="true" tabindex="-1"></a>\mu(\hat{\theta}, w_n) = \frac{1}{N} \sum_{n=1} ^ N \hat{\mu}_n</span>
<span id="cb57-2026"><a href="#cb57-2026" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2027"><a href="#cb57-2027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2028"><a href="#cb57-2028" aria-hidden="true" tabindex="-1"></a>and the hypotheses won't be rejected if $\hat{\tau}$  is</span>
<span id="cb57-2029"><a href="#cb57-2029" aria-hidden="true" tabindex="-1"></a>sufficiently close to a vector of 0. The derivation of its variance is</span>
<span id="cb57-2030"><a href="#cb57-2030" aria-hidden="true" tabindex="-1"></a>quite complicated because there are two sources of stochastic</span>
<span id="cb57-2031"><a href="#cb57-2031" aria-hidden="true" tabindex="-1"></a>variations, as both $\hat{\theta}$ and $\hat{\mu}_n$ are</span>
<span id="cb57-2032"><a href="#cb57-2032" aria-hidden="true" tabindex="-1"></a>random.^<span class="co">[</span><span class="ot">@CAME:TRIV:05, page 260.\index[author]{Cameron}\index[author]{Trivedi}</span><span class="co">]</span> Using a first-order Taylor expansion around the true value $\theta_0$, we have:</span>
<span id="cb57-2033"><a href="#cb57-2033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2034"><a href="#cb57-2034" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2035"><a href="#cb57-2035" aria-hidden="true" tabindex="-1"></a>\hat{\tau} =  \frac{1}{N}m(\theta_0, Z) + \frac{1}{N}\frac{\partial</span>
<span id="cb57-2036"><a href="#cb57-2036" aria-hidden="true" tabindex="-1"></a>m}{\partial \theta^\top}(\bar{\theta}, Z)</span>
<span id="cb57-2037"><a href="#cb57-2037" aria-hidden="true" tabindex="-1"></a>(\hat{\theta} - \theta_0) </span>
<span id="cb57-2038"><a href="#cb57-2038" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2039"><a href="#cb57-2039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2040"><a href="#cb57-2040" aria-hidden="true" tabindex="-1"></a>Denote: $\mathcal{W} = \displaystyle\lim_{N\rightarrow \infty}</span>
<span id="cb57-2041"><a href="#cb57-2041" aria-hidden="true" tabindex="-1"></a>\frac{\partial m(\theta, Z)}{\partial \theta^\top} =</span>
<span id="cb57-2042"><a href="#cb57-2042" aria-hidden="true" tabindex="-1"></a>\displaystyle\lim_{N\rightarrow \infty} \frac{1}{N} \sum_{n=1} ^ N\frac{\partial</span>
<span id="cb57-2043"><a href="#cb57-2043" aria-hidden="true" tabindex="-1"></a>\mu(\theta, z_n)}{\partial \theta^\top}$. As the estimator is</span>
<span id="cb57-2044"><a href="#cb57-2044" aria-hidden="true" tabindex="-1"></a>consistent, we have:</span>
<span id="cb57-2045"><a href="#cb57-2045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2046"><a href="#cb57-2046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2047"><a href="#cb57-2047" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2048"><a href="#cb57-2048" aria-hidden="true" tabindex="-1"></a>\hat{\tau} \overset{a}{=} \frac{1}{N}m(\theta_0, Z) + \mathcal{W}</span>
<span id="cb57-2049"><a href="#cb57-2049" aria-hidden="true" tabindex="-1"></a>(\hat{\theta} - \theta_0) </span>
<span id="cb57-2050"><a href="#cb57-2050" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2051"><a href="#cb57-2051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2052"><a href="#cb57-2052" aria-hidden="true" tabindex="-1"></a>Using @eq-nondeg_theta:</span>
<span id="cb57-2053"><a href="#cb57-2053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2054"><a href="#cb57-2054" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2055"><a href="#cb57-2055" aria-hidden="true" tabindex="-1"></a>\hat{\tau} \overset{a}{=} </span>
<span id="cb57-2056"><a href="#cb57-2056" aria-hidden="true" tabindex="-1"></a>\frac{m(\theta_0, Z)}{N} - \mathcal{W}\mathcal{H}^{-1}\frac{g(\theta_0, Z)}{N}=</span>
<span id="cb57-2057"><a href="#cb57-2057" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sum_{n=1} ^ N \mu(\theta_0, z_n) -</span>
<span id="cb57-2058"><a href="#cb57-2058" aria-hidden="true" tabindex="-1"></a>\mathcal{W}\mathcal{H}^{-1}\frac{1}{N} \sum_{n=1}^N \gamma(\theta_0, Z)</span>
<span id="cb57-2059"><a href="#cb57-2059" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2060"><a href="#cb57-2060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2061"><a href="#cb57-2061" aria-hidden="true" tabindex="-1"></a>Or:</span>
<span id="cb57-2062"><a href="#cb57-2062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2063"><a href="#cb57-2063" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2064"><a href="#cb57-2064" aria-hidden="true" tabindex="-1"></a>\sqrt{N}\hat{\tau} \overset{a}{=} \left[I,</span>
<span id="cb57-2065"><a href="#cb57-2065" aria-hidden="true" tabindex="-1"></a>-\mathcal{W}\mathcal{H}^{-1}\right]</span>
<span id="cb57-2066"><a href="#cb57-2066" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{c}\frac{\sum_n \mu(\theta_0, z_n)}{\sqrt{N}} <span class="sc">\\</span></span>
<span id="cb57-2067"><a href="#cb57-2067" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n \gamma(\theta_0, z_n)}{\sqrt{N}}\end{array}\right)</span>
<span id="cb57-2068"><a href="#cb57-2068" aria-hidden="true" tabindex="-1"></a>$$ {#eq-tau_cmtest}</span>
<span id="cb57-2069"><a href="#cb57-2069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2070"><a href="#cb57-2070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2071"><a href="#cb57-2071" aria-hidden="true" tabindex="-1"></a>Define $V$ as the variance of the vector in parentheses in @eq-tau_cmtest, obtained by concatenating $g(\theta_0, Z)$ (the gradient for</span>
<span id="cb57-2072"><a href="#cb57-2072" aria-hidden="true" tabindex="-1"></a>the true value of the parameters) and $m(\theta_0, Z)$, both divided</span>
<span id="cb57-2073"><a href="#cb57-2073" aria-hidden="true" tabindex="-1"></a>$\sqrt{N}$. The probability limit of $V$ is:</span>
<span id="cb57-2074"><a href="#cb57-2074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2075"><a href="#cb57-2075" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2076"><a href="#cb57-2076" aria-hidden="true" tabindex="-1"></a>\mathcal{V} = \lim_{N\rightarrow \infty} \frac{1}{N}</span>
<span id="cb57-2077"><a href="#cb57-2077" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-2078"><a href="#cb57-2078" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-2079"><a href="#cb57-2079" aria-hidden="true" tabindex="-1"></a>\sum_n \mu_n \mu_n ^ {\top} &amp; \sum_n \mu_n \gamma_n ^ {\top} <span class="sc">\\</span></span>
<span id="cb57-2080"><a href="#cb57-2080" aria-hidden="true" tabindex="-1"></a>\sum_n \gamma_n \mu_n ^ {\top} &amp; \sum_n \gamma_n \gamma_n ^ {\top} <span class="sc">\\</span></span>
<span id="cb57-2081"><a href="#cb57-2081" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2082"><a href="#cb57-2082" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb57-2083"><a href="#cb57-2083" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2084"><a href="#cb57-2084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2085"><a href="#cb57-2085" aria-hidden="true" tabindex="-1"></a>The probability limit of the variance of $\sqrt{N} \hat{\tau}$ is</span>
<span id="cb57-2086"><a href="#cb57-2086" aria-hidden="true" tabindex="-1"></a>therefore:</span>
<span id="cb57-2087"><a href="#cb57-2087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2088"><a href="#cb57-2088" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2089"><a href="#cb57-2089" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\sqrt{N} \hat{\tau}) \overset{p}{\rightarrow} </span>
<span id="cb57-2090"><a href="#cb57-2090" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot">I, -\mathcal{W}\mathcal{H}^{-1}\right</span><span class="co">]</span></span>
<span id="cb57-2091"><a href="#cb57-2091" aria-hidden="true" tabindex="-1"></a>\mathcal{V} \left[I,</span>
<span id="cb57-2092"><a href="#cb57-2092" aria-hidden="true" tabindex="-1"></a>-\mathcal{W}\mathcal{H}^{-1}\right] ^ \top</span>
<span id="cb57-2093"><a href="#cb57-2093" aria-hidden="true" tabindex="-1"></a>$$ {#eq-var_sqrtN_tau}</span>
<span id="cb57-2094"><a href="#cb57-2094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2095"><a href="#cb57-2095" aria-hidden="true" tabindex="-1"></a>and $\mathcal{V}$ can be</span>
<span id="cb57-2096"><a href="#cb57-2096" aria-hidden="true" tabindex="-1"></a>consistently estimated by:</span>
<span id="cb57-2097"><a href="#cb57-2097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2098"><a href="#cb57-2098" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2099"><a href="#cb57-2099" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}</span>
<span id="cb57-2100"><a href="#cb57-2100" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-2101"><a href="#cb57-2101" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-2102"><a href="#cb57-2102" aria-hidden="true" tabindex="-1"></a>\hat{M}^\top \hat{M} &amp; \hat{M} ^ \top \hat{G} <span class="sc">\\</span></span>
<span id="cb57-2103"><a href="#cb57-2103" aria-hidden="true" tabindex="-1"></a>\hat{G}^\top \hat{M} &amp; \hat{G}^\top \hat{G}</span>
<span id="cb57-2104"><a href="#cb57-2104" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2105"><a href="#cb57-2105" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb57-2106"><a href="#cb57-2106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2107"><a href="#cb57-2107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2108"><a href="#cb57-2108" aria-hidden="true" tabindex="-1"></a>with $\hat{M}$ the $N \times J$ matrix containing the individual</span>
<span id="cb57-2109"><a href="#cb57-2109" aria-hidden="true" tabindex="-1"></a>contributions to $\hat{m}$ (with a $n$^th^ row equal to</span>
<span id="cb57-2110"><a href="#cb57-2110" aria-hidden="true" tabindex="-1"></a>$\hat{\mu}_n^\top$) and $\hat{G}$ the $N \times (K+1)$ matrix containing</span>
<span id="cb57-2111"><a href="#cb57-2111" aria-hidden="true" tabindex="-1"></a>the individual contributions to the gradient (with a $n$^th^ row equal</span>
<span id="cb57-2112"><a href="#cb57-2112" aria-hidden="true" tabindex="-1"></a>to $\hat{\gamma}_n^\top$).</span>
<span id="cb57-2113"><a href="#cb57-2113" aria-hidden="true" tabindex="-1"></a>Replacing $\mathcal{V}$ in @eq-var_sqrtN_tau, developing the quadratic form</span>
<span id="cb57-2114"><a href="#cb57-2114" aria-hidden="true" tabindex="-1"></a>and regrouping terms, we finally get:^<span class="co">[</span><span class="ot">@SKEE:VELL:99\index[author]{Skeels}\index[author]{Vella}, eq. 2.13.</span><span class="co">]</span></span>
<span id="cb57-2115"><a href="#cb57-2115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2116"><a href="#cb57-2116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2117"><a href="#cb57-2117" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\sqrt{N}\hat{\tau})</span>
<span id="cb57-2118"><a href="#cb57-2118" aria-hidden="true" tabindex="-1"></a>\overset{p}{\rightarrow} </span>
<span id="cb57-2119"><a href="#cb57-2119" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)^\top</span>
<span id="cb57-2120"><a href="#cb57-2120" aria-hidden="true" tabindex="-1"></a>\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W} ^ \top\right)</span>
<span id="cb57-2121"><a href="#cb57-2121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2122"><a href="#cb57-2122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2123"><a href="#cb57-2123" aria-hidden="true" tabindex="-1"></a>and the statistic is:</span>
<span id="cb57-2124"><a href="#cb57-2124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2125"><a href="#cb57-2125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2126"><a href="#cb57-2126" aria-hidden="true" tabindex="-1"></a>\hat{\tau} ^ \top \left[\frac{1}{N^2}\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)^\top</span>
<span id="cb57-2127"><a href="#cb57-2127" aria-hidden="true" tabindex="-1"></a>\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)\right] ^{-1}\hat{\tau}</span>
<span id="cb57-2128"><a href="#cb57-2128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2129"><a href="#cb57-2129" aria-hidden="true" tabindex="-1"></a>or, in terms of $\hat{m} = N \hat{\tau}$:</span>
<span id="cb57-2130"><a href="#cb57-2130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2131"><a href="#cb57-2131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2132"><a href="#cb57-2132" aria-hidden="true" tabindex="-1"></a>\hat{m} ^ \top\left[\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)^\top</span>
<span id="cb57-2133"><a href="#cb57-2133" aria-hidden="true" tabindex="-1"></a>\left(\hat{M} - \hat{G}\mathcal{H}^{-1}\mathcal{W}^ \top\right)\right]^{-1} \hat{m} </span>
<span id="cb57-2134"><a href="#cb57-2134" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cmtest_m}</span>
<span id="cb57-2135"><a href="#cb57-2135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2136"><a href="#cb57-2136" aria-hidden="true" tabindex="-1"></a>which is, under H~0~, a chi-squared with $J$ degrees of freedom.</span>
<span id="cb57-2137"><a href="#cb57-2137" aria-hidden="true" tabindex="-1"></a>Different flavors of the test are obtained using different estimators</span>
<span id="cb57-2138"><a href="#cb57-2138" aria-hidden="true" tabindex="-1"></a>of $\mathcal{W}$ and $\mathcal{H}$:</span>
<span id="cb57-2139"><a href="#cb57-2139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2140"><a href="#cb57-2140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the first uses the expected value of the estimators of</span>
<span id="cb57-2141"><a href="#cb57-2141" aria-hidden="true" tabindex="-1"></a>  $\mathcal{H}$ and $\mathcal{W}$ which are respectively:</span>
<span id="cb57-2142"><a href="#cb57-2142" aria-hidden="true" tabindex="-1"></a>  $\mbox{E} \frac{\partial \ln L}{\partial \theta \partial \theta ^</span>
<span id="cb57-2143"><a href="#cb57-2143" aria-hidden="true" tabindex="-1"></a>  \top}(\hat{\theta},Z) / N$ and</span>
<span id="cb57-2144"><a href="#cb57-2144" aria-hidden="true" tabindex="-1"></a>  $\mbox{E} \frac{\partial m(\hat{\theta}, Z)}{\partial</span>
<span id="cb57-2145"><a href="#cb57-2145" aria-hidden="true" tabindex="-1"></a>  \theta} / N$,</span>
<span id="cb57-2146"><a href="#cb57-2146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the second uses the same expressions without the expectation:</span>
<span id="cb57-2147"><a href="#cb57-2147" aria-hidden="true" tabindex="-1"></a>  $\frac{\partial \ln L}{\partial \theta \partial \theta ^</span>
<span id="cb57-2148"><a href="#cb57-2148" aria-hidden="true" tabindex="-1"></a>  \top}(\hat{\theta},Z)/N$ and</span>
<span id="cb57-2149"><a href="#cb57-2149" aria-hidden="true" tabindex="-1"></a>  $\frac{\partial m(\hat{\theta}, Z)}{\partial</span>
<span id="cb57-2150"><a href="#cb57-2150" aria-hidden="true" tabindex="-1"></a>  \theta} / N$,</span>
<span id="cb57-2151"><a href="#cb57-2151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the third uses the information equality to estimate $\mathcal{H}$</span>
<span id="cb57-2152"><a href="#cb57-2152" aria-hidden="true" tabindex="-1"></a>  by $- \hat{G}^\top \hat{G} / N$ and the generalized information</span>
<span id="cb57-2153"><a href="#cb57-2153" aria-hidden="true" tabindex="-1"></a>  equality to estimate $\mathcal{W}$ by $- \hat{G}^\top \hat{M}/N$.</span>
<span id="cb57-2154"><a href="#cb57-2154" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-2155"><a href="#cb57-2155" aria-hidden="true" tabindex="-1"></a>The last one is particularly convenient, as it only requires the</span>
<span id="cb57-2156"><a href="#cb57-2156" aria-hidden="true" tabindex="-1"></a>$\hat{G}$ matrix of the contributions to the gradient and the</span>
<span id="cb57-2157"><a href="#cb57-2157" aria-hidden="true" tabindex="-1"></a>$\hat{M}$ matrix containing the contributions to the empirical moment</span>
<span id="cb57-2158"><a href="#cb57-2158" aria-hidden="true" tabindex="-1"></a>vector. Rearranging terms and using the fact that $\hat{m} = \hat{M} j$ with $j$ a vector of 1s, @eq-cmtest_m becomes:</span>
<span id="cb57-2159"><a href="#cb57-2159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2160"><a href="#cb57-2160" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2161"><a href="#cb57-2161" aria-hidden="true" tabindex="-1"></a>j^\top \hat{M} \left[M^\top\left(I - \hat{G}(\hat{G}^\top \hat{G})^{-1}</span>
<span id="cb57-2162"><a href="#cb57-2162" aria-hidden="true" tabindex="-1"></a>\hat{G}^\top\right)\hat{M}\right] ^ {-1} \hat{M}^\top j</span>
<span id="cb57-2163"><a href="#cb57-2163" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-opg_cmtest}</span>
<span id="cb57-2164"><a href="#cb57-2164" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-2165"><a href="#cb57-2165" aria-hidden="true" tabindex="-1"></a>which is just the explained sum of squares of a regression of a vector</span>
<span id="cb57-2166"><a href="#cb57-2166" aria-hidden="true" tabindex="-1"></a>of 1s on $\hat{G}$ and $\hat{M}$. To see that, start with the</span>
<span id="cb57-2167"><a href="#cb57-2167" aria-hidden="true" tabindex="-1"></a>expression of the explained sum of squares which is, denoting $y$ the</span>
<span id="cb57-2168"><a href="#cb57-2168" aria-hidden="true" tabindex="-1"></a>response and $X$ the matrix of covariates: $y^\top X(X^\top</span>
<span id="cb57-2169"><a href="#cb57-2169" aria-hidden="true" tabindex="-1"></a>X)^{-1}X^\top y$. In our case, the response is $\iota$ and the matrix</span>
<span id="cb57-2170"><a href="#cb57-2170" aria-hidden="true" tabindex="-1"></a>of covariates $(\hat{G}\; \hat{M})$. Therefore, the explained sum of</span>
<span id="cb57-2171"><a href="#cb57-2171" aria-hidden="true" tabindex="-1"></a>squares is:</span>
<span id="cb57-2172"><a href="#cb57-2172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2173"><a href="#cb57-2173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2174"><a href="#cb57-2174" aria-hidden="true" tabindex="-1"></a>j^\top (\hat{G}\; \hat{M})</span>
<span id="cb57-2175"><a href="#cb57-2175" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-2176"><a href="#cb57-2176" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb57-2177"><a href="#cb57-2177" aria-hidden="true" tabindex="-1"></a>\hat{G}^\top \hat{G} &amp; \hat{G}^\top \hat{M}<span class="sc">\\</span></span>
<span id="cb57-2178"><a href="#cb57-2178" aria-hidden="true" tabindex="-1"></a>\hat{M}^\top \hat{G} &amp; \hat{M}^\top \hat{M}</span>
<span id="cb57-2179"><a href="#cb57-2179" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2180"><a href="#cb57-2180" aria-hidden="true" tabindex="-1"></a>\right)  ^ {-1}</span>
<span id="cb57-2181"><a href="#cb57-2181" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb57-2182"><a href="#cb57-2182" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb57-2183"><a href="#cb57-2183" aria-hidden="true" tabindex="-1"></a>\hat{G}^\top <span class="sc">\\</span></span>
<span id="cb57-2184"><a href="#cb57-2184" aria-hidden="true" tabindex="-1"></a>\hat{M}^\top</span>
<span id="cb57-2185"><a href="#cb57-2185" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2186"><a href="#cb57-2186" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb57-2187"><a href="#cb57-2187" aria-hidden="true" tabindex="-1"></a>j</span>
<span id="cb57-2188"><a href="#cb57-2188" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-ess_cmtest}</span>
<span id="cb57-2189"><a href="#cb57-2189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2190"><a href="#cb57-2190" aria-hidden="true" tabindex="-1"></a>But all the columns of $\hat{G}$ sum to 0 ($\hat{G}^\top j=0$),</span>
<span id="cb57-2191"><a href="#cb57-2191" aria-hidden="true" tabindex="-1"></a>so that @eq-ess_cmtest reduces $j ^ \top \hat{M} C \hat{M} ^ \top j$, where $C$ is the lower right square matrix in the formula of</span>
<span id="cb57-2192"><a href="#cb57-2192" aria-hidden="true" tabindex="-1"></a>the partitioned inverse of the matrix in @eq-ess_cmtest and is the matrix in bracket</span>
<span id="cb57-2193"><a href="#cb57-2193" aria-hidden="true" tabindex="-1"></a>in @eq-opg_cmtest (see for example @GREE:03\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Greene}, equation A-74, page 824).\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{inverse of a partitioned matrix}</span>
<span id="cb57-2194"><a href="#cb57-2194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2195"><a href="#cb57-2195" aria-hidden="true" tabindex="-1"></a>The conditional moment test can be used to test the hypothesis of normality, homoskedasticity and omitted variables. For the normality hypothesis,  the theoretical moments are $\mbox{E}(\epsilon_n ^ 3 \mid x_n) = 0$ and $\mbox{E}(\epsilon_n ^ 4 - 3 \sigma ^ 2 \mid x_n) = 0$ and the empirical counterparts are:</span>
<span id="cb57-2196"><a href="#cb57-2196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2197"><a href="#cb57-2197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2198"><a href="#cb57-2198" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb57-2199"><a href="#cb57-2199" aria-hidden="true" tabindex="-1"></a>\begin{array}{l}</span>
<span id="cb57-2200"><a href="#cb57-2200" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sum_{n} \hat{\epsilon}_n ^ 3 <span class="sc">\\</span></span>
<span id="cb57-2201"><a href="#cb57-2201" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sum_{n} \hat{\epsilon}_n ^ 4 - 3 \hat{\sigma} ^ 2<span class="sc">\\</span></span>
<span id="cb57-2202"><a href="#cb57-2202" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2203"><a href="#cb57-2203" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb57-2204"><a href="#cb57-2204" aria-hidden="true" tabindex="-1"></a>$$ {#eq-emp_moments_normal}</span>
<span id="cb57-2205"><a href="#cb57-2205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2206"><a href="#cb57-2206" aria-hidden="true" tabindex="-1"></a>For the homoskedasticity hypothesis, the theoretical moments are $\mbox{E}\left(w_n(\epsilon_n ^ 2 - \sigma ^ 2) | x_n\right)=0$, where, under the alternative hypothesis, $w$ are variables that enter the skedastic function and the empirical counterparts are:</span>
<span id="cb57-2207"><a href="#cb57-2207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2208"><a href="#cb57-2208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2209"><a href="#cb57-2209" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sum_{n} w_n(\hat{\epsilon}_n ^2 -\hat{\sigma} ^ 2)</span>
<span id="cb57-2210"><a href="#cb57-2210" aria-hidden="true" tabindex="-1"></a>$$ {#eq-emp_moments_homosc}</span>
<span id="cb57-2211"><a href="#cb57-2211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2212"><a href="#cb57-2212" aria-hidden="true" tabindex="-1"></a>For the omitted variables test,  the theoretical moments are $\mbox{E}(w_n\hat{\epsilon}_n | x_n)=0$ and the empirical counterparts are: </span>
<span id="cb57-2213"><a href="#cb57-2213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2214"><a href="#cb57-2214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2215"><a href="#cb57-2215" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\sum_{n} w_n\hat{\epsilon}_n</span>
<span id="cb57-2216"><a href="#cb57-2216" aria-hidden="true" tabindex="-1"></a>$$ {#eq-emp_moments_omit_var}</span>
<span id="cb57-2217"><a href="#cb57-2217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2218"><a href="#cb57-2218" aria-hidden="true" tabindex="-1"></a>As an example, we test the normality hypothesis in the context of the</span>
<span id="cb57-2219"><a href="#cb57-2219" aria-hidden="true" tabindex="-1"></a>generalized production function previously estimated:</span>
<span id="cb57-2220"><a href="#cb57-2220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2221"><a href="#cb57-2221" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2222"><a href="#cb57-2222" aria-hidden="true" tabindex="-1"></a>\epsilon = \ln y + \theta \ln y - \left(\alpha + \sum_{j=1} ^ {J-1} \beta_j</span>
<span id="cb57-2223"><a href="#cb57-2223" aria-hidden="true" tabindex="-1"></a>\ln z_{j} ^ * + \beta_J^*\ln z_J + \ln z_J\right) \sim \mathcal{N} (0, \sigma^2)</span>
<span id="cb57-2224"><a href="#cb57-2224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2225"><a href="#cb57-2225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2226"><a href="#cb57-2226" aria-hidden="true" tabindex="-1"></a>We first extract the fitted coefficients of the model <span class="in">`htheta`</span>, we</span>
<span id="cb57-2227"><a href="#cb57-2227" aria-hidden="true" tabindex="-1"></a>matrix of the individual contribution to the gradient <span class="in">`G`</span> and the</span>
<span id="cb57-2228"><a href="#cb57-2228" aria-hidden="true" tabindex="-1"></a>hessian <span class="in">`H`</span>:</span>
<span id="cb57-2229"><a href="#cb57-2229" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-2230"><a href="#cb57-2230" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{zellner<span class="sc">\_</span>revankar}{micsr}\idxfun{attr}{base}</span>
<span id="cb57-2231"><a href="#cb57-2231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2232"><a href="#cb57-2232" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2233"><a href="#cb57-2233" aria-hidden="true" tabindex="-1"></a>htheta <span class="ot">&lt;-</span> <span class="fu">coef</span>(gpf)</span>
<span id="cb57-2234"><a href="#cb57-2234" aria-hidden="true" tabindex="-1"></a>lnlest <span class="ot">&lt;-</span> <span class="fu">zellner_revankar</span>(htheta, <span class="at">y =</span> y, <span class="at">Z =</span> Z)</span>
<span id="cb57-2235"><a href="#cb57-2235" aria-hidden="true" tabindex="-1"></a>G <span class="ot">&lt;-</span> <span class="fu">attr</span>(lnlest, <span class="st">"gradient"</span>)</span>
<span id="cb57-2236"><a href="#cb57-2236" aria-hidden="true" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="fu">attr</span>(lnlest, <span class="st">"hessian"</span>)</span>
<span id="cb57-2237"><a href="#cb57-2237" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2238"><a href="#cb57-2238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2239"><a href="#cb57-2239" aria-hidden="true" tabindex="-1"></a>Working on the reparametrized version of the model, we then transform</span>
<span id="cb57-2240"><a href="#cb57-2240" aria-hidden="true" tabindex="-1"></a>the matrix of covariates, compute $\epsilon$ the</span>
<span id="cb57-2241"><a href="#cb57-2241" aria-hidden="true" tabindex="-1"></a>matrix of the individual contribution to the moments $M$ and the</span>
<span id="cb57-2242"><a href="#cb57-2242" aria-hidden="true" tabindex="-1"></a>moment conditions $m$ which is a vector containing the sum of the columns of $M$.</span>
<span id="cb57-2243"><a href="#cb57-2243" aria-hidden="true" tabindex="-1"></a>\idxfun{drop}{base}\idxfun{cbind}{base}\idxfun{apply}{base}</span>
<span id="cb57-2244"><a href="#cb57-2244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2245"><a href="#cb57-2245" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2246"><a href="#cb57-2246" aria-hidden="true" tabindex="-1"></a>Zm <span class="ot">&lt;-</span> Z</span>
<span id="cb57-2247"><a href="#cb57-2247" aria-hidden="true" tabindex="-1"></a>Zm[, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>] <span class="ot">&lt;-</span> Z[, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>] <span class="sc">-</span> Z[, <span class="dv">4</span>]</span>
<span id="cb57-2248"><a href="#cb57-2248" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">drop</span>(Zm <span class="sc">%*%</span> htheta[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb57-2249"><a href="#cb57-2249" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> (<span class="fu">log</span>(y) <span class="sc">-</span> Z[, <span class="dv">4</span>] <span class="sc">+</span> htheta[<span class="st">"lambda"</span>] <span class="sc">*</span> y <span class="sc">-</span> mu)</span>
<span id="cb57-2250"><a href="#cb57-2250" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fu">cbind</span>(eps <span class="sc">^</span> <span class="dv">3</span>, eps <span class="sc">^</span> <span class="dv">4</span> <span class="sc">-</span> <span class="dv">3</span> <span class="sc">*</span> htheta[<span class="st">"sigma"</span>] <span class="sc">^</span> <span class="dv">4</span>)</span>
<span id="cb57-2251"><a href="#cb57-2251" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> M <span class="sc">%&gt;%</span> <span class="fu">apply</span>(<span class="dv">2</span>, sum)</span>
<span id="cb57-2252"><a href="#cb57-2252" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2253"><a href="#cb57-2253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2254"><a href="#cb57-2254" aria-hidden="true" tabindex="-1"></a>Then, the derivatives of $m$ with respect to the parameters of the model are computed to obtain the $W$ matrix.</span>
<span id="cb57-2255"><a href="#cb57-2255" aria-hidden="true" tabindex="-1"></a>\idxfun{matrix}{base}\idxfun{apply}{base}</span>
<span id="cb57-2256"><a href="#cb57-2256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2257"><a href="#cb57-2257" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2258"><a href="#cb57-2258" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span> <span class="sc">*</span> <span class="fu">apply</span>(eps <span class="sc">^</span> <span class="dv">2</span> <span class="sc">*</span> Z, <span class="dv">2</span>, sum),</span>
<span id="cb57-2259"><a href="#cb57-2259" aria-hidden="true" tabindex="-1"></a>              <span class="dv">3</span> <span class="sc">*</span> <span class="fu">sum</span>(eps <span class="sc">^</span> <span class="dv">2</span> <span class="sc">*</span> y), <span class="dv">0</span>,</span>
<span id="cb57-2260"><a href="#cb57-2260" aria-hidden="true" tabindex="-1"></a>              <span class="sc">-</span><span class="dv">4</span> <span class="sc">*</span> <span class="fu">apply</span>(eps <span class="sc">^</span> <span class="dv">3</span> <span class="sc">*</span> Z, <span class="dv">2</span>, sum),</span>
<span id="cb57-2261"><a href="#cb57-2261" aria-hidden="true" tabindex="-1"></a>              <span class="dv">4</span> <span class="sc">*</span> <span class="fu">sum</span>(eps <span class="sc">^</span> <span class="dv">3</span> <span class="sc">*</span> y),</span>
<span id="cb57-2262"><a href="#cb57-2262" aria-hidden="true" tabindex="-1"></a>              <span class="sc">-</span> <span class="dv">12</span> <span class="sc">*</span> htheta[<span class="st">"sigma"</span>] <span class="sc">^</span> <span class="dv">3</span>),</span>
<span id="cb57-2263"><a href="#cb57-2263" aria-hidden="true" tabindex="-1"></a>            <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb57-2264"><a href="#cb57-2264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2265"><a href="#cb57-2265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2266"><a href="#cb57-2266" aria-hidden="true" tabindex="-1"></a>We first compute the test using minus the hessian to estimate the</span>
<span id="cb57-2267"><a href="#cb57-2267" aria-hidden="true" tabindex="-1"></a>information and the matrix of the analytical derivatives of $m$ just</span>
<span id="cb57-2268"><a href="#cb57-2268" aria-hidden="true" tabindex="-1"></a>computed:</span>
<span id="cb57-2269"><a href="#cb57-2269" aria-hidden="true" tabindex="-1"></a>\idxfun{length}{base}\idxfun{crossprod}{base}\idxfun{drop}{base}\idxfun{solve}{base}</span>
<span id="cb57-2270"><a href="#cb57-2270" aria-hidden="true" tabindex="-1"></a>\idxfun{quad<span class="sc">\_</span>form}{micsr}</span>
<span id="cb57-2271"><a href="#cb57-2271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2272"><a href="#cb57-2272" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2273"><a href="#cb57-2273" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-2274"><a href="#cb57-2274" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb57-2275"><a href="#cb57-2275" aria-hidden="true" tabindex="-1"></a>Q1 <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(M <span class="sc">-</span> G <span class="sc">%*%</span> <span class="fu">solve</span>(H) <span class="sc">%*%</span> W)</span>
<span id="cb57-2276"><a href="#cb57-2276" aria-hidden="true" tabindex="-1"></a><span class="fu">quad_form</span>(m, Q1)</span>
<span id="cb57-2277"><a href="#cb57-2277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2278"><a href="#cb57-2278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2279"><a href="#cb57-2279" aria-hidden="true" tabindex="-1"></a>The statistic is <span class="in">`r round(quad_form(m, Q1), 2)`</span>, which</span>
<span id="cb57-2280"><a href="#cb57-2280" aria-hidden="true" tabindex="-1"></a>is far less than the critical value for a $\chi^2$ with 2 degrees of</span>
<span id="cb57-2281"><a href="#cb57-2281" aria-hidden="true" tabindex="-1"></a>freedom, even at the 10% level. The hypothesis of normality is</span>
<span id="cb57-2282"><a href="#cb57-2282" aria-hidden="true" tabindex="-1"></a>therefore not rejected.</span>
<span id="cb57-2283"><a href="#cb57-2283" aria-hidden="true" tabindex="-1"></a>We then compute the version of the test based on the OPG to</span>
<span id="cb57-2284"><a href="#cb57-2284" aria-hidden="true" tabindex="-1"></a>estimate the information matrix and on $G^\top M$ to estimate $W$. The</span>
<span id="cb57-2285"><a href="#cb57-2285" aria-hidden="true" tabindex="-1"></a>statistic can be computed by using matrix algebra or by taking the explained</span>
<span id="cb57-2286"><a href="#cb57-2286" aria-hidden="true" tabindex="-1"></a>sum of squares in a regression of a column of 1 on $G$ and $M$:</span>
<span id="cb57-2287"><a href="#cb57-2287" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}\idxfun{solve}{base}\idxfun{drop}{base}\idxfun{rep}{base}\idxfun{rsq}{micsr}</span>
<span id="cb57-2288"><a href="#cb57-2288" aria-hidden="true" tabindex="-1"></a>\idxfun{quad<span class="sc">\_</span>form}{micsr}</span>
<span id="cb57-2289"><a href="#cb57-2289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2290"><a href="#cb57-2290" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2291"><a href="#cb57-2291" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-2292"><a href="#cb57-2292" aria-hidden="true" tabindex="-1"></a>Q2 <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(M <span class="sc">-</span> G <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">crossprod</span>(G)) <span class="sc">%*%</span></span>
<span id="cb57-2293"><a href="#cb57-2293" aria-hidden="true" tabindex="-1"></a>                <span class="fu">crossprod</span>(G, M))</span>
<span id="cb57-2294"><a href="#cb57-2294" aria-hidden="true" tabindex="-1"></a><span class="fu">quad_form</span>(m, Q2)</span>
<span id="cb57-2295"><a href="#cb57-2295" aria-hidden="true" tabindex="-1"></a>j <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, N)</span>
<span id="cb57-2296"><a href="#cb57-2296" aria-hidden="true" tabindex="-1"></a><span class="fu">rsq</span>(<span class="fu">lm</span>(j <span class="sc">~</span> G <span class="sc">+</span> M <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> N</span>
<span id="cb57-2297"><a href="#cb57-2297" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2298"><a href="#cb57-2298" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-2299"><a href="#cb57-2299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2300"><a href="#cb57-2300" aria-hidden="true" tabindex="-1"></a>Note that this second version of the test leads to a much higher value</span>
<span id="cb57-2301"><a href="#cb57-2301" aria-hidden="true" tabindex="-1"></a>of the statistic and a probability value of</span>
<span id="cb57-2302"><a href="#cb57-2302" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(pchisq(drop(crossprod(m, solve(Q2, m))), df= 2, lower.tail =FALSE), 3)`</span>.</span>
<span id="cb57-2303"><a href="#cb57-2303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2304"><a href="#cb57-2304" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{conditional moment test|)}</span>
<span id="cb57-2305"><a href="#cb57-2305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2306"><a href="#cb57-2306" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tests for non-nested models</span></span>
<span id="cb57-2307"><a href="#cb57-2307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2308"><a href="#cb57-2308" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Vuong test|(}</span>
<span id="cb57-2309"><a href="#cb57-2309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2310"><a href="#cb57-2310" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Vuong test</span></span>
<span id="cb57-2311"><a href="#cb57-2311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2312"><a href="#cb57-2312" aria-hidden="true" tabindex="-1"></a>@VUON:89\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Vuong} proposed a test for non-nested models. He considered two</span>
<span id="cb57-2313"><a href="#cb57-2313" aria-hidden="true" tabindex="-1"></a>competing models characterized by densities $f(y|z; \beta)$ and $g(y|z; \gamma)$. Denoting $h(y | z)$ the true conditional density, the distance of the first model to the true model is measured by the minimum Kullback-Leibler information criterion (**KLIC**):</span>
<span id="cb57-2314"><a href="#cb57-2314" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Kullback-Leibler information criterion}</span>
<span id="cb57-2315"><a href="#cb57-2315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2316"><a href="#cb57-2316" aria-hidden="true" tabindex="-1"></a>D_f = \mbox{E}^0\left<span class="co">[</span><span class="ot">\ln h(y\mid z)\right</span><span class="co">]</span> - \mbox{E}^0\left[\ln f(y\mid z;</span>
<span id="cb57-2317"><a href="#cb57-2317" aria-hidden="true" tabindex="-1"></a>\beta_*)\right]</span>
<span id="cb57-2318"><a href="#cb57-2318" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2319"><a href="#cb57-2319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2320"><a href="#cb57-2320" aria-hidden="true" tabindex="-1"></a>where $\mbox{E}^0$ is the expected value using the true joint</span>
<span id="cb57-2321"><a href="#cb57-2321" aria-hidden="true" tabindex="-1"></a>distribution of $(y, X)$ and $\beta_*$ is the pseudo-true value of</span>
<span id="cb57-2322"><a href="#cb57-2322" aria-hidden="true" tabindex="-1"></a>$\beta$.^[$\beta_*$ is called the pseudo-true value because $f$ may</span>
<span id="cb57-2323"><a href="#cb57-2323" aria-hidden="true" tabindex="-1"></a>be an incorrect model.] As the true model is unobserved, denoting</span>
<span id="cb57-2324"><a href="#cb57-2324" aria-hidden="true" tabindex="-1"></a>$\theta^\top = (\beta ^ \top, \gamma ^ \top)$, we consider the</span>
<span id="cb57-2325"><a href="#cb57-2325" aria-hidden="true" tabindex="-1"></a>difference of the KLIC distance to the true model of model $G_\gamma$</span>
<span id="cb57-2326"><a href="#cb57-2326" aria-hidden="true" tabindex="-1"></a>and model $F_\beta$:</span>
<span id="cb57-2327"><a href="#cb57-2327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2328"><a href="#cb57-2328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2329"><a href="#cb57-2329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2330"><a href="#cb57-2330" aria-hidden="true" tabindex="-1"></a>\Lambda(\theta) = D_g - D_f = \mbox{E}^0\left[\ln f(y\mid z;</span>
<span id="cb57-2331"><a href="#cb57-2331" aria-hidden="true" tabindex="-1"></a>\beta_*)\right]- \mbox{E}^0\left[\ln g(y\mid z; \gamma_*)\right] =</span>
<span id="cb57-2332"><a href="#cb57-2332" aria-hidden="true" tabindex="-1"></a>\mbox{E}^0\left[\ln \frac{f(y\mid z; \beta_*)}{g(y\mid z;</span>
<span id="cb57-2333"><a href="#cb57-2333" aria-hidden="true" tabindex="-1"></a>\gamma_*)}\right]</span>
<span id="cb57-2334"><a href="#cb57-2334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2335"><a href="#cb57-2335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2336"><a href="#cb57-2336" aria-hidden="true" tabindex="-1"></a>The null hypothesis is that the distances of the two models to the true</span>
<span id="cb57-2337"><a href="#cb57-2337" aria-hidden="true" tabindex="-1"></a>models are equal or, equivalently, that: $\Lambda=0$. The alternative</span>
<span id="cb57-2338"><a href="#cb57-2338" aria-hidden="true" tabindex="-1"></a>hypothesis is either $\Lambda&gt;0$, which means that the first model is</span>
<span id="cb57-2339"><a href="#cb57-2339" aria-hidden="true" tabindex="-1"></a>better than the second or $\Lambda&lt;0$, which means that the second model is better than the first. Denoting, for a given random sample of</span>
<span id="cb57-2340"><a href="#cb57-2340" aria-hidden="true" tabindex="-1"></a>size $N$, $\hat{\beta}$ and $\hat{\gamma}$ the maximum likelihood</span>
<span id="cb57-2341"><a href="#cb57-2341" aria-hidden="true" tabindex="-1"></a>estimators of the two models and $\ln L_f(\hat{\beta})$ and $\ln</span>
<span id="cb57-2342"><a href="#cb57-2342" aria-hidden="true" tabindex="-1"></a>L_g(\hat{\gamma})$ the corresponding values of the log-likelihood functions, $\Lambda$ can be</span>
<span id="cb57-2343"><a href="#cb57-2343" aria-hidden="true" tabindex="-1"></a>consistently estimated by:</span>
<span id="cb57-2344"><a href="#cb57-2344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2345"><a href="#cb57-2345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2346"><a href="#cb57-2346" aria-hidden="true" tabindex="-1"></a>\hat{\Lambda}_N = \frac{1}{N} \sum_{n = 1}^N \left(\ln f(y_n \mid</span>
<span id="cb57-2347"><a href="#cb57-2347" aria-hidden="true" tabindex="-1"></a>x_n, \hat{\beta}) - \ln g(y_n \mid x_n, \hat{\gamma})\right) =</span>
<span id="cb57-2348"><a href="#cb57-2348" aria-hidden="true" tabindex="-1"></a>\frac{1}{N} \left(\ln L_f(\hat{\beta}) - \ln L_g(\hat{\gamma})\right)</span>
<span id="cb57-2349"><a href="#cb57-2349" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2350"><a href="#cb57-2350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2351"><a href="#cb57-2351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2352"><a href="#cb57-2352" aria-hidden="true" tabindex="-1"></a>which is the likelihood ratio divided by the sample size. Note that</span>
<span id="cb57-2353"><a href="#cb57-2353" aria-hidden="true" tabindex="-1"></a>the statistic of the standard likelihood ratio test, suitable for</span>
<span id="cb57-2354"><a href="#cb57-2354" aria-hidden="true" tabindex="-1"></a>nested models is $2 \left(\ln L^f(\hat{\beta}) - \ln</span>
<span id="cb57-2355"><a href="#cb57-2355" aria-hidden="true" tabindex="-1"></a>L^g(\hat{\gamma})\right)$, which is $2 N \hat{\Lambda}_N$.</span>
<span id="cb57-2356"><a href="#cb57-2356" aria-hidden="true" tabindex="-1"></a>The variance of $\Lambda$ is:</span>
<span id="cb57-2357"><a href="#cb57-2357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2358"><a href="#cb57-2358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2359"><a href="#cb57-2359" aria-hidden="true" tabindex="-1"></a>\omega^2_* = \mbox{V}^o \left[\ln \frac{f(y \mid x; \beta_*)}{g(y</span>
<span id="cb57-2360"><a href="#cb57-2360" aria-hidden="true" tabindex="-1"></a>\mid x; \gamma_*)}\right]</span>
<span id="cb57-2361"><a href="#cb57-2361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2362"><a href="#cb57-2362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2363"><a href="#cb57-2363" aria-hidden="true" tabindex="-1"></a>which can be consistently estimated by:</span>
<span id="cb57-2364"><a href="#cb57-2364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2365"><a href="#cb57-2365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2366"><a href="#cb57-2366" aria-hidden="true" tabindex="-1"></a>\hat{\omega}_N^2 = \frac{1}{N} \sum_{n = 1} ^ N  \left(\ln f(y_n \mid</span>
<span id="cb57-2367"><a href="#cb57-2367" aria-hidden="true" tabindex="-1"></a>x_n, \hat{\beta}) - \ln g(y_n \mid x,_n \hat{\gamma})\right) ^ 2 -</span>
<span id="cb57-2368"><a href="#cb57-2368" aria-hidden="true" tabindex="-1"></a>\hat{\Lambda}_N ^ 2</span>
<span id="cb57-2369"><a href="#cb57-2369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2370"><a href="#cb57-2370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2371"><a href="#cb57-2371" aria-hidden="true" tabindex="-1"></a>Three different cases should be considered when the two models are:</span>
<span id="cb57-2372"><a href="#cb57-2372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2373"><a href="#cb57-2373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>nested, $\omega^2_*$ is necessarily 0,</span>
<span id="cb57-2374"><a href="#cb57-2374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>overlapping (which means than the models</span>
<span id="cb57-2375"><a href="#cb57-2375" aria-hidden="true" tabindex="-1"></a>  coincide for some values of the parameters), $\omega^2_*$ *may be*</span>
<span id="cb57-2376"><a href="#cb57-2376" aria-hidden="true" tabindex="-1"></a>  equal to 0 or not,</span>
<span id="cb57-2377"><a href="#cb57-2377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>strictly non-nested, $\omega^2_*$ is</span>
<span id="cb57-2378"><a href="#cb57-2378" aria-hidden="true" tabindex="-1"></a>  necessarily strictly positive.</span>
<span id="cb57-2379"><a href="#cb57-2379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2380"><a href="#cb57-2380" aria-hidden="true" tabindex="-1"></a>The distribution of the statistic depends on whether $\omega^2_*$ is</span>
<span id="cb57-2381"><a href="#cb57-2381" aria-hidden="true" tabindex="-1"></a>zero or positive.  If $\omega^2_*$ is positive, the statistic is</span>
<span id="cb57-2382"><a href="#cb57-2382" aria-hidden="true" tabindex="-1"></a>$\hat{T}_N = \sqrt{N}\frac{\hat{\Lambda}_N}{\hat{\omega}_N}$ and,</span>
<span id="cb57-2383"><a href="#cb57-2383" aria-hidden="true" tabindex="-1"></a>under the null hypothesis that the two models are equivalent,</span>
<span id="cb57-2384"><a href="#cb57-2384" aria-hidden="true" tabindex="-1"></a>follows a standard normal distribution. This is the case for</span>
<span id="cb57-2385"><a href="#cb57-2385" aria-hidden="true" tabindex="-1"></a>strictly non-nested models.</span>
<span id="cb57-2386"><a href="#cb57-2386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2387"><a href="#cb57-2387" aria-hidden="true" tabindex="-1"></a>On the contrary, if $\omega^2_* = 0$, the distribution is much more</span>
<span id="cb57-2388"><a href="#cb57-2388" aria-hidden="true" tabindex="-1"></a>complicated. We need to define two matrices: $A$ contains the expected</span>
<span id="cb57-2389"><a href="#cb57-2389" aria-hidden="true" tabindex="-1"></a>values of the second derivatives of $\Lambda$:</span>
<span id="cb57-2390"><a href="#cb57-2390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2391"><a href="#cb57-2391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2392"><a href="#cb57-2392" aria-hidden="true" tabindex="-1"></a>A(\theta_*) = \mbox{E}^0\left[\frac{\partial^2 \Lambda}{\partial \theta</span>
<span id="cb57-2393"><a href="#cb57-2393" aria-hidden="true" tabindex="-1"></a>\partial \theta ^ \top}\right] =</span>
<span id="cb57-2394"><a href="#cb57-2394" aria-hidden="true" tabindex="-1"></a>\mbox{E}^0\left[\begin{array}{cc}</span>
<span id="cb57-2395"><a href="#cb57-2395" aria-hidden="true" tabindex="-1"></a>\frac{\partial^2 \ln f}{\partial \beta \partial \beta ^</span>
<span id="cb57-2396"><a href="#cb57-2396" aria-hidden="true" tabindex="-1"></a>\top} &amp; 0 <span class="sc">\\</span></span>
<span id="cb57-2397"><a href="#cb57-2397" aria-hidden="true" tabindex="-1"></a>0 &amp; -\frac{\partial^2 \ln g}{\partial \beta \partial \beta ^</span>
<span id="cb57-2398"><a href="#cb57-2398" aria-hidden="true" tabindex="-1"></a>\top}</span>
<span id="cb57-2399"><a href="#cb57-2399" aria-hidden="true" tabindex="-1"></a>\end{array}\right]</span>
<span id="cb57-2400"><a href="#cb57-2400" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb57-2401"><a href="#cb57-2401" aria-hidden="true" tabindex="-1"></a>\left[</span>
<span id="cb57-2402"><a href="#cb57-2402" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb57-2403"><a href="#cb57-2403" aria-hidden="true" tabindex="-1"></a>A_f(\beta_*) &amp; 0 <span class="sc">\\</span></span>
<span id="cb57-2404"><a href="#cb57-2404" aria-hidden="true" tabindex="-1"></a>0 &amp; - A_g(\gamma_*)</span>
<span id="cb57-2405"><a href="#cb57-2405" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2406"><a href="#cb57-2406" aria-hidden="true" tabindex="-1"></a>\right]</span>
<span id="cb57-2407"><a href="#cb57-2407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2408"><a href="#cb57-2408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2409"><a href="#cb57-2409" aria-hidden="true" tabindex="-1"></a>and $B$ the variance of its first derivatives:</span>
<span id="cb57-2410"><a href="#cb57-2410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2411"><a href="#cb57-2411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2412"><a href="#cb57-2412" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb57-2413"><a href="#cb57-2413" aria-hidden="true" tabindex="-1"></a>B(\theta_*) =</span>
<span id="cb57-2414"><a href="#cb57-2414" aria-hidden="true" tabindex="-1"></a>\mbox{E}^0\left[\frac{\partial \Lambda}{\partial</span>
<span id="cb57-2415"><a href="#cb57-2415" aria-hidden="true" tabindex="-1"></a>\theta}\frac{\partial \Lambda}{\partial \theta ^ \top}\right]&amp;=&amp;</span>
<span id="cb57-2416"><a href="#cb57-2416" aria-hidden="true" tabindex="-1"></a>\mbox{E}^0\left[</span>
<span id="cb57-2417"><a href="#cb57-2417" aria-hidden="true" tabindex="-1"></a>\left(\frac{\partial \ln f}{\partial \beta},</span>
<span id="cb57-2418"><a href="#cb57-2418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\frac{\partial \ln g}{\partial \gamma} \right)</span>
<span id="cb57-2419"><a href="#cb57-2419" aria-hidden="true" tabindex="-1"></a>\left(\frac{\partial \ln f}{\partial \beta ^ \top},</span>
<span id="cb57-2420"><a href="#cb57-2420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\frac{\partial \ln g}{\partial \gamma ^ \top} \right)</span>
<span id="cb57-2421"><a href="#cb57-2421" aria-hidden="true" tabindex="-1"></a>\right]<span class="sc">\\</span></span>
<span id="cb57-2422"><a href="#cb57-2422" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \mbox{E}^0\left[</span>
<span id="cb57-2423"><a href="#cb57-2423" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb57-2424"><a href="#cb57-2424" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln f}{\partial \beta} \frac{\partial \ln f}{\partial</span>
<span id="cb57-2425"><a href="#cb57-2425" aria-hidden="true" tabindex="-1"></a>\beta^\top} &amp;</span>
<span id="cb57-2426"><a href="#cb57-2426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\frac{\partial \ln f}{\partial \beta} \frac{\partial \ln g}{\partial</span>
<span id="cb57-2427"><a href="#cb57-2427" aria-hidden="true" tabindex="-1"></a>\gamma ^ \top} <span class="sc">\\</span></span>
<span id="cb57-2428"><a href="#cb57-2428" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\frac{\partial \ln g}{\partial \gamma} \frac{\partial \ln f}{\partial</span>
<span id="cb57-2429"><a href="#cb57-2429" aria-hidden="true" tabindex="-1"></a>  \beta^\top} &amp;</span>
<span id="cb57-2430"><a href="#cb57-2430" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln g}{\partial \gamma} \frac{\partial \ln g}{\partial \gamma^\top}</span>
<span id="cb57-2431"><a href="#cb57-2431" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2432"><a href="#cb57-2432" aria-hidden="true" tabindex="-1"></a>\right]</span>
<span id="cb57-2433"><a href="#cb57-2433" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2434"><a href="#cb57-2434" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2435"><a href="#cb57-2435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2436"><a href="#cb57-2436" aria-hidden="true" tabindex="-1"></a>or:</span>
<span id="cb57-2437"><a href="#cb57-2437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2438"><a href="#cb57-2438" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2439"><a href="#cb57-2439" aria-hidden="true" tabindex="-1"></a>B(\theta_*) =</span>
<span id="cb57-2440"><a href="#cb57-2440" aria-hidden="true" tabindex="-1"></a>\left[</span>
<span id="cb57-2441"><a href="#cb57-2441" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb57-2442"><a href="#cb57-2442" aria-hidden="true" tabindex="-1"></a>B_f(\beta_*) &amp; - B_{fg}(\beta_*, \gamma_*) <span class="sc">\\</span></span>
<span id="cb57-2443"><a href="#cb57-2443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>B_{gf}(\beta_*, \gamma_*) &amp; B_g(\gamma_*)</span>
<span id="cb57-2444"><a href="#cb57-2444" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2445"><a href="#cb57-2445" aria-hidden="true" tabindex="-1"></a>\right]</span>
<span id="cb57-2446"><a href="#cb57-2446" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2447"><a href="#cb57-2447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2448"><a href="#cb57-2448" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb57-2449"><a href="#cb57-2449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2450"><a href="#cb57-2450" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2451"><a href="#cb57-2451" aria-hidden="true" tabindex="-1"></a>W(\theta_*) =  B(\theta_*) \left[-A(\theta_*)\right] ^ {-1}=</span>
<span id="cb57-2452"><a href="#cb57-2452" aria-hidden="true" tabindex="-1"></a>\left[</span>
<span id="cb57-2453"><a href="#cb57-2453" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb57-2454"><a href="#cb57-2454" aria-hidden="true" tabindex="-1"></a>-B_f(\beta_*) A^{-1}_f(\beta_*) &amp; - B_{fg}(\beta_*, \gamma_*)</span>
<span id="cb57-2455"><a href="#cb57-2455" aria-hidden="true" tabindex="-1"></a>A^{-1}_g(\gamma_*) <span class="sc">\\</span></span>
<span id="cb57-2456"><a href="#cb57-2456" aria-hidden="true" tabindex="-1"></a>B_{gf}(\gamma_*, \beta_*) A^{-1}_f(\beta_*) &amp; B_g(\gamma_*)</span>
<span id="cb57-2457"><a href="#cb57-2457" aria-hidden="true" tabindex="-1"></a>A^{-1}_g(\gamma_*)</span>
<span id="cb57-2458"><a href="#cb57-2458" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb57-2459"><a href="#cb57-2459" aria-hidden="true" tabindex="-1"></a>\right]</span>
<span id="cb57-2460"><a href="#cb57-2460" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2461"><a href="#cb57-2461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2462"><a href="#cb57-2462" aria-hidden="true" tabindex="-1"></a>Denote $\lambda_*$ the eigenvalues of $W$.  When $\omega_*^2 = 0$</span>
<span id="cb57-2463"><a href="#cb57-2463" aria-hidden="true" tabindex="-1"></a>(which is always the case for nested models), the statistic is the one</span>
<span id="cb57-2464"><a href="#cb57-2464" aria-hidden="true" tabindex="-1"></a>used in the standard likelihood ratio test: $2 (\ln L_f - \ln L_g) = 2</span>
<span id="cb57-2465"><a href="#cb57-2465" aria-hidden="true" tabindex="-1"></a>N \hat{\Lambda}_N$ which, under the null, follows a weighted $\chi ^</span>
<span id="cb57-2466"><a href="#cb57-2466" aria-hidden="true" tabindex="-1"></a>2$ distribution with weights equal to $\lambda_*$. The Vuong test can</span>
<span id="cb57-2467"><a href="#cb57-2467" aria-hidden="true" tabindex="-1"></a>be seen in this context as a more robust version of the standard</span>
<span id="cb57-2468"><a href="#cb57-2468" aria-hidden="true" tabindex="-1"></a>likelihood ratio test, because it doesn't assume, under the null, that</span>
<span id="cb57-2469"><a href="#cb57-2469" aria-hidden="true" tabindex="-1"></a>the larger model is correctly specified.</span>
<span id="cb57-2470"><a href="#cb57-2470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2471"><a href="#cb57-2471" aria-hidden="true" tabindex="-1"></a>Note that, if the larger model is correctly specified, the information</span>
<span id="cb57-2472"><a href="#cb57-2472" aria-hidden="true" tabindex="-1"></a>matrix equality implies that $B_f(\theta_*)=-A_f(\theta_*)$. In this</span>
<span id="cb57-2473"><a href="#cb57-2473" aria-hidden="true" tabindex="-1"></a>case, the two matrices on the diagonal of $W$ reduces to $-I_{K_f}$ and</span>
<span id="cb57-2474"><a href="#cb57-2474" aria-hidden="true" tabindex="-1"></a>$I_{K_g}$, the trace of $W$ to $K_g - K_f$ and the distribution of the</span>
<span id="cb57-2475"><a href="#cb57-2475" aria-hidden="true" tabindex="-1"></a>statistic under the null reduce to a $\chi^2$ with $K_g - K_f$ degrees</span>
<span id="cb57-2476"><a href="#cb57-2476" aria-hidden="true" tabindex="-1"></a>of freedom.</span>
<span id="cb57-2477"><a href="#cb57-2477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2478"><a href="#cb57-2478" aria-hidden="true" tabindex="-1"></a>The $W$ matrix can be consistently estimated by computing the first</span>
<span id="cb57-2479"><a href="#cb57-2479" aria-hidden="true" tabindex="-1"></a>and the second derivatives of the likelihood functions of the two</span>
<span id="cb57-2480"><a href="#cb57-2480" aria-hidden="true" tabindex="-1"></a>models for $\hat{\theta}$. For example,</span>
<span id="cb57-2481"><a href="#cb57-2481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2482"><a href="#cb57-2482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2483"><a href="#cb57-2483" aria-hidden="true" tabindex="-1"></a>\hat{A}_f(\hat{\beta}) = \frac{1}{N}</span>
<span id="cb57-2484"><a href="#cb57-2484" aria-hidden="true" tabindex="-1"></a>\sum_{n= 1} ^ N \frac{\partial^2 \ln f}{\partial \beta \partial</span>
<span id="cb57-2485"><a href="#cb57-2485" aria-hidden="true" tabindex="-1"></a>\beta ^ \top}(\hat{\beta}, x_n, y_n)</span>
<span id="cb57-2486"><a href="#cb57-2486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2487"><a href="#cb57-2487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2488"><a href="#cb57-2488" aria-hidden="true" tabindex="-1"></a>$$ \hat{B}_{fg}(\hat{\theta})= \frac{1}{N} \sum_{n=1}^N</span>
<span id="cb57-2489"><a href="#cb57-2489" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln f}{\partial \beta}(\hat{\beta}, x_n, y_n)</span>
<span id="cb57-2490"><a href="#cb57-2490" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln g}{\partial \gamma^\top}(\hat{\gamma}, x_n, y_n)</span>
<span id="cb57-2491"><a href="#cb57-2491" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2492"><a href="#cb57-2492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2493"><a href="#cb57-2493" aria-hidden="true" tabindex="-1"></a>For the overlapping case, the test should be performed in two steps:</span>
<span id="cb57-2494"><a href="#cb57-2494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2495"><a href="#cb57-2495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the first step consists of testing whether $\omega_*^*$ is 0 or</span>
<span id="cb57-2496"><a href="#cb57-2496" aria-hidden="true" tabindex="-1"></a>  not. This hypothesis is based on the statistic $N \hat{\omega} ^ 2$</span>
<span id="cb57-2497"><a href="#cb57-2497" aria-hidden="true" tabindex="-1"></a>  which, under the null ($\omega_*^2=0$) follows a weighted $\chi ^ 2$</span>
<span id="cb57-2498"><a href="#cb57-2498" aria-hidden="true" tabindex="-1"></a>  distributions with weights equal to $\lambda_* ^ 2$. If the null</span>
<span id="cb57-2499"><a href="#cb57-2499" aria-hidden="true" tabindex="-1"></a>  hypothesis is not rejected, the test stops at this step, and the</span>
<span id="cb57-2500"><a href="#cb57-2500" aria-hidden="true" tabindex="-1"></a>  conclusion is that the two models are equivalent;</span>
<span id="cb57-2501"><a href="#cb57-2501" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>if the null hypothesis is rejected, the second step consists of</span>
<span id="cb57-2502"><a href="#cb57-2502" aria-hidden="true" tabindex="-1"></a>  applying the test for non-nested models previously described.</span>
<span id="cb57-2503"><a href="#cb57-2503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2504"><a href="#cb57-2504" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!Vuong test|(}</span>
<span id="cb57-2505"><a href="#cb57-2505" aria-hidden="true" tabindex="-1"></a>@SHI:15\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Shi} provides an example of simulations of non-nested linear models</span>
<span id="cb57-2506"><a href="#cb57-2506" aria-hidden="true" tabindex="-1"></a>that shows that the distribution of the Vuong statistic can be very</span>
<span id="cb57-2507"><a href="#cb57-2507" aria-hidden="true" tabindex="-1"></a>different from a standard normal. The data generating process used for</span>
<span id="cb57-2508"><a href="#cb57-2508" aria-hidden="true" tabindex="-1"></a>the simulations is:</span>
<span id="cb57-2509"><a href="#cb57-2509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2510"><a href="#cb57-2510" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2511"><a href="#cb57-2511" aria-hidden="true" tabindex="-1"></a>y = 1 + \sum_{k = 1} ^ {K_f} z^f_k + \sum_{k = 1} ^ {K_g} z^g_k + \epsilon</span>
<span id="cb57-2512"><a href="#cb57-2512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2513"><a href="#cb57-2513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2514"><a href="#cb57-2514" aria-hidden="true" tabindex="-1"></a>where $z^f$ is the set of $K_f$ covariates that are used in the first</span>
<span id="cb57-2515"><a href="#cb57-2515" aria-hidden="true" tabindex="-1"></a>model and $z^g$ the set of $K_g$ covariates used in the second model</span>
<span id="cb57-2516"><a href="#cb57-2516" aria-hidden="true" tabindex="-1"></a>and $\epsilon \sim N(0, 1 - a ^ 2)$. $z^f_k \sim N(0, a / \sqrt{K_f})$</span>
<span id="cb57-2517"><a href="#cb57-2517" aria-hidden="true" tabindex="-1"></a>and $z^g_k \sim N(0, a / \sqrt{K_g})$, so that the variance</span>
<span id="cb57-2518"><a href="#cb57-2518" aria-hidden="true" tabindex="-1"></a>explained by the two competing models is the same (equal to $a ^ 2$)</span>
<span id="cb57-2519"><a href="#cb57-2519" aria-hidden="true" tabindex="-1"></a>and the null hypothesis of the Vuong test is true. The <span class="in">`micsr::vuong_sim`</span></span>
<span id="cb57-2520"><a href="#cb57-2520" aria-hidden="true" tabindex="-1"></a>enables to simulate values of the Vuong test. As in @SHI:15\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Shi}, we use a</span>
<span id="cb57-2521"><a href="#cb57-2521" aria-hidden="true" tabindex="-1"></a>very different degree of parametrization for the two models, with $K_f</span>
<span id="cb57-2522"><a href="#cb57-2522" aria-hidden="true" tabindex="-1"></a>= 15$ and $K_G = 1$.</span>
<span id="cb57-2523"><a href="#cb57-2523" aria-hidden="true" tabindex="-1"></a>\idxfun{vuong<span class="sc">\_</span>sim}{micsr}\idxfun{head}{utils}</span>
<span id="cb57-2524"><a href="#cb57-2524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2525"><a href="#cb57-2525" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2526"><a href="#cb57-2526" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-2527"><a href="#cb57-2527" aria-hidden="true" tabindex="-1"></a>Vuong <span class="ot">&lt;-</span> <span class="fu">vuong_sim</span>(<span class="at">N =</span> <span class="dv">100</span>, <span class="at">R =</span> <span class="dv">1000</span>,</span>
<span id="cb57-2528"><a href="#cb57-2528" aria-hidden="true" tabindex="-1"></a>                   <span class="at">Kf =</span> <span class="dv">15</span>, <span class="at">Kg =</span> <span class="dv">1</span>, <span class="at">a =</span> <span class="fl">0.5</span>)</span>
<span id="cb57-2529"><a href="#cb57-2529" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Vuong)</span>
<span id="cb57-2530"><a href="#cb57-2530" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Vuong)</span>
<span id="cb57-2531"><a href="#cb57-2531" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(Vuong) <span class="sc">&gt;</span> <span class="fl">1.96</span>)</span>
<span id="cb57-2532"><a href="#cb57-2532" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2533"><a href="#cb57-2533" aria-hidden="true" tabindex="-1"></a>We can see that the mean of the statistic for the 1000 replications is</span>
<span id="cb57-2534"><a href="#cb57-2534" aria-hidden="true" tabindex="-1"></a>far away from 0, which means that the numerator of the Vuong statistic</span>
<span id="cb57-2535"><a href="#cb57-2535" aria-hidden="true" tabindex="-1"></a>is seriously biased. A total of <span class="in">`r round(mean(abs(Vuong) &gt; 1.96) * 100, 1)`</span>% of the</span>
<span id="cb57-2536"><a href="#cb57-2536" aria-hidden="true" tabindex="-1"></a>values of the statistic are greater than the critical value so that</span>
<span id="cb57-2537"><a href="#cb57-2537" aria-hidden="true" tabindex="-1"></a>the Vuong test will lead in such context to a noticeable</span>
<span id="cb57-2538"><a href="#cb57-2538" aria-hidden="true" tabindex="-1"></a>over-rejection. The empirical density function is shown in @fig-vuong_stat_dist,</span>
<span id="cb57-2539"><a href="#cb57-2539" aria-hidden="true" tabindex="-1"></a>along with the normal density.</span>
<span id="cb57-2540"><a href="#cb57-2540" aria-hidden="true" tabindex="-1"></a>\idxfun{ggplot}{ggplot2}\idxfun{geom<span class="sc">\_</span>density}{ggplot2}\idxfun{geom<span class="sc">\_</span>function}{ggplot2}\idxfun{aes}{ggplot2}\idxfun{data.frame}{base}</span>
<span id="cb57-2541"><a href="#cb57-2541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2542"><a href="#cb57-2542" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2543"><a href="#cb57-2543" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-2544"><a href="#cb57-2544" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-vuong_stat_dist</span></span>
<span id="cb57-2545"><a href="#cb57-2545" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of the Vuong statistic"</span></span>
<span id="cb57-2546"><a href="#cb57-2546" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">Vuong =</span> Vuong)) <span class="sc">+</span></span>
<span id="cb57-2547"><a href="#cb57-2547" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> Vuong)) <span class="sc">+</span></span>
<span id="cb57-2548"><a href="#cb57-2548" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_function</span>(<span class="at">fun =</span> dnorm, <span class="at">linetype =</span> <span class="st">"dotted"</span>)</span>
<span id="cb57-2549"><a href="#cb57-2549" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2550"><a href="#cb57-2550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2551"><a href="#cb57-2551" aria-hidden="true" tabindex="-1"></a>@SHI:15\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Shi} proposed a non-degenerate Vuong test which corrects the small sample bias of the numerator of the Vuong statistic and inflates the denominator by adding a constant. </span>
<span id="cb57-2552"><a href="#cb57-2552" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simulations!Vuong test|)}</span>
<span id="cb57-2553"><a href="#cb57-2553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2554"><a href="#cb57-2554" aria-hidden="true" tabindex="-1"></a><span class="fu">#### An example: generalized production function vs. translog function</span></span>
<span id="cb57-2555"><a href="#cb57-2555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2556"><a href="#cb57-2556" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Vuong test!generalized production function|(}</span>
<span id="cb57-2557"><a href="#cb57-2557" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Vuong test!translog production function|(}</span>
<span id="cb57-2558"><a href="#cb57-2558" aria-hidden="true" tabindex="-1"></a>A popular alternative to the generalized production function is the</span>
<span id="cb57-2559"><a href="#cb57-2559" aria-hidden="true" tabindex="-1"></a>translog function, which is:</span>
<span id="cb57-2560"><a href="#cb57-2560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2561"><a href="#cb57-2561" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2562"><a href="#cb57-2562" aria-hidden="true" tabindex="-1"></a>\ln y = \alpha + \sum_{j=1} ^ J \beta_j \ln q_j + \frac{1}{2}</span>
<span id="cb57-2563"><a href="#cb57-2563" aria-hidden="true" tabindex="-1"></a>\sum_{j=1}^J \sum_{k=1} ^ J \beta_{jk} \ln q_j \ln q_k</span>
<span id="cb57-2564"><a href="#cb57-2564" aria-hidden="true" tabindex="-1"></a>$$ {#eq-translog_production}</span>
<span id="cb57-2565"><a href="#cb57-2565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2566"><a href="#cb57-2566" aria-hidden="true" tabindex="-1"></a>The elasticity of the production with a factor is:</span>
<span id="cb57-2567"><a href="#cb57-2567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2568"><a href="#cb57-2568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2569"><a href="#cb57-2569" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln y}{\partial \ln x_j} = \beta_j + \sum_{k=1} ^ J</span>
<span id="cb57-2570"><a href="#cb57-2570" aria-hidden="true" tabindex="-1"></a>\beta_{jk} \ln q_k</span>
<span id="cb57-2571"><a href="#cb57-2571" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2572"><a href="#cb57-2572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2573"><a href="#cb57-2573" aria-hidden="true" tabindex="-1"></a>and the scale elasticity is just the sum of these $J$ elasticities:</span>
<span id="cb57-2574"><a href="#cb57-2574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2575"><a href="#cb57-2575" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2576"><a href="#cb57-2576" aria-hidden="true" tabindex="-1"></a>\epsilon = \sum_{j=1} ^ J \beta_j + \sum_{j=1} ^ J \sum_{k=1} ^ J</span>
<span id="cb57-2577"><a href="#cb57-2577" aria-hidden="true" tabindex="-1"></a>\beta_{jk} \ln q_k = </span>
<span id="cb57-2578"><a href="#cb57-2578" aria-hidden="true" tabindex="-1"></a>\sum_{j=1} ^ J \beta_j + \sum_{k=1} ^ J \ln q_k \sum_{j=1} ^ J\beta_{jk}</span>
<span id="cb57-2579"><a href="#cb57-2579" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-2580"><a href="#cb57-2580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2581"><a href="#cb57-2581" aria-hidden="true" tabindex="-1"></a>The constant returns to scale hypothesis therefore implies that $\sum_j \beta_j =</span>
<span id="cb57-2582"><a href="#cb57-2582" aria-hidden="true" tabindex="-1"></a>1$ and $\sum_{j=1} ^ J \beta_{jl} = 0 \;\forall\;k$. We fit @eq-translog_production using <span class="in">`loglm`</span>:</span>
<span id="cb57-2583"><a href="#cb57-2583" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- With the same trick as the one used in @sec-system_equation, @eq-translog_production can be rewritten as: --&gt;</span></span>
<span id="cb57-2584"><a href="#cb57-2584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2585"><a href="#cb57-2585" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $$ --&gt;</span></span>
<span id="cb57-2586"><a href="#cb57-2586" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \ln y =  --&gt;</span></span>
<span id="cb57-2587"><a href="#cb57-2587" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \sum_j ^{J-1}\beta_j q_j^* +  --&gt;</span></span>
<span id="cb57-2588"><a href="#cb57-2588" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \frac{1}{2} \sum_{j=1} ^ {J - 1} \beta_{jj} q_j ^ * +   --&gt;</span></span>
<span id="cb57-2589"><a href="#cb57-2589" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \sum_{j=1} ^ {J - 1} \sum_{k&gt;j} ^ {J - 1} \beta_{jk}  --&gt;</span></span>
<span id="cb57-2590"><a href="#cb57-2590" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- q_j ^ * q_k ^ * + \ln q_J \sum_{j=1} ^ {J-1} \beta_{jJ} ^ * q_j^* + \beta_J ^ *\ln q_J + \frac{1}{2}\beta_{JJ}^* \ln^2q_J + \ln q_j --&gt;</span></span>
<span id="cb57-2591"><a href="#cb57-2591" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $$ --&gt;</span></span>
<span id="cb57-2592"><a href="#cb57-2592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2593"><a href="#cb57-2593" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- with $q_j ^ * = \ln q_j / q_I$, $\beta_J ^ * = 1 -\sum_{n=1}^{J-1}\beta_j$, $\beta_{jJ}^* = \sum_{k=1}^{J-1}\beta_{jk}$ and $\beta_{JJ} ^ * sum_{j=1}^J \beta_{jJ} ^ *$ and the constant return to scales hypothesis is $\beta_J ^ *, \beta_{jJ}^* = \beta_{JJ} = 0$. --&gt;</span></span>
<span id="cb57-2594"><a href="#cb57-2594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2595"><a href="#cb57-2595" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r} --&gt;</span></span>
<span id="cb57-2596"><a href="#cb57-2596" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- #| echo: false --&gt;</span></span>
<span id="cb57-2597"><a href="#cb57-2597" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- llcont.micsr &lt;- function(x) x$logLik --&gt;</span></span>
<span id="cb57-2598"><a href="#cb57-2598" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- llcont.maxLik &lt;- function(x) x$objectiveFn(x$estimate, Z = Z, y = y, gradient = FALSE, hessian = FALSE) --&gt;</span></span>
<span id="cb57-2599"><a href="#cb57-2599" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb57-2600"><a href="#cb57-2600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2601"><a href="#cb57-2601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2602"><a href="#cb57-2602" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- And the constant return to scale hypothesis implies that $\beta_\lambda ^ *$ and $\beta_{jJ}^*, \forall j = 1\ldots J-1$ equal 0. --&gt;</span></span>
<span id="cb57-2603"><a href="#cb57-2603" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb57-2604"><a href="#cb57-2604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2605"><a href="#cb57-2605" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2606"><a href="#cb57-2606" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-2607"><a href="#cb57-2607" aria-hidden="true" tabindex="-1"></a>aps <span class="ot">&lt;-</span> aps <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">km =</span> <span class="fu">log</span>(k <span class="sc">/</span> m), <span class="at">lm =</span> <span class="fu">log</span>(l <span class="sc">/</span> m))</span>
<span id="cb57-2608"><a href="#cb57-2608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2609"><a href="#cb57-2609" aria-hidden="true" tabindex="-1"></a>free <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> km <span class="sc">+</span> lm <span class="sc">+</span> <span class="fu">I</span>(km <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(lm <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> km<span class="sc">:</span>lm <span class="sc">+</span> km<span class="sc">:</span><span class="fu">log</span>(m) <span class="sc">+</span> lm<span class="sc">:</span><span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(m) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(m)), aps)</span>
<span id="cb57-2610"><a href="#cb57-2610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2611"><a href="#cb57-2611" aria-hidden="true" tabindex="-1"></a>crs <span class="ot">&lt;-</span>  <span class="fu">loglm</span>(y <span class="sc">~</span> km <span class="sc">+</span> <span class="fu">I</span>(km <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> lm <span class="sc">+</span> <span class="fu">I</span>(lm <span class="sc">^</span> <span class="dv">2</span>)<span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(m)), aps)</span>
<span id="cb57-2612"><a href="#cb57-2612" aria-hidden="true" tabindex="-1"></a>cd <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> km <span class="sc">+</span> lm <span class="sc">+</span> <span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(m)), aps)</span>
<span id="cb57-2613"><a href="#cb57-2613" aria-hidden="true" tabindex="-1"></a>cd_crs <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> km <span class="sc">+</span> lm <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(m)), aps)</span>
<span id="cb57-2614"><a href="#cb57-2614" aria-hidden="true" tabindex="-1"></a>trsl <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> <span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(k) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(l) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(m) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">log</span>(k)<span class="sc">:</span><span class="fu">log</span>(l) <span class="sc">+</span></span>
<span id="cb57-2615"><a href="#cb57-2615" aria-hidden="true" tabindex="-1"></a>                <span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">log</span>(m)<span class="sc">:</span>(<span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l)), aps)</span>
<span id="cb57-2616"><a href="#cb57-2616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2617"><a href="#cb57-2617" aria-hidden="true" tabindex="-1"></a>trsl <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> <span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">log</span>(k)<span class="sc">:</span><span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">log</span>(k)<span class="sc">:</span><span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">log</span>(l)<span class="sc">:</span><span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(k) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(l) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(m) <span class="sc">^</span> <span class="dv">2</span>), aps)</span>
<span id="cb57-2618"><a href="#cb57-2618" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2619"><a href="#cb57-2619" aria-hidden="true" tabindex="-1"></a>\idxfun{loglm}{micsr}\idxfun{I}{base}</span>
<span id="cb57-2620"><a href="#cb57-2620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2623"><a href="#cb57-2623" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-2624"><a href="#cb57-2624" aria-hidden="true" tabindex="-1"></a>trsl <span class="ot">&lt;-</span> <span class="fu">loglm</span>(y <span class="sc">~</span> <span class="fu">log</span>(k) <span class="sc">+</span> <span class="fu">log</span>(l) <span class="sc">+</span> <span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">log</span>(k)<span class="sc">:</span><span class="fu">log</span>(l) <span class="sc">+</span> </span>
<span id="cb57-2625"><a href="#cb57-2625" aria-hidden="true" tabindex="-1"></a>                <span class="fu">log</span>(k)<span class="sc">:</span><span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">log</span>(l)<span class="sc">:</span><span class="fu">log</span>(m) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(k) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb57-2626"><a href="#cb57-2626" aria-hidden="true" tabindex="-1"></a>                <span class="fu">I</span>(<span class="fu">log</span>(l) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(m) <span class="sc">^</span> <span class="dv">2</span>), aps)</span>
<span id="cb57-2627"><a href="#cb57-2627" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2628"><a href="#cb57-2628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2629"><a href="#cb57-2629" aria-hidden="true" tabindex="-1"></a>All that is required to compute the Vuong test for non-nested models is the</span>
<span id="cb57-2630"><a href="#cb57-2630" aria-hidden="true" tabindex="-1"></a>contributions to the log-likelihood for both models:^<span class="co">[</span><span class="ot">The object returned by `loglm` contains an element called `values` that is a vector of individual contributions to the log-likelihood.</span><span class="co">]</span></span>
<span id="cb57-2631"><a href="#cb57-2631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2632"><a href="#cb57-2632" aria-hidden="true" tabindex="-1"></a>\idxfun{zellner<span class="sc">\_</span>revankar}{micsr}\idxfun{as.numeric}{base}\idxfun{coef}{stats}</span>
<span id="cb57-2633"><a href="#cb57-2633" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2634"><a href="#cb57-2634" aria-hidden="true" tabindex="-1"></a>est_gpf <span class="ot">&lt;-</span> <span class="fu">zellner_revankar</span>(<span class="fu">coef</span>(gpf), <span class="at">y =</span> y, <span class="at">Z =</span> Z)</span>
<span id="cb57-2635"><a href="#cb57-2635" aria-hidden="true" tabindex="-1"></a>lnl_gpf <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(est_gpf)</span>
<span id="cb57-2636"><a href="#cb57-2636" aria-hidden="true" tabindex="-1"></a>lnl_trsl <span class="ot">&lt;-</span> trsl<span class="sc">$</span>value</span>
<span id="cb57-2637"><a href="#cb57-2637" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2638"><a href="#cb57-2638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2639"><a href="#cb57-2639" aria-hidden="true" tabindex="-1"></a>We can then compute the average likelihood ratio statistic (<span class="in">`L`</span>), its</span>
<span id="cb57-2640"><a href="#cb57-2640" aria-hidden="true" tabindex="-1"></a>variance (<span class="in">`w2`</span>) and the statistic:</span>
<span id="cb57-2641"><a href="#cb57-2641" aria-hidden="true" tabindex="-1"></a>\idxfun{length}{base}</span>
<span id="cb57-2642"><a href="#cb57-2642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2643"><a href="#cb57-2643" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2644"><a href="#cb57-2644" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-2645"><a href="#cb57-2645" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(lnl_gpf)</span>
<span id="cb57-2646"><a href="#cb57-2646" aria-hidden="true" tabindex="-1"></a>L <span class="ot">&lt;-</span> <span class="fu">mean</span>(lnl_gpf) <span class="sc">-</span> <span class="fu">mean</span>(lnl_trsl)</span>
<span id="cb57-2647"><a href="#cb57-2647" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">mean</span>((lnl_gpf <span class="sc">-</span> lnl_trsl) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">-</span> L <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb57-2648"><a href="#cb57-2648" aria-hidden="true" tabindex="-1"></a>vuong_stat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(N) <span class="sc">*</span> L <span class="sc">/</span> <span class="fu">sqrt</span>(w2)</span>
<span id="cb57-2649"><a href="#cb57-2649" aria-hidden="true" tabindex="-1"></a>vuong_stat</span>
<span id="cb57-2650"><a href="#cb57-2650" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2651"><a href="#cb57-2651" aria-hidden="true" tabindex="-1"></a>The probability value is <span class="in">`r round(pnorm(abs(vuong_stat), lower.tail = FALSE), 3)`</span></span>
<span id="cb57-2652"><a href="#cb57-2652" aria-hidden="true" tabindex="-1"></a>so that the test concludes that the two models are indistinguishable,</span>
<span id="cb57-2653"><a href="#cb57-2653" aria-hidden="true" tabindex="-1"></a>although the difference of their log-likelihood is quite high:</span>
<span id="cb57-2654"><a href="#cb57-2654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2655"><a href="#cb57-2655" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb57-2656"><a href="#cb57-2656" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb57-2657"><a href="#cb57-2657" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(lnl_gpf)</span>
<span id="cb57-2658"><a href="#cb57-2658" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(lnl_trsl)</span>
<span id="cb57-2659"><a href="#cb57-2659" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2660"><a href="#cb57-2660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-2663"><a href="#cb57-2663" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-2664"><a href="#cb57-2664" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb57-2665"><a href="#cb57-2665" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb57-2666"><a href="#cb57-2666" aria-hidden="true" tabindex="-1"></a>nonnest2<span class="sc">::</span><span class="fu">vuongtest</span>(gpf, free)</span>
<span id="cb57-2667"><a href="#cb57-2667" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-2668"><a href="#cb57-2668" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Vuong test!generalized production function|(}</span>
<span id="cb57-2669"><a href="#cb57-2669" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Vuong test!translog production function|(}</span>
<span id="cb57-2670"><a href="#cb57-2670" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Vuong test|)}</span>
<span id="cb57-2671"><a href="#cb57-2671" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>