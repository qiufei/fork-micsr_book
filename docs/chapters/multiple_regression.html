<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Microeconometrics with R - 3&nbsp; Multiple regression model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/coefficients.html" rel="next">
<link href="../chapters/simple_regression_properties.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Microeconometrics with R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../OLS.html" class="sidebar-item-text sidebar-link">Ordinary least squares estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/multiple_regression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coefficients.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../beyond_OLS.html" class="sidebar-item-text sidebar-link">Beyond the OLS estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/maximum_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/non_spherical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/endogeneity.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/treateff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/spatial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Spatial econometrics</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../special_responses.html" class="sidebar-item-text sidebar-link">Special responses</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/binomial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Binomial models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/tobit.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Censored and truncated models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/count.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Count data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/duration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Duration models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discrete choice models</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-model_data_multiple" id="toc-sec-model_data_multiple" class="nav-link active" data-scroll-target="#sec-model_data_multiple"><span class="toc-section-number">3.1</span>  Model and data set</a>
  <ul class="collapse">
<li><a href="#structural-model" id="toc-structural-model" class="nav-link" data-scroll-target="#structural-model">Structural model</a></li>
  <li><a href="#sec-data_solow_mrw" id="toc-sec-data_solow_mrw" class="nav-link" data-scroll-target="#sec-data_solow_mrw">Data set</a></li>
  </ul>
</li>
  <li><a href="#sec-comp_ols_mult" id="toc-sec-comp_ols_mult" class="nav-link" data-scroll-target="#sec-comp_ols_mult"><span class="toc-section-number">3.2</span>  Computation of the OLS estimator</a></li>
  <li>
<a href="#sec-geometry_multiple_ols" id="toc-sec-geometry_multiple_ols" class="nav-link" data-scroll-target="#sec-geometry_multiple_ols"><span class="toc-section-number">3.3</span>  Geometry of least squares</a>
  <ul class="collapse">
<li><a href="#geometry-of-the-multiple-regression-model" id="toc-geometry-of-the-multiple-regression-model" class="nav-link" data-scroll-target="#geometry-of-the-multiple-regression-model">Geometry of the multiple regression model</a></li>
  <li><a href="#frisch-waugh-theorem" id="toc-frisch-waugh-theorem" class="nav-link" data-scroll-target="#frisch-waugh-theorem">Frisch-Waugh theorem</a></li>
  </ul>
</li>
  <li>
<a href="#sec-computation_R_multiple" id="toc-sec-computation_R_multiple" class="nav-link" data-scroll-target="#sec-computation_R_multiple"><span class="toc-section-number">3.4</span>  Computation with R</a>
  <ul class="collapse">
<li><a href="#computation-using-matrix-algebra" id="toc-computation-using-matrix-algebra" class="nav-link" data-scroll-target="#computation-using-matrix-algebra">Computation using matrix algebra</a></li>
  <li><a href="#efficient-computation-qr-decomposition" id="toc-efficient-computation-qr-decomposition" class="nav-link" data-scroll-target="#efficient-computation-qr-decomposition">Efficient computation: QR decomposition</a></li>
  </ul>
</li>
  <li>
<a href="#sec-properties_ols_multiple" id="toc-sec-properties_ols_multiple" class="nav-link" data-scroll-target="#sec-properties_ols_multiple"><span class="toc-section-number">3.5</span>  Properties of the estimators</a>
  <ul class="collapse">
<li><a href="#unbiasedness-of-the-ols-estimator" id="toc-unbiasedness-of-the-ols-estimator" class="nav-link" data-scroll-target="#unbiasedness-of-the-ols-estimator">Unbiasedness of the OLS estimator</a></li>
  <li><a href="#sec-variance_ols" id="toc-sec-variance_ols" class="nav-link" data-scroll-target="#sec-variance_ols">Variance of the OLS estimator</a></li>
  <li><a href="#the-ols-estimator-is-blue" id="toc-the-ols-estimator-is-blue" class="nav-link" data-scroll-target="#the-ols-estimator-is-blue">The OLS estimator is BLUE</a></li>
  <li><a href="#asymptotic-properties-of-the-ols-estimator" id="toc-asymptotic-properties-of-the-ols-estimator" class="nav-link" data-scroll-target="#asymptotic-properties-of-the-ols-estimator">Asymptotic properties of the OLS estimator</a></li>
  <li><a href="#the-coefficient-of-determination" id="toc-the-coefficient-of-determination" class="nav-link" data-scroll-target="#the-coefficient-of-determination">The coefficient of determination</a></li>
  </ul>
</li>
  <li>
<a href="#sec-confint_test_multiple" id="toc-sec-confint_test_multiple" class="nav-link" data-scroll-target="#sec-confint_test_multiple"><span class="toc-section-number">3.6</span>  Confidence interval and test</a>
  <ul class="collapse">
<li><a href="#simple-confidence-interval-and-test" id="toc-simple-confidence-interval-and-test" class="nav-link" data-scroll-target="#simple-confidence-interval-and-test">Simple confidence interval and test</a></li>
  <li><a href="#joint-confidence-interval-and-test-of-joint-hypothesis" id="toc-joint-confidence-interval-and-test-of-joint-hypothesis" class="nav-link" data-scroll-target="#joint-confidence-interval-and-test-of-joint-hypothesis">Joint confidence interval and test of joint hypothesis</a></li>
  <li><a href="#sec-three_tests" id="toc-sec-three_tests" class="nav-link" data-scroll-target="#sec-three_tests">The three tests</a></li>
  <li><a href="#computation-of-the-three-tests" id="toc-computation-of-the-three-tests" class="nav-link" data-scroll-target="#computation-of-the-three-tests">Computation of the three tests</a></li>
  <li><a href="#testing-that-all-the-slopes-are-0" id="toc-testing-that-all-the-slopes-are-0" class="nav-link" data-scroll-target="#testing-that-all-the-slopes-are-0">Testing that all the slopes are 0</a></li>
  </ul>
</li>
  <li>
<a href="#sec-system_equation" id="toc-sec-system_equation" class="nav-link" data-scroll-target="#sec-system_equation"><span class="toc-section-number">3.7</span>  System estimation and constrained least squares</a>
  <ul class="collapse">
<li><a href="#sec-sys_eq_ols" id="toc-sec-sys_eq_ols" class="nav-link" data-scroll-target="#sec-sys_eq_ols">System of equations</a></li>
  <li><a href="#sec-constrained_ls" id="toc-sec-constrained_ls" class="nav-link" data-scroll-target="#sec-constrained_ls">Constrained least squares</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-mult_reg_chapter" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>In this chapter, we’ll analyze the computation and the properties of the OLS estimator when the number of covariates (<span class="math inline">\(K\)</span>) is at least 2. Actually, we’ll analyze in depth the case when <span class="math inline">\(K=2\)</span> (generalizing from <span class="math inline">\(K=2\)</span> to <span class="math inline">\(K&gt;2\)</span> being quite simple) and, compared to the previous two chapters, we’ll insist on two important points:</p>
<ul>
<li>the use of matrix algebra, which makes the computation of the estimator and the analysis of its properties elegant and compact,</li>
<li>the correlation between the covariates; actually we’ll show that the multiple and the simple linear models are different only if there exists such a correlation.</li>
</ul>
<p>We’ll roughly follow the same plan as for the simple linear model: <a href="#sec-model_data_multiple"><span>Section&nbsp;3.1</span></a> presents the structural model and the data set that we’ll use throughout the chapter, <a href="#sec-comp_ols_mult"><span>Section&nbsp;3.2</span></a> the computation of the estimator, <a href="#sec-geometry_multiple_ols"><span>Section&nbsp;3.3</span></a> the geometry of the multiple linear model, <a href="#sec-computation_R_multiple"><span>Section&nbsp;3.4</span></a> the computation of the estimator with <strong>R</strong>, <a href="#sec-properties_ols_multiple"><span>Section&nbsp;3.5</span></a> its statistical properties and <a href="#sec-confint_test_multiple"><span>Section&nbsp;3.6</span></a> the inference methods (confidence interval and tests). Finally, <a href="#sec-system_equation"><span>Section&nbsp;3.7</span></a> presents system estimation and the constrained least squares estimator.</p>
<section id="sec-model_data_multiple" class="level2" data-number="3.1"><h2 data-number="3.1" class="anchored" data-anchor-id="sec-model_data_multiple">
<span class="header-section-number">3.1</span> Model and data set</h2>
<p>To illustrate the multiple regression model, we’ll use the example of the estimation of a model explaining economic growth, using a cross-section of countries.</p>
<section id="structural-model" class="level3"><h3 class="anchored" data-anchor-id="structural-model">Structural model</h3>
<p>One of the most popular growth models in the economic literature is the Solow-Swan model. The production <span class="math inline">\(Y\)</span> (or more precisely the added value or GDP) is performed using two production factors, labor <span class="math inline">\(L\)</span> and capital <span class="math inline">\(K\)</span>. Physical labor is transformed in effective labor using a term called <span class="math inline">\(A\)</span>. <span class="math inline">\(A\)</span> is time-varying (typically increasing) and therefore represents the effect of technical progress which increases the productivity of labor. The functional form of the production function is a Cobb-Douglas:</p>
<p><span id="eq-production_cobb_douglas"><span class="math display">\[
Y(t) = K(t) ^ \kappa \left[A(t)L(t)\right]^{1 - \kappa}
\tag{3.1}\]</span></span></p>
<p>Coefficients for capital and labor are respectively <span class="math inline">\(\kappa\)</span> and <span class="math inline">\(1-\kappa\)</span>. They represent the elasticity of the production respective to each factor, but also the share of each factor in the national income (<span class="math inline">\(\kappa\)</span> is therefore the share of profits and <span class="math inline">\(1-\kappa\)</span> the share of wages).</p>
<p>Each variable is a continuous function of time <span class="math inline">\(t\)</span>. We’ll denote, for each variable <span class="math inline">\(\dot{V}=\frac{d V}{d t}\)</span> the derivative with respect to time. Finally, we’ll denote <span class="math inline">\(y(t) = \frac{Y(t)}{A(t)L(t)}\)</span> and <span class="math inline">\(k(t)=\frac{Y(t)}{A(t)L(t)}\)</span> production and capital per unit of effective labor. We therefore have:</p>
<p><span id="eq-production_function"><span class="math display">\[
y(t) = k(t) ^ \kappa
\tag{3.2}\]</span></span></p>
<p>We’ll hereafter omit <span class="math inline">\((t)\)</span> to make the notation less cluttered. Variation of capital is investment less depreciation. We assume that investment equals savings and that a constant percentage of income (<span class="math inline">\(i\)</span>) is saved every year. The depreciation rate is denoted by <span class="math inline">\(\delta\)</span>. We then have:</p>
<p><span class="math display">\[
\dot{K} = \frac{d K}{d t} = i Y - \delta K
\]</span></p>
<p>The growth of the capital stock per unit of effective labor <span class="math inline">\(k\)</span> is then:</p>
<p><span class="math display">\[
\dot{k} = \frac{d \frac{K}{AL}}{dt}=\frac{\dot{K}AL - (A\dot{L} +
\dot{A}L)K}{A^2L^2}= \frac{\dot{K}}{AL} -
\frac{K}{AL}\left(\frac{\dot{A}}{A} + \frac{\dot{L}}{L}\right) = (iy - \delta k)- k\left(\frac{\dot{A}}{A} + \frac{\dot{L}}{L}\right)
\]</span></p>
<p>Denoting <span class="math inline">\(n = \frac{\dot{L}}{L}\)</span> and <span class="math inline">\(g = \frac{\dot{A}}{A}\)</span> the growth rates of <span class="math inline">\(L\)</span> and <span class="math inline">\(A\)</span>, i.e., the demographic growth rate and the technological progress rate, we finally have:</p>
<p><span class="math display">\[
\dot{k}(t)=iy(t)-(n+g+\delta)k(t) = ik(t) ^ \kappa - (n+g+\delta)k(t)
\]</span></p>
<p>At the steady state, the growth rate of capital per unit of effective labor is 0. Solving <span class="math inline">\(\dot{k}(t)=0\)</span> we get the steady state value of <span class="math inline">\(k(t)\)</span>, denoted by <span class="math inline">\(k^*\)</span>:</p>
<p><span class="math display">\[
k^* = \left(\frac{i}{n + g+ \delta}\right) ^ \frac{1}{1-\kappa}
\]</span></p>
<p>Or:</p>
<p><span class="math display">\[
\left(\frac{K}{Y}\right)^*= \frac{k^*}{y^*}=k^{*(1-\kappa)} = \frac{i}{n + g+ \delta}
\]</span></p>
<p>From <a href="#eq-production_cobb_douglas">Equation&nbsp;<span>3.1</span></a>, the production per capita is:</p>
<p><span class="math display">\[
\frac{Y(t)}{L(t)} = A(t) k(t) ^ {\kappa}
\]</span></p>
<p>Replacing <span class="math inline">\(k(t)\)</span> by <span class="math inline">\(k^*\)</span>, <span class="math inline">\(A(t)\)</span> by <span class="math inline">\(A(0)e^{gt}\)</span> and taking logs, we get:</p>
<p><span class="math display">\[
\ln\frac{Y(t)}{L(t)} = \ln A(0) + g t   + \frac{\kappa}{1-\kappa} \ln i
- \frac{\kappa}{1 - \kappa} \ln (n + g + \delta)
\]</span></p>
<p>Finally, let’s denote <span class="math inline">\(\ln A(0) = a + \epsilon\)</span>. <span class="math inline">\(\epsilon\)</span> is an error term which represents the initial dispersion between countries in terms of initial value of technical progress. With <span class="math inline">\(C=a + gt\)</span>, and <span class="math inline">\(v = \ln(n + g + \delta)\)</span>, the linear model that we wish to estimate is finally:</p>
<p><span id="eq-solow_equation"><span class="math display">\[
\ln\frac{Y}{L} = C + \frac{\kappa}{1-\kappa} \ln i - \frac{\kappa}{1 - \kappa} \ln v + \epsilon
\tag{3.3}\]</span></span></p>
<p>We therefore get a multiple regression model for which the response is the log of GPD per capita, (<span class="math inline">\(\ln \frac{Y}{L})\)</span> and the two covariates are <span class="math inline">\(\ln i_n\)</span> (the saving rate) and <span class="math inline">\(\ln v_n\)</span> (the sum of the demographic growth, the technical progress and the depreciation rates). Moreover, the structural model imposes some restrictions on the coefficients that can be tested. The two slopes are, in terms of the structural parameters of the theoretical model: <span class="math inline">\(\beta_i = \frac{\kappa}{1-\kappa}\)</span> and <span class="math inline">\(\beta_v = -\frac{\kappa}{1 - \kappa}\)</span>. Moreover, <span class="math inline">\(\kappa\)</span> is the elasticity of the GDP with the capital and also the share of profits in GDP. A common approximation for the value of this parameter is about 1/3, which implies: <span class="math inline">\(\beta_i = - \beta_v = \frac{\kappa}{1-\kappa}=0.5\)</span>.</p>
<p><span class="citation" data-cites="MANK:ROME:WEIL:92">Mankiw, Romer, and Weil (<a href="#ref-MANK:ROME:WEIL:92" role="doc-biblioref">1992</a>)</span> proposed a generalization of the Solow-Swan model that includes human capital, denoted by <span class="math inline">\(H\)</span>. The production function is now:</p>
<p><span class="math display">\[
Y(t) = K(t) ^ \kappa H(t) ^ \lambda \left[A(t)L(t)\right]^{1 - \kappa - \lambda}
\]</span></p>
<p><span class="math inline">\(\lambda\)</span> is the share of human capital in the GDP and the share of labor is now <span class="math inline">\((1 - \kappa - \lambda)\)</span>. The model is very similar to the one we previously developed. We first compute the growth rate of physical and human capital (<span class="math inline">\(\dot{k}\)</span> and <span class="math inline">\(\dot{h}\)</span>) per unit of effective labor, we set these two growth rates to 0 to get the stocks of physical and human capital at the steady state per unit of effective labor (<span class="math inline">\(k^*\)</span> and <span class="math inline">\(h^*\)</span>) and we introduce these two values in the production function to get:</p>
<p><span id="eq-growth_equation"><span class="math display">\[
\ln\frac{Y}{L} = C + \frac{\kappa}{1-\kappa-\lambda} \ln i +
\frac{\lambda}{1-\kappa-\lambda} \ln e - \frac{\kappa + \lambda}{1 - \kappa-\lambda} \ln v + \epsilon
\tag{3.4}\]</span></span></p>
<p>where <span class="math inline">\(e\)</span> is the per capita level of human capital. The model now contains three covariates and three slopes (<span class="math inline">\(\beta_i\)</span>, <span class="math inline">\(\beta_e\)</span> and <span class="math inline">\(\beta_v\)</span>). Moreover, the structural model implies a structural restriction (<span class="math inline">\(\beta_i + \beta_e + \beta_v = 0\)</span>) that is testable.</p>
</section><section id="sec-data_solow_mrw" class="level3"><h3 class="anchored" data-anchor-id="sec-data_solow_mrw">Data set</h3>
<p><code>growth</code> contains the data used by <span class="citation" data-cites="MANK:ROME:WEIL:92">Mankiw, Romer, and Weil (<a href="#ref-MANK:ROME:WEIL:92" role="doc-biblioref">1992</a>)</span>. It consists of 121 countries for 1985.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">growth</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 121 × 9
  country group  gdp60 gdp85 gdpgwth popgwth   inv school  growth
  &lt;chr&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1 Algeria other   2485  4371     4.8   0.026 0.241  0.045  0.565 
2 Angola  lqdata  1588  1171     0.8   0.021 0.058  0.018 -0.305 
3 Benin   lqdata  1116  1071     2.2   0.024 0.108  0.018 -0.0412
# ℹ 118 more rows</code></pre>
</div>
</div>
<p>This data set contains a variable called <code>group</code> which enables the selection of subsamples. The modalities of this variable are:</p>
<ul>
<li>
<code>"oil"</code>: for countries whose most part of the GDP is linked to oil extraction,</li>
<li>
<code>"oecd"</code>: for OECD countries,</li>
<li>
<code>"lqdata"</code>: for countries with low quality data,</li>
<li>
<code>"other"</code>: for other countries.</li>
</ul>
<p>The variables used in the following regressions are per capita GDP in 1985 (<code>gdp85</code>), investment rate (<code>inv</code>) and growth rate of the population (<code>popgwth</code>). To get the variable denoted by <code>v</code> in the previous section, we need to add to the growth rate of the population the technical progress rate and the rate of depreciation. As these two variables are difficult to measure consistently, the authors assume that they don’t exhibit any cross-country variation and that they sum to 5%. Therefore, <code>v</code> equals <code>popgwth + 0.05</code>.</p>
<p>We first investigate the relationship between the two covariates: <code>inv</code> and <code>popgwth</code>. <a href="#fig-invpop">Figure&nbsp;<span>3.1</span></a> presents the scatterplot, the size of the points being proportional to GDP per capita.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">growth</span> <span class="op">%&gt;%</span> <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">popgwth</span>, <span class="va">inv</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>size <span class="op">=</span> <span class="va">gdp85</span>, shape <span class="op">=</span> <span class="va">group</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">stat_ellipse</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_smooth</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-invpop" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="multiple_regression_files/figure-html/fig-invpop-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.1: Investment rate and demographic growth</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>There is a weak negative correlation between the two variables and rich countries are in general characterized by a low demographic growth rate and a high investment rate. We also remark that there is an outlier, Kuwait, which has a very high demographic growth rate. We then compute the variable <code>v</code> and we rename <code>inv</code> and <code>school</code> in <code>i</code> and <code>e</code> as in <a href="#eq-growth_equation">Equation&nbsp;<span>3.4</span></a>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="fu">mutate</span><span class="op">(</span><span class="va">growth</span>, v <span class="op">=</span> <span class="va">popgwth</span> <span class="op">+</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">rename</span><span class="op">(</span>i <span class="op">=</span> <span class="va">inv</span>, e <span class="op">=</span> <span class="va">school</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="sec-comp_ols_mult" class="level2" data-number="3.2"><h2 data-number="3.2" class="anchored" data-anchor-id="sec-comp_ols_mult">
<span class="header-section-number">3.2</span> Computation of the OLS estimator</h2>
<p>In this section, we present the computation of the OLS estimator with a number of covariates <span class="math inline">\(K \geq 2\)</span>. We start with the case where <span class="math inline">\(K=2\)</span>, for which it is possible to compute the estimators using roughly the same notations as the one used for the simple linear regression model. Then we’ll perform the computation in the general case using matrix algebra.</p>
<p>For one observation, denoting <span class="math inline">\(x_{n1}\)</span> and <span class="math inline">\(x_{n2}\)</span> the two covariates for observation <span class="math inline">\(n\)</span>, the model is:</p>
<p><span class="math display">\[
  y_{n}=\alpha+\beta_1 x_{n1}+\beta_2 x_{n2}+\epsilon_{n}
\]</span></p>
<p>Each observation is now a point in the 3-D space defined by <span class="math inline">\((x_1, x_2, y)\)</span> and <span class="math inline">\(\gamma ^ \top = (\alpha, \beta_1, \beta_2)\)</span> are the coordinates of a plane that returns the expected value of <span class="math inline">\(y\)</span> for a given value of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The residual sum of squares is:</p>
<p><span class="math display">\[f(\alpha, \beta) =\sum_{n = 1} ^  N\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right)^2\]</span> which leads to the following three first-order conditions:</p>
<p><span id="eq-multiple_ols_foc"><span class="math display">\[
\left\{
\begin{array}{rcl}
\frac{\partial f}{\partial \alpha} &amp;=&amp; -2 \sum_{n = 1} ^ N
\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right) = 0 \\
\frac{\partial f}{\partial \beta_1}  &amp;=&amp;
-2\sum_{n = 1} ^  N x_{n1}\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right)=0 \\
\frac{\partial f}{\partial \beta_2}  &amp;=&amp;
-2\sum_{n = 1} ^  N
x_{n2}\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right)=0
\end{array}
\right.
\tag{3.5}\]</span></span></p>
<p>Dividing the first line of <a href="#eq-multiple_ols_foc">Equation&nbsp;<span>3.5</span></a> by the sample size, we get:</p>
<p><span id="eq-multiple_ols_mean_point"><span class="math display">\[
\bar{y} - \alpha - \beta_1 \bar{x}_1 - \beta_2 \bar{x}_2 = 0
\tag{3.6}\]</span></span></p>
<p>which means that the sample mean is on the regression plane, or that the sum of the residuals is zero. For the last two lines of <a href="#eq-multiple_ols_foc">Equation&nbsp;<span>3.5</span></a>, the terms in parentheses are the residual for one observation and as its mean is 0, they indicate that the sample covariance between the residuals and both covariates should be 0. Therefore, we get exactly the same conditions as the one obtained for the simple linear regression model. Subtracting <a href="#eq-multiple_ols_mean_point">Equation&nbsp;<span>3.6</span></a> from the last two lines of <a href="#eq-multiple_ols_foc">Equation&nbsp;<span>3.5</span></a>, we get:</p>
<p><span class="math display">\[
\left\{
\begin{array}{l}
\sum_{n = 1} ^  N x_{n1}\left[(y_{n}-\bar{y})-\beta_1(x_{n1}-\bar{x}_1) -
\beta_2(x_{n2} - \bar{x}_2)\right]=0 \\
\sum_{n = 1} ^  N x_{n2}\left[(y_{n}-\bar{y})-\beta_1(x_{n1}-\bar{x}_1) -
\beta_2(x_{n2} - \bar{x}_2)\right]=0 \\
\end{array}
\right.
\]</span> or replacing <span class="math inline">\(x_{nk}\)</span> by <span class="math inline">\(x_{nk} - \bar{x}_k\)</span> and developing terms:</p>
<p><span id="eq-multiple_ols_foc_covariates"><span class="math display">\[
\small{
  \left\{
\begin{array}{lcl}
\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(y_{n} - \bar{y}) &amp;=&amp;
\beta_1\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) ^ 2 +
\beta_2 \sum_{n = 1} ^ {N} (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2) \\
\sum_{n = 1} ^  N
(x_{n2}- \bar{x}_2)(y_{n}-\bar{y}) &amp;=&amp;
\beta_1\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2)  +
\beta_2 \sum_{n = 1} ^  N (x_{n2} - \bar{x}_2) ^ 2  \\
\end{array}
\right.}
\tag{3.7}\]</span></span> </p>
<p>We therefore have a system of two linear equations with two unknown parameters (<span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>) that could be solved, for example, by substitution. However, the use of matrix algebra enables to solve such a problem in a much simpler way. Denote:</p>
<p><span class="math display">\[
X=\left(\begin{array}{cc}
x_{11} &amp; x_{12} \\
x_{21} &amp; x_{22} \\
\vdots &amp; \vdots \\
x_{N1} &amp; x_{N2} \\
\end{array}\right)
\;\;
y=\left(\begin{array}{c}
y_1 \\
y_2  \\
\vdots \\
y_N \\
\end{array}\right)
\;\;
\beta=\left(\begin{array}{c}
\beta_1 \\
\beta_2  \\
\end{array}\right)
\]</span> <span class="math inline">\(X\)</span> is a matrix with two columns (<span class="math inline">\(K\)</span> in the general case) and <span class="math inline">\(N\)</span> lines (the number of observations) and <span class="math inline">\(y\)</span> is a vector of length <span class="math inline">\(N\)</span>. We define <span class="math inline">\(\tilde{I} = I - J / N\)</span> where <span class="math inline">\(I\)</span> is a <span class="math inline">\(N \times N\)</span> identity matrix and <span class="math inline">\(J\)</span> a <span class="math inline">\(N\times N\)</span> matrix of ones. For example, for <span class="math inline">\(N = 3\)</span>:</p>
<p><span class="math display">\[
\tilde{I} = I - J / N =
\left(
\begin{array}{cccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array}
\right) -
\left(
\begin{array}{cccc}
\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} \\
\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} \\
\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3}
\end{array}
\right)
\]</span></p>
<p>Premultiplying a vector <span class="math inline">\(z\)</span> of length <span class="math inline">\(N\)</span> by <span class="math inline">\(J/N\)</span>, we get a vector of length <span class="math inline">\(N\)</span> containing the sample mean of <span class="math inline">\(z\)</span> <span class="math inline">\(\bar{z}\)</span> repeated <span class="math inline">\(N\)</span> times. As premultiplying <span class="math inline">\(z\)</span> by the identity matrix returns <span class="math inline">\(z\)</span>, premultiplying <span class="math inline">\(z\)</span> by <span class="math inline">\(\tilde{I}\)</span> returns a vector of length <span class="math inline">\(N\)</span> containing the <span class="math inline">\(N\)</span> values of <span class="math inline">\(z\)</span> in difference from the sample mean. Note that <span class="math inline">\(\tilde{I}\)</span> is <strong>idempotent</strong>, which means that <span class="math inline">\(\tilde{I} \times \tilde{I}\)</span>. It can be checked by using direct multiplication, but also by reminding that premultiplying a vector by <span class="math inline">\(\tilde{I}\)</span> removes the sample mean from the values of the vector. The transformed vector then has a zero mean, so that applying the same premultiplication one more time will leave it unchanged. Therefore, <span class="math inline">\(\tilde{I}(\tilde{I} z) = \tilde{I}z\)</span>. Denoting <span class="math inline">\(\tilde{z} = \tilde{I} z\)</span>, we get:</p>
<p><span class="math display">\[
\tilde{X}= \tilde{I} X =
\left(\begin{array}{cc}
x_{11} - \bar{x}_1 &amp; x_{12} - \bar{x}_2\\
x_{21} - \bar{x}_1 &amp; x_{22} - \bar{x}_2\\
\vdots &amp; \vdots \\
x_{N1} - \bar{x}_1 &amp; x_{N2} - \bar{x}_2\\
\end{array}\right)
,\;\;
\tilde{y} = \tilde{I} y =\left(\begin{array}{c}
y_1 - \bar{y}\\
y_2 - \bar{y}\\
\vdots \\
y_N - \bar{y}\\
\end{array}\right)
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\tilde{X} ^ \top \tilde{X} =
\left(\begin{array}{cc}
\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1) ^ 2 &amp; \sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2)\\
\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2) &amp; \sum_{n = 1} ^  N (x_{n2} - \bar{x}_2) ^ 2\\
\end{array}
\right)
=
\left(\begin{array}{cc}
S_{11} &amp; S_{12}\\
S_{12} &amp; S_{22}\\
\end{array}
\right)
\]</span> and <span class="math display">\[
\tilde{X}^\top \tilde{y} =
\left(\begin{array}{c}
\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(y_{n}- \bar{y})\\
\sum_{n = 1} ^  N (x_{n2} - \bar{x}_2)(y_{n}- \bar{y})\\
\end{array}
\right)
=
\left(\begin{array}{cc}
S_{1y} \\
S_{2y} \\
\end{array}
\right)
\]</span></p>
<p><span class="math inline">\(S_{kk}\)</span> is the total variation of <span class="math inline">\(x_k\)</span>, <span class="math inline">\(S_{kl}\)</span> the covariation of <span class="math inline">\(x_k\)</span> and <span class="math inline">\(x_l\)</span> and <span class="math inline">\(S_{ky}\)</span> the covariation between covariate <span class="math inline">\(k\)</span> and the response. Note that the quantities <span class="math inline">\(S_{kk}\)</span> and <span class="math inline">\(S_{ky}\)</span> were already present in the simple linear model as <span class="math inline">\(S_{xx}\)</span> and <span class="math inline">\(S_{xy}\)</span> (there are now two of them). The new term is <span class="math inline">\(S_{kl}\)</span>, which measures the correlation between the two covariates.</p>
<p><a href="#eq-multiple_ols_foc_covariates">Equation&nbsp;<span>3.7</span></a> can then be written in matrix form as:</p>
<p><span class="math display">\[
\tilde{X}^\top \tilde{y} = \tilde{X} ^ \top \tilde{X} \beta
\]</span></p>
<p>And the OLS estimator is obtained by premultiplying both sides of the equation by the inverse of <span class="math inline">\(\tilde{X} ^ \top \tilde{X}\)</span>:</p>
<p><span id="eq-mls"><span class="math display">\[
\hat{\beta} = \left(\tilde{X} ^ \top \tilde{X}\right) ^ {- 1} \tilde{X}^\top \tilde{y}
\tag{3.8}\]</span></span></p>
<p>Note that premultiplying the vector <span class="math inline">\(\tilde{X}^\top \tilde{y}\)</span> by the inverse of <span class="math inline">\(\tilde{X} ^ \top \tilde{X}\)</span> is a natural extension of the computation we’ve performed for the simple linear model, which consisted of dividing <span class="math inline">\(S_{xy}\)</span> by <span class="math inline">\(S_{xx}\)</span>. To understand this formula, we write <span class="math inline">\(\tilde{X} ^ \top \tilde{X}\)</span> and <span class="math inline">\(\tilde{X} ^ \top \tilde{y}\)</span> as:</p>
<p><span id="eq-XpX"><span class="math display">\[
\tilde{X} ^ \top \tilde{X} =
\left(\begin{array}{cc}
S_{11} &amp; S_{12}\\
S_{12} &amp; S_{22}\\
\end{array}
\right)
=
N\left(\begin{array}{cc}
\hat{\sigma}_1^2 &amp; \hat{\sigma}_{12}\\
\hat{\sigma}_{12} &amp; \hat{\sigma}_2^2\\
\end{array}
\right)
=
N\hat{\sigma}_1\hat{\sigma}_2
\left(\begin{array}{cc}
\frac{\hat{\sigma}_1}{\hat{\sigma}_2} &amp; \hat{\rho}_{12}\\
\hat{\rho}_{12} &amp; \frac{\hat{\sigma}_2}{\hat{\sigma}_1}\\
\end{array}
\right)
\tag{3.9}\]</span></span></p>
<p>and</p>
<p><span class="math display">\[
X^\top y =
\left(
\begin{array}{cc}
S_{1y} \\
S_{2y} \\
\end{array}
\right)
=
N\left(
\begin{array}{cc}
\hat{\sigma}_{1y} \\
\hat{\sigma}_{2y} \\
\end{array}
\right)
=
N\hat{
\sigma}_y
\left(\begin{array}{cc}
\hat{\sigma}_1\hat{\rho}_{1y} \\
\hat{\sigma}_2\hat{\rho}_{2y} \\
\end{array}
\right)
\]</span></p>
<ul>
<li>the first formulation uses the total sample variations / covariations,</li>
<li>the second one divides every term by <span class="math inline">\(N\)</span> to obtain sample variances and covariances,</li>
<li>the third one divides the covariances by the product of the standard deviations to get sample coefficients of correlation.</li>
</ul>
<p>To compute the estimator, we need to compute the inverse of <span class="math inline">\(\tilde{X} ^ \top \tilde{X}\)</span>, which is:</p>
<p><span id="eq-XpXm1"><span class="math display">\[
\left(\tilde{X} ^ \top \tilde{X}\right) ^ {- 1} =
\frac{
\left(\begin{array}{cc}
S_{22} &amp; -S_{12}\\
-S_{12} &amp; S_{11}\\
\end{array}
\right)}
{S_{11} S_{22} - S_{12} ^ 2}
=
\frac{
\left(\begin{array}{cc}
\hat{\sigma}_2^2 &amp; -\hat{\sigma}_{12}\\
-\hat{\sigma}_{12} &amp; \hat{\sigma}_1^2\\
\end{array}
\right)
}
{N (\hat{\sigma}_1 ^ 2 \hat{\sigma}_2 ^ 2 - \hat{\sigma}_{12}^2)}
=
\frac{
\left(\begin{array}{cc}
\displaystyle\frac{\hat{\sigma}_2}{\hat{\sigma}_1} &amp; - \hat{\rho}_{12}\\
- \hat{\rho}_{12} &amp; \displaystyle\frac{\hat{\sigma}_1}{\hat{\sigma}_2}\\
\end{array}
\right)
}
{N \hat{\sigma}_1 \hat{\sigma}_2 (1 - \hat{\rho}_{12} ^ 2)}
\tag{3.10}\]</span></span></p>
<p><a href="#eq-mls">Equation&nbsp;<span>3.8</span></a> finally gives:</p>
<p><span class="math display">\[
\left\{\begin{array}{l}
\hat{\beta}_1 = \displaystyle
\frac{S_{22}S_{1y} - S_{12}S_{2y}}{S_{11}S_{22} - S_{12} ^ 2} =
\frac{\hat{\sigma}_2 ^ 2 \hat{\sigma}_{1y} - \hat{\sigma}_{12} \hat{\sigma}_{2y}}
{\hat{\sigma}_1 ^ 2 \hat{\sigma}_2 ^ 2 - \hat{\sigma} _ {12} ^ 2}
= \frac{\hat{\rho}_{1y} - \hat{\rho}_{12}\hat{\rho}_{2y}}{1 - \hat{\rho}_{12} ^ 2}
\frac{\hat{\sigma}_y}{\hat{\sigma}_1} \\
\hat{\beta}_2 = \displaystyle
\frac{S_{11}S_{2y} - S_{12}S_{1y}}{S_{11}S_{22} - S_{12} ^ 2} =
\frac{\hat{\sigma}_1 ^ 2 \hat{\sigma}_{2y} - \hat{\sigma}_{12} \hat{\sigma}_{1y}}
{\hat{\sigma}_1 ^ 2 \hat{\sigma}_2 ^ 2 - \hat{\sigma}_{12} ^ 2}
= \frac{\hat{\rho}_{2y} - \hat{\rho}_{12} \hat{\rho}_{1y}}
{1-\hat{\rho}_{12}^2} \frac{\hat{\sigma}_y}{\hat{\sigma}_2}
\\
\end{array}
\right.
\]</span></p>
<p>If the two covariates are uncorrelated in the sample (<span class="math inline">\(S_{12} = \hat{\sigma}_{12} = \hat{\rho}_{12} = 0\)</span>), we have:</p>
<p><span class="math display">\[
\left\{
\begin{array}{l}
\hat{\beta}_1 = \displaystyle
\frac{S_{1y}}{S_{11}} =
\frac{\hat{\sigma}_{1y}}
{\hat{\sigma}_1 ^ 2}
= \hat{\rho}_{1y}
\frac{\hat{\sigma}_y}{\hat{\sigma}_1} \\
\hat{\beta}_2 = \displaystyle
\frac{S_{2y}}{S_{22}} =
\frac{\hat{\sigma}_{2y}}
{\hat{\sigma}_2 ^ 2}
= \hat{\rho}_{2y}
\frac{\hat{\sigma}_y}{\hat{\sigma}_2}
\end{array}
\right.
\]</span></p>
<p>which is exactly the same formula that we had for the unique slope in the case of the simple regression model. This means that, if <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are uncorrelated in the sample, regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span> or on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> leads exactly to the same estimator for the slope of <span class="math inline">\(x_1\)</span>.</p>
<p>The general formula for <span class="math inline">\(\hat{\beta}_1\)</span> in term of the estimator of the simple linear model <span class="math inline">\(\hat{\beta}_1^S\)</span> (<a href="simple_regression.html#eq-slrbeta">Equation&nbsp;<span>1.10</span></a>) is:</p>
<p><span id="eq-mult_simple"><span class="math display">\[
\hat{\beta}_1 =
\frac{\hat{\rho}_{1y} - \hat{\rho}_{12}\hat{\rho}_{2y}}{1 - \hat{\rho}_{12} ^ 2}
\frac{\hat{\sigma}_y}{\hat{\sigma}_1} =
\hat{\beta}_1^s \frac{1 - \displaystyle\frac{\hat{\rho}_{12}\hat{\rho}_{2y}}{\hat{\rho}_{1y}}}
{1 - \hat{\rho}_{12} ^ 2}=
\hat{\beta}_1^s \frac{1 - \hat{\rho}_{12} ^ 2 \displaystyle\frac{\hat{\rho}_{2y}}{\hat{\rho}_{12}\hat{\rho}_{1y}}}
{1 - \hat{\rho}_{12} ^ 2}
\tag{3.11}\]</span></span></p>
<p>We have <span class="math inline">\(\mid\hat{\rho}_{12} \hat{\rho}_{1y}\mid \leq \mid\hat{\rho}_{2y}\mid\)</span>, or <span class="math inline">\(\left| \frac{\hat{\rho}_{2y}}{\hat{\rho}_{12}\hat{\rho}_{1y}}\right| \geq 1\)</span>.</p>
<p>Consider the case where the two covariates are positively correlated with the response. As an example, consider the wage equation with <span class="math inline">\(x_1\)</span> education and <span class="math inline">\(x_2\)</span> a dummy for males. Two cases should then be analyzed:</p>
<ul>
<li>the two covariates are positively correlated (males are more educated than females on average). In this case <span class="math inline">\(\hat{\rho}_{2y}/(\hat{\rho}_{12}\hat{\rho}_{1y})&gt;1\)</span>, and the numerator of <a href="#eq-mult_simple">Equation&nbsp;<span>3.11</span></a> is lower than <span class="math inline">\(1 - \hat{\rho}_{12} ^ 2\)</span>, so that <span class="math inline">\(\hat{\beta}_1&lt;\hat{\beta}_1^S\)</span>. <span class="math inline">\(\hat{\beta}_1 ^ S\)</span> is upward biased because it estimates the sum of the positive direct effect of education on wage and a positive indirect effect (more education leads to a subpopulation with a higher share of males and therefore higher wages),</li>
<li>the two covariates are negatively correlated (males are less educated than females on average). In this case, <span class="math inline">\(\hat{\rho}_{2y}/(\hat{\rho}_{12}\hat{\rho}_{1y})&lt;0\)</span> and the numerator of <a href="#eq-mult_simple">Equation&nbsp;<span>3.11</span></a> is greater than 1, so that <span class="math inline">\(\hat{\beta}_1&gt;\hat{\beta}_1^S\)</span>. <span class="math inline">\(\hat{\beta}_1 ^ S\)</span> is downward biased because it estimates the sum of the positive direct effect of education on wage and a negative indirect effect (more education leads to a subpopulation with a lower share of males and therefore lower wages).</li>
</ul>
<p>The general derivation of the OLS estimator can be performed using matrix algebra, denoting <span class="math inline">\(j_N\)</span> a vector of 1 of length <span class="math inline">\(N\)</span>, <span class="math inline">\(Z = (j_N, X)\)</span> a vector formed by binding a vector of 1 to the matrix of covariates and <span class="math inline">\(\gamma^\top = (\alpha, \beta ^ \top)\)</span> the vector of parameters obtained by adding the intercept <span class="math inline">\(\alpha\)</span> to the vector of slopes:</p>
<p><span class="math display">\[
f(\gamma) = (y - Z\gamma)^\top (y - Z\gamma) = y ^ \top y +  \gamma ^ \top Z
^ \top Z \gamma -2 \gamma ^ \top Z ^ \top y
\]</span></p>
<p>The <span class="math inline">\(K+1\)</span> first-order conditions are:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial \gamma} = 2 Z ^ \top Z \gamma - 2 Z ^ \top
y = -2 Z ^ \top (y - Z\gamma) = 0
\]</span></p>
<p>The last expression indicates that all the columns of <span class="math inline">\(Z\)</span> are orthogonal to the vector of residuals <span class="math inline">\(y - Z\gamma\)</span>. Solving for <span class="math inline">\(\gamma\)</span>, we get:</p>
<p><span id="eq-multiple_ols_gamma"><span class="math display">\[
\hat{\gamma} = (Z ^ \top Z) ^ {- 1} Z ^ \top y
\tag{3.12}\]</span></span></p>
<p>The matrix of second derivatives is:</p>
<p><span class="math display">\[
\frac{\partial^2 f}{\partial \gamma\partial^\top \gamma} =
2 Z ^ \top Z
\]</span></p>
<p>It is a positive semi-definite matrix, so that <span class="math inline">\(\hat{\gamma}\)</span> is a minimum of <span class="math inline">\(f\)</span>. Comparing <a href="#eq-multiple_ols_gamma">Equation&nbsp;<span>3.12</span></a> and <a href="#eq-mls">Equation&nbsp;<span>3.8</span></a>, we can see that the formula of the OLS is the same with different matrices (respectively <span class="math inline">\(\tilde{X}\)</span> and <span class="math inline">\(Z\)</span>), the first equation returning <span class="math inline">\(\hat{\beta}\)</span> and the second one <span class="math inline">\(\hat{\gamma} ^ \top = (\hat{\alpha}, \hat{\beta} ^ \top)\)</span>.</p>
</section><section id="sec-geometry_multiple_ols" class="level2" data-number="3.3"><h2 data-number="3.3" class="anchored" data-anchor-id="sec-geometry_multiple_ols">
<span class="header-section-number">3.3</span> Geometry of least squares</h2>
<section id="geometry-of-the-multiple-regression-model" class="level3"><h3 class="anchored" data-anchor-id="geometry-of-the-multiple-regression-model">Geometry of the multiple regression model</h3>
<p></p>
<p>The geometry of the multiple regression model is presented in <a href="#fig-multregmodel">Figure&nbsp;<span>3.2</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-multregmodel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="./tikz/fig/OLS3D.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.2: Geometry of the multiple regression model</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We now have <span class="math inline">\(N = 3\)</span> and <span class="math inline">\(K = 2\)</span> and therefore each variable is a vector in the 3-D space. As the two covariates <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are linearly independent, they span a subspace of dimension 2, which is a plane in this 3-D space. <span class="math inline">\(\hat{y}\)</span> is the orthogonal projection of <span class="math inline">\(y\)</span> on this subspace and <span class="math inline">\(\hat{\epsilon}\)</span>, is the vector that links <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>. <span class="math inline">\(\hat{\epsilon}\)</span> is therefore the projection of <span class="math inline">\(y\)</span> on the complement to the subspace defined by <span class="math inline">\((x_1, x_2)\)</span>, which is a straight line orthogonal to the plane spanned by <span class="math inline">\((x_1, x_2)\)</span>. Therefore <span class="math inline">\(\hat{\epsilon}\)</span> is orthogonal to <span class="math inline">\(x_1\)</span> and to <span class="math inline">\(x_2\)</span>, which means that the residuals are uncorrelated with the two covariates. The decomposition of <span class="math inline">\(y\)</span> on the sum of two orthogonal vectors <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span> doesn’t depend on the two variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> per se, but on the subspace spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. This means that any couple of independent linear combination of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> will leads to the same subspace as the one defined by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and therefore to the same residuals and the same fitted values.</p>
<p>More formally, as <span class="math inline">\(\hat{\beta} = (X ^ \top X) ^ {-1} X ^ \top y\)</span>, we have <span class="math inline">\(\hat{y} = X \hat{\beta} = X (X ^ \top X) ^ {-1} X ^ \top y = P_X y\)</span>. <span class="math inline">\(P\)</span> is sometimes called the “hat” matrix, as it “puts a hat” on <span class="math inline">\(y\)</span>. This matrix transforms the vector of response on a vector of prediction. As <span class="math inline">\(\hat{\epsilon} = y - \hat{y}\)</span>, we also have <span class="math inline">\(\hat{\epsilon} = y - P_X y = (I-P_X) y=M_Xy\)</span>. We therefore consider two matrices <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span>:</p>
<p><span id="eq-projmatrix"><span class="math display">\[
\left\{
\begin{array}{rcl}
P_X &amp;=&amp; X (X ^ \top X) ^ {-1} X ^ \top \\
M_X &amp;=&amp; I - X (X ^ \top X) ^ {-1} X ^ \top \\
\end{array}
\right.
\tag{3.13}\]</span></span></p>
<p>that are square and symmetric of dimension <span class="math inline">\(N \times N\)</span>, which means that they are in practice large matrices, and are therefore never computed in practice. However, they have very interesting analytical features. First, they are idempotent, which means that <span class="math inline">\(P_X \times P_X = P_X\)</span> and <span class="math inline">\(M_X \times M_X = M_X\)</span>. This means that while premultiplying a vector by such a matrix, this vector is projected in a subspace. For example, premultiplying <span class="math inline">\(y\)</span> by <span class="math inline">\(P_X\)</span> gives <span class="math inline">\(\hat{y}\)</span>, the vector of fitted values. It is a linear combination of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and therefore belongs to the subspace spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. Therefore, <span class="math inline">\(P_X \hat{y}\)</span> obviously equal <span class="math inline">\(\hat{y}\)</span> and therefore <span class="math inline">\(P_X \times P_X = P_X\)</span>. Except for the identity matrix, idempotent matrices are not full rank. Their rank can be easily computed using the fact that the rank of a matrix is equal to its trace (the sum of the diagonal elements) and that the trace of a product of matrices is invariant to any permutation of the matrices: <span class="math inline">\(\mbox{tr} ABC = \mbox{tr} BCA = \mbox{tr} CAB\)</span>.</p>
<p>For a regression with an intercept, the model matrix <span class="math inline">\(Z\)</span> has <span class="math inline">\(K + 1\)</span> column, the first one being a column of one. In this case, the rank of <span class="math inline">\(P_Z\)</span> and <span class="math inline">\(M_Z\)</span> are: <span class="math inline">\(\mbox{rank} \,P_Z = \mbox{tr}\, P_Z = \mbox{tr}\, Z (Z ^ \top Z) ^ {-1} Z ^ \top = \mbox{tr}\, (Z ^ \top Z) ^ {-1} Z ^ \top Z = \mbox{tr}\, I_{K+1} = K + 1\)</span> and <span class="math inline">\(\mbox{rank} \,M_Z = \mbox{tr}\, (I_N - P_X) = \mbox{tr}\, I_N - \mbox{tr}\, P_Z = N - K - 1\)</span>.</p>
<p>Finally, the two matrices are orthogonal: <span class="math inline">\(P_ZM_Z= P_Z(I - P_Z)=P_Z-P_Z=0\)</span>, which means that they perform the projection of a vector on two orthogonal subspaces.</p>
<p>Getting back to <a href="#fig-multregmodel">Figure&nbsp;<span>3.2</span></a>, <span class="math inline">\(P_X\)</span> project <span class="math inline">\(y\)</span> on the 2-D subspace (a plane) spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and <span class="math inline">\(M_X\)</span> project <span class="math inline">\(y\)</span> on a 1-D subspace (the straight line orthogonal to the previous plane). <span class="math inline">\(M_X\)</span> and <span class="math inline">\(P_X\)</span> perform therefore an orthogonal decomposition of <span class="math inline">\(y\)</span> in <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span>, which means that <span class="math inline">\(\hat{y} + \hat{\epsilon} = y\)</span> and that <span class="math inline">\(\hat{y}^\top\hat{\epsilon} = 0\)</span>. </p>
</section><section id="frisch-waugh-theorem" class="level3"><h3 class="anchored" data-anchor-id="frisch-waugh-theorem">Frisch-Waugh theorem</h3>
<p> Consider the regression of <span class="math inline">\(y\)</span> on a set of regressors <span class="math inline">\(X\)</span> which, for some reasons, is separated in two subsets <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Suppose that we are only interested in the coefficients <span class="math inline">\(\beta_2\)</span> associated with <span class="math inline">\(X_2\)</span>. The Frisch-Waugh theorem states that the same estimator <span class="math inline">\(\hat{\beta}_2\)</span> is obtained:</p>
<ul>
<li>by regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>,</li>
<li>by first regressing <span class="math inline">\(y\)</span> and each column of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_1\)</span>, then taking the residuals <span class="math inline">\(M_1y\)</span> and <span class="math inline">\(M_1{X}_2\)</span> of these regressions and finally regressing <span class="math inline">\(M_1y\)</span> on <span class="math inline">\(M_1X_2\)</span>.</li>
</ul>
<p><a href="#fig-Frischwaugh">Figure&nbsp;<span>3.3</span></a> illustrates the Frisch-Waugh theorem.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-Frischwaugh" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="./tikz/fig/frishWaugh.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.3: Frisch-Waugh theorem</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The <strong>first regression</strong> is the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. We then get an orthogonal decomposition of <span class="math inline">\(y\)</span> in the vector of fitted values <span class="math inline">\(\hat{y}=P_{12}y\)</span> and of the residuals <span class="math inline">\(\hat{\epsilon}_{12}=M_{12}y\)</span>. We also show in this figure the decomposition of <span class="math inline">\(\hat{y}\)</span> in <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, which is represented by the sum of the two vectors <span class="math inline">\(\hat{\beta}_1 x_1\)</span> and <span class="math inline">\(\hat{\beta}_2 x_2\)</span>. <span class="math inline">\(\hat{\beta}_2\)</span> is the estimator of <span class="math inline">\(x_2\)</span> on this first regression and is represented by the ratio between <span class="math inline">\(\hat{\beta}_2 x_2\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<p>The <strong>second regression</strong> is the regression of <span class="math inline">\(M_1y\)</span> on <span class="math inline">\(M_1x_2\)</span>. <span class="math inline">\(M_1x_2\)</span> is the residual of the regression of <span class="math inline">\(x_2\)</span> on <span class="math inline">\(x_1\)</span>. Therefore, this vector lies in the line that is in the plane spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and is orthogonal to <span class="math inline">\(x_1\)</span>. <span class="math inline">\(M_1y\)</span> is the residual of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span>; it is therefore orthogonal to <span class="math inline">\(x_1\)</span>.</p>
<p>As both <span class="math inline">\(M_1y\)</span> and <span class="math inline">\(M_{12}y\)</span> are orthogonal to <span class="math inline">\(x_1\)</span>, so is the vector that joins those two vectors. Therefore, this vector is parallel to <span class="math inline">\(M_1x_2\)</span>, and it is therefore the fitted value of the second regression (<span class="math inline">\(M_1y\)</span> on <span class="math inline">\(M_1x_2\)</span>), <span class="math inline">\(P_{M_1x_2}y\)</span>. It is also equal to <span class="math inline">\(\tilde{\beta}_2 M_1x_2\)</span>, <span class="math inline">\(\tilde{\beta}_2\)</span> being the estimator of <span class="math inline">\(x_2\)</span> on the second regression. Note also that <span class="math inline">\(\hat{\epsilon}_{12} = M_{12}y\)</span> is the residual of the second regression, which is therefore the same as the residuals of the first regression.</p>
<p>Finally, consider the regression of <span class="math inline">\(\hat{\beta}_2 x_2\)</span> on <span class="math inline">\(x_1\)</span>. The residual of this regression is <span class="math inline">\(\hat{\beta}_2M_1x_2\)</span>. As it lies on the plane spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and is orthogonal to <span class="math inline">\(x_1\)</span>, it is parallel to <span class="math inline">\(P_{M_1x_2}y=\tilde{\beta}_2M_1x_2\)</span>. Moreover, the Frisch-Waugh theorem states that both vectors have the same length and are therefore identical, which means that <span class="math inline">\(\tilde{\beta}_2 = \hat{\beta}_2\)</span> and that the two regressions give identical estimators.</p>
<p>The Frisch-Waugh is easily demonstrated using some geometric arguments. Consider the regression with all the covariates:</p>
<p><span class="math display">\[
y = X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2 + M_{12}y
\]</span></p>
<p>Then, premultiply both sides of the model by <span class="math inline">\(M_1\)</span>:</p>
<p><span class="math display">\[
M_1 y = M_1 X_1 \hat{\beta}_1 + M_1 X_2 \hat{\beta}_2 + M_1 M_{12}y
\]</span></p>
<p><span class="math inline">\(M_1 X_1 \hat{\beta}_1\)</span> is 0 as <span class="math inline">\(X_1 \hat{\beta}_1\)</span> is obviously in the subset spanned by <span class="math inline">\(X_1\)</span> and therefore its projection on the orthogonal complement is 0. <span class="math inline">\(M_{12} y\)</span> is orthogonal to the subset spanned by <span class="math inline">\(X\)</span> and is therefore also orthogonal to the subset spanned by <span class="math inline">\(X_1\)</span>. Therefore <span class="math inline">\(M_1 M_{12} y = M_{12} y\)</span>. We therefore have:</p>
<p><span class="math display">\[
M_1 y = M_1 X_2 \hat{\beta}_2 + M_{12}y
\]</span></p>
<p>For which the estimation is:</p>
<p><span class="math display">\[
\hat{\beta}_2 = (X_2^\top M_1 X_2) ^ {-1} X_2 ^ \top M_1 y
\]</span></p>
<p>which is exactly the estimation obtained by regressing <span class="math inline">\(M_1 y\)</span> on <span class="math inline">\(M_1 X_2\)</span>. We finally note (an important result that will be used in <a href="#sec-three_tests"><span>Section&nbsp;3.6.3</span></a>), that <span class="math inline">\(\hat{\epsilon}_1=M_1y\)</span>, <span class="math inline">\(\hat{\epsilon}_{12}=M_{12}y\)</span> and <span class="math inline">\(P_{M_1x_2}y\)</span> form a right triangle, <span class="math inline">\(\hat{\epsilon}_1\)</span> being the hypotenuse. Therefore, using the Pythagorean theorem, we have:</p>
<p><span id="eq-pyth_ssr"><span class="math display">\[
\mid\mid \hat{\epsilon}_{12}^2\mid\mid + \mid\mid P_{M_1X_2}y\mid\mid = \mid\mid \hat{\epsilon}_{1}^2\mid\mid
\tag{3.14}\]</span></span> </p>
<div style="page-break-after: always;"></div>
</section></section><section id="sec-computation_R_multiple" class="level2" data-number="3.4"><h2 data-number="3.4" class="anchored" data-anchor-id="sec-computation_R_multiple">
<span class="header-section-number">3.4</span> Computation with R</h2>
<p>To estimate the multiple linear model, we use as for the single linear model the <code>lm</code> function; the difference being that now, on the right side of the formula, we have several variables (here two), separated by the <code>+</code> operator. Actually, formulas have a much richer syntax that includes other operators, for example <code>*</code> and <code>:</code>. This will be discussed in <a href="coefficients.html">Chapter&nbsp;<span>4</span></a>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slw_tot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span>, <span class="va">growth</span><span class="op">)</span></span>
<span><span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">## (Intercept)      log(i)      log(v) </span></span>
<span><span class="co">##      9.6293      1.4780     -0.4573</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="computation-using-matrix-algebra" class="level3"><h3 class="anchored" data-anchor-id="computation-using-matrix-algebra">Computation using matrix algebra</h3>
<p>The estimator can also be computed “by hand”, using matrix algebra. To start, we use the <code>model.frame</code> function which, as <code>lm</code>, has <code>formula</code> and <code>data</code> arguments. For pedagogical purposes, we add the <code>group</code> variable in the formula. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span> <span class="op">+</span> <span class="va">group</span>, <span class="va">growth</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">mf</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">##   log(gdp85) log(i) log(v)  group</span></span>
<span><span class="co">## 1      8.383 -1.423 -2.577  other</span></span>
<span><span class="co">## 2      7.066 -2.847 -2.645 lqdata</span></span>
<span><span class="co">## 3      6.976 -2.226 -2.604 lqdata</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">mf</span><span class="op">)</span></span>
<span><span class="co">## [1] 107</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">growth</span><span class="op">)</span></span>
<span><span class="co">## [1] 121</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>model.frame</code> returns a data frame that contains the data required for estimating the model described in the formula. More precisely, it performs three tasks:</p>
<ul>
<li>it selects only the columns of the initial data frame that are required for the estimation,</li>
<li>it transforms these variables if required, here <code>gdp85</code>, <code>i</code> and <code>v</code> are transformed in logarithms and the columns are renamed accordingly,</li>
<li>it selects only the observations for which there are no missing values for the relevant variables; note here that <code>growth</code> has 121 rows and <code>mf</code> only 107 rows.</li>
</ul>
<p>Several interesting elements of the model can be extracted from the model frame. The <span class="math inline">\(Z\)</span> matrix is obtained using the <code>model.matrix</code> function, which also uses a <code>formula</code>/<code>data</code> interface, the data being the model frame <code>mf</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span> <span class="op">+</span> <span class="va">group</span>, <span class="va">mf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Z</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) log(i) log(v) groupother grouplqdata groupoil
1           1 -1.423 -2.577          1           0        0
2           1 -2.847 -2.645          0           1        0
3           1 -2.226 -2.604          0           1        0</code></pre>
</div>
</div>
<p>Note that the model matrix includes an intercept<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and the <code>group</code> variable, which is a categorical variable is transformed into a set of dummy variables. More precisely, a dummy variable is created for all the modalities except the first one (<code>"oecd"</code>). The response is obtained using the <code>model.response</code> function: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.extract.html">model.response</a></span><span class="op">(</span><span class="va">mf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">y</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">##     1     2     3 </span></span>
<span><span class="co">## 8.383 7.066 6.976</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once the model matrix and the response vector are created, the estimator can easily be computed using matrix operators provided by <code>R</code>. In particular:</p>
<ul>
<li>
<code>%*%</code> is the matrix product operator (<code>*</code> performs an element per element product),</li>
<li>
<code><a href="https://rdrr.io/r/base/t.html">t()</a></code> transposes a matrix,</li>
<li>
<code><a href="https://rdrr.io/r/base/solve.html">solve()</a></code> solves a linear system of equation or computes the inverse of a matrix,</li>
<li>
<code>crossprod</code> takes the inner products of two matrices (or of one matrix and a vector).</li>
</ul>
<p>The most straightforward formula to get the OLS estimator is: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               [,1]
(Intercept) 15.0991
log(i)       0.8332
log(v)       1.6089
groupother  -1.4135
grouplqdata -2.1321
groupoil    -0.8888</code></pre>
</div>
</div>
<p>But <code>crossprod</code> is more efficient, <span class="math inline">\(A^\top B\)</span> being obtained using <code>crossprod(A, B)</code> and <span class="math inline">\(A^\top A\)</span> is either <code>crossprod(A, A)</code> or <code>crossprod(A)</code>. Therefore, <span class="math inline">\(Z^\top Z\)</span> and <span class="math inline">\(Z^\top y\)</span> are respectively obtained using: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span>, <span class="va">y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Moreover, <code>solve</code> can be used to solve a system of linear equations: <code>solve(A, z)</code> compute the vector <span class="math inline">\(w\)</span> such that <span class="math inline">\(Aw=z\)</span>, which is <span class="math inline">\(w=A^{-1}z\)</span>. Therefore, the OLS estimator can be computed using the more efficient and compact following code: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="efficient-computation-qr-decomposition" class="level3"><h3 class="anchored" data-anchor-id="efficient-computation-qr-decomposition">Efficient computation: QR decomposition</h3>
<p> The efficient method used by <code>lm</code> to compute the OLS estimate is the QR decomposition:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> the matrix of covariates <span class="math inline">\(Z\)</span> can be written as the product of an orthonormal matrix <span class="math inline">\(Q\)</span> of dimension <span class="math inline">\(N \times (K + 1)\)</span> (therefore <span class="math inline">\(Q^\top Q = I\)</span>) and an upper triangular matrix <span class="math inline">\(R\)</span> of dimension <span class="math inline">\((K+1) \times (K+1)\)</span>. Then, the linear model can be written as:</p>
<p><span class="math display">\[
y = Z\gamma + \epsilon = QR\gamma + \epsilon = Q \delta + \epsilon
\]</span></p>
<p>with <span class="math inline">\(\delta = R \gamma\)</span>. With <span class="math inline">\(Q\)</span> as the matrix of covariates, the OLS estimator is <span class="math inline">\(\hat{\delta} = (Q ^ \top Q ) ^ {-1} Q ^ \top y = Q^\top y\)</span>, which is obtained without matrix inversion. Then <span class="math inline">\(\hat{y} = Q \hat{\gamma} = Q Q ^ \top y\)</span> and <span class="math inline">\(\hat{\epsilon} = (I - Q Q ^ \top) y\)</span>. With <span class="math inline">\(Z\)</span> as the matrix of covariates, the OLS estimator is <span class="math inline">\(\hat{\gamma} = R ^ {-1} \delta\)</span>. The inverse of a triangular matrix can be very easily obtained with a very high numerical accuracy. Moreover, <span class="math inline">\(\hat{\gamma}\)</span> can be obtained without inverting <span class="math inline">\(R\)</span>. With <span class="math inline">\(K = 2\)</span>, we have:</p>
<p><span class="math display">\[
\left(\begin{array}{c} \delta_0 \\ \delta_1 \\ \delta_2 \end{array}\right) =
\left(
\begin{array}{ccc}
r_{11} &amp; r_{12} &amp; r_{13} \\
0 &amp; r_{22} &amp; r_{23} \\
0 &amp; 0 &amp; r_{33}
\end{array}
\right)
\left(\begin{array}{c} \alpha \\ \beta_1 \\ \beta_2 \end{array}\right)
=
\left(\begin{array}{c}
r_{11} \alpha + r_{12} \beta_1 + r_{13} \beta_2 \\
r_{22} \beta_1 + r_{23} \beta_2 \\
r_{33} \beta_2
\end{array}\right)
\]</span> which can be solved recursively:</p>
<ul>
<li>
<span class="math inline">\(\beta_2 = \delta_2 / r_{33}\)</span>,</li>
<li>
<span class="math inline">\(\beta_1 = (\delta_1 - r_{23}\beta_2) / r_{22}\)</span>,</li>
<li>
<span class="math inline">\(\alpha = (\delta_0 - r_{12} \beta_1 - r_{13} \beta_2) / r_{11}\)</span>.</li>
</ul>
<p>Finally, <span class="math inline">\((X^\top X) ^ {-1} = (R^\top Q^ \top Q R) ^ {-1} = (R ^ \top R) ^{-1} = R ^ {-1} R ^ {- 1 \top}\)</span>.</p>
<p>We illustrate the computation of the OLS estimator using the QR decomposition and using the previously estimated growth model, without the <code>group</code> covariate. The QR decomposition is performed using the <code>qr</code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span>, <span class="va">mf</span><span class="op">)</span></span>
<span><span class="va">qrZ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/qr.html">qr</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>qr</code> returns an object of class <code>qr</code>, and the <code>qr.R</code> and <code>qr.Q</code> function can be used to retrieve the two matrices. We check that <span class="math inline">\(R\)</span> is upper triangular and that <span class="math inline">\(Q^\top Q = I\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/qraux.html">qr.R</a></span><span class="op">(</span><span class="va">qrZ</span><span class="op">)</span></span>
<span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/qraux.html">qr.Q</a></span><span class="op">(</span><span class="va">qrZ</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Q</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]
[1,] 1.000e+00 3.849e-17 8.088e-17
[2,] 3.849e-17 1.000e+00 2.971e-17
[3,] 8.088e-17 2.971e-17 1.000e+00</code></pre>
</div>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) log(i)  log(v)
1      -10.34 19.193 27.1993
2        0.00  5.084 -0.4102
3        0.00  0.000  1.3600</code></pre>
</div>
</div>
<p><span class="math inline">\(\hat{\delta}\)</span> is then obtained as the cross-products of <span class="math inline">\(Q\)</span> and <span class="math inline">\(y\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hdelta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">y</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">drop</span></span>
<span><span class="va">hdelta</span></span>
<span><span class="co">## [1] -83.678   7.702  -0.622</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and <span class="math inline">\(\hat{\gamma}\)</span> is obtained by solving recursively <span class="math inline">\(\hat{\delta} = R \hat{\gamma}\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">beta_2</span> <span class="op">&lt;-</span> <span class="va">hdelta</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span> <span class="op">/</span> <span class="va">R</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">hdelta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">R</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">beta_2</span><span class="op">)</span> <span class="op">/</span> <span class="va">R</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">hdelta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">beta_1</span>  <span class="op">-</span> <span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">beta_2</span><span class="op">)</span> <span class="op">/</span> <span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha</span>, <span class="va">beta_1</span>, <span class="va">beta_2</span><span class="op">)</span></span>
<span><span class="co">## [1]  9.6293  1.4780 -0.4573</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section></section><section id="sec-properties_ols_multiple" class="level2" data-number="3.5"><h2 data-number="3.5" class="anchored" data-anchor-id="sec-properties_ols_multiple">
<span class="header-section-number">3.5</span> Properties of the estimators</h2>
<p>In this section, we’ll briefly analyze the statistical properties of the OLS estimator with more than one covariate. Most of these properties are similar to the one we have described in <a href="simple_regression_properties.html"><span>Chapter&nbsp;2</span></a>. They’ll be presented in this section using matrix algebra.</p>
<section id="unbiasedness-of-the-ols-estimator" class="level3"><h3 class="anchored" data-anchor-id="unbiasedness-of-the-ols-estimator">Unbiasedness of the OLS estimator</h3>
<p></p>
<p>The vector of slopes can be written as a linear combination of the vector of response and then of vector of the errors:</p>
<p><span id="eq-hbeta"><span class="math display">\[
\begin{array}{rcl}
\hat{\beta}&amp;=&amp;(X^\top\tilde{I}X)^{-1}X^\top\tilde{I}y \\
&amp;=&amp;(X^\top\tilde{I}X)^{-1}X^\top\tilde{I}(X\beta+\epsilon)\\
&amp;=&amp;\beta+(X^\top\tilde{I}X)^{-1}X^\top\tilde{I}\epsilon \\
&amp;=&amp;\beta+(\tilde{X}^\top\tilde{X})^{-1}\tilde{X}^\top\epsilon \\
\end{array}
\tag{3.15}\]</span></span></p>
<!-- $\tilde{I} X ^ \top \epsilon = \tilde{X}^\top\epsilon$ is a $K$-length vector containing the product of -->
<!-- every covariates (the column of $X$) in deviation from the sample mean -->
<!-- and the vector of errors: -->
<!-- $$ -->
<!-- \tilde{X}^\top\epsilon = -->
<!-- \left( -->
<!-- \begin{array}{c} -->
<!-- \sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n \\ -->
<!-- \sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n \\ -->
<!-- \vdots \\ -->
<!-- \sum_{n=1} ^ N (x_{n1} - \bar{x}_K) \epsilon_n -->
<!-- \end{array} -->
<!-- \right)  -->
<!-- = \sum_{n = 1} ^ N \psi_n -->
<!-- $$ -->
<!-- $\sum_{n = 1} ^ N \psi_n$, evaluated for $\hat{\beta}$ the vector of -->
<!-- slopes estimates is a K-length vector of 0 (i.e., the vector of the -->
<!-- first-order conditions for minimizing the sum of squares residuals, also -->
<!-- called the vector of scores). -->
<p>The expected value of <span class="math inline">\(\hat{\beta}\)</span> conditional on <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
\mbox{E}(\hat{\beta}\mid X) = \beta +
(\tilde{X}^\top\tilde{X})^{-1}\tilde{X}^\top\mbox{E}(\epsilon \mid X)
\]</span></p>
<div style="page-break-after: always;"></div>
<p>The unbiasedness condition is therefore that <span class="math inline">\(\mbox{E}(\epsilon \mid X) = 0\)</span>, which is a direct generalization of the result obtained for the simple linear regression model, namely <span class="math inline">\(\epsilon\)</span> has a constant expected value (that can be set to 0 without any restriction) whatever the value of the covariates. It implies also that the population covariance between the errors and any of the covariates is 0. </p>
</section><section id="sec-variance_ols" class="level3"><h3 class="anchored" data-anchor-id="sec-variance_ols">Variance of the OLS estimator</h3>
<p>The variance of <span class="math inline">\(\hat{\beta}\)</span> is now a matrix of variances and covariances:</p>
<p><span class="math display">\[\mbox{V}(\hat{\beta}\mid X) =
\mbox{E}\left[(\hat{\beta}- \beta)(\hat{\beta}- \beta)^\top\mid X\right]
\]</span></p>
<p>Using <a href="#eq-hbeta">Equation&nbsp;<span>3.15</span></a>:</p>
<p><span class="math display">\[\mbox{V}(\hat{\beta}\mid X) =
\mbox{E}\left[(\tilde{X} ^ \top\tilde{X})^{-1}\tilde{X}^\top\epsilon \epsilon ^ \top
\tilde{X} (\tilde{X}^\top\tilde{X})^{-1} \mid X\right]
\]</span></p>
<p><span id="eq-general_variance"><span class="math display">\[
\mbox{V}(\hat{\beta}\mid X)=
\frac{1}{N}
\left(\frac{1}{N}
\tilde{X}^\top\tilde{X}\right)^{-1}\left[\frac{1}{N}\mbox{E}(\tilde{X}^\top \epsilon \epsilon ^
\top \tilde{X} \mid X)\right] \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right) ^ {-1}
\tag{3.16}\]</span></span></p>
<p> This is a <strong>sandwich</strong> formula, the <strong>meat</strong>: <span class="math inline">\(\frac{1}{N}\mbox{E}\left(X'\bar{I}\epsilon \epsilon ^ \top \bar{I} X \mid X\right)\)</span> being surrounded by two slices of <strong>bread</strong>: <span class="math inline">\(\left(\frac{1}{N} \tilde{X}^\top\tilde{X}\right)^{-1}\)</span>. Note that the two matrices are square and of dimension <span class="math inline">\(K\)</span>. The bread is just the inverse of the covariance matrix of the covariates. The meat is the variance of the score vector, i.e., the vector of the first-order conditions. For <span class="math inline">\(K = 2\)</span>, it is the expected value of:</p>
<p><span class="math display">\[
\small{
\frac{1}{N}
\left(
\begin{array}{cccc}
\left(\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n)\right) ^ 2 &amp;
\left(\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n)\right)
\left(\sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n)\right)  \\
\left(\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n)\right)
\left(\sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n)\right) &amp;
\left(\sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n)\right) ^ 2
\end{array}
\right)
}
\]</span></p>
<p>which is a generalization of the single regression case where the “meat” reduces to the scalar <span class="math inline">\(\left(\sum_{n=1} ^ N (x_n - \bar{x}) \epsilon_n)\right) ^ 2\)</span>. As for the simple regression model, the formula of the variance simplifies with the hypothesis that the errors are homoskedastic (<span class="math inline">\(\mbox{E}(\epsilon_n ^ 2 \mid x) = \sigma_\epsilon ^ 2\)</span>) and uncorrelated (<span class="math inline">\(\mbox{E}(\epsilon_n \epsilon_m \mid x) = 0 \; \forall \; m \neq n\)</span>). In this case, the meat reduces to <span class="math inline">\(\sigma_\epsilon ^ 2 \frac{1}{N} \tilde{X} ^ \top \tilde{X}\)</span>, i.e., up to a scalar to the matrix of covariance of the covariates, and <a href="#eq-general_variance">Equation&nbsp;<span>3.16</span></a> becomes: </p>
<p><span id="eq-vbeta"><span class="math display">\[
V(\hat{\beta})= \sigma_\epsilon^2(\tilde{X}^\top\tilde{X})^{-1}
\tag{3.17}\]</span></span></p>
<p>which can be rewritten, using <a href="#eq-XpXm1">Equation&nbsp;<span>3.10</span></a>:</p>
<p><span class="math display">\[
\mbox{V}(\hat{\beta})=
\frac{\sigma_\epsilon^2}{N\hat{\sigma}_1\hat{\sigma}_2(1-\hat{\rho}_{12}^2)}
\left(\begin{array}{cc}
\frac{\hat{\sigma}_2}{\hat{\sigma}_1} &amp; -\hat{\rho}_{12}\\
-\hat{\rho}_{12} &amp; \frac{\hat{\sigma}_1}{\hat{\sigma}_2}\\
\end{array}
\right)
\]</span></p>
<p>from which we get:</p>
<p><span id="eq-var_covar_slopes"><span class="math display">\[
\sigma_{\hat{\beta}_k}=
\frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_k\sqrt{1-\hat{\rho}_{12}^2}}
\mbox{ for }k = 1, 2,\;
\hat{\sigma}_{\hat{\beta}_1\hat{\beta}_2}=
-\frac{\hat{\rho}_{12}\sigma_\epsilon^2}{N\hat{\sigma}_1\hat{\sigma}_2(1-\hat{\rho}_{12}^2)}
\mbox{ and } \hat{\rho}_{\hat{\beta}_1\hat{\beta}_2} = -\hat{\rho}_{12}
\tag{3.18}\]</span></span></p>
<p>First remark that if <span class="math inline">\(\hat{\rho}_{12} = 0\)</span>, which means that the two covariates are uncorrelated, the formula for the standard deviation of a slope in the multiple regression model reduces to the formula of the single regression model, which means that the standard deviation is proportional to:</p>
<ul>
<li>the standard deviation of the error,</li>
<li>the inverse of the standard deviation of the corresponding covariate,</li>
<li>the inverse of the square root of the sample size.</li>
</ul>
<p>When the two covariates are correlated, the last term <span class="math inline">\(1/\sqrt{1 - \hat{\rho}_{12}^2}\)</span> is added and inflates the standard deviation. This means that the more the covariates are correlated (whatever the sign of the correlation) the larger is the standard deviation of the slope. The intuition is that, if the two covariates are highly correlated, it is difficult to estimate precisely the separate effect of each of them.</p>
<p><span class="math inline">\(\sigma_\epsilon^2\)</span> being unknown, <span class="math inline">\(\sigma^2_{\hat{\beta}}\)</span> can’t be computed. If the error were observed, a natural estimator of <span class="math inline">\(\sigma_\epsilon^2\)</span> would be <span class="math inline">\(\sum_{n=1}^N (\epsilon_n-\bar{\epsilon}) ^ 2/N\)</span>. As the errors are unknown, one can use the residuals instead, which are related to the errors by the relation: <span class="math inline">\(\hat{\epsilon} = M_Z y = M_Z (Z\gamma + \epsilon) = M_Z \epsilon\)</span>, the last equality standing because <span class="math inline">\(Z\gamma\)</span> is a vector of the subspace defined by the columns of <span class="math inline">\(Z\)</span> and therefore <span class="math inline">\(M_ZZ\gamma = 0\)</span>. Therefore, we have: <span class="math inline">\(\hat{\epsilon} ^ \top \hat{\epsilon} = \epsilon ^ \top M_Z \epsilon\)</span>; as it is a scalar, it is equal to its trace. Using the rule of permutation, we get: <span class="math inline">\(\hat{\epsilon} ^ \top \hat{\epsilon} = \epsilon ^ \top M_Z \epsilon = \mbox{tr}\, M_Z \epsilon \epsilon ^ \top\)</span>. With spherical disturbances, we have <span class="math inline">\(\mbox{E}(\hat{\epsilon} ^ \top \hat{\epsilon}) = \mbox{tr}\, M_Z \sigma_\epsilon ^ 2 I = \sigma_\epsilon ^ 2 \mbox{tr} M_Z = (N - K - 1) \sigma_\epsilon ^ 2\)</span>. Therefore, an unbiased estimator of <span class="math inline">\(\sigma_\epsilon^2\)</span> is:</p>
<p><span id="eq-unbiased_res_se"><span class="math display">\[
\dot{\sigma}_\epsilon^2 = \frac{\hat{\epsilon}^\top\hat{\epsilon}}{N - K - 1} =
\frac{\sum_{n=1}^N \hat{\epsilon}_n ^ 2}{N - K - 1}
\tag{3.19}\]</span></span></p>
<p>and replacing <span class="math inline">\(\sigma_\epsilon ^ 2\)</span> by <span class="math inline">\(\dot{\sigma}_\epsilon ^ 2\)</span> in <a href="#eq-vbeta">Equation&nbsp;<span>3.17</span></a>, we get an unbiased estimator of the covariance matrix of the estimators:</p>
<p><span id="eq-hvbeta"><span class="math display">\[
\hat{V}(\hat{\beta}) = \dot{\sigma}_\epsilon^2(\tilde{X}^\top\tilde{X})^{-1}
\tag{3.20}\]</span></span></p>
<p>We now go back to the estimation of the growth model. As in <span class="citation" data-cites="MANK:ROME:WEIL:92">Mankiw, Romer, and Weil (<a href="#ref-MANK:ROME:WEIL:92" role="doc-biblioref">1992</a>)</span>, we use a restricted sample by excluding countries for which most of the GDP is linked to oil extraction and those with low quality data. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">growth_sub</span> <span class="op">&lt;-</span> <span class="va">growth</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="op">!</span> <span class="va">group</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"oil"</span>, <span class="st">"lqdata"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">slw_tot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span>, <span class="va">growth_sub</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The covariance matrix of the estimators is obtained using the <code>vcov</code> function: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            (Intercept)  log(i)  log(v)
(Intercept)      2.3811 0.13554 0.80956
log(i)           0.1355 0.02922 0.03213
log(v)           0.8096 0.03213 0.28501</code></pre>
</div>
</div>
<p>The <code>summary</code> method computes detailed results of the regression, in particular the table of coefficients, a matrix that can be extracted using the <code>coef</code> method: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)    5.346     1.5431   3.464 8.990e-04
log(i)         1.318     0.1709   7.708 5.383e-11
log(v)        -2.017     0.5339  -3.778 3.224e-04</code></pre>
</div>
</div>
</section><section id="the-ols-estimator-is-blue" class="level3"><h3 class="anchored" data-anchor-id="the-ols-estimator-is-blue">The OLS estimator is BLUE</h3>
<p> The demonstration made in <a href="simple_regression_properties.html#sec-simple_ols_blue"><span>Section&nbsp;2.1.4</span></a> for the simple linear model can easily be extended to the multiple regression model. The OLS estimator is: <span class="math inline">\(\hat{\gamma} = (Z^\top Z)^{-1}Z^\top y = \gamma + (Z^\top Z)^{-1}Z^\top \epsilon\)</span>. Consider another linear estimator:</p>
<p><span class="math display">\[
\tilde{\gamma}=Ay=\left[(Z^\top Z)^{-1}Z^\top + D\right]y = (I + DZ)\gamma + \left[(Z^\top Z)^{-1}Z^\top + D\right]\epsilon
\]</span></p>
<p>The unbiasedness of <span class="math inline">\(\tilde{\gamma}\)</span> implies that <span class="math inline">\(DZ=0\)</span>, so that:</p>
<p><span class="math display">\[
\tilde{\gamma}-\gamma= \left[(Z^\top Z)^{-1}Z^\top + D\right]\epsilon = (\hat{\gamma}- \gamma) + D\epsilon = (\hat{\gamma}- \gamma) + Dy
\]</span> because <span class="math inline">\(DZ=0\)</span> implies that <span class="math inline">\(D\epsilon=Dy\)</span>. Therefore, <span class="math inline">\(\tilde{\gamma}- \hat{\gamma}=Dy\)</span>. The covariance between the OLS estimator and the vector of differences of the two estimators is:</p>
<p><span class="math display">\[
\mbox{E}\left[(\tilde{\gamma}-\hat{\gamma})(\hat{\gamma} - \gamma)^\top\right] = \mbox{E}\left[D\epsilon\epsilon^\top Z(Z^\top Z) ^ {-1}\right]
\]</span> with spherical disturbances, this reduces to:</p>
<p><span class="math display">\[
\mbox{E}\left[(\tilde{\gamma}-\hat{\gamma})(\hat{\gamma} - \gamma)^\top\right] = \sigma_\epsilon ^ 2DZ(Z^\top Z) ^ {-1} =0
\]</span> Therefore, we can write the variance of <span class="math inline">\(\tilde{\gamma}\)</span> as:</p>
<p><span class="math display">\[
\mbox{V}(\tilde{\gamma}) = \mbox{V}\left[\hat{\gamma} + (\tilde{\gamma}-\hat{\gamma})\right] = \mbox{V}(\hat{\gamma}) + \mbox{V}(Dy)
\]</span> as <span class="math inline">\(\mbox{V}(Dy)\)</span> is a covariance matrix, it is semi-definite positive and, therefore, the difference between the variance matrix of any unbiased linear estimator and the OLS estimator is a semi-definite positive matrix, which means that the OLS estimator is the most efficient linear unbiased estimator. </p>
</section><section id="asymptotic-properties-of-the-ols-estimator" class="level3"><h3 class="anchored" data-anchor-id="asymptotic-properties-of-the-ols-estimator">Asymptotic properties of the OLS estimator</h3>
<p> Asymptotic properties of the multiple regression model are direct extensions of those we have seen for the simple regression model. With <span class="math inline">\(\mbox{E}(\hat{\beta}) = \beta\)</span> and <span class="math inline">\(\mbox{V}(\hat{\beta}) = \frac{\sigma_\epsilon ^ 2}{N} \left(\frac{1}{N}\tilde{X}^ \top X\right) ^ {- 1}\)</span>, the OLS estimator is consistent (<span class="math inline">\(\mbox{plim} \;\hat{\beta} = \beta\)</span>) if the covariance matrix of the covariates <span class="math inline">\(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\)</span> converges to a finite matrix. The central-limit theorem implies that:</p>
<p><span class="math display">\[
\sqrt{N}(\hat{\beta}_N - \beta) \xrightarrow{d} \mathcal{N}\left(0,
\sigma_\epsilon ^ 2 \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right)^ {-1}\right)
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\hat{\beta}_N \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_\epsilon ^ 2}{N} \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right)^ {-1}\right)
\]</span> </p>
</section><section id="the-coefficient-of-determination" class="level3"><h3 class="anchored" data-anchor-id="the-coefficient-of-determination">The coefficient of determination</h3>
<p>From <a href="simple_regression.html#eq-r2_slrm">Equation&nbsp;<span>1.14</span></a>, one way to write the coefficient of determination for the simple linear regression model is:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{\sum_{n=1} ^ N \hat{\epsilon}_n^2}{\sum_{n=1} ^ N (y_n - \hat{y})^2} = 1 - \frac{\hat{\sigma}_\epsilon^2}{\hat{\sigma}_y^2}
\]</span></p>
<p>The last term is obtained by dividing the residual sum of squares and the total variation of <span class="math inline">\(y\)</span> by the sample size <span class="math inline">\(N\)</span>. This gives rise to biased estimators of the variances of the errors and of the response. Consider now the unbiased estimators: the numerator should then be divided by <span class="math inline">\(N - K - 1\)</span> and <span class="math inline">\(\dot{\sigma}_\epsilon ^ 2\)</span> is obtained. Similarly, the denominator should be divided by <span class="math inline">\(N-1\)</span> to obtain an unbiased estimation of the variance of the response. We then obtain the <strong>adjusted coefficient of determination</strong>:</p>
<p><span class="math display">\[
\bar{R}^2 = 1 - \frac{\dot{\sigma}_\epsilon^2}{\dot{\sigma}_y^2}= 1 - \frac{\sum_{n=1} ^ N \hat{\epsilon}_n^2 / (N-K-1)}{\sum_{n=1} ^ N (y_n - \hat{y})^2 / (N-1)} =
1 - \frac{N - 1}{N - K - 1}\frac{\sum_{n=1} ^ N \hat{\epsilon}_n^2}{\sum_{n=1} ^ N (y_n - \hat{y})^2}
\]</span> <span class="math inline">\(R^2\)</span> necessarily increases when one more covariate is added to the regression because, even if this covariate is irrelevant, its sample correlation with the response will never be exactly 0 and therefore the sum of square will decrease. This is not the case with <span class="math inline">\(\bar{R}^2\)</span> because <span class="math inline">\(\dot{\sigma}^2_\epsilon\)</span> is the ratio of two terms which both increase when a covariate is added. Note also that <span class="math inline">\(\bar{R}^2\)</span> is not necessarily positive.</p>
<div style="page-break-after: always;"></div>
</section></section><section id="sec-confint_test_multiple" class="level2" data-number="3.6"><h2 data-number="3.6" class="anchored" data-anchor-id="sec-confint_test_multiple">
<span class="header-section-number">3.6</span> Confidence interval and test</h2>
<p></p>
<p>In <a href="simple_regression_properties.html#sec-confint_test_slm"><span>Section&nbsp;2.3</span></a>, we have seen how to compute a confidence interval for a single parameter and how to perform a test for one hypothesis. The confidence interval was a segment, i.e., a range of values that contains the true value of the parameter with a given probability, and the tests were performed using a normal or a Student distribution. Of course, the same kind of analysis can be performed with a multiple regression. But, in this latter case:</p>
<ul>
<li>a confidence interval can also be computed for several coefficients one at a time,</li>
<li>tests of multiple hypotheses can be performed, using a chi-squared or a Fisher distribution.</li>
</ul>
<p>To illustrate these points, we’ll use the Solow model. Remember that the model to estimate is:</p>
<p><span class="math display">\[
\ln y = \alpha + \beta_i \ln i + \beta_v \ln v + \epsilon
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the per capita gdp, <span class="math inline">\(i\)</span> the investment rate and <span class="math inline">\(v\)</span> the sum of the labor force growth rate, the depreciation rate and the technical progress rate. Moreover, the relation between <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_v\)</span> and the structural parameter <span class="math inline">\(\kappa\)</span>, which is the share of profits in the GDP is <span class="math inline">\(\beta_i = - \beta_v =\kappa / (1 -\kappa)\)</span>. We’ll analyze the confidence interval for the couple of coefficients <span class="math inline">\((\beta_i, \beta_v)\)</span> and we’ll test two hypotheses:</p>
<ul>
<li>the first is imposed by the model; we must have <span class="math inline">\(\beta_i + \beta_v = 0\)</span>,</li>
<li>the second corresponds to a reasonable value of the share of profits, which is approximately one-third; therefore, we’ll test the hypothesis that <span class="math inline">\(\kappa = 1/3\)</span>, which implies that <span class="math inline">\(\beta_i = 0.5\)</span>.</li>
</ul>
<p>Note that these two hypotheses imply that <span class="math inline">\(\beta_v=-0.5\)</span>.</p>
<section id="simple-confidence-interval-and-test" class="level3"><h3 class="anchored" data-anchor-id="simple-confidence-interval-and-test">Simple confidence interval and test</h3>
<p>The asymptotic distribution of the estimators is a multivariate normal distribution.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The distribution of one estimator (say <span class="math inline">\(\hat{\beta}_1\)</span>)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> is a univariate normal distribution with, for the two covariates case, a standard deviation equal to: <span class="math inline">\(\sigma_{\hat{\beta}_1} = \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_1\sqrt{1 - \hat{\rho}_{12} ^ 2}}\)</span>. Therefore <span class="math inline">\((\hat{\beta}_1 - \beta_1) / \sigma_{\hat{\beta}_1}\)</span> follows (exactly if <span class="math inline">\(\epsilon\)</span> is normal) a standard normal distribution. Confidence interval and tests for a coefficient are therefore computed exactly as for the case of the simple linear regression model. In particular, using <a href="#eq-var_covar_slopes">Equation&nbsp;<span>3.18</span></a> and <a href="#eq-unbiased_res_se">Equation&nbsp;<span>3.19</span></a>, we get:</p>
<p><span id="eq-student_multiple"><span class="math display">\[
t_k = \frac{\hat{\beta}_k - \beta_k}{\hat{\sigma}_{\hat{\beta}_k}} =
\frac{\hat{\beta}_k - \beta_k}{\frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_1\sqrt{1 -\hat{\rho}_{12}^2}}}
\tag{3.21}\]</span></span></p>
<p>that follows exactly a Student distribution with <span class="math inline">\(N-K-1\)</span> degrees of freedom if the errors are normal and asymptotically a normal distribution whatever the distribution of the errors. Therefore, <span class="math inline">\(1 - \alpha\)</span> confidence intervals are: <span class="math inline">\(\hat{\beta}_k \pm \mbox{cv}_{1-\alpha / 2} \dot{\sigma}_{\hat{\beta}_k}\)</span> where <span class="math inline">\(\mbox{cv}_{1-\alpha / 2}\)</span> is the critical value of either a Student or a normal distribution. For a given hypothesis: <span class="math inline">\(H_0:\beta_k = \beta_{k0}\)</span>, <span class="math inline">\(t_{k0}=(\hat{\beta}_k - \beta_{k0})/\hat{\sigma}_{\hat{\beta}_k}\)</span> is a draw on a normal or a Student distribution if <span class="math inline">\(H_0\)</span> is true.</p>
<p>Remember that “reasonable” values of the two slopes in our growth model should be <span class="math inline">\(+/-0.5\)</span>. We check whether these values are in the confidence intervals, using the <code>confint</code> function: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">slw_tot</span>, level <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              2.5 % 97.5 %
(Intercept)  2.2698  8.422
log(i)       0.9768  1.658
log(v)      -3.0814 -0.953</code></pre>
</div>
</div>
<p><span class="math inline">\(0.5\)</span> and <span class="math inline">\(-0.5\)</span> are not in the 95% confidence interval for respectively <span class="math inline">\(\hat{\beta}_i\)</span> and <span class="math inline">\(\hat{\beta}_v\)</span>. The hypotheses that the true values of the parameters are 0 is very easy to compute, as they are based on the t statistics that are routinely returned by the <code>summary</code> method for <code>lm</code> objects. But, in our example, the hypothesis that <span class="math inline">\(\beta_i = 0\)</span> is the hypothesis that the share of profits is 0, which is of little interest. More interestingly, we could check the hypothesis that the coefficients are equal to <span class="math inline">\(\pm 0.5\)</span>. In this case, we can “manually” compute the test statistics by extracting the relevant elements in the matrix returned by <code>coef(summary(x))</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">v</span> <span class="op">&lt;-</span> <span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="op">(</span><span class="va">v</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="op">-</span> <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">v</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="co">## log(i) log(v) </span></span>
<span><span class="co">##  4.783 -2.842</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which confirms that both hypotheses are rejected. The same kind of linear hypothesis on one coefficient can be simpler tested by using a different parametrization of the same model. The trick is to write the model in such a way that a subset of coefficients are 0 if the hypothesis is true. For example, in order to test the hypothesis that <span class="math inline">\(\beta_i = 0.5\)</span>, we must have in the model <span class="math inline">\((\beta_i - 0.5)\ln i\)</span>; therefore, the term <span class="math inline">\(-0.5\ln i\)</span> is added on the right side of the formula and should therefore be added also on the left side. Adding also <span class="math inline">\(0.5 \ln v\)</span> on both sides of the formula, we finally get:</p>
<p><span class="math display">\[
(\ln y - 0.5 \ln i + 0.5 \ln v) = \alpha + (\beta_i - 0.5) \ln i + (\beta_v + 0.5) \ln v + \epsilon
\]</span></p>
<p>and the two slopes are now equal to 0 under <span class="math inline">\(H_0\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slw_totb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">v</span> <span class="op">/</span> <span class="va">i</span><span class="op">)</span> <span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span>, <span class="va">growth_sub</span><span class="op">)</span></span>
<span><span class="va">slw_totb</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)   5.3459     1.5431   3.464 8.990e-04
log(i)        0.8176     0.1709   4.783 8.924e-06
log(v)       -1.5172     0.5339  -2.842 5.830e-03</code></pre>
</div>
</div>
<p>which gives exactly the same values for the <span class="math inline">\(t\)</span> statistics.</p>
<p>A simple hypothesis may also concern a linear combination of several coefficients and not the value of one coefficient. For example, the structural growth model implies that <span class="math inline">\(\beta_i + \beta_v = 0\)</span>. If the hypothesis is true, <span class="math inline">\(\mbox{E}(\hat{\beta}_i + \hat{\beta}_v) = 0\)</span>, the variance being <span class="math inline">\(\mbox{V}(\hat{\beta}_i + \hat{\beta}_v) = \hat{\sigma}_{\hat{\beta}_1}^2 + \hat{\sigma}_{\hat{\beta}_2}^2 + 2 \hat{\sigma}_{\hat{\beta}_1\hat{\beta}_2}\)</span>, the statistic can then be computed by extracting the relevant elements of the covariance matrix of the fitted model: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">v</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span></span>
<span><span class="va">v_sum</span> <span class="op">&lt;-</span> <span class="va">v</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="va">v</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">v</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">stat_sum</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span><span class="va">t_sum</span> <span class="op">&lt;-</span> <span class="va">stat_sum</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">v_sum</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span><span class="va">pval_sum</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">t_sum</span><span class="op">)</span>, df <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span>, </span>
<span>                   lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="va">stat_sum</span>, t <span class="op">=</span> <span class="va">t_sum</span>, pv <span class="op">=</span> <span class="va">pval_sum</span><span class="op">)</span></span>
<span><span class="co">##    stat       t      pv </span></span>
<span><span class="co">## -0.6996 -1.1372  0.2592</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The hypothesis is therefore not rejected, even at the 10% level. Once again, such a linear hypothesis can be more easily tested using a different parametrization. Introducing in the model the term <span class="math inline">\((\beta_i + \beta_v) \ln i\)</span>, subtracting <span class="math inline">\(\beta_v \ln_i\)</span> and rearranging terms:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\ln y &amp;=&amp; \alpha + (\beta_i + \beta_v) \ln i + \beta_v \ln v -
\beta_v \ln i+ \epsilon \\
&amp; = &amp; \alpha + (\beta_i + \beta_v) \ln i + \beta_v
\ln \frac{v}{i} + \epsilon
\end{array}
\]</span> We then have a model for which the two covariates are now <span class="math inline">\(\ln i\)</span> and <span class="math inline">\(\ln v / i\)</span>, the hypothesis being that the coefficient associated to <span class="math inline">\(\ln i\)</span> is equal to 0. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slw_totc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span> <span class="op">/</span> <span class="va">i</span><span class="op">)</span>, <span class="va">growth_sub</span><span class="op">)</span></span>
<span><span class="va">slw_totc</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)   5.3459     1.5431   3.464 0.0008990
log(i)       -0.6996     0.6152  -1.137 0.2592063
log(v/i)     -2.0172     0.5339  -3.778 0.0003224</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">

</div>
</section><section id="joint-confidence-interval-and-test-of-joint-hypothesis" class="level3"><h3 class="anchored" data-anchor-id="joint-confidence-interval-and-test-of-joint-hypothesis">Joint confidence interval and test of joint hypothesis</h3>
<section id="joint-confidence-interval" class="level4"><h4 class="anchored" data-anchor-id="joint-confidence-interval">Joint confidence interval</h4>
<p>We now consider the computation of confidence interval for more than one parameter. The distribution of <span class="math inline">\(\hat{\beta}\)</span> is:</p>
<p><span class="math display">\[
\hat{\beta} \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_\epsilon ^ 2}{N} \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right)^ {-1}\right)
\]</span></p>
<p>In the simple regression model, subtracting the expected value and dividing by the standard deviation, we get a standard normal deviate. Taking the square, we get a <span class="math inline">\(\chi^2\)</span> with 1 degree of freedom. If the <span class="math inline">\(K\)</span> slopes are uncorrelated, <span class="math inline">\(\sum_{k=1} ^ K(\hat{\beta}_k - \beta_{k0}) ^ 2/\sigma_{\hat{\beta}_k} ^ 2\)</span> is a <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(K\)</span> degrees of freedom. If the slopes are correlated, this correlation should be “corrected”; more precisely, a quadratic form of the vector of slopes in deviation from its expectation with the inverse of its variance should be computed:</p>
<p><span class="math display">\[
q_K = (\hat{\beta}-\beta) ^ \top
\mbox{V}(\hat{\beta})^{-1} (\hat{\beta}-\beta)=
(\hat{\beta}-\beta) ^ \top \frac{N}{\sigma_\epsilon ^ 2}
\left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right) (\hat{\beta}-\beta)
\sim \chi^2_K
\]</span> <!-- Note that, if $K=1$, $\hat{\beta}- \beta$ and $\mbox{V}(\hat{\beta})$ are scalars and the expression reduce to $(\hat{\beta}- \beta) / \sigma_{\hat{\beta}}^2$, which is a square of the t statistic. --></p>
<p>For <span class="math inline">\(K=2\)</span>, we have:</p>
<p><span class="math display">\[
q_2 = \left(\hat{\beta}_1 - \beta_1, \hat{\beta}_2 - \beta_2 \right)
\frac{N}{\sigma_\epsilon ^ 2}
\left(
\begin{array}{cc}
\hat{\sigma}_1 ^ 2 &amp; \hat{\sigma}_{12} \\ \
\hat{\sigma}_{12} &amp; \hat{\sigma}_2 ^ 2
\end{array}
\right)
\left(
\begin{array}{c}
\hat{\beta}_1 - \beta_1 \\
\hat{\beta}_2 - \beta_2
\end{array}
\right)
\]</span></p>
<p><span id="eq-ellipsebeta"><span class="math display">\[
q_2 = \frac{N}{\sigma_\epsilon ^ 2}
\left[
\hat{\sigma}_1^2(\hat{\beta}_1 - \beta_1) ^ 2+
\hat{\sigma}_2^2(\hat{\beta}_2 - \beta_2) ^ 2+
2 \hat{\sigma}_{12}(\hat{\beta}_1 - \beta_1)(\hat{\beta}_2 - \beta_2)
\right]
\tag{3.22}\]</span></span></p>
<p>Using <a href="#eq-student_multiple">Equation&nbsp;<span>3.21</span></a>, this expression can also be rewritten in terms of the Student statistics <span class="math inline">\(t_k\)</span>:</p>
<p><span id="eq-ellipse"><span class="math display">\[
q_2 = \frac{1}{1 - \hat{\rho}_{12} ^ 2}\left(t_1 ^ 2 + t_2 ^ 2 +
2 \hat{\rho}_{12} t_1 t_2\right) =
\frac{1}{1 - \hat{\rho}_{\hat{\beta}_1\hat{\beta}_2} ^ 2}\left(t_1 ^ 2 + t_2 ^ 2 -
2 \hat{\rho}_{\hat{\beta}_1\hat{\beta}_2} t_1 t_2\right)
\tag{3.23}\]</span></span></p>
<p>the last equality resulting from the fact that the coefficient of correlation of two slopes is the opposite of the coefficient of correlation of the corresponding covariates (see <a href="#eq-var_covar_slopes">Equation&nbsp;<span>3.18</span></a>). Therefore, a <span class="math inline">\((1-\alpha)\)</span> confidence interval for a couple of coefficients <span class="math inline">\((\beta_1, \beta_2)\)</span> is the set of values for which <a href="#eq-ellipse">Equation&nbsp;<span>3.23</span></a> is lower than the critical value for a <span class="math inline">\(\chi ^ 2\)</span> with 2 degrees of freedom, which is, for example, 5.99 for two degrees of freedom and <span class="math inline">\(1-\alpha = 0.95\)</span>. Equating <a href="#eq-ellipse">Equation&nbsp;<span>3.23</span></a> to this critical value, we get the equation of an ellipse, with two particular nested cases:</p>
<ul>
<li>if <span class="math inline">\(\hat{\rho}_{12} = 0\)</span>, i.e., if the two covariates and therefore the two coefficients are uncorrelated, the expression reduces to the sum of squares of the t statistics, which is an ellipse with vertical and horizontal tangents,</li>
<li>if <span class="math inline">\(\hat{\rho}_{12} = 0\)</span> and <span class="math inline">\(\hat{\sigma}_1 = \hat{\sigma}_2\)</span>, the expression reduces further to the equation of a circle.</li>
</ul>
<p><a href="#eq-ellipse">Equation&nbsp;<span>3.23</span></a> can’t be computed as it depends on <span class="math inline">\(\sigma_\epsilon\)</span> which is unknown. Replacing <span class="math inline">\(\sigma_\epsilon\)</span> by <span class="math inline">\(\dot{\sigma}_\epsilon\)</span>, we get:</p>
<p><span id="eq-ellipse_est"><span class="math display">\[
\hat{q}_2 = \frac{1}{1 - \hat{\rho}_{12} ^ 2}\left(\hat{t}_1 ^ 2 + \hat{t}_2 ^ 2 +
2 \hat{\rho}_{12} \hat{t}_1 \hat{t}_2\right)
\tag{3.24}\]</span></span></p>
<p>where <span class="math inline">\(\hat{q}_2\)</span> now follows asymptotically a <span class="math inline">\(\chi ^ 2\)</span> distribution with 2 degrees of freedom. If the errors are normal, dividing by <span class="math inline">\(K\)</span> (here 2), we get an exact Fisher <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(2\)</span> and <span class="math inline">\(N - K - 1\)</span> degrees of freedom. In the simple case of no correlation between the covariates, the <span class="math inline">\(\chi^2\)</span> and the <span class="math inline">\(F\)</span> statistics are therefore the sum and the mean of the squares of the t statistics. As for the Student distribution, which converges in distribution to a normal distribution, <span class="math inline">\(K \times\)</span> the <span class="math inline">\(F\)</span> statistic converges in distribution to a <span class="math inline">\(\chi ^ 2\)</span> with <span class="math inline">\(K\)</span> degrees of freedom. For example, with <span class="math inline">\(K=2\)</span>, the critical value for a <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(2\)</span> and <span class="math inline">\(\infty\)</span> degrees of freedom is half the corresponding <span class="math inline">\(\chi^2\)</span> value with 2 degrees of freedom (5.99 for the 95% confidence level), which is <span class="math inline">\(2.996\)</span>. The confidence interval is represented in <a href="#fig-ellipse">Figure&nbsp;<span>3.4</span></a>; the circle point is the point estimation, the vertical and horizontal segments are the separate confidence intervals for both coefficients at the 95% level. We’ve also added a diamond point that corresponds to the hypothesis <span class="math inline">\(\kappa = 1/3\)</span>, which implies that <span class="math inline">\(\beta_i = - \beta_v = \frac{\kappa}{1-\kappa} = 0.5\)</span>. Finally, we add a line with a slope equal to <span class="math inline">\(-1\)</span> and an intercept equal to 0 which corresponds to the hypothesis that <span class="math inline">\(\beta_i = - \beta_v\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ellipse" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="multiple_regression_files/figure-html/fig-ellipse-1.png" class="img-fluid figure-img" style="width:30.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.4: Ellipse of confidence for the two coefficients</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The confidence ellipse is a “fat sausage”<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> with the long part of the sausage slightly oriented in the lower-left/upper-right direction. This is because, as we have seen previously (see <a href="#fig-invpop">Figure&nbsp;<span>3.1</span></a>), the two covariates exhibit a small negative correlation, which implies a small positive correlations between <span class="math inline">\(\hat{\beta}_i\)</span> and <span class="math inline">\(\hat{\beta}_v\)</span>. The ellipse is also “higher” than “wide”, because <span class="math inline">\(v\)</span> has a smaller variance than <span class="math inline">\(i\)</span> and therefore <span class="math inline">\(\hat{\beta}_v\)</span> has a larger variance than <span class="math inline">\(\hat{\beta}_i\)</span>. Note the difference between a set of two simple hypotheses and a joint hypothesis. Consider for example:</p>
<ul>
<li>
<span class="math inline">\((\beta_i = 1.1, \beta_v = -1)\)</span> represented by a square point: both simple hypotheses are not rejected (the two values are in the unidimensional confidence interval), but the joint hypothesis is rejected, as the corresponding square point is outside the 95% confidence interval ellipse,</li>
<li>
<span class="math inline">\((\beta_i = 1.25, \beta_v = -3.1)\)</span> represented by a triangle point: the simple hypothesis <span class="math inline">\(\beta_i = 1.25\)</span> is not rejected, the simple hypothesis <span class="math inline">\(\beta_b = -3.1\)</span> is rejected, but the joint hypothesis is not rejected, the triangle point being inside the 95% confidence interval ellipse.</li>
</ul>
<p>The hypothesis that the two coefficients sum to 0 is not rejected as some points of the straight line that figures this hypothesis are in the confidence ellipse. Concerning the hypothesis that <span class="math inline">\(\beta_i = 0.5\)</span> and <span class="math inline">\(\beta_v = -0.5\)</span>, the two simple hypotheses and the joint hypothesis are rejected at the 95% confidence level; the two estimates are neither in the segments representing the simple confidence interval nor inside the ellipse figuring the joint confidence interval.</p>
<p></p>
</section><section id="joint-hypothesis" class="level4"><h4 class="anchored" data-anchor-id="joint-hypothesis">Joint hypothesis</h4>
<p> To test a joint hypothesis for the values of a couple of parameters <span class="math inline">\((\beta_{10}, \beta_{20})\)</span>, we have just seen that we can simply check whether the corresponding point is inside or outside the confidence interval ellipse. We can also compute the statistic given in <a href="#eq-ellipse_est">Equation&nbsp;<span>3.24</span></a> for <span class="math inline">\((\beta_1=\beta_{10}, \beta_2=\beta_{20})\)</span> and compare it to the critical value.</p>
<p>The statistic is computed using the elements of the matrix returned by <code>coef(summary(x))</code>, which contains in particular the estimations and their standard deviations. We first compute the t statistics corresponding to the two simple hypothesis <span class="math inline">\(\beta_i = 0.5\)</span> and <span class="math inline">\(\beta_v = -0.5\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sc</span> <span class="op">&lt;-</span> <span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="va">t_i</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">sc</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">/</span> <span class="va">sc</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">t_v</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">sc</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">/</span> <span class="va">sc</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then apply the simplified formula, which is the sum or the mean of the squares of the t statistics:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">St2</span> <span class="op">&lt;-</span> <span class="va">t_i</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">t_v</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="va">Mt2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">t_i</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">t_v</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">St2</span>, <span class="va">Mt2</span><span class="op">)</span></span>
<span><span class="co">## [1] 30.95 15.47</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>for which the distributions under <span class="math inline">\(H_0\)</span> are respectively <span class="math inline">\(\chi ^ 2\)</span> and <span class="math inline">\(F\)</span>, with the following critical values: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">qchisq</a></span><span class="op">(</span><span class="fl">0.95</span>, df <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">## [1] 5.991</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">qf</a></span><span class="op">(</span><span class="fl">0.95</span>, df1 <span class="op">=</span> <span class="fl">2</span>, df2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## [1] 3.124</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The critical values being much smaller than the computed statistics, the joint hypothesis is clearly rejected. The exact formula corrects the correlation between the two estimators using the coefficient of correlation between the two covariates. We compute this coefficient using the data frame of the fitted model, which is obtained using the <code>model.frame</code> function. Using the initial data frame <code>growth</code> wouldn’t give the correct value, as the estimation is not performed on the full data set because of missing data and because the estimation is performed on a subsample with some groups of countries that are excluded: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">slw_tot</span><span class="op">)</span></span>
<span><span class="va">r_iv</span> <span class="op">&lt;-</span> <span class="fu">summarise</span><span class="op">(</span><span class="va">mf</span>, r <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">`log(i)`</span>, <span class="va">`log(v)`</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">r</span><span class="op">)</span></span>
<span><span class="va">r_iv</span></span>
<span><span class="co">## [1] -0.352</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the names of the covariates are not regular names as they contain parentheses, therefore they should be surrounded by the sign. The coefficient of correlation between the two covariates is <span class="math inline">\(-0.352\)</span>. We obtain:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Fstat</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">t_i</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">t_v</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">r_iv</span> <span class="op">*</span> <span class="va">t_i</span> <span class="op">*</span> <span class="va">t_v</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">r_iv</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="va">Fstat</span></span>
<span><span class="co">## [1] 23.13</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The statistic (23.125) is slightly greater than the approximative value (15.475) which was previously computed; we therefore reject once again the null hypothesis. </p>
</section></section><section id="sec-three_tests" class="level3"><h3 class="anchored" data-anchor-id="sec-three_tests">The three tests</h3>
<p></p>
<!-- !!! clarifier les notations Z vs X -->
<p>In the previous subsection, we started with a general (unconstrained) model, we constructed a confidence ellipse for the two parameters, and we were able to test a set of hypotheses, either by checking whether the values of the parameters corresponding to the hypotheses were inside the confidence ellipse, or by computing the value of the statistic for the tested values of the parameters. Actually, this testing principle, based on the unconstrained model, is just one way of testing a hypothesis. The geometry of least squares and the Frisch-Waugh theorem highlights the fact that any set of hypotheses can be tested using the fact that this set of hypotheses gives rise to two models: a <strong>constrained model</strong>, which imposes the hypotheses and an <strong>unconstrained model</strong>, which doesn’t impose the hypotheses. The same test can be performed using the constrained model, the unconstrained model or both, which give rise to three test principles:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<ul>
<li>
<strong>Wald</strong> test is based only on the unconstrained model,</li>
<li>
<strong>lagrange multiplier</strong> or <strong>score</strong> test is based on the constrained model,</li>
<li>
<strong>likelihood ratio</strong> test is based on the comparison between the two models.</li>
</ul>
<p>A set of <span class="math inline">\(J\)</span> linear hypotheses is written as:</p>
<p><span class="math display">\[
R \gamma = q
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is a matrix of dimension <span class="math inline">\(J \times (K + 1)\)</span> and <span class="math inline">\(q\)</span> is a vector of length <span class="math inline">\(J\)</span>. <span class="math inline">\(J\)</span>, the number of hypotheses, is necessarily lower or equal to <span class="math inline">\(K\)</span>. Actually, a set of <span class="math inline">\(J\)</span> hypotheses can always be rewritten as a model of the form <span class="math inline">\(y = X_1\beta_1 + X_2\beta_2 + \epsilon\)</span>, the hypothesis being <span class="math inline">\(\mbox{H}_0: \beta_2 = 0\)</span>. In this setting, the three tests are easily constructed using the Frisch-Waugh theorem, with <span class="math inline">\(H_0: \beta_2 = 0\)</span>.</p>
<section id="wald-test" class="level4"><h4 class="anchored" data-anchor-id="wald-test">Wald test</h4>
<p></p>
<p>The Wald test is based on the unconstrained model, for which a vector of slopes <span class="math inline">\(\hat{\beta}_2\)</span> is estimated. Using the Frisch-Waugh theorem, this vector can be obtained as the regression of the residuals of <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> (<span class="math inline">\(M_1y\)</span>) on the residuals of every column of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_1\)</span> (<span class="math inline">\(M_1X_2\)</span>). Then, <span class="math inline">\(\hat{\beta}_2 = (X_2^\top M_1 X_2) ^ {-1} X_2^\top M_1 y\)</span>, with expected value and variance equal to <span class="math inline">\(\beta_2\)</span> (0 under <span class="math inline">\(H_0\)</span>) and <span class="math inline">\(\mbox{V}(\hat{\beta}_2) = \sigma_\epsilon^2 (X_2^\top M_1 X_2) ^ {-1}\)</span>. Convergence in distribution implies that:</p>
<p><span class="math display">\[
\hat{\beta}_2 \stackrel{a}{\sim} \mathcal{N} \left(0, \mbox{V}(\hat{\beta}_2)\right)
\]</span></p>
<p>The distribution of the quadratic form of a centered vector of normal random variables of length <span class="math inline">\(J\)</span> with the inverse of its covariance matrix is a <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(J\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\mbox{wald} = \hat{\beta}_2 ^ \top \mbox{V}(\hat{\beta}_2) ^ {-1}
\hat{\beta}_2
=
\frac{\hat{\beta}_2 ^ \top
(X_2^\top M_1 X_2)
\hat{\beta}_2}
{\sigma_\epsilon^2}
\]</span></p>
<p>which is also, replacing <span class="math inline">\(\hat{\beta}_2\)</span> by its expression:</p>
<p><span class="math display">\[
\mbox{wald} =
\frac{y ^ \top M_1 X_2 (X_2^\top M_1 X_2) ^ {-1} X_2^\top M_1 y}
{\sigma_\epsilon^2} =
\frac{y ^ \top P_{M_1X_2} y}
{\sigma_\epsilon^2}
\]</span> </p>
</section><section id="lagrange-multiplier-or-score-test" class="level4"><h4 class="anchored" data-anchor-id="lagrange-multiplier-or-score-test">Lagrange multiplier or score test</h4>
<p></p>
<p>Consider the constrained model, which imposes <span class="math inline">\(\beta_2 = 0\)</span>. It is therefore obtained by regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> only, and the vector of residuals is <span class="math inline">\(\hat{\epsilon}_1 = M_1 y\)</span>. The idea of this test is that, if <span class="math inline">\(H_0\)</span> is true, <span class="math inline">\(X_2 ^ \top \hat{\epsilon}_1\)</span> should be close to zero, <span class="math inline">\(\hat{\epsilon}_1\)</span> being “almost” orthogonal to the subspace spanned by <span class="math inline">\(X_2\)</span>. Therefore, we consider the vector <span class="math inline">\(X_2 ^ \top \hat{\epsilon}_1 = X_2 ^ \top M_1 y\)</span>, which, under <span class="math inline">\(H_0\)</span>, should have a 0 expected value. The variance of this vector is: <span class="math inline">\(\sigma_\epsilon ^ 2 (X_2^\top M_1X_2)^{-1}\)</span> so that, applying the central-limit theorem:</p>
<p><span class="math display">\[
X_2 ^ \top \hat{\epsilon}_1 \equiv X_2 ^ \top M_1 y \stackrel{a}{\sim} \mathcal{N}
\left(0, \sigma_\epsilon ^ 2 X_2 ^ \top M_1 X_2\right)
\]</span></p>
<p>The statistic is obtained, as previously, by computing the quadratic form:</p>
<p><span class="math display">\[
\mbox{score}=y ^ \top M_1 X_2  \left(\sigma_\epsilon ^ 2 X_2 ^ \top M_1
X_2\right) ^ {-1} X_2 ^ \top M_1 y=
\frac{y ^ \top M_1 X_2 (X_2^\top M_1 X_2) ^ {-1} X_2^\top M_1 y}
{\sigma_\epsilon^2} =
\frac{y ^ \top P_{M_1X_2} y}
{\sigma_\epsilon^2}
\]</span> </p>
</section><section id="likelihood-ratio-test" class="level4"><h4 class="anchored" data-anchor-id="likelihood-ratio-test">Likelihood ratio test</h4>
<p></p>
<p>The likelihood ratio test is based on the comparison of the objective function (the sum of square residuals) for the constrained and the unconstrained model. Remember, from <a href="#eq-pyth_ssr">Equation&nbsp;<span>3.14</span></a>, that:</p>
<p><span class="math display">\[
\mid\mid \hat{\epsilon}_{12}^2\mid\mid + \mid\mid P_{M_1X_2}y\mid\mid = \mid\mid \hat{\epsilon}_{1}^2\mid\mid
\]</span></p>
<p>The first term on the left and the term on the right are residual sums of squares (respectively for the unconstrained and the constrained model). Therefore, the likelihood ratio test is based on: <span class="math inline">\(\mbox{SSR}_c - \mbox{SSR}_{nc} = ||P_{M_1X_2}y|| ^ 2 = y ^ \top P_{M_1X_2} y\)</span>. Dividing by <span class="math inline">\(\sigma_\epsilon^2\)</span>, we get exactly the same statistic as previously.</p>
<p> </p>
</section></section><section id="computation-of-the-three-tests" class="level3"><h3 class="anchored" data-anchor-id="computation-of-the-three-tests">Computation of the three tests</h3>
<p>Consider the augmented Solow model: <span class="math inline">\(y = \alpha + \beta_i i + \beta_v v + \beta_e e + \epsilon\)</span>, with <span class="math inline">\(\beta_i = \frac{\kappa}{1 - \kappa - \lambda}\)</span>, <span class="math inline">\(\beta_v = -\frac{\kappa + \lambda}{1 - \kappa - \lambda}\)</span> and <span class="math inline">\(\beta_e = \frac{\lambda}{1 - \kappa - \lambda}\)</span>. For convenience, we compute the logarithm of the variables before using <code>lm</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">growth_sub2</span> <span class="op">&lt;-</span> <span class="va">growth_sub</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp85</span><span class="op">)</span>, i <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span>, v <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span>, e <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">mrw2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">i</span> <span class="op">+</span> <span class="va">v</span> <span class="op">+</span> <span class="va">e</span>, <span class="va">growth_sub2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We consider two hypotheses:</p>
<ul>
<li>
<span class="math inline">\(\beta_i + \beta_v + \beta_e = 0\)</span>, this hypothesis is directly implied by the structural model,</li>
<li>
<span class="math inline">\(\kappa = 1 /3\)</span>, the share of profits has a reasonable value.</li>
</ul>
<p>The second hypothesis implies that <span class="math inline">\(\beta_i = \frac{1/3}{2/3 - \lambda}\)</span> and <span class="math inline">\(\beta_e = \frac{\lambda}{2/3 - \lambda}\)</span>, and therefore that <span class="math inline">\(\beta_e = 2 \beta_i - 1\)</span>. The model can be reparametrized in such way that two slopes are 0 if the two hypotheses are satisfied:</p>
<p><span class="math display">\[
y + e - v = \alpha + \beta_i (i + 2e - 3v) + (\beta_e-2\beta_i+1)(e-v)+(\beta_i + \beta_e+\beta_v)v
\]</span></p>
<p>Therefore, the unconstrained model can be written as a model with <span class="math inline">\(y + e - v\)</span> as the response and <span class="math inline">\(i + 2 e - 3 v\)</span>, <span class="math inline">\(e-v\)</span> and <span class="math inline">\(v\)</span> as the three covariates and the constrained model as a model with the same response but with <span class="math inline">\(i + 2 e - 3\)</span> as the unique covariate. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">growth_sub2</span> <span class="op">&lt;-</span> <span class="va">growth_sub2</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>y2 <span class="op">=</span> <span class="va">y</span> <span class="op">+</span> <span class="va">e</span> <span class="op">-</span> <span class="va">v</span>, i2 <span class="op">=</span> <span class="va">i</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">e</span> <span class="op">-</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">v</span>, e2 <span class="op">=</span> <span class="va">e</span> <span class="op">-</span> <span class="va">v</span><span class="op">)</span></span>
<span><span class="va">mrw_c</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y2</span> <span class="op">~</span> <span class="va">i2</span>,         <span class="va">growth_sub2</span><span class="op">)</span></span>
<span><span class="va">mrw_nc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y2</span> <span class="op">~</span> <span class="va">i2</span> <span class="op">+</span> <span class="va">e2</span> <span class="op">+</span> <span class="va">v</span>, <span class="va">growth_sub2</span><span class="op">)</span></span>
<span><span class="va">coef_nc</span> <span class="op">&lt;-</span> <span class="va">mrw_nc</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="va">coef_nc</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)  7.79131     1.1924  6.5340 8.301e-09
i2           0.70037     0.1506  4.6510 1.488e-05
e2           0.32981     0.3611  0.9133 3.642e-01
v           -0.06886     0.4654 -0.1480 8.828e-01</code></pre>
</div>
</div>
<p>The two hypotheses can be tested one by one, as the test is that:</p>
<ul>
<li>the slope of <span class="math inline">\(e2 = e-v\)</span> equals 0 for the hypothesis that <span class="math inline">\(\kappa = 1/3\)</span>,</li>
<li>the slope of <span class="math inline">\(v\)</span> equals 0 for the hypothesis that <span class="math inline">\(\beta_i+\beta_e+\beta_v=0\)</span>.</li>
</ul>
<p>Both hypotheses are not rejected, even at the 10% level. To test the joint hypothesis, we can first use the approximate formula which is the sum or the mean of the squares of the t statistics (respectively <span class="math inline">\(\chi^2\)</span> with 2 degrees of freedom and F with 2 and 71 degrees of freedom): </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">appr_chisq</span> <span class="op">&lt;-</span> <span class="va">coef_nc</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">coef_nc</span><span class="op">[</span><span class="fl">4</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="va">appr_f</span> <span class="op">&lt;-</span> <span class="va">appr_chisq</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">appr_chisq</span>, <span class="va">appr_f</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.856 0.428</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">appr_chisq</span>, df <span class="op">=</span> <span class="fl">2</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.6518</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">pf</a></span><span class="op">(</span><span class="va">appr_f</span>, df1 <span class="op">=</span> <span class="fl">2</span>, df2 <span class="op">=</span> <span class="fl">71</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.6535</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Based on this approximation, the joint hypothesis is clearly not rejected. We now turn to the computation of the statistics, using the three test principles.</p>
<section id="sec-wald_test_example" class="level4"><h4 class="anchored" data-anchor-id="sec-wald_test_example">Wald test</h4>
<p></p>
<p>Considering the initial unconstrained model, for which the formula is <span class="math inline">\(y \sim i + v + e\)</span>, the set of the two hypotheses can be written in matrix form as:</p>
<p><span class="math display">\[
\left(
\begin{array}{cccc}
0 &amp; 2 &amp; 0 &amp; - 1 \\
0 &amp; 1 &amp; 1 &amp;   1 \\
\end{array}
\right)
\left(
\begin{array}{c}
\alpha \\ \beta_i \\ \beta_v \\ \beta_e
\end{array}
\right) =
\left(
\begin{array}{c}
1 \\ 0
\end{array}
\right)
\]</span> The matrix <span class="math inline">\(R\)</span> and the vector <span class="math inline">\(q\)</span> are created in <strong>R</strong>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">4</span><span class="op">)</span> ; <span class="va">q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">2</span> ; <span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">4</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fl">1</span> ; <span class="va">R</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then have <span class="math inline">\(H_0: R\gamma - q = 0\)</span> and, for the fitted unconstrained model, we get: <span class="math inline">\(R\hat{\gamma} - q\)</span>. Under <span class="math inline">\(H_0\)</span> the expected value of this vector is 0 and its estimated variance is <span class="math inline">\(R^\top \hat{V}_{\hat{\gamma}}R=\hat{\sigma}_\epsilon ^ 2R^\top(Z^\top Z)^{-1}R\)</span>. Then:</p>
<p><span id="eq-wald_test_formula"><span class="math display">\[
(R\hat{\gamma} - q)^\top \left[R \hat{V}_{\hat{\gamma}}R ^ \top\right]^{-1}(R\hat{\gamma} - q)
\tag{3.25}\]</span></span></p>
<p>is asymptotically a <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(J=2\)</span> degrees of freedom. Dividing by <span class="math inline">\(J\)</span>, we get a F statistic with <span class="math inline">\(2\)</span> and <span class="math inline">\(71\)</span> degrees of freedom. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mrw2</span><span class="op">)</span> <span class="op">-</span> <span class="va">q</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">mrw2</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span>  </span>
<span>  <span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mrw2</span><span class="op">)</span> <span class="op">-</span> <span class="va">q</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="co">##        [,1]</span></span>
<span><span class="co">## [1,] 0.4233</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">car::linearHypothesis</a></code> performs Wald tests with a nice syntax: the first argument is a fitted model, and the second one is a vector of characters which contains the character representation of the hypothesis: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">mrw2</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"i + v + e = 0"</span>, <span class="st">"e = 2 * i - 1"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## F = 0.423, df: 2-71, pval = 0.657</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section><section id="likelihood-ratio-test-1" class="level4"><h4 class="anchored" data-anchor-id="likelihood-ratio-test-1">Likelihood ratio test</h4>
<p></p>
<p>The computation of the likelihood ratio statistic is very simple once the two models have been estimated. The residual sums of squares of the two models are extracted using the <code>deviance</code> method, and we divide the difference of the two sums of squares by <span class="math inline">\(\dot{\sigma}_\epsilon^2\)</span> (the <code>sigma</code> method is used to extract the residual standard error) and by 2 to get an F statistic. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">mrw_c</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">mrw2</span><span class="op">)</span><span class="op">)</span><span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">mrw2</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="co">## [1] 0.4233</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p></p>
</section><section id="score-test" class="level4"><h4 class="anchored" data-anchor-id="score-test">Score test</h4>
<p></p>
<p>For the score test, we consider the reparametrized model and we define <span class="math inline">\(Z_1\)</span> as a matrix containing a column of one and <span class="math inline">\(i + 2 e - 3 v\)</span> and <span class="math inline">\(X_2\)</span> as a matrix containing <span class="math inline">\(e-v\)</span> and <span class="math inline">\(v\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Z1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="op">~</span> <span class="va">i2</span>, <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">mrw_nc</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="op">~</span> <span class="va">e2</span> <span class="op">+</span> <span class="va">v</span> <span class="op">-</span> <span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">mrw_nc</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The test is based on the vector <span class="math inline">\(X_2 ^ \top \hat{\epsilon}_c\)</span>, where <span class="math inline">\(\hat{\epsilon}_c\)</span> is the vector of the residuals for the constrained model. Under <span class="math inline">\(H_0\)</span>, the expected value of <span class="math inline">\(X_2 ^ \top \hat{\epsilon}_c\)</span> is 0 and its variance is <span class="math inline">\(\sigma_\epsilon ^ 2 X_2^ \top M_1 X_2\)</span>. <span class="math inline">\(M_1 X_2\)</span> is a matrix of residuals of all the columns of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(Z_1\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">mrw_c</span><span class="op">)</span></span>
<span><span class="va">M1X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">X2</span> <span class="op">~</span> <span class="va">Z1</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we’ve used <code>lm</code> with only a formula argument. In this case, the response and the covariates are vectors or matrices and not columns of a tibble. Note also that the left-hand side of a formula is a matrix and not a vector; in this case, each column is supposed to be a response and <code>lm</code> fit as many models than there are columns in this matrix. Then, the <code>resid</code> method no longer returns a vector, but a matrix, each column being a vector of residuals for one of the fitted models.</p>
<p>The statistic is then <span class="math inline">\(\hat{\epsilon}_c^\top X_2 \left[X_2^ \top M_1 X_2\right]^{-1} X_2 \hat{\epsilon}_c / \sigma_\epsilon^2\)</span> and it is computed using an estimator of <span class="math inline">\(\sigma_\epsilon^2\)</span>, and dividing by <span class="math inline">\(J=2\)</span> to get an F statistic. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">X2</span>, <span class="va">ec</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">M1X2</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">X2</span>, <span class="va">ec</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">mrw_c</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="co">##        [,1]</span></span>
<span><span class="co">## [1,] 0.4301</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The statistic is slightly different from the one computed previously, the difference being only due to the fact that the estimation of <span class="math inline">\(\sigma_\epsilon\)</span> is based, for the score test, on the constrained model.</p>
<p></p>
</section></section><section id="testing-that-all-the-slopes-are-0" class="level3"><h3 class="anchored" data-anchor-id="testing-that-all-the-slopes-are-0">Testing that all the slopes are 0</h3>
<p> The test that all the slopes are 0 is routinely reported by software performing OLS estimation. It can be computed using any of the three test principles, but the likelihood ratio test is particularly appealing in this context, the constrained model being a model with only an intercept: <span class="math inline">\(y_n = \alpha + \epsilon_n\)</span>. In this case, <span class="math inline">\(\hat{\alpha} = \bar{y}\)</span> and <span class="math inline">\(\hat{\epsilon} = (y_n - \bar{y})\)</span>. Therefore, the residual sum of squares for the constrained model is <span class="math inline">\(S_{yy} = \sum_n (y_n - \bar{y}) ^ 2\)</span> and is also denoted by <span class="math inline">\(\mbox{TSS}\)</span> for total sum of squares. The statistic is then: <span class="math inline">\((\mbox{TSS} - \mbox{RSS})/\sigma_\epsilon ^ 2 \sim \chi ^ 2_K\)</span>, which is a <span class="math inline">\(\chi ^ 2\)</span> with <span class="math inline">\(K\)</span> degrees of freedom if the hypothesis that all the slopes are 0 is true. To compute this statistic, <span class="math inline">\(\sigma_\epsilon^2\)</span> has to be estimated. A natural estimator is <span class="math inline">\(\mbox{RSS} / (N - K - 1)\)</span> but, if <span class="math inline">\(H_0\)</span> is true, <span class="math inline">\(\mbox{TSS} / (N - 1)\)</span> is also an unbiased estimator. Moreover, dividing by the sample size (<span class="math inline">\(N\)</span>) and not by the number of degrees of freedom leads to biased but consistent estimators. Using the first estimator of <span class="math inline">\(\sigma_\epsilon^2\)</span> and dividing by <span class="math inline">\(K\)</span>, we get the <span class="math inline">\(F\)</span> statistic with <span class="math inline">\(K\)</span> and <span class="math inline">\(N-K-1\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\frac{\mbox{TSS} -\mbox{RSS}}{\mbox{RSS}}\frac{N - K - 1}{K} \sim F_{K, N-K-1}
\]</span></p>
<p>Using <span class="math inline">\(\mbox{TSS} / N\)</span> as an estimator of <span class="math inline">\(\sigma_\epsilon^2\)</span>, we get a very simple statistic that is asymptotically a <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(K\)</span> degrees of freedom:</p>
<p><span class="math display">\[
N \frac{\mbox{TSS} -\mbox{RSS}}{\mbox{TSS}}  \stackrel{a}{\sim} \chi^2_{K}
\]</span></p>
<p>These two statistics are closely related to the <span class="math inline">\(R^2\)</span> which is, using this notation, equal to <span class="math inline">\(1 - \mbox{RSS} / \mbox{TSS} = (\mbox{TSS} - \mbox{RSS}) / \mbox{TSS}\)</span>. We can then write the <span class="math inline">\(F\)</span> statistic as:</p>
<p><span class="math display">\[
\frac{R^2}{1 - R ^ 2}\frac{N - K - 1}{K}
\]</span> and the asymptotic <span class="math inline">\(\chi^2\)</span> statistic as <span class="math inline">\(N R^2\)</span>.</p>
<p>There is no easy way to extract the <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(F\)</span> statistic with <strong>R</strong>. Both are computed by the <code>summary</code> method of <code>lm</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slm</span> <span class="op">&lt;-</span> <span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">summary</span></span>
<span><span class="va">slm</span> <span class="op">%&gt;%</span> <span class="va">names</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "call"          "terms"         "residuals"     "coefficients" 
 [5] "aliased"       "sigma"         "df"            "r.squared"    
 [9] "adj.r.squared" "fstatistic"    "cov.unscaled" </code></pre>
</div>
</div>
<p>and can be extracted “manually” from the list returned by <code>summary</code> using the <code>$</code> operator:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slm</span><span class="op">$</span><span class="va">r.squared</span></span>
<span><span class="co">## [1] 0.5989</span></span>
<span><span class="va">slm</span><span class="op">$</span><span class="va">adj.r.squared</span></span>
<span><span class="co">## [1] 0.5878</span></span>
<span><span class="va">slm</span><span class="op">$</span><span class="va">fstatistic</span></span>
<span><span class="co">## value numdf dendf </span></span>
<span><span class="co">## 53.76  2.00 72.00</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>More simply, <code>micsr:rsq</code> and <code>micsr:ftest</code> can be used: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">rsq</span></span>
<span><span class="co">## [1] 0.5989</span></span>
<span><span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="fu">rsq</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"adj"</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.5878</span></span>
<span><span class="va">slw_tot</span> <span class="op">%&gt;%</span> <span class="va">ftest</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## F = 53.755, df: 2-72, pval = 0.000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The interest of this testing strategy is not limited to the test that all the slopes of a real model are 0. It can also be used to test any set of hypotheses, using reparametrization and the Frisch-Waugh theorem. Consider for example the hypothesis that <span class="math inline">\(\kappa = 0.5\)</span> and <span class="math inline">\(\beta_i + \beta_e + \beta_v=0\)</span>. We have seen previously that, after reparametrization, this corresponds to the model: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mrw_nc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y2</span> <span class="op">~</span> <span class="va">i2</span> <span class="op">+</span> <span class="va">e2</span> <span class="op">+</span> <span class="va">v</span>, <span class="va">growth_sub2</span><span class="op">)</span></span>
<span><span class="va">mrw_nc</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">## (Intercept)          i2          e2           v </span></span>
<span><span class="co">##     7.79131     0.70037     0.32981    -0.06886</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>with, if the two hypotheses are true, the two slopes associated with <code>e2</code> and <code>v</code> equal to 0. Now, using the Frisch-Waugh theorem, and denoting <span class="math inline">\(Z_1 = (j, i_2)\)</span> and <span class="math inline">\(X_2 = (e_2, v)\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">y2b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y2</span> <span class="op">~</span> <span class="va">i2</span>, <span class="va">growth_sub2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">resid</span></span>
<span><span class="va">e2b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">e2</span> <span class="op">~</span> <span class="va">i2</span>, <span class="va">growth_sub2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">resid</span></span>
<span><span class="va">vb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">v</span> <span class="op">~</span> <span class="va">i2</span>, <span class="va">growth_sub2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">resid</span></span>
<span><span class="va">mrw_ncb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y2b</span> <span class="op">~</span> <span class="va">e2b</span> <span class="op">+</span> <span class="va">vb</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">mrw_ncb</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">##      e2b       vb </span></span>
<span><span class="co">##  0.32981 -0.06886</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We get exactly the same estimators as previously for <code>e2</code> and <code>v</code>, but now the joint hypothesis is that all the slopes of the second model are 0. Therefore, the test is based on the F statistic that is returned by <code>summary(lm(x))</code> and doesn’t require any further calculus:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mrw_ncb</span> <span class="op">%&gt;%</span> <span class="va">ftest</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## F = 0.435, df: 2-73, pval = 0.649</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Actually, a degrees of freedom correction should be performed to get exactly the same results because <code>mrw_ncb</code> has <span class="math inline">\(75 - 2 = 73\)</span> degrees of freedom, as the real number of degrees of freedom is <span class="math inline">\(75 - 4 = 71\)</span>.</p>
<p> </p>
</section></section><section id="sec-system_equation" class="level2" data-number="3.7"><h2 data-number="3.7" class="anchored" data-anchor-id="sec-system_equation">
<span class="header-section-number">3.7</span> System estimation and constrained least squares</h2>
<p> Very often in economics, the phenomenon under investigation is not well described by a single equation, but by a system of equations. Moreover, there may be inter-equations constraints on the coefficients. It is particularly the case in the field of the microeconometrics of consumption or production. For example, the behavior of a producer is described by a minimum cost equation along with equations of factor demand and the behavior of a consumer is described by a set of demand equations.</p>
<section id="sec-sys_eq_ols" class="level3"><h3 class="anchored" data-anchor-id="sec-sys_eq_ols">System of equations</h3>
<p>We consider therefore a system of <span class="math inline">\(L\)</span> equations denoted by <span class="math inline">\(y_l=Z_l\beta_l+\epsilon_l\)</span>, with <span class="math inline">\(l=1\ldots L\)</span>. In matrix form, the system can be written as follows:</p>
<p><span id="eq-system_equation"><span class="math display">\[
\left(
  \begin{array}{c}
    y_1 \\ y_2 \\ \vdots \\ y_L
  \end{array}
\right)
=
\left(
  \begin{array}{ccccc}
    Z_1 &amp; 0 &amp; \ldots &amp; 0 \\
    0 &amp; Z_2 &amp; \ldots &amp; 0 \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; \ldots &amp; Z_L
  \end{array}
\right)
\left(
  \begin{array}{c}
    \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_L
  \end{array}
\right)
+
\left(
  \begin{array}{c}
    \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_L
  \end{array}
\right)
\tag{3.26}\]</span></span></p>
<p>Therefore, the whole system can be estimated directly by stacking the vector of responses and by constructing a block-diagonal matrix of covariates, each block being the matrix of covariates for one equation.</p>
<p>As an example, consider the analysis of production characteristics (returns to scale, elasticities of substitution between pairs of inputs). The modern approach of production analysis consists of first considering the minimum cost function, which depends on the level of production and on input unit prices <span class="math inline">\(C(y, p_1, \ldots p_J)\)</span> and then of deriving the demands for input using Shepard Lemma:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial p_j} = x_j(y, p_1, \ldots, p_J)
\]</span></p>
<p>The cost function is obviously homogeneous of degree 1 in input unit prices, which means that, for a given level of input, if all the prices increase proportionally, the quantity of the different inputs are the same and therefore the cost function increases by the same percentage. This writes: <span class="math inline">\(C(y, \lambda p_1, \ldots \lambda p_J) = \lambda C(y, p_1, \ldots, p_J)\)</span> and <span class="math inline">\(x_j(y, \lambda p_1, \ldots, \lambda p _J) = x_j(y, p_1, \ldots p_J)\)</span>; the latter relation indicating that the demands for input are homogeneous of degree 0 in input unit prices. Among the different functional forms that have been proposed to estimate the cost function, the translog specification is the most popular. It can be considered as the second-order approximation of a general cost function:</p>
<p><span class="math display">\[
\ln C = \alpha + \beta_y \ln y + \frac{1}{2}\beta_{yy} \ln^2 y +\sum_i \beta_i \ln p_i + \frac{1}{2}
\sum_i\sum_j \beta_{ij} \ln p_i \ln p_j
\]</span></p>
<p>Using Shephard Lemma, the cost share of input <span class="math inline">\(i\)</span> is the derivative of <span class="math inline">\(\ln C\)</span> with <span class="math inline">\(\ln p_i\)</span>.</p>
<p><span class="math display">\[
s_i = \frac{\ln C}{\ln p_i} = \frac{p_i x_i}{C} = \beta_i + \beta_{ii} \ln p_i + \frac{1}{2}\sum_{j\neq i} (\beta_{ij} + \beta_{ji}) \ln p_j
\]</span></p>
<p>Homogeneity of degree 1 in input prices implies that the cost shares don’t depend on the level of prices. Therefore, <span class="math inline">\(\sum_j^I \beta_{ij} = 0\)</span>, or <span class="math inline">\(\beta_{iI} = - \sum_j^{I-1} \beta_{ij}\)</span> and:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\sum_i^I\sum_j^I \beta_{ij} \ln p_i \ln p_j &amp;=&amp;
\sum_i^I \ln p_i \left[\sum_j^{I-1} \beta_{ij}\ln p_j  + \beta_{iJ}\ln
p_I\right] \\
&amp;=&amp; \sum_i^I \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} \\
&amp;=&amp; \sum_i^{I-1} \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} +
\ln p_I \sum_j^{I-1} \beta_{Ij} \ln\frac{p_j}{p_I} \\
&amp;=&amp; \sum_i^{I-1} \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} +
\ln p_I \sum_j^{I-1} \beta_{jI} \ln\frac{p_j}{p_I} \\
&amp;=&amp; \sum_i^{I-1} \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} -
\ln p_I \sum_i^{I-1} \sum_j^{I-1}\beta_{ji} \ln\frac{p_j}{p_I} \\
&amp;=&amp; \sum_i^{I-1} \sum_j^{I-1} \beta_{ij} \ln \frac{p_i}{p_I}\ln\frac{p_j}{p_I}
\end{array}
\]</span></p>
<p>Moreover, the cost shares sum to 1 whatever the value of the prices, so that <span class="math inline">\(\sum_i \beta_i = 1\)</span>. Therefore, the cost function can be rewritten as:</p>
<p><span class="math display">\[
C^* = \alpha + \beta_y \ln y + \frac{1}{2}\beta_{yy} \ln^2 y + \sum_i ^{I-1}\beta_i p_i^* +
\frac{1}{2} \sum_{i=1} ^ {I - 1} \beta_{ii} {p_i^*} ^ 2 +  
\sum_{i=1} ^ {I - 1} \sum_{j&gt;i} ^ {I - 1} \beta_{ij} p_i^* p_j^*
\]</span></p>
<p>where <span class="math inline">\(z^* = \ln (z / p_I)\)</span> and the cost shares are:</p>
<p><span class="math display">\[
s_i = \beta_i + \sum_{j=1} ^ {I-1} \beta_{ij} p_j ^ *
\]</span></p>
<p>Consider the case where <span class="math inline">\(I=3\)</span>. In this case, the complete system of equations is:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
C^* &amp;=&amp; \alpha + \beta_y \ln y + \frac{1}{2}\beta_{yy} \ln^2 y + \beta_1 p_1 ^ * + \beta_2 p_2 ^ *
+ \frac{1}{2} \beta_{11} {p_1^*} ^ 2 + \beta_{12} p_1^* p_2^* +
  \frac{1}{2} \beta_{22} {p_2^*} ^ 2 \\
s_1 &amp;=&amp; \beta_1 + \beta_{11} p_1 ^ * + \beta_{12} p_2 ^ * \\
s_2 &amp;=&amp; \beta_2 + \beta_{12} p_1 ^ * + \beta_{22} p_2 ^* \\
\end{array}
\right.
\]</span></p>
<p>There are 14 parameters to estimate in total (8 in the cost function and 3 in each of the cost share equations), but there are 6 linear restrictions; for example, the coefficient of <span class="math inline">\(p_1^*\)</span> in the cost equation should be equal to the intercept of the cost share for the first factor.</p>
<p>We estimate the translog cost function with the <code>apples</code> data set of <span class="citation" data-cites="IVAL:LADO:OSSA:SIMI:96">Ivaldi et al. (<a href="#ref-IVAL:LADO:OSSA:SIMI:96" role="doc-biblioref">1996</a>)</span> who studied the production cost of apple producers. Farms in this sample produce apples and other fruits (respectively <code>apples</code> and <code>otherprod</code>). The authors observe the sales of apples and other fruits as well as the quantity of apple produced. Therefore, they are able to compute the unit price of apples. Both sales are divided by this unit price, so that <code>apples</code> is measured in apple quantity, and <code>otherprod</code> is measured in “equivalent” apple quantities. Therefore, they can be summed in order to have a unique output variable <code>y</code>. The expenses in the tree factors are given by <code>capital</code>, <code>labor</code> and <code>materials</code> and the corresponding unit prices are <code>pc</code>, <code>pl</code> and <code>pm</code>. The data set is an unbalanced panel of 173 farms observed for three years (1984, 1985 and 1986). We consider only one year (1985) and, for a reason that will be clear later, we divide all the variables by their sample mean: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ap</span> <span class="op">&lt;-</span> <span class="va">apples</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">year</span> <span class="op">==</span> <span class="fl">1985</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">transmute</span><span class="op">(</span>y <span class="op">=</span> <span class="va">otherprod</span> <span class="op">+</span> <span class="va">apples</span>,</span>
<span>              ct <span class="op">=</span> <span class="va">capital</span> <span class="op">+</span> <span class="va">labor</span> <span class="op">+</span> <span class="va">materials</span>,</span>
<span>              sl <span class="op">=</span> <span class="va">labor</span> <span class="op">/</span> <span class="va">ct</span>, sm <span class="op">=</span> <span class="va">materials</span> <span class="op">/</span> <span class="va">ct</span>,</span>
<span>              pk <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">pc</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pc</span><span class="op">)</span><span class="op">)</span>, pl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">pl</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pl</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">pk</span>,</span>
<span>              pm <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">pm</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pm</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">pk</span>, ct <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">ct</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">pk</span>,</span>
<span>              y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>, y2 <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">y</span> <span class="op">^</span> <span class="fl">2</span>,</span>
<span>              ct <span class="op">=</span> <span class="va">ct</span>, pl2 <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">pl</span> <span class="op">^</span> <span class="fl">2</span>,</span>
<span>              pm2 <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">pm</span> <span class="op">^</span> <span class="fl">2</span>, plm <span class="op">=</span> <span class="va">pl</span> <span class="op">*</span> <span class="va">pm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then create three formulas corresponding to the system of three equations (the cost function and the two factor shares):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">eq_ct</span> <span class="op">&lt;-</span> <span class="va">ct</span> <span class="op">~</span> <span class="va">y</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">pl</span> <span class="op">+</span> <span class="va">pm</span> <span class="op">+</span> <span class="va">pl2</span> <span class="op">+</span> <span class="va">plm</span> <span class="op">+</span> <span class="va">pm2</span></span>
<span><span class="va">eq_sl</span> <span class="op">&lt;-</span> <span class="va">sl</span> <span class="op">~</span> <span class="va">pl</span> <span class="op">+</span> <span class="va">pm</span></span>
<span><span class="va">eq_sm</span> <span class="op">&lt;-</span> <span class="va">sm</span> <span class="op">~</span> <span class="va">pl</span> <span class="op">+</span> <span class="va">pm</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These equations can be estimated one by one using OLS, but in this case, the trans-equations restrictions are ignored. The whole system can also be estimated directly by stacking the three vectors of responses and constructing a block diagonal matrix of covariates, each block being the relevant set of covariates for one equation. We use for this purpose the <strong>Formula</strong> package <span class="citation" data-cites="ZEIL:CROI:10">(<a href="#ref-ZEIL:CROI:10" role="doc-biblioref">Zeileis and Croissant 2010</a>)</span>. This package extends usual formulas in two directions: first, several covariates can be indicated on the left- hand side of the formula (using the <code>+</code> operator), and several parts can be defined on both sides of the formula, using the <code>|</code> sign. For example:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">y_1</span> <span class="op">+</span> <span class="va">y_2</span> <span class="op">|</span> <span class="va">y_3</span> <span class="op">~</span> <span class="va">x_1</span> <span class="op">+</span> <span class="va">x_2</span> <span class="op">|</span> <span class="va">x_3</span> <span class="op">|</span> <span class="va">x_4</span> <span class="op">+</span> <span class="va">x_5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This formula has two sets of responses, the first containing <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>, and the second <span class="math inline">\(y_3\)</span>. Three sets of covariates are defined on the right-hand side of the formula.</p>
<p>For our production analysis, we first create a “meta” formula which contains the three responses on the left side and the whole set of covariates on the right side. Then, we extract, using <code>model.matrix</code>, the three model matrices for the three equations (<code>Z_c</code>, <code>Z_l</code> and <code>Z_m</code> respectively for the cost, labor share and material share equations). The column names of these matrices are customized using the <code>nms_cols</code> function which, for example, turns the original column names of <code>Z_l</code> (<code>(Intercept)</code>, <code>pl</code> and <code>pm</code>) to <code>sl_cst</code>, <code>sl_pl</code> and <code>sl_pm</code>. We then construct the block diagonal matrix (using the <code><a href="https://rdrr.io/pkg/Matrix/man/bdiag.html">Matrix::bdiag</a></code> function) and use our customized names: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">Formula</span><span class="op">)</span></span>
<span><span class="va">eq_sys</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Formula/man/Formula.html">Formula</a></span><span class="op">(</span><span class="va">ct</span> <span class="op">+</span> <span class="va">sl</span> <span class="op">+</span> <span class="va">sm</span> <span class="op">~</span> <span class="va">y</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">pl</span> <span class="op">+</span> <span class="va">pm</span> <span class="op">+</span> <span class="va">pl2</span> <span class="op">+</span> <span class="va">plm</span> <span class="op">+</span> <span class="va">pm2</span><span class="op">)</span></span>
<span><span class="va">mf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">eq_sys</span>, <span class="va">ap</span><span class="op">)</span>  ; <span class="va">Z_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_ct</span>, <span class="va">mf</span><span class="op">)</span> </span>
<span><span class="va">Z_l</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_sl</span>, <span class="va">mf</span><span class="op">)</span> ; <span class="va">Z_m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_sm</span>, <span class="va">mf</span><span class="op">)</span></span>
<span><span class="va">nms_cols</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">label</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">label</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"cst"</span>, <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, sep <span class="op">=</span> <span class="st">"_"</span><span class="op">)</span></span>
<span><span class="va">nms_c</span> <span class="op">&lt;-</span> <span class="fu">nms_cols</span><span class="op">(</span><span class="va">Z_c</span>, <span class="st">"cost"</span><span class="op">)</span> ; <span class="va">nms_l</span> <span class="op">&lt;-</span> <span class="fu">nms_cols</span><span class="op">(</span><span class="va">Z_l</span>, <span class="st">"sl"</span><span class="op">)</span></span>
<span><span class="va">nms_m</span> <span class="op">&lt;-</span> <span class="fu">nms_cols</span><span class="op">(</span><span class="va">Z_m</span>, <span class="st">"sm"</span><span class="op">)</span></span>
<span><span class="va">Zs</span> <span class="op">&lt;-</span> <span class="fu">Matrix</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/bdiag.html">bdiag</a></span><span class="op">(</span><span class="va">Z_c</span>, <span class="va">Z_l</span>, <span class="va">Z_m</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">as.matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">Zs</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">nms_c</span>, <span class="va">nms_l</span>, <span class="va">nms_m</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Zs</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     cost_cst  cost_y cost_y2 cost_pl cost_pm cost_pl2 cost_plm
[1,]        1  0.1661  0.0138 -0.3187 -0.0787  0.05077  0.02508
[2,]        1 -1.2880  0.8294 -0.3480 -0.9062  0.06056  0.31537
     cost_pm2 sl_cst sl_pl sl_pm sm_cst sm_pl sm_pm
[1,] 0.003097      0     0     0      0     0     0
[2,] 0.410581      0     0     0      0     0     0</code></pre>
</div>
</div>
<p><code><a href="https://rdrr.io/pkg/Formula/man/model.frame.Formula.html">Formula::model.part</a></code> enables to retrieve any part of the model. Here, we want to extract the three responses which are on the only left side of the formula. Therefore, we set <code>lrs</code> and <code>rhs</code> respectively to 1 and 0. The result is a data frame with three variables <code>ct</code>, <code>sl</code> and <code>sm</code>. We then use <code>dplyr::pivot_longer</code>, to stack the three responses in one column. Note the use of the optional argument <code>cols_vary</code> that is set to <code>"slowest"</code>, so that the elements are stacked columnwise: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Formula/man/model.frame.Formula.html">model.part</a></span><span class="op">(</span><span class="va">eq_sys</span>, <span class="va">mf</span>, rhs <span class="op">=</span> <span class="fl">0</span>, lhs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">Y</span> <span class="op">%&gt;%</span> <span class="fu">pivot_longer</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, cols_vary <span class="op">=</span> <span class="st">"slowest"</span>, </span>
<span>                         names_to <span class="op">=</span> <span class="st">"equation"</span>, values_to <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ys</span>, n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 411 × 2
  equation response
  &lt;chr&gt;       &lt;dbl&gt;
1 ct          0.232
2 ct         -1.25 
# ℹ 409 more rows</code></pre>
</div>
</div>
<p>The estimation can be performed using the response vector and the matrix of covariates: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ys</span><span class="op">$</span><span class="va">response</span> <span class="op">~</span> <span class="va">Zs</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>However, nicer input is obtained by constructing a tibble by binding the columns of <code>ys</code> and <code>Zs</code> and then using the usual formula-data interface. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">stack_data</span> <span class="op">&lt;-</span> <span class="va">ys</span> <span class="op">%&gt;%</span> <span class="fu">bind_cols</span><span class="op">(</span><span class="va">Zs</span><span class="op">)</span></span>
<span><span class="va">stack_data</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 411 × 16
  equation response cost_cst cost_y cost_y2 cost_pl cost_pm cost_pl2
  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
1 ct          0.232        1  0.166  0.0138  -0.319 -0.0787   0.0508
2 ct         -1.25         1 -1.29   0.829   -0.348 -0.906    0.0606
# ℹ 409 more rows
# ℹ 8 more variables: cost_plm &lt;dbl&gt;, cost_pm2 &lt;dbl&gt;, sl_cst &lt;dbl&gt;,
#   sl_pl &lt;dbl&gt;, sl_pm &lt;dbl&gt;, sm_cst &lt;dbl&gt;, sm_pl &lt;dbl&gt;,
#   sm_pm &lt;dbl&gt;</code></pre>
</div>
</div>
<p>In order to avoid having to write the whole long list of covariates, the dot can be used on the right-hand side of the formula, which means in this context all the variables (except the response which is on the left-hand side of the formula). The intercept and the <code>equation</code> variable should be omitted from the regression. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols_unconst</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">equation</span>, <span class="va">stack_data</span><span class="op">)</span></span>
<span><span class="va">ols_unconst</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">## cost_cst   cost_y  cost_y2  cost_pl  cost_pm cost_pl2 cost_plm </span></span>
<span><span class="co">##  0.01742  0.43634  0.11320  0.39270  0.46545  0.23015 -0.21353 </span></span>
<span><span class="co">## cost_pm2   sl_cst    sl_pl    sl_pm   sm_cst    sm_pl    sm_pm </span></span>
<span><span class="co">##  0.18063  0.49383  0.11957 -0.10219  0.32165 -0.09014  0.10369</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="sec-constrained_ls" class="level3"><h3 class="anchored" data-anchor-id="sec-constrained_ls">Constrained least squares</h3>
<p></p>
<p>Linear restrictions on the vector of coefficients to be estimated can be represented using a matrix <span class="math inline">\(R\)</span> and a numeric vector <span class="math inline">\(q\)</span>: <span class="math inline">\(R\gamma = q\)</span>,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> where <span class="math inline">\(\gamma ^ \top = (\gamma_1 ^ \top, \ldots, \gamma_L ^ \top)\)</span> is the stacked vector of the coefficients for the whole system of equations. The OLS estimator is now the solution of a constrained optimization problem. Denoting <span class="math inline">\(\lambda\)</span> a vector of Lagrange multipliers,<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and <span class="math inline">\(\epsilon\)</span> the errors, the objective function is:</p>
<p><span class="math display">\[
L = \epsilon^\top \epsilon + 2\lambda^\top(R\gamma-q)
\]</span></p>
<p>The Lagrangian can also be written as:</p>
<p><span class="math display">\[
L = y^\top y - 2 \gamma^\top Z^\top y + \gamma^\top Z^\top Z \gamma +
2\lambda (R\gamma-q)
\]</span></p>
<p>The first-order conditions are:</p>
<p><span class="math display">\[
\left\{
  \begin{array}{rcl}
    \frac{\partial L}{\partial \gamma}&amp;=&amp;-2Z^\top y + 2 Z^\top Z \gamma + 2R^\top \lambda =0\\
    \frac{\partial L}{\partial \lambda}&amp;=&amp;2(R\gamma-q)=0
  \end{array}
\right.
\]</span></p>
<p>which can also be written in matrix form as:</p>
<p><span class="math display">\[
\left(
\begin{array}{cc}
  Z^\top Z &amp; R^\top \\
  R &amp; 0 \\
\end{array}
\right)
\left(
\begin{array}{c}
  \gamma \\ \lambda
\end{array}
\right)
=
\left(
\begin{array}{c}
  Z^\top y\\ q
\end{array}
\right)
\]</span></p>
<p>The constrained OLS estimator can be obtained using the formula for the inverse of a partitioned matrix:<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> </p>
<p><span class="math display">\[
\left(
  \begin{array}{cc}
    A_{11} &amp; A_{12} \\
    A_{21} &amp; A_{22}
  \end{array}
\right)^{-1}
=
\left(
  \begin{array}{cc}
    B_{11} &amp; B_{12} \\
    B_{21} &amp; B_{22}
  \end{array}
\right)
=
\left(
  \begin{array}{cc}
    A_{11}^{-1}(I+A_{12}F_2A_{21}A_{11}^{-1}) &amp; - A_{11}^{-1}A_{12}F_2 \\
    -F_2A_{21}A_{11}^{-1} &amp; F_2
  \end{array}
\right)
\]</span></p>
<p>with <span class="math inline">\(F_2=\left(A_{22}-A_{21}A_{11}^{-1}A_{12}\right)^{-1}\)</span>. We have here <span class="math inline">\(F_2=-\left(R(Z^\top Z)^{-1}R^\top\right)^{-1}\)</span>. The constrained estimator is then: <span class="math inline">\(\hat{\gamma}_c=B_{11}Z^\top y+ B_{12}q\)</span>, with <span class="math inline">\(B_{11} = (Z^\top Z)^{-1}\left(I-R^\top(R(Z^\top Z)^{-1}R^\top)^{-1}R(Z^\top Z)^{-1}\right)\)</span> and <span class="math inline">\(B_{12}=(Z^\top Z)^{-1}R^\top\left(R(Z^\top Z)^{-1}R^\top\right)^{-1}\)</span></p>
<p>The unconstrained estimator being <span class="math inline">\(\hat{\beta}_{nc}=\left(Z^\top Z\right)^{-1}Z^\top y\)</span>, we finally get:</p>
<p><span id="eq-const_lm"><span class="math display">\[
\hat{\gamma}_c=\hat{\gamma}_{nc} - (Z^\top Z)^{-1}R^\top(R(Z^\top
Z)^{-1}R^\top)^{-1}(R\hat{\gamma}_{nc}-q)
\tag{3.27}\]</span></span></p>
<p></p>
<p>The difference between the constrained and unconstrained estimators is then a linear combination of the excess of the linear constraints of the model evaluated for the unconstrained model. For the system of cost and factor shares for apple production previously described, we have 14 coefficients and 6 restrictions: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, nrow <span class="op">=</span> <span class="fl">6</span>, ncol <span class="op">=</span> <span class="fl">14</span><span class="op">)</span></span>
<span><span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,  <span class="fl">9</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">R</span><span class="op">[</span><span class="fl">2</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">12</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">R</span><span class="op">[</span><span class="fl">3</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">6</span>, <span class="fl">10</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> </span>
<span>  <span class="va">R</span><span class="op">[</span><span class="fl">4</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">7</span>, <span class="fl">11</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">R</span><span class="op">[</span><span class="fl">5</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">7</span>, <span class="fl">13</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> </span>
<span>  <span class="va">R</span><span class="op">[</span><span class="fl">6</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">8</span>, <span class="fl">14</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example, the first line of <code>R</code> returns the difference between the fourth coefficient (<code>cost_pl</code>) and the ninth coefficient (<code>sl_cst</code>). As the vector <span class="math inline">\(q\)</span> is 0 in our example, this means that the coefficient of <span class="math inline">\(p_l^*\)</span> in the cost equation should equal the intercept in the labor share equation. Applying <a href="#eq-const_lm">Equation&nbsp;<span>3.27</span></a>, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">excess</span> <span class="op">&lt;-</span> <span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ols_unconst</span><span class="op">)</span></span>
<span><span class="va">XpX</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">ols_unconst</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">beta_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ols_unconst</span><span class="op">)</span> <span class="op">-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">XpX</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span></span>
<span>         <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">R</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">XpX</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">R</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">excess</span><span class="op">)</span></span>
<span><span class="va">beta_c</span></span>
<span><span class="co">## cost_cst   cost_y  cost_y2  cost_pl  cost_pm cost_pl2 cost_plm </span></span>
<span><span class="co">##  0.02019  0.44485  0.11118  0.49570  0.34208  0.11690 -0.10409 </span></span>
<span><span class="co">## cost_pm2   sl_cst    sl_pl    sl_pm   sm_cst    sm_pl    sm_pm </span></span>
<span><span class="co">##  0.10775  0.49570  0.11690 -0.10409  0.34208 -0.10409  0.10775</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>More simply, <code><a href="https://rdrr.io/pkg/micsr/man/clm.html">micsr::clm</a></code> can be used, which computes the constrained least squares estimator with, as arguments, a <code>lm</code> object (the unconstrained model) and <code>R</code>, the matrix of restrictions (and optionally a <code>q</code> vector):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols_const</span> <span class="op">&lt;-</span> <span class="fu">clm</span><span class="op">(</span><span class="va">ols_unconst</span>, <span class="va">R</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and returns a <code>lm</code> object.</p>
<p>Finally, the <strong>systemfit</strong> package <span class="citation" data-cites="HENN:HAMA:07">(<a href="#ref-HENN:HAMA:07" role="doc-biblioref">Henningsen and Hamann 2007</a>)</span> is devoted to system estimation and provides a <code>systemfit</code> function. Its main arguments are a list of equations and a data frame, but it also has <code>restrict.matrix</code> and <code>restrict.rhs</code> arguments to provide respectively <span class="math inline">\(R\)</span> and <span class="math inline">\(q\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">systemfit</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/systemfit/man/systemfit.html">systemfit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>cost <span class="op">=</span> <span class="va">eq_ct</span>, labor <span class="op">=</span> <span class="va">eq_sl</span>, </span>
<span>                          materials <span class="op">=</span> <span class="va">eq_sm</span><span class="op">)</span>,</span>
<span>                     data <span class="op">=</span> <span class="va">ap</span>, restrict.matrix <span class="op">=</span> <span class="va">R</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output of <code>systemfit</code> is large and is not reproduced here. The full strength of this package will be presented in later chapters, while describing the seemingly unrelated regression (<a href="non_spherical.html#sec-sur"><span>Section&nbsp;6.4.4</span></a>) and the three-stage least squares (<a href="endogeneity.html#sec-three_sls"><span>Section&nbsp;7.4</span></a>) estimators.</p>
<!-- Once the constrained least squares estimator has been computed, one can test the validity of the constraints, using @eq-wald_test_formula: -->
<!-- ```{r} -->
<!-- R %*% vcov(ols_const) %*% t(R) -->
<!-- ``` -->
<p></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-DAVI:MACK:93" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, and James G. MacKinnon. 1993. <em>Estimation and Inference in Econometrics</em>. New-York: Oxford University Press.
</div>
<div id="ref-GREE:18" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometrics Analysis</em>. 8th ed. Pearson.
</div>
<div id="ref-HENN:HAMA:07" class="csl-entry" role="doc-biblioentry">
Henningsen, Arne, and Jeff D. Hamann. 2007. <span>“Systemfit: A Package for Estimating Systems of Simultaneous Equations in r.”</span> <em>Journal of Statistical Software</em> 23 (4): 1–40. <a href="https://www.jstatsoft.org/v23/i04/">https://www.jstatsoft.org/v23/i04/</a>.
</div>
<div id="ref-IVAL:LADO:OSSA:SIMI:96" class="csl-entry" role="doc-biblioentry">
Ivaldi, Marc, Norbert Ladoux, Hervé Ossard, and Michel Simioni. 1996. <span>“Comparing Fourier and Translog Specifications of Multiproduct Technology: Evidence from an Incomplete Panel of French Farmers.”</span> <em>Journal of Applied Econometrics</em> 11 (6): 649–67. <a href="https://doi.org/10.1002/(sici)1099-1255(199611)11:6<649::aid-jae416>3.0.co;2-4">https://doi.org/10.1002/(sici)1099-1255(199611)11:6&lt;649::aid-jae416&gt;3.0.co;2-4</a>.
</div>
<div id="ref-MANK:ROME:WEIL:92" class="csl-entry" role="doc-biblioentry">
Mankiw, N. Gregory, David Romer, and David N. Weil. 1992. <span>“<span class="nocase">A Contribution to the Empirics of Economic Growth</span>.”</span> <em>The Quarterly Journal of Economics</em> 107 (2): 407–37. <a href="https://ideas.repec.org/a/oup/qjecon/v107y1992i2p407-437..html">https://ideas.repec.org/a/oup/qjecon/v107y1992i2p407-437..html</a>.
</div>
<div id="ref-STOC:WATS:15" class="csl-entry" role="doc-biblioentry">
Stock, James H., and Mark W. Watson. 2015. <em>Introduction to Econometrics</em>. Pearson.
</div>
<div id="ref-ZEIL:CROI:10" class="csl-entry" role="doc-biblioentry">
Zeileis, Achim, and Yves Croissant. 2010. <span>“Extended Model Formulas in <span>R</span>: Multiple Parts and Multiple Responses.”</span> <em>Journal of Statistical Software</em> 34 (1): 1–13. <a href="https://www.jstatsoft.org/v34/i01/">https://www.jstatsoft.org/v34/i01/</a>.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>As seen in <a href="simple_regression_properties.html#sec-remove_intercept"><span>Section&nbsp;2.1.4.2</span></a>; to remove it, one has to use either <code>+ 0</code> or <code>- 1</code> in the formula.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See <span class="citation" data-cites="DAVI:MACK:93">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:93" role="doc-biblioref">1993</a>)</span>, section 1.5, pp.&nbsp;25-31.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>If the errors are normal, the exact distribution of the estimators is normal (see <a href="simple_regression_properties.html#sec-clt"><span>Section&nbsp;2.2.2</span></a>).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Or of a linear combination of several estimators.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="STOC:WATS:15">Stock and Watson (<a href="#ref-STOC:WATS:15" role="doc-biblioref">2015</a>)</span>, p.&nbsp;235.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This section is largely based on <span class="citation" data-cites="DAVI:MACK:93">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:93" role="doc-biblioref">1993</a>)</span>, section 3.6, pp.&nbsp;88-94. The three “classical” tests are often understood as tests suitable for models estimated by maximum likelihood (see <a href="maximum_likelihood.html#sec-three_tests_ml"><span>Section&nbsp;5.3.1</span></a>). <span class="citation" data-cites="DAVI:MACK:93">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:93" role="doc-biblioref">1993</a>)</span> advocate the presentation of the three test principles for other estimators, including the linear regression model.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>See <a href="#sec-wald_test_example"><span>Section&nbsp;3.6.4.1</span></a>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>These multipliers are multiplied by 2 in order to simplify the first-order conditions.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>See <span class="citation" data-cites="GREE:18">Greene (<a href="#ref-GREE:18" role="doc-biblioref">2018</a>)</span>, online appendix p.&nbsp;1076.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../chapters/simple_regression_properties.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/coefficients.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb72" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Multiple regression model {#sec-mult_reg_chapter}</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: multiple_regression</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../_commonR.R"</span>)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>In this chapter, we'll analyze the computation and the properties of</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>the OLS estimator when the number of covariates ($K$) is at</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>least 2. Actually, we'll analyze in depth the case when $K=2$</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>(generalizing from $K=2$ to $K&gt;2$ being quite simple) and, compared</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>to the previous two chapters, we'll insist on two important points:</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the use of matrix algebra, which makes the</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>  computation of the estimator and the analysis of its properties elegant and compact,</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the correlation between the covariates; actually we'll show that the multiple and the simple linear models are different only if there exists such a correlation.</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>We'll roughly follow the same plan as for the simple linear model: @sec-model_data_multiple </span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>presents the structural model and the data set that we'll use</span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>throughout the chapter, @sec-comp_ols_mult the computation of the</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>estimator, @sec-geometry_multiple_ols the</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>geometry of the multiple linear model, @sec-computation_R_multiple the computation of the estimator with **R**, @sec-properties_ols_multiple its statistical properties and</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>@sec-confint_test_multiple the inference methods (confidence interval and tests). Finally, @sec-system_equation presents system estimation and the constrained least squares estimator.</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model and data set {#sec-model_data_multiple}</span></span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a>To illustrate the multiple regression model, we'll use the example of</span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a>the estimation of a model explaining economic growth, using a</span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a>cross-section of countries.</span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a><span class="fu">### Structural model</span></span>
<span id="cb72-33"><a href="#cb72-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-34"><a href="#cb72-34" aria-hidden="true" tabindex="-1"></a>One of the most popular growth models in the economic literature is the Solow-Swan model.</span>
<span id="cb72-35"><a href="#cb72-35" aria-hidden="true" tabindex="-1"></a>The production $Y$ (or more precisely the added value or GDP) is</span>
<span id="cb72-36"><a href="#cb72-36" aria-hidden="true" tabindex="-1"></a>performed using two production factors, labor $L$ and capital $K$.</span>
<span id="cb72-37"><a href="#cb72-37" aria-hidden="true" tabindex="-1"></a>Physical labor is transformed in effective labor using a term called</span>
<span id="cb72-38"><a href="#cb72-38" aria-hidden="true" tabindex="-1"></a>$A$. $A$ is time-varying (typically increasing) and therefore represents</span>
<span id="cb72-39"><a href="#cb72-39" aria-hidden="true" tabindex="-1"></a>the effect of technical progress which increases the productivity of</span>
<span id="cb72-40"><a href="#cb72-40" aria-hidden="true" tabindex="-1"></a>labor. The functional form of the production function is a Cobb-Douglas:</span>
<span id="cb72-41"><a href="#cb72-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-42"><a href="#cb72-42" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-43"><a href="#cb72-43" aria-hidden="true" tabindex="-1"></a>Y(t) = K(t) ^ \kappa \left<span class="co">[</span><span class="ot">A(t)L(t)\right</span><span class="co">]</span>^{1 - \kappa}</span>
<span id="cb72-44"><a href="#cb72-44" aria-hidden="true" tabindex="-1"></a>$$ {#eq-production_cobb_douglas}</span>
<span id="cb72-45"><a href="#cb72-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-46"><a href="#cb72-46" aria-hidden="true" tabindex="-1"></a>Coefficients for capital and labor are respectively $\kappa$ and</span>
<span id="cb72-47"><a href="#cb72-47" aria-hidden="true" tabindex="-1"></a>$1-\kappa$. They represent the elasticity of the production respective</span>
<span id="cb72-48"><a href="#cb72-48" aria-hidden="true" tabindex="-1"></a>to each factor, but also the share of each factor in the national income</span>
<span id="cb72-49"><a href="#cb72-49" aria-hidden="true" tabindex="-1"></a>($\kappa$ is therefore the share of profits and $1-\kappa$ the share of</span>
<span id="cb72-50"><a href="#cb72-50" aria-hidden="true" tabindex="-1"></a>wages).</span>
<span id="cb72-51"><a href="#cb72-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-52"><a href="#cb72-52" aria-hidden="true" tabindex="-1"></a>Each variable is a continuous function of time $t$. We'll denote, for</span>
<span id="cb72-53"><a href="#cb72-53" aria-hidden="true" tabindex="-1"></a>each variable $\dot{V}=\frac{d V}{d t}$ the derivative with respect</span>
<span id="cb72-54"><a href="#cb72-54" aria-hidden="true" tabindex="-1"></a>to time. Finally, we'll denote $y(t) = \frac{Y(t)}{A(t)L(t)}$ and $k(t)=\frac{Y(t)}{A(t)L(t)}$</span>
<span id="cb72-55"><a href="#cb72-55" aria-hidden="true" tabindex="-1"></a>production and capital per unit of effective labor. We therefore have:</span>
<span id="cb72-56"><a href="#cb72-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-57"><a href="#cb72-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-58"><a href="#cb72-58" aria-hidden="true" tabindex="-1"></a>y(t) = k(t) ^ \kappa</span>
<span id="cb72-59"><a href="#cb72-59" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-production_function}</span>
<span id="cb72-60"><a href="#cb72-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-61"><a href="#cb72-61" aria-hidden="true" tabindex="-1"></a>We'll hereafter omit $(t)$ to make the notation less cluttered.</span>
<span id="cb72-62"><a href="#cb72-62" aria-hidden="true" tabindex="-1"></a>Variation of capital is investment less depreciation. We assume that</span>
<span id="cb72-63"><a href="#cb72-63" aria-hidden="true" tabindex="-1"></a>investment equals savings and that a constant percentage of income ($i$) is</span>
<span id="cb72-64"><a href="#cb72-64" aria-hidden="true" tabindex="-1"></a>saved every year. The depreciation rate is denoted by $\delta$. We then</span>
<span id="cb72-65"><a href="#cb72-65" aria-hidden="true" tabindex="-1"></a>have:</span>
<span id="cb72-66"><a href="#cb72-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-67"><a href="#cb72-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-68"><a href="#cb72-68" aria-hidden="true" tabindex="-1"></a>\dot{K} = \frac{d K}{d t} = i Y - \delta K</span>
<span id="cb72-69"><a href="#cb72-69" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-70"><a href="#cb72-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-71"><a href="#cb72-71" aria-hidden="true" tabindex="-1"></a>The growth of the capital stock per unit of effective labor $k$ is then:</span>
<span id="cb72-72"><a href="#cb72-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-73"><a href="#cb72-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-74"><a href="#cb72-74" aria-hidden="true" tabindex="-1"></a>\dot{k} = \frac{d \frac{K}{AL}}{dt}=\frac{\dot{K}AL - (A\dot{L} +</span>
<span id="cb72-75"><a href="#cb72-75" aria-hidden="true" tabindex="-1"></a>\dot{A}L)K}{A^2L^2}= \frac{\dot{K}}{AL} -</span>
<span id="cb72-76"><a href="#cb72-76" aria-hidden="true" tabindex="-1"></a>\frac{K}{AL}\left(\frac{\dot{A}}{A} + \frac{\dot{L}}{L}\right) = (iy - \delta k)- k\left(\frac{\dot{A}}{A} + \frac{\dot{L}}{L}\right)</span>
<span id="cb72-77"><a href="#cb72-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-78"><a href="#cb72-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-79"><a href="#cb72-79" aria-hidden="true" tabindex="-1"></a>Denoting $n = \frac{\dot{L}}{L}$ and $g = \frac{\dot{A}}{A}$ the growth rates of</span>
<span id="cb72-80"><a href="#cb72-80" aria-hidden="true" tabindex="-1"></a>$L$ and $A$, i.e., the demographic growth rate and the technological</span>
<span id="cb72-81"><a href="#cb72-81" aria-hidden="true" tabindex="-1"></a>progress rate, we finally have:</span>
<span id="cb72-82"><a href="#cb72-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-83"><a href="#cb72-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-84"><a href="#cb72-84" aria-hidden="true" tabindex="-1"></a>\dot{k}(t)=iy(t)-(n+g+\delta)k(t) = ik(t) ^ \kappa - (n+g+\delta)k(t)</span>
<span id="cb72-85"><a href="#cb72-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-86"><a href="#cb72-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-87"><a href="#cb72-87" aria-hidden="true" tabindex="-1"></a>At the steady state, the growth rate of capital per unit of effective</span>
<span id="cb72-88"><a href="#cb72-88" aria-hidden="true" tabindex="-1"></a>labor is 0. Solving $\dot{k}(t)=0$ we get the steady state value of</span>
<span id="cb72-89"><a href="#cb72-89" aria-hidden="true" tabindex="-1"></a>$k(t)$, denoted by $k^*$:</span>
<span id="cb72-90"><a href="#cb72-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-91"><a href="#cb72-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-92"><a href="#cb72-92" aria-hidden="true" tabindex="-1"></a>k^* = \left(\frac{i}{n + g+ \delta}\right) ^ \frac{1}{1-\kappa}</span>
<span id="cb72-93"><a href="#cb72-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-94"><a href="#cb72-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-95"><a href="#cb72-95" aria-hidden="true" tabindex="-1"></a>Or:</span>
<span id="cb72-96"><a href="#cb72-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-97"><a href="#cb72-97" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-98"><a href="#cb72-98" aria-hidden="true" tabindex="-1"></a>\left(\frac{K}{Y}\right)^*= \frac{k^*}{y^*}=k^{*(1-\kappa)} = \frac{i}{n + g+ \delta}</span>
<span id="cb72-99"><a href="#cb72-99" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-100"><a href="#cb72-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-101"><a href="#cb72-101" aria-hidden="true" tabindex="-1"></a>From @eq-production_cobb_douglas, the production per capita is:</span>
<span id="cb72-102"><a href="#cb72-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-103"><a href="#cb72-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-104"><a href="#cb72-104" aria-hidden="true" tabindex="-1"></a>\frac{Y(t)}{L(t)} = A(t) k(t) ^ {\kappa}</span>
<span id="cb72-105"><a href="#cb72-105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-106"><a href="#cb72-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-107"><a href="#cb72-107" aria-hidden="true" tabindex="-1"></a>Replacing $k(t)$ by $k^*$, $A(t)$ by $A(0)e^{gt}$ and taking logs, we get:</span>
<span id="cb72-108"><a href="#cb72-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-109"><a href="#cb72-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-110"><a href="#cb72-110" aria-hidden="true" tabindex="-1"></a>\ln\frac{Y(t)}{L(t)} = \ln A(0) + g t   + \frac{\kappa}{1-\kappa} \ln i</span>
<span id="cb72-111"><a href="#cb72-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\frac{\kappa}{1 - \kappa} \ln (n + g + \delta)</span>
<span id="cb72-112"><a href="#cb72-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-113"><a href="#cb72-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-114"><a href="#cb72-114" aria-hidden="true" tabindex="-1"></a>Finally, let's denote $\ln A(0) = a + \epsilon$. $\epsilon$ is an error term which represents the initial dispersion between</span>
<span id="cb72-115"><a href="#cb72-115" aria-hidden="true" tabindex="-1"></a>countries in terms of initial value of technical progress. With</span>
<span id="cb72-116"><a href="#cb72-116" aria-hidden="true" tabindex="-1"></a>$C=a + gt$, and $v = \ln(n + g + \delta)$, the linear model that we wish to estimate is finally:</span>
<span id="cb72-117"><a href="#cb72-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-118"><a href="#cb72-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-119"><a href="#cb72-119" aria-hidden="true" tabindex="-1"></a>\ln\frac{Y}{L} = C + \frac{\kappa}{1-\kappa} \ln i - \frac{\kappa}{1 - \kappa} \ln v + \epsilon</span>
<span id="cb72-120"><a href="#cb72-120" aria-hidden="true" tabindex="-1"></a>$$ {#eq-solow_equation}</span>
<span id="cb72-121"><a href="#cb72-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-122"><a href="#cb72-122" aria-hidden="true" tabindex="-1"></a>We therefore get a multiple regression model for which the response is</span>
<span id="cb72-123"><a href="#cb72-123" aria-hidden="true" tabindex="-1"></a>the log of GPD per capita, ($\ln \frac{Y}{L})$ and the two covariates are</span>
<span id="cb72-124"><a href="#cb72-124" aria-hidden="true" tabindex="-1"></a>$\ln i_n$ (the saving rate) and $\ln v_n$ (the sum of the demographic growth, the technical progress and the depreciation rates). Moreover, the structural model imposes some</span>
<span id="cb72-125"><a href="#cb72-125" aria-hidden="true" tabindex="-1"></a>restrictions on the coefficients that can be tested. The two slopes are,</span>
<span id="cb72-126"><a href="#cb72-126" aria-hidden="true" tabindex="-1"></a>in terms of the structural parameters of the theoretical model:</span>
<span id="cb72-127"><a href="#cb72-127" aria-hidden="true" tabindex="-1"></a>$\beta_i = \frac{\kappa}{1-\kappa}$ and $\beta_v = -\frac{\kappa}{1 - \kappa}$. Moreover, $\kappa$ is the</span>
<span id="cb72-128"><a href="#cb72-128" aria-hidden="true" tabindex="-1"></a>elasticity of the GDP with the capital and also the share of</span>
<span id="cb72-129"><a href="#cb72-129" aria-hidden="true" tabindex="-1"></a>profits in GDP. A common approximation for the value of this parameter is about</span>
<span id="cb72-130"><a href="#cb72-130" aria-hidden="true" tabindex="-1"></a>1/3, which implies: $\beta_i = - \beta_v = \frac{\kappa}{1-\kappa}=0.5$.</span>
<span id="cb72-131"><a href="#cb72-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-132"><a href="#cb72-132" aria-hidden="true" tabindex="-1"></a>@MANK:ROME:WEIL:92\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Mankiw}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Romer}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Weil} proposed a generalization of the Solow-Swan model</span>
<span id="cb72-133"><a href="#cb72-133" aria-hidden="true" tabindex="-1"></a>that includes human capital, denoted by $H$. The production function is</span>
<span id="cb72-134"><a href="#cb72-134" aria-hidden="true" tabindex="-1"></a>now:</span>
<span id="cb72-135"><a href="#cb72-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-136"><a href="#cb72-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-137"><a href="#cb72-137" aria-hidden="true" tabindex="-1"></a>Y(t) = K(t) ^ \kappa H(t) ^ \lambda \left<span class="co">[</span><span class="ot">A(t)L(t)\right</span><span class="co">]</span>^{1 - \kappa - \lambda}</span>
<span id="cb72-138"><a href="#cb72-138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-139"><a href="#cb72-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-140"><a href="#cb72-140" aria-hidden="true" tabindex="-1"></a>$\lambda$ is the share of human capital in the GDP and the</span>
<span id="cb72-141"><a href="#cb72-141" aria-hidden="true" tabindex="-1"></a>share of labor is now $(1 - \kappa - \lambda)$. The model is very</span>
<span id="cb72-142"><a href="#cb72-142" aria-hidden="true" tabindex="-1"></a>similar to the one we previously developed. We first compute the growth rate of physical</span>
<span id="cb72-143"><a href="#cb72-143" aria-hidden="true" tabindex="-1"></a>and human capital ($\dot{k}$ and $\dot{h}$) per unit of effective labor, we set these</span>
<span id="cb72-144"><a href="#cb72-144" aria-hidden="true" tabindex="-1"></a>two growth rates to 0 to get the stocks of physical and human capital at the</span>
<span id="cb72-145"><a href="#cb72-145" aria-hidden="true" tabindex="-1"></a>steady state per unit of effective labor ($k^*$ and $h^*$) and we introduce these two values in the</span>
<span id="cb72-146"><a href="#cb72-146" aria-hidden="true" tabindex="-1"></a>production function to get:</span>
<span id="cb72-147"><a href="#cb72-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-148"><a href="#cb72-148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-149"><a href="#cb72-149" aria-hidden="true" tabindex="-1"></a>\ln\frac{Y}{L} = C + \frac{\kappa}{1-\kappa-\lambda} \ln i + </span>
<span id="cb72-150"><a href="#cb72-150" aria-hidden="true" tabindex="-1"></a> \frac{\lambda}{1-\kappa-\lambda} \ln e - \frac{\kappa + \lambda}{1 - \kappa-\lambda} \ln v + \epsilon </span>
<span id="cb72-151"><a href="#cb72-151" aria-hidden="true" tabindex="-1"></a>$$ {#eq-growth_equation}</span>
<span id="cb72-152"><a href="#cb72-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-153"><a href="#cb72-153" aria-hidden="true" tabindex="-1"></a>where $e$ is the per capita level of human capital. The model now contains three covariates and three slopes ($\beta_i$, $\beta_e$ and $\beta_v$). Moreover, the structural model implies a structural restriction ($\beta_i + \beta_e + \beta_v = 0$) that is testable.</span>
<span id="cb72-154"><a href="#cb72-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-155"><a href="#cb72-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data set {#sec-data_solow_mrw}</span></span>
<span id="cb72-156"><a href="#cb72-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-157"><a href="#cb72-157" aria-hidden="true" tabindex="-1"></a><span class="in">`growth`</span>\idxdata{growth}{micsr.data} contains the data used by @MANK:ROME:WEIL:92\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Mankiw}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Romer}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Weil}. It</span>
<span id="cb72-158"><a href="#cb72-158" aria-hidden="true" tabindex="-1"></a>consists of 121 countries for 1985.</span>
<span id="cb72-159"><a href="#cb72-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-160"><a href="#cb72-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-161"><a href="#cb72-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: growth_print</span></span>
<span id="cb72-162"><a href="#cb72-162" aria-hidden="true" tabindex="-1"></a>growth <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb72-163"><a href="#cb72-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-164"><a href="#cb72-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-165"><a href="#cb72-165" aria-hidden="true" tabindex="-1"></a>This data set contains a variable called <span class="in">`group`</span> which enables the</span>
<span id="cb72-166"><a href="#cb72-166" aria-hidden="true" tabindex="-1"></a>selection of subsamples. The modalities of this variable are:</span>
<span id="cb72-167"><a href="#cb72-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-168"><a href="#cb72-168" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`"oil"`</span>: for countries whose most part of the GDP is linked to oil</span>
<span id="cb72-169"><a href="#cb72-169" aria-hidden="true" tabindex="-1"></a>    extraction,</span>
<span id="cb72-170"><a href="#cb72-170" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`"oecd"`</span>: for OECD countries,</span>
<span id="cb72-171"><a href="#cb72-171" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`"lqdata"`</span>: for countries with low quality data,</span>
<span id="cb72-172"><a href="#cb72-172" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`"other"`</span>: for other countries.</span>
<span id="cb72-173"><a href="#cb72-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-174"><a href="#cb72-174" aria-hidden="true" tabindex="-1"></a>The variables used in the following regressions are per capita GDP in</span>
<span id="cb72-175"><a href="#cb72-175" aria-hidden="true" tabindex="-1"></a>1985 (<span class="in">`gdp85`</span>), investment rate (<span class="in">`inv`</span>) and growth rate of the</span>
<span id="cb72-176"><a href="#cb72-176" aria-hidden="true" tabindex="-1"></a>population (<span class="in">`popgwth`</span>). To get the variable denoted by <span class="in">`v`</span> in the previous</span>
<span id="cb72-177"><a href="#cb72-177" aria-hidden="true" tabindex="-1"></a>section, we need to add to the growth rate of the population the</span>
<span id="cb72-178"><a href="#cb72-178" aria-hidden="true" tabindex="-1"></a>technical progress rate and the rate of depreciation. As these two</span>
<span id="cb72-179"><a href="#cb72-179" aria-hidden="true" tabindex="-1"></a>variables are difficult to measure consistently, the authors assume that</span>
<span id="cb72-180"><a href="#cb72-180" aria-hidden="true" tabindex="-1"></a>they don't exhibit any cross-country variation and that they sum to 5%. Therefore,</span>
<span id="cb72-181"><a href="#cb72-181" aria-hidden="true" tabindex="-1"></a><span class="in">`v`</span> equals <span class="in">`popgwth + 0.05`</span>.</span>
<span id="cb72-182"><a href="#cb72-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-183"><a href="#cb72-183" aria-hidden="true" tabindex="-1"></a>We first investigate the relationship between the two covariates: <span class="in">`inv`</span></span>
<span id="cb72-184"><a href="#cb72-184" aria-hidden="true" tabindex="-1"></a>and <span class="in">`popgwth`</span>. @fig-invpop presents the scatterplot, the size</span>
<span id="cb72-185"><a href="#cb72-185" aria-hidden="true" tabindex="-1"></a>of the points being proportional to GDP per capita.</span>
<span id="cb72-186"><a href="#cb72-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-187"><a href="#cb72-187" aria-hidden="true" tabindex="-1"></a>\idxfun{ggplot}{ggplot2}\idxfun{geom<span class="sc">\_</span>point}{ggplot2}\idxfun{geom<span class="sc">\_</span>smooth}{ggplot2}\idxfun{stat<span class="sc">\_</span>ellipse}{ggplot2}</span>
<span id="cb72-188"><a href="#cb72-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-189"><a href="#cb72-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-190"><a href="#cb72-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-invpop</span></span>
<span id="cb72-191"><a href="#cb72-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.cap: "Investment rate and demographic growth"</span></span>
<span id="cb72-192"><a href="#cb72-192" aria-hidden="true" tabindex="-1"></a>growth <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(popgwth, inv)) <span class="sc">+</span></span>
<span id="cb72-193"><a href="#cb72-193" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">size =</span> gdp85, <span class="at">shape =</span> group)) <span class="sc">+</span> <span class="fu">stat_ellipse</span>() <span class="sc">+</span></span>
<span id="cb72-194"><a href="#cb72-194" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">color =</span> <span class="st">"black"</span>)</span>
<span id="cb72-195"><a href="#cb72-195" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-196"><a href="#cb72-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-197"><a href="#cb72-197" aria-hidden="true" tabindex="-1"></a>There is a weak negative correlation between the two variables and rich</span>
<span id="cb72-198"><a href="#cb72-198" aria-hidden="true" tabindex="-1"></a>countries are in general characterized by a low demographic growth rate</span>
<span id="cb72-199"><a href="#cb72-199" aria-hidden="true" tabindex="-1"></a>and a high investment rate. We also remark that there is an outlier,</span>
<span id="cb72-200"><a href="#cb72-200" aria-hidden="true" tabindex="-1"></a>Kuwait, which has a very high demographic growth rate. We then compute</span>
<span id="cb72-201"><a href="#cb72-201" aria-hidden="true" tabindex="-1"></a>the variable <span class="in">`v`</span> and we rename <span class="in">`inv`</span> and <span class="in">`school`</span> in <span class="in">`i`</span> and <span class="in">`e`</span> as in @eq-growth_equation:</span>
<span id="cb72-202"><a href="#cb72-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-203"><a href="#cb72-203" aria-hidden="true" tabindex="-1"></a>\idxfun{mutate}{dplyr}\idxfun{rename}{dplyr}</span>
<span id="cb72-204"><a href="#cb72-204" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-205"><a href="#cb72-205" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: mutate_and_rename_growth</span></span>
<span id="cb72-206"><a href="#cb72-206" aria-hidden="true" tabindex="-1"></a>growth <span class="ot">&lt;-</span> <span class="fu">mutate</span>(growth, <span class="at">v =</span> popgwth <span class="sc">+</span> <span class="fl">0.05</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb72-207"><a href="#cb72-207" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">i =</span> inv, <span class="at">e =</span> school)</span>
<span id="cb72-208"><a href="#cb72-208" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-209"><a href="#cb72-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-210"><a href="#cb72-210" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computation of the OLS estimator {#sec-comp_ols_mult}</span></span>
<span id="cb72-211"><a href="#cb72-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-212"><a href="#cb72-212" aria-hidden="true" tabindex="-1"></a>In this section, we present the computation of the OLS estimator with a number of covariates $K \geq 2$. We start with the case where $K=2$, for which it is possible to compute the estimators using roughly the same notations as the one used for the simple linear regression model. Then we'll perform the computation in the general case using matrix algebra.</span>
<span id="cb72-213"><a href="#cb72-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-214"><a href="#cb72-214" aria-hidden="true" tabindex="-1"></a>For one observation, denoting $x_{n1}$ and $x_{n2}$ the two covariates for observation $n$, the model is:</span>
<span id="cb72-215"><a href="#cb72-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-216"><a href="#cb72-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-217"><a href="#cb72-217" aria-hidden="true" tabindex="-1"></a>  y_{n}=\alpha+\beta_1 x_{n1}+\beta_2 x_{n2}+\epsilon_{n}</span>
<span id="cb72-218"><a href="#cb72-218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-219"><a href="#cb72-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-220"><a href="#cb72-220" aria-hidden="true" tabindex="-1"></a>Each observation is now a point in the 3-D space defined by $(x_1, x_2, y)$ and $\gamma ^ \top = (\alpha,  \beta_1, \beta_2)$ are the coordinates of a plane that returns the expected value of $y$ for a given value of $x_1$ and $x_2$. The residual sum of squares is:</span>
<span id="cb72-221"><a href="#cb72-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-222"><a href="#cb72-222" aria-hidden="true" tabindex="-1"></a>$$f(\alpha, \beta) =\sum_{n = 1} ^  N\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right)^2$$</span>
<span id="cb72-223"><a href="#cb72-223" aria-hidden="true" tabindex="-1"></a>which leads to the following three first-order conditions:</span>
<span id="cb72-224"><a href="#cb72-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-225"><a href="#cb72-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-226"><a href="#cb72-226" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-227"><a href="#cb72-227" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-228"><a href="#cb72-228" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial \alpha} &amp;=&amp; -2 \sum_{n = 1} ^ N </span>
<span id="cb72-229"><a href="#cb72-229" aria-hidden="true" tabindex="-1"></a>\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right) = 0 <span class="sc">\\</span></span>
<span id="cb72-230"><a href="#cb72-230" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial \beta_1}  &amp;=&amp; </span>
<span id="cb72-231"><a href="#cb72-231" aria-hidden="true" tabindex="-1"></a>-2\sum_{n = 1} ^  N x_{n1}\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right)=0 <span class="sc">\\</span></span>
<span id="cb72-232"><a href="#cb72-232" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial \beta_2}  &amp;=&amp; </span>
<span id="cb72-233"><a href="#cb72-233" aria-hidden="true" tabindex="-1"></a>-2\sum_{n = 1} ^  N</span>
<span id="cb72-234"><a href="#cb72-234" aria-hidden="true" tabindex="-1"></a>x_{n2}\left(y_{n}-\alpha-\beta_1x_{n1}-\beta_2x_{n2}\right)=0 </span>
<span id="cb72-235"><a href="#cb72-235" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-236"><a href="#cb72-236" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-237"><a href="#cb72-237" aria-hidden="true" tabindex="-1"></a>$$ {#eq-multiple_ols_foc}</span>
<span id="cb72-238"><a href="#cb72-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-239"><a href="#cb72-239" aria-hidden="true" tabindex="-1"></a>Dividing the first line of @eq-multiple_ols_foc by the sample size, we get:</span>
<span id="cb72-240"><a href="#cb72-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-241"><a href="#cb72-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-242"><a href="#cb72-242" aria-hidden="true" tabindex="-1"></a>\bar{y} - \alpha - \beta_1 \bar{x}_1 - \beta_2 \bar{x}_2 = 0</span>
<span id="cb72-243"><a href="#cb72-243" aria-hidden="true" tabindex="-1"></a>$$ {#eq-multiple_ols_mean_point}</span>
<span id="cb72-244"><a href="#cb72-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-245"><a href="#cb72-245" aria-hidden="true" tabindex="-1"></a>which means that the sample mean is on the regression plane, or that the sum of the residuals is zero. </span>
<span id="cb72-246"><a href="#cb72-246" aria-hidden="true" tabindex="-1"></a>For the last two lines of @eq-multiple_ols_foc, the terms in parentheses are the residual for one observation and as its mean is 0, they indicate that the sample covariance between the residuals and both covariates should be 0. Therefore, we get exactly the same conditions as the one obtained for the simple linear regression model. Subtracting @eq-multiple_ols_mean_point from the last two lines of @eq-multiple_ols_foc, we get:</span>
<span id="cb72-247"><a href="#cb72-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-248"><a href="#cb72-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-249"><a href="#cb72-249" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-250"><a href="#cb72-250" aria-hidden="true" tabindex="-1"></a>\begin{array}{l}</span>
<span id="cb72-251"><a href="#cb72-251" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N x_{n1}\left[(y_{n}-\bar{y})-\beta_1(x_{n1}-\bar{x}_1) -</span>
<span id="cb72-252"><a href="#cb72-252" aria-hidden="true" tabindex="-1"></a>\beta_2(x_{n2} - \bar{x}_2)\right]=0 <span class="sc">\\</span></span>
<span id="cb72-253"><a href="#cb72-253" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N x_{n2}\left[(y_{n}-\bar{y})-\beta_1(x_{n1}-\bar{x}_1) -</span>
<span id="cb72-254"><a href="#cb72-254" aria-hidden="true" tabindex="-1"></a>\beta_2(x_{n2} - \bar{x}_2)\right]=0 <span class="sc">\\</span></span>
<span id="cb72-255"><a href="#cb72-255" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-256"><a href="#cb72-256" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-257"><a href="#cb72-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-258"><a href="#cb72-258" aria-hidden="true" tabindex="-1"></a>or replacing $x_{nk}$ by $x_{nk} - \bar{x}_k$ and developing terms:</span>
<span id="cb72-259"><a href="#cb72-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-260"><a href="#cb72-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-261"><a href="#cb72-261" aria-hidden="true" tabindex="-1"></a>\small{</span>
<span id="cb72-262"><a href="#cb72-262" aria-hidden="true" tabindex="-1"></a>  \left<span class="sc">\{</span></span>
<span id="cb72-263"><a href="#cb72-263" aria-hidden="true" tabindex="-1"></a>\begin{array}{lcl}</span>
<span id="cb72-264"><a href="#cb72-264" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(y_{n} - \bar{y}) &amp;=&amp; </span>
<span id="cb72-265"><a href="#cb72-265" aria-hidden="true" tabindex="-1"></a>\beta_1\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) ^ 2 +</span>
<span id="cb72-266"><a href="#cb72-266" aria-hidden="true" tabindex="-1"></a>\beta_2 \sum_{n = 1} ^ {N} (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2) <span class="sc">\\</span></span>
<span id="cb72-267"><a href="#cb72-267" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N </span>
<span id="cb72-268"><a href="#cb72-268" aria-hidden="true" tabindex="-1"></a>(x_{n2}- \bar{x}_2)(y_{n}-\bar{y}) &amp;=&amp; </span>
<span id="cb72-269"><a href="#cb72-269" aria-hidden="true" tabindex="-1"></a>\beta_1\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2)  +</span>
<span id="cb72-270"><a href="#cb72-270" aria-hidden="true" tabindex="-1"></a>\beta_2 \sum_{n = 1} ^  N (x_{n2} - \bar{x}_2) ^ 2  <span class="sc">\\</span></span>
<span id="cb72-271"><a href="#cb72-271" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-272"><a href="#cb72-272" aria-hidden="true" tabindex="-1"></a>\right.}</span>
<span id="cb72-273"><a href="#cb72-273" aria-hidden="true" tabindex="-1"></a>$$ {#eq-multiple_ols_foc_covariates}</span>
<span id="cb72-274"><a href="#cb72-274" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb72-275"><a href="#cb72-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-276"><a href="#cb72-276" aria-hidden="true" tabindex="-1"></a>We therefore have a system of two linear equations with two unknown</span>
<span id="cb72-277"><a href="#cb72-277" aria-hidden="true" tabindex="-1"></a>parameters ($\beta_1$ and $\beta_2$) that could be solved, for</span>
<span id="cb72-278"><a href="#cb72-278" aria-hidden="true" tabindex="-1"></a>example, by substitution. However, the use of matrix algebra enables</span>
<span id="cb72-279"><a href="#cb72-279" aria-hidden="true" tabindex="-1"></a>to solve such a problem in a much simpler way. Denote:</span>
<span id="cb72-280"><a href="#cb72-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-281"><a href="#cb72-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-282"><a href="#cb72-282" aria-hidden="true" tabindex="-1"></a>X=\left(\begin{array}{cc}</span>
<span id="cb72-283"><a href="#cb72-283" aria-hidden="true" tabindex="-1"></a>x_{11} &amp; x_{12} <span class="sc">\\</span></span>
<span id="cb72-284"><a href="#cb72-284" aria-hidden="true" tabindex="-1"></a>x_{21} &amp; x_{22} <span class="sc">\\</span></span>
<span id="cb72-285"><a href="#cb72-285" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-286"><a href="#cb72-286" aria-hidden="true" tabindex="-1"></a>x_{N1} &amp; x_{N2} <span class="sc">\\</span></span>
<span id="cb72-287"><a href="#cb72-287" aria-hidden="true" tabindex="-1"></a>\end{array}\right)</span>
<span id="cb72-288"><a href="#cb72-288" aria-hidden="true" tabindex="-1"></a>\;\;</span>
<span id="cb72-289"><a href="#cb72-289" aria-hidden="true" tabindex="-1"></a>y=\left(\begin{array}{c}</span>
<span id="cb72-290"><a href="#cb72-290" aria-hidden="true" tabindex="-1"></a>y_1 <span class="sc">\\</span></span>
<span id="cb72-291"><a href="#cb72-291" aria-hidden="true" tabindex="-1"></a>y_2  <span class="sc">\\</span></span>
<span id="cb72-292"><a href="#cb72-292" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb72-293"><a href="#cb72-293" aria-hidden="true" tabindex="-1"></a>y_N <span class="sc">\\</span></span>
<span id="cb72-294"><a href="#cb72-294" aria-hidden="true" tabindex="-1"></a>\end{array}\right)</span>
<span id="cb72-295"><a href="#cb72-295" aria-hidden="true" tabindex="-1"></a>\;\;</span>
<span id="cb72-296"><a href="#cb72-296" aria-hidden="true" tabindex="-1"></a>\beta=\left(\begin{array}{c}</span>
<span id="cb72-297"><a href="#cb72-297" aria-hidden="true" tabindex="-1"></a>\beta_1 <span class="sc">\\</span></span>
<span id="cb72-298"><a href="#cb72-298" aria-hidden="true" tabindex="-1"></a>\beta_2  <span class="sc">\\</span></span>
<span id="cb72-299"><a href="#cb72-299" aria-hidden="true" tabindex="-1"></a>\end{array}\right)</span>
<span id="cb72-300"><a href="#cb72-300" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-301"><a href="#cb72-301" aria-hidden="true" tabindex="-1"></a>$X$ is a matrix with two columns ($K$ in the general case) and $N$ lines (the number of observations) and $y$ is a vector of length $N$. We define $\tilde{I} = I - J / N$ where $I$ is a $N \times N$ identity matrix and $J$ a $N\times N$ matrix of ones. For example, for $N = 3$:</span>
<span id="cb72-302"><a href="#cb72-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-303"><a href="#cb72-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-304"><a href="#cb72-304" aria-hidden="true" tabindex="-1"></a>\tilde{I} = I - J / N = </span>
<span id="cb72-305"><a href="#cb72-305" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-306"><a href="#cb72-306" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb72-307"><a href="#cb72-307" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-308"><a href="#cb72-308" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-309"><a href="#cb72-309" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1</span>
<span id="cb72-310"><a href="#cb72-310" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-311"><a href="#cb72-311" aria-hidden="true" tabindex="-1"></a>\right) - </span>
<span id="cb72-312"><a href="#cb72-312" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-313"><a href="#cb72-313" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb72-314"><a href="#cb72-314" aria-hidden="true" tabindex="-1"></a>\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} <span class="sc">\\</span></span>
<span id="cb72-315"><a href="#cb72-315" aria-hidden="true" tabindex="-1"></a>\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} <span class="sc">\\</span></span>
<span id="cb72-316"><a href="#cb72-316" aria-hidden="true" tabindex="-1"></a>\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3}</span>
<span id="cb72-317"><a href="#cb72-317" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-318"><a href="#cb72-318" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-319"><a href="#cb72-319" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-320"><a href="#cb72-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-321"><a href="#cb72-321" aria-hidden="true" tabindex="-1"></a>Premultiplying a vector $z$ of length $N$ by $J/N$, we get a vector</span>
<span id="cb72-322"><a href="#cb72-322" aria-hidden="true" tabindex="-1"></a>of length $N$ containing the sample mean of $z$ $\bar{z}$ repeated $N$</span>
<span id="cb72-323"><a href="#cb72-323" aria-hidden="true" tabindex="-1"></a>times. As premultiplying $z$ by the identity matrix returns $z$,</span>
<span id="cb72-324"><a href="#cb72-324" aria-hidden="true" tabindex="-1"></a>premultiplying $z$ by $\tilde{I}$ returns a vector of length $N$</span>
<span id="cb72-325"><a href="#cb72-325" aria-hidden="true" tabindex="-1"></a>containing the $N$ values of $z$ in difference from the sample mean.</span>
<span id="cb72-326"><a href="#cb72-326" aria-hidden="true" tabindex="-1"></a>Note that $\tilde{I}$ is **idempotent**\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{indempotent matrix}, which means that $\tilde{I} \times \tilde{I}$. It can be checked by using direct multiplication, but also by reminding that premultiplying a vector by $\tilde{I}$ removes the sample mean from the values of the vector. The transformed vector then has a zero mean, so that applying the same premultiplication one more time will leave it unchanged. Therefore, $\tilde{I}(\tilde{I} z) = \tilde{I}z$. Denoting $\tilde{z} = \tilde{I} z$, we get:</span>
<span id="cb72-327"><a href="#cb72-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-328"><a href="#cb72-328" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-329"><a href="#cb72-329" aria-hidden="true" tabindex="-1"></a>\tilde{X}= \tilde{I} X =</span>
<span id="cb72-330"><a href="#cb72-330" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-331"><a href="#cb72-331" aria-hidden="true" tabindex="-1"></a>x_{11} - \bar{x}_1 &amp; x_{12} - \bar{x}_2<span class="sc">\\</span></span>
<span id="cb72-332"><a href="#cb72-332" aria-hidden="true" tabindex="-1"></a>x_{21} - \bar{x}_1 &amp; x_{22} - \bar{x}_2<span class="sc">\\</span></span>
<span id="cb72-333"><a href="#cb72-333" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-334"><a href="#cb72-334" aria-hidden="true" tabindex="-1"></a>x_{N1} - \bar{x}_1 &amp; x_{N2} - \bar{x}_2<span class="sc">\\</span></span>
<span id="cb72-335"><a href="#cb72-335" aria-hidden="true" tabindex="-1"></a>\end{array}\right)</span>
<span id="cb72-336"><a href="#cb72-336" aria-hidden="true" tabindex="-1"></a>,\;\;</span>
<span id="cb72-337"><a href="#cb72-337" aria-hidden="true" tabindex="-1"></a>\tilde{y} = \tilde{I} y =\left(\begin{array}{c}</span>
<span id="cb72-338"><a href="#cb72-338" aria-hidden="true" tabindex="-1"></a>y_1 - \bar{y}<span class="sc">\\</span></span>
<span id="cb72-339"><a href="#cb72-339" aria-hidden="true" tabindex="-1"></a>y_2 - \bar{y}<span class="sc">\\</span></span>
<span id="cb72-340"><a href="#cb72-340" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb72-341"><a href="#cb72-341" aria-hidden="true" tabindex="-1"></a>y_N - \bar{y}<span class="sc">\\</span></span>
<span id="cb72-342"><a href="#cb72-342" aria-hidden="true" tabindex="-1"></a>\end{array}\right)</span>
<span id="cb72-343"><a href="#cb72-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-344"><a href="#cb72-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-345"><a href="#cb72-345" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb72-346"><a href="#cb72-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-347"><a href="#cb72-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-348"><a href="#cb72-348" aria-hidden="true" tabindex="-1"></a>\tilde{X} ^ \top \tilde{X} = </span>
<span id="cb72-349"><a href="#cb72-349" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-350"><a href="#cb72-350" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1) ^ 2 &amp; \sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2)<span class="sc">\\</span></span>
<span id="cb72-351"><a href="#cb72-351" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(x_{n2} - \bar{x}_2) &amp; \sum_{n = 1} ^  N (x_{n2} - \bar{x}_2) ^ 2<span class="sc">\\</span></span>
<span id="cb72-352"><a href="#cb72-352" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-353"><a href="#cb72-353" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-354"><a href="#cb72-354" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-355"><a href="#cb72-355" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-356"><a href="#cb72-356" aria-hidden="true" tabindex="-1"></a>S_{11} &amp; S_{12}<span class="sc">\\</span></span>
<span id="cb72-357"><a href="#cb72-357" aria-hidden="true" tabindex="-1"></a>S_{12} &amp; S_{22}<span class="sc">\\</span></span>
<span id="cb72-358"><a href="#cb72-358" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-359"><a href="#cb72-359" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-360"><a href="#cb72-360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-361"><a href="#cb72-361" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb72-362"><a href="#cb72-362" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-363"><a href="#cb72-363" aria-hidden="true" tabindex="-1"></a>\tilde{X}^\top \tilde{y} =</span>
<span id="cb72-364"><a href="#cb72-364" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{c}</span>
<span id="cb72-365"><a href="#cb72-365" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N (x_{n1} - \bar{x}_1)(y_{n}- \bar{y})<span class="sc">\\</span></span>
<span id="cb72-366"><a href="#cb72-366" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^  N (x_{n2} - \bar{x}_2)(y_{n}- \bar{y})<span class="sc">\\</span></span>
<span id="cb72-367"><a href="#cb72-367" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-368"><a href="#cb72-368" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-369"><a href="#cb72-369" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-370"><a href="#cb72-370" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-371"><a href="#cb72-371" aria-hidden="true" tabindex="-1"></a>S_{1y} <span class="sc">\\</span></span>
<span id="cb72-372"><a href="#cb72-372" aria-hidden="true" tabindex="-1"></a>S_{2y} <span class="sc">\\</span></span>
<span id="cb72-373"><a href="#cb72-373" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-374"><a href="#cb72-374" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-375"><a href="#cb72-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-376"><a href="#cb72-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-377"><a href="#cb72-377" aria-hidden="true" tabindex="-1"></a>$S_{kk}$ is the total variation of $x_k$, $S_{kl}$ the </span>
<span id="cb72-378"><a href="#cb72-378" aria-hidden="true" tabindex="-1"></a>covariation of $x_k$ and $x_l$ and $S_{ky}$ the covariation between covariate $k$ and the response. Note that the quantities $S_{kk}$ and $S_{ky}$ were already present in the simple linear model as $S_{xx}$ and $S_{xy}$ (there are now two of them). The new term is $S_{kl}$, which measures the correlation between the two covariates. </span>
<span id="cb72-379"><a href="#cb72-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-380"><a href="#cb72-380" aria-hidden="true" tabindex="-1"></a>@eq-multiple_ols_foc_covariates can then be written in matrix form as:</span>
<span id="cb72-381"><a href="#cb72-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-382"><a href="#cb72-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-383"><a href="#cb72-383" aria-hidden="true" tabindex="-1"></a>\tilde{X}^\top \tilde{y} = \tilde{X} ^ \top \tilde{X} \beta</span>
<span id="cb72-384"><a href="#cb72-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-385"><a href="#cb72-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-386"><a href="#cb72-386" aria-hidden="true" tabindex="-1"></a>And the OLS estimator is obtained by premultiplying both sides of the equation by the inverse of $\tilde{X} ^ \top \tilde{X}$:</span>
<span id="cb72-387"><a href="#cb72-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-388"><a href="#cb72-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-389"><a href="#cb72-389" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \left(\tilde{X} ^ \top \tilde{X}\right) ^ {- 1} \tilde{X}^\top \tilde{y}</span>
<span id="cb72-390"><a href="#cb72-390" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mls}</span>
<span id="cb72-391"><a href="#cb72-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-392"><a href="#cb72-392" aria-hidden="true" tabindex="-1"></a>Note that premultiplying the vector $\tilde{X}^\top \tilde{y}$ by the inverse of $\tilde{X} ^ \top \tilde{X}$ is a natural extension of the computation we've performed for the simple linear model, which consisted of dividing $S_{xy}$ by $S_{xx}$. </span>
<span id="cb72-393"><a href="#cb72-393" aria-hidden="true" tabindex="-1"></a>To understand this formula, we write $\tilde{X} ^ \top \tilde{X}$</span>
<span id="cb72-394"><a href="#cb72-394" aria-hidden="true" tabindex="-1"></a>and $\tilde{X} ^ \top \tilde{y}$ as:</span>
<span id="cb72-395"><a href="#cb72-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-396"><a href="#cb72-396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-397"><a href="#cb72-397" aria-hidden="true" tabindex="-1"></a>\tilde{X} ^ \top \tilde{X} = </span>
<span id="cb72-398"><a href="#cb72-398" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-399"><a href="#cb72-399" aria-hidden="true" tabindex="-1"></a>S_{11} &amp; S_{12}<span class="sc">\\</span></span>
<span id="cb72-400"><a href="#cb72-400" aria-hidden="true" tabindex="-1"></a>S_{12} &amp; S_{22}<span class="sc">\\</span></span>
<span id="cb72-401"><a href="#cb72-401" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-402"><a href="#cb72-402" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-403"><a href="#cb72-403" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-404"><a href="#cb72-404" aria-hidden="true" tabindex="-1"></a>N\left(\begin{array}{cc}</span>
<span id="cb72-405"><a href="#cb72-405" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_1^2 &amp; \hat{\sigma}_{12}<span class="sc">\\</span></span>
<span id="cb72-406"><a href="#cb72-406" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{12} &amp; \hat{\sigma}_2^2<span class="sc">\\</span></span>
<span id="cb72-407"><a href="#cb72-407" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-408"><a href="#cb72-408" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-409"><a href="#cb72-409" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-410"><a href="#cb72-410" aria-hidden="true" tabindex="-1"></a>N\hat{\sigma}_1\hat{\sigma}_2</span>
<span id="cb72-411"><a href="#cb72-411" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-412"><a href="#cb72-412" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_1}{\hat{\sigma}_2} &amp; \hat{\rho}_{12}<span class="sc">\\</span></span>
<span id="cb72-413"><a href="#cb72-413" aria-hidden="true" tabindex="-1"></a>\hat{\rho}_{12} &amp; \frac{\hat{\sigma}_2}{\hat{\sigma}_1}<span class="sc">\\</span></span>
<span id="cb72-414"><a href="#cb72-414" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-415"><a href="#cb72-415" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-416"><a href="#cb72-416" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-XpX}</span>
<span id="cb72-417"><a href="#cb72-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-418"><a href="#cb72-418" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb72-419"><a href="#cb72-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-420"><a href="#cb72-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-421"><a href="#cb72-421" aria-hidden="true" tabindex="-1"></a>X^\top y =</span>
<span id="cb72-422"><a href="#cb72-422" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-423"><a href="#cb72-423" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb72-424"><a href="#cb72-424" aria-hidden="true" tabindex="-1"></a>S_{1y} <span class="sc">\\</span></span>
<span id="cb72-425"><a href="#cb72-425" aria-hidden="true" tabindex="-1"></a>S_{2y} <span class="sc">\\</span></span>
<span id="cb72-426"><a href="#cb72-426" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-427"><a href="#cb72-427" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-428"><a href="#cb72-428" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-429"><a href="#cb72-429" aria-hidden="true" tabindex="-1"></a>N\left(</span>
<span id="cb72-430"><a href="#cb72-430" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb72-431"><a href="#cb72-431" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{1y} <span class="sc">\\</span></span>
<span id="cb72-432"><a href="#cb72-432" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{2y} <span class="sc">\\</span></span>
<span id="cb72-433"><a href="#cb72-433" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-434"><a href="#cb72-434" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-435"><a href="#cb72-435" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-436"><a href="#cb72-436" aria-hidden="true" tabindex="-1"></a>N\hat{</span>
<span id="cb72-437"><a href="#cb72-437" aria-hidden="true" tabindex="-1"></a>\sigma}_y</span>
<span id="cb72-438"><a href="#cb72-438" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-439"><a href="#cb72-439" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_1\hat{\rho}_{1y} <span class="sc">\\</span></span>
<span id="cb72-440"><a href="#cb72-440" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_2\hat{\rho}_{2y} <span class="sc">\\</span></span>
<span id="cb72-441"><a href="#cb72-441" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-442"><a href="#cb72-442" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-443"><a href="#cb72-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-444"><a href="#cb72-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-445"><a href="#cb72-445" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the first formulation uses the total sample variations / covariations,</span>
<span id="cb72-446"><a href="#cb72-446" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the second one divides every term by $N$ to obtain sample variances</span>
<span id="cb72-447"><a href="#cb72-447" aria-hidden="true" tabindex="-1"></a>    and covariances,</span>
<span id="cb72-448"><a href="#cb72-448" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the third one divides the covariances by the product of the standard</span>
<span id="cb72-449"><a href="#cb72-449" aria-hidden="true" tabindex="-1"></a>    deviations to get sample coefficients of correlation.</span>
<span id="cb72-450"><a href="#cb72-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-451"><a href="#cb72-451" aria-hidden="true" tabindex="-1"></a>To compute the estimator, we need to compute the inverse of $\tilde{X} ^ \top \tilde{X}$, which is:</span>
<span id="cb72-452"><a href="#cb72-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-453"><a href="#cb72-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-454"><a href="#cb72-454" aria-hidden="true" tabindex="-1"></a>\left(\tilde{X} ^ \top \tilde{X}\right) ^ {- 1} = </span>
<span id="cb72-455"><a href="#cb72-455" aria-hidden="true" tabindex="-1"></a>\frac{</span>
<span id="cb72-456"><a href="#cb72-456" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-457"><a href="#cb72-457" aria-hidden="true" tabindex="-1"></a>S_{22} &amp; -S_{12}<span class="sc">\\</span></span>
<span id="cb72-458"><a href="#cb72-458" aria-hidden="true" tabindex="-1"></a>-S_{12} &amp; S_{11}<span class="sc">\\</span></span>
<span id="cb72-459"><a href="#cb72-459" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-460"><a href="#cb72-460" aria-hidden="true" tabindex="-1"></a>\right)}</span>
<span id="cb72-461"><a href="#cb72-461" aria-hidden="true" tabindex="-1"></a>{S_{11} S_{22} - S_{12} ^ 2}</span>
<span id="cb72-462"><a href="#cb72-462" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-463"><a href="#cb72-463" aria-hidden="true" tabindex="-1"></a>\frac{</span>
<span id="cb72-464"><a href="#cb72-464" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-465"><a href="#cb72-465" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_2^2 &amp; -\hat{\sigma}_{12}<span class="sc">\\</span></span>
<span id="cb72-466"><a href="#cb72-466" aria-hidden="true" tabindex="-1"></a>-\hat{\sigma}_{12} &amp; \hat{\sigma}_1^2<span class="sc">\\</span></span>
<span id="cb72-467"><a href="#cb72-467" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-468"><a href="#cb72-468" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-469"><a href="#cb72-469" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-470"><a href="#cb72-470" aria-hidden="true" tabindex="-1"></a>{N (\hat{\sigma}_1 ^ 2 \hat{\sigma}_2 ^ 2 - \hat{\sigma}_{12}^2)}</span>
<span id="cb72-471"><a href="#cb72-471" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-472"><a href="#cb72-472" aria-hidden="true" tabindex="-1"></a>\frac{</span>
<span id="cb72-473"><a href="#cb72-473" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-474"><a href="#cb72-474" aria-hidden="true" tabindex="-1"></a>\displaystyle\frac{\hat{\sigma}_2}{\hat{\sigma}_1} &amp; - \hat{\rho}_{12}<span class="sc">\\</span></span>
<span id="cb72-475"><a href="#cb72-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\hat{\rho}_{12} &amp; \displaystyle\frac{\hat{\sigma}_1}{\hat{\sigma}_2}<span class="sc">\\</span></span>
<span id="cb72-476"><a href="#cb72-476" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-477"><a href="#cb72-477" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-478"><a href="#cb72-478" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-479"><a href="#cb72-479" aria-hidden="true" tabindex="-1"></a>{N \hat{\sigma}_1 \hat{\sigma}_2 (1 - \hat{\rho}_{12} ^ 2)}</span>
<span id="cb72-480"><a href="#cb72-480" aria-hidden="true" tabindex="-1"></a>$$ {#eq-XpXm1}</span>
<span id="cb72-481"><a href="#cb72-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-482"><a href="#cb72-482" aria-hidden="true" tabindex="-1"></a>@eq-mls finally gives:</span>
<span id="cb72-483"><a href="#cb72-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-484"><a href="#cb72-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-485"><a href="#cb72-485" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span>\begin{array}{l}</span>
<span id="cb72-486"><a href="#cb72-486" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1 = \displaystyle</span>
<span id="cb72-487"><a href="#cb72-487" aria-hidden="true" tabindex="-1"></a>\frac{S_{22}S_{1y} - S_{12}S_{2y}}{S_{11}S_{22} - S_{12} ^ 2} = </span>
<span id="cb72-488"><a href="#cb72-488" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_2 ^ 2 \hat{\sigma}_{1y} - \hat{\sigma}_{12} \hat{\sigma}_{2y}}</span>
<span id="cb72-489"><a href="#cb72-489" aria-hidden="true" tabindex="-1"></a>{\hat{\sigma}_1 ^ 2 \hat{\sigma}_2 ^ 2 - \hat{\sigma} _ {12} ^ 2}</span>
<span id="cb72-490"><a href="#cb72-490" aria-hidden="true" tabindex="-1"></a>= \frac{\hat{\rho}_{1y} - \hat{\rho}_{12}\hat{\rho}_{2y}}{1 - \hat{\rho}_{12} ^ 2}</span>
<span id="cb72-491"><a href="#cb72-491" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_y}{\hat{\sigma}_1} <span class="sc">\\</span></span>
<span id="cb72-492"><a href="#cb72-492" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2 = \displaystyle</span>
<span id="cb72-493"><a href="#cb72-493" aria-hidden="true" tabindex="-1"></a>\frac{S_{11}S_{2y} - S_{12}S_{1y}}{S_{11}S_{22} - S_{12} ^ 2} = </span>
<span id="cb72-494"><a href="#cb72-494" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_1 ^ 2 \hat{\sigma}_{2y} - \hat{\sigma}_{12} \hat{\sigma}_{1y}} </span>
<span id="cb72-495"><a href="#cb72-495" aria-hidden="true" tabindex="-1"></a>{\hat{\sigma}_1 ^ 2 \hat{\sigma}_2 ^ 2 - \hat{\sigma}_{12} ^ 2}</span>
<span id="cb72-496"><a href="#cb72-496" aria-hidden="true" tabindex="-1"></a>= \frac{\hat{\rho}_{2y} - \hat{\rho}_{12} \hat{\rho}_{1y}}</span>
<span id="cb72-497"><a href="#cb72-497" aria-hidden="true" tabindex="-1"></a>{1-\hat{\rho}_{12}^2} \frac{\hat{\sigma}_y}{\hat{\sigma}_2}</span>
<span id="cb72-498"><a href="#cb72-498" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb72-499"><a href="#cb72-499" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-500"><a href="#cb72-500" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-501"><a href="#cb72-501" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-502"><a href="#cb72-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-503"><a href="#cb72-503" aria-hidden="true" tabindex="-1"></a>If the two covariates are uncorrelated in the sample</span>
<span id="cb72-504"><a href="#cb72-504" aria-hidden="true" tabindex="-1"></a>($S_{12} = \hat{\sigma}_{12} = \hat{\rho}_{12} = 0$), we have:</span>
<span id="cb72-505"><a href="#cb72-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-506"><a href="#cb72-506" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-507"><a href="#cb72-507" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-508"><a href="#cb72-508" aria-hidden="true" tabindex="-1"></a>\begin{array}{l}</span>
<span id="cb72-509"><a href="#cb72-509" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1 = \displaystyle</span>
<span id="cb72-510"><a href="#cb72-510" aria-hidden="true" tabindex="-1"></a>\frac{S_{1y}}{S_{11}} = </span>
<span id="cb72-511"><a href="#cb72-511" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{1y}}</span>
<span id="cb72-512"><a href="#cb72-512" aria-hidden="true" tabindex="-1"></a>{\hat{\sigma}_1 ^ 2}</span>
<span id="cb72-513"><a href="#cb72-513" aria-hidden="true" tabindex="-1"></a>= \hat{\rho}_{1y}</span>
<span id="cb72-514"><a href="#cb72-514" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_y}{\hat{\sigma}_1} <span class="sc">\\</span></span>
<span id="cb72-515"><a href="#cb72-515" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2 = \displaystyle</span>
<span id="cb72-516"><a href="#cb72-516" aria-hidden="true" tabindex="-1"></a>\frac{S_{2y}}{S_{22}} = </span>
<span id="cb72-517"><a href="#cb72-517" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{2y}} </span>
<span id="cb72-518"><a href="#cb72-518" aria-hidden="true" tabindex="-1"></a>{\hat{\sigma}_2 ^ 2}</span>
<span id="cb72-519"><a href="#cb72-519" aria-hidden="true" tabindex="-1"></a>= \hat{\rho}_{2y}</span>
<span id="cb72-520"><a href="#cb72-520" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_y}{\hat{\sigma}_2}</span>
<span id="cb72-521"><a href="#cb72-521" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-522"><a href="#cb72-522" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-523"><a href="#cb72-523" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-524"><a href="#cb72-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-525"><a href="#cb72-525" aria-hidden="true" tabindex="-1"></a>which is exactly the same formula that we had for the unique slope in</span>
<span id="cb72-526"><a href="#cb72-526" aria-hidden="true" tabindex="-1"></a>the case of the simple regression model. This means that, if $x_1$ and</span>
<span id="cb72-527"><a href="#cb72-527" aria-hidden="true" tabindex="-1"></a>$x_2$ are uncorrelated in the sample, regressing $y$ on $x_1$ or on</span>
<span id="cb72-528"><a href="#cb72-528" aria-hidden="true" tabindex="-1"></a>$x_1$ and $x_2$ leads exactly to the same estimator for the slope of</span>
<span id="cb72-529"><a href="#cb72-529" aria-hidden="true" tabindex="-1"></a>$x_1$.</span>
<span id="cb72-530"><a href="#cb72-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-531"><a href="#cb72-531" aria-hidden="true" tabindex="-1"></a>The general formula for $\hat{\beta}_1$ in term of the estimator of the simple linear model $\hat{\beta}_1^S$ (@eq-slrbeta) is:</span>
<span id="cb72-532"><a href="#cb72-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-533"><a href="#cb72-533" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-534"><a href="#cb72-534" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1 = </span>
<span id="cb72-535"><a href="#cb72-535" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\rho}_{1y} - \hat{\rho}_{12}\hat{\rho}_{2y}}{1 - \hat{\rho}_{12} ^ 2}</span>
<span id="cb72-536"><a href="#cb72-536" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_y}{\hat{\sigma}_1} =</span>
<span id="cb72-537"><a href="#cb72-537" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1^s \frac{1 - \displaystyle\frac{\hat{\rho}_{12}\hat{\rho}_{2y}}{\hat{\rho}_{1y}}}</span>
<span id="cb72-538"><a href="#cb72-538" aria-hidden="true" tabindex="-1"></a>{1 - \hat{\rho}_{12} ^ 2}=</span>
<span id="cb72-539"><a href="#cb72-539" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1^s \frac{1 - \hat{\rho}_{12} ^ 2 \displaystyle\frac{\hat{\rho}_{2y}}{\hat{\rho}_{12}\hat{\rho}_{1y}}}</span>
<span id="cb72-540"><a href="#cb72-540" aria-hidden="true" tabindex="-1"></a>{1 - \hat{\rho}_{12} ^ 2}</span>
<span id="cb72-541"><a href="#cb72-541" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-mult_simple}</span>
<span id="cb72-542"><a href="#cb72-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-543"><a href="#cb72-543" aria-hidden="true" tabindex="-1"></a>We have</span>
<span id="cb72-544"><a href="#cb72-544" aria-hidden="true" tabindex="-1"></a>$\mid\hat{\rho}_{12} \hat{\rho}_{1y}\mid \leq \mid\hat{\rho}_{2y}\mid$, or</span>
<span id="cb72-545"><a href="#cb72-545" aria-hidden="true" tabindex="-1"></a>$\left| \frac{\hat{\rho}_{2y}}{\hat{\rho}_{12}\hat{\rho}_{1y}}\right| \geq 1$.</span>
<span id="cb72-546"><a href="#cb72-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-547"><a href="#cb72-547" aria-hidden="true" tabindex="-1"></a>Consider the case where the two covariates are positively correlated</span>
<span id="cb72-548"><a href="#cb72-548" aria-hidden="true" tabindex="-1"></a>with the response. As an example, consider the wage equation with $x_1$ education and $x_2$ a dummy for males. Two cases should then be analyzed:</span>
<span id="cb72-549"><a href="#cb72-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-550"><a href="#cb72-550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the two covariates are positively correlated (males are more educated than females on average). In this case $\hat{\rho}_{2y}/(\hat{\rho}_{12}\hat{\rho}_{1y})&gt;1$, and the numerator of @eq-mult_simple is lower than $1 - \hat{\rho}_{12} ^ 2$, so that $\hat{\beta}_1&lt;\hat{\beta}_1^S$. $\hat{\beta}_1 ^ S$ is upward biased because it estimates the sum of the positive direct effect of education on wage and a positive indirect effect (more education leads to a subpopulation with a higher share of males and therefore higher wages),</span>
<span id="cb72-551"><a href="#cb72-551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the two covariates are negatively correlated (males are less educated than females on average). In this case, $\hat{\rho}_{2y}/(\hat{\rho}_{12}\hat{\rho}_{1y})&lt;0$ and the numerator of @eq-mult_simple is greater than 1, so that $\hat{\beta}_1&gt;\hat{\beta}_1^S$. $\hat{\beta}_1 ^ S$ is downward biased because it estimates the sum of the positive direct effect of education on wage and a negative indirect effect (more education leads to a subpopulation with a lower share of males and therefore lower wages).</span>
<span id="cb72-552"><a href="#cb72-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-553"><a href="#cb72-553" aria-hidden="true" tabindex="-1"></a>The general derivation of the OLS estimator can be performed using matrix algebra, denoting $j_N$ a vector of 1 of length $N$, $Z = (j_N, X)$ a vector formed by binding a vector of 1 to the matrix of covariates and $\gamma^\top = (\alpha, \beta ^ \top)$ the vector of parameters obtained by adding the intercept $\alpha$ to the vector of slopes:</span>
<span id="cb72-554"><a href="#cb72-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-555"><a href="#cb72-555" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-556"><a href="#cb72-556" aria-hidden="true" tabindex="-1"></a>f(\gamma) = (y - Z\gamma)^\top (y - Z\gamma) = y ^ \top y +  \gamma ^ \top Z</span>
<span id="cb72-557"><a href="#cb72-557" aria-hidden="true" tabindex="-1"></a>^ \top Z \gamma -2 \gamma ^ \top Z ^ \top y</span>
<span id="cb72-558"><a href="#cb72-558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-559"><a href="#cb72-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-560"><a href="#cb72-560" aria-hidden="true" tabindex="-1"></a>The $K+1$ first-order conditions are:</span>
<span id="cb72-561"><a href="#cb72-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-562"><a href="#cb72-562" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-563"><a href="#cb72-563" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial \gamma} = 2 Z ^ \top Z \gamma - 2 Z ^ \top</span>
<span id="cb72-564"><a href="#cb72-564" aria-hidden="true" tabindex="-1"></a>y = -2 Z ^ \top (y - Z\gamma) = 0</span>
<span id="cb72-565"><a href="#cb72-565" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-566"><a href="#cb72-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-567"><a href="#cb72-567" aria-hidden="true" tabindex="-1"></a>The last expression indicates that all the columns of $Z$ are orthogonal</span>
<span id="cb72-568"><a href="#cb72-568" aria-hidden="true" tabindex="-1"></a>to the vector of residuals $y - Z\gamma$. Solving for $\gamma$, we</span>
<span id="cb72-569"><a href="#cb72-569" aria-hidden="true" tabindex="-1"></a>get:</span>
<span id="cb72-570"><a href="#cb72-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-571"><a href="#cb72-571" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-572"><a href="#cb72-572" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = (Z ^ \top Z) ^ {- 1} Z ^ \top y</span>
<span id="cb72-573"><a href="#cb72-573" aria-hidden="true" tabindex="-1"></a>$$ {#eq-multiple_ols_gamma}</span>
<span id="cb72-574"><a href="#cb72-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-575"><a href="#cb72-575" aria-hidden="true" tabindex="-1"></a>The matrix of second derivatives is:</span>
<span id="cb72-576"><a href="#cb72-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-577"><a href="#cb72-577" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-578"><a href="#cb72-578" aria-hidden="true" tabindex="-1"></a>\frac{\partial^2 f}{\partial \gamma\partial^\top \gamma} = </span>
<span id="cb72-579"><a href="#cb72-579" aria-hidden="true" tabindex="-1"></a>2 Z ^ \top Z</span>
<span id="cb72-580"><a href="#cb72-580" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-581"><a href="#cb72-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-582"><a href="#cb72-582" aria-hidden="true" tabindex="-1"></a>It is a positive semi-definite matrix, so that $\hat{\gamma}$ is a minimum of $f$.</span>
<span id="cb72-583"><a href="#cb72-583" aria-hidden="true" tabindex="-1"></a>Comparing @eq-multiple_ols_gamma and @eq-mls, we can see that the formula of the OLS is the same with different matrices (respectively $\tilde{X}$ and $Z$), the first equation returning $\hat{\beta}$ and the second one $\hat{\gamma} ^ \top = (\hat{\alpha}, \hat{\beta} ^ \top)$.</span>
<span id="cb72-584"><a href="#cb72-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-585"><a href="#cb72-585" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geometry of least squares {#sec-geometry_multiple_ols}</span></span>
<span id="cb72-586"><a href="#cb72-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-587"><a href="#cb72-587" aria-hidden="true" tabindex="-1"></a><span class="fu">### Geometry of the multiple regression model</span></span>
<span id="cb72-588"><a href="#cb72-588" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{geometry of least squares!multiple linear regression model|(}</span>
<span id="cb72-589"><a href="#cb72-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-590"><a href="#cb72-590" aria-hidden="true" tabindex="-1"></a>The geometry of the multiple regression model is presented in</span>
<span id="cb72-591"><a href="#cb72-591" aria-hidden="true" tabindex="-1"></a>@fig-multregmodel.</span>
<span id="cb72-592"><a href="#cb72-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-593"><a href="#cb72-593" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-594"><a href="#cb72-594" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-multregmodel</span></span>
<span id="cb72-595"><a href="#cb72-595" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Geometry of the multiple regression model"</span></span>
<span id="cb72-596"><a href="#cb72-596" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb72-597"><a href="#cb72-597" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"./tikz/fig/OLS3D.png"</span>, <span class="at">auto_pdf =</span> <span class="cn">TRUE</span>)</span>
<span id="cb72-598"><a href="#cb72-598" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-599"><a href="#cb72-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-600"><a href="#cb72-600" aria-hidden="true" tabindex="-1"></a>We now have $N = 3$ and $K = 2$ and therefore each variable is a vector</span>
<span id="cb72-601"><a href="#cb72-601" aria-hidden="true" tabindex="-1"></a>in the 3-D space. As the two covariates $x_1$ and $x_2$ are linearly</span>
<span id="cb72-602"><a href="#cb72-602" aria-hidden="true" tabindex="-1"></a>independent, they span a subspace of dimension 2, which is a plane in this</span>
<span id="cb72-603"><a href="#cb72-603" aria-hidden="true" tabindex="-1"></a>3-D space. $\hat{y}$ is the orthogonal projection of $y$ on this subspace</span>
<span id="cb72-604"><a href="#cb72-604" aria-hidden="true" tabindex="-1"></a>and $\hat{\epsilon}$, is the vector that links $\hat{y}$ and $y$.</span>
<span id="cb72-605"><a href="#cb72-605" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon}$ is therefore the projection of $y$ on the complement to</span>
<span id="cb72-606"><a href="#cb72-606" aria-hidden="true" tabindex="-1"></a>the subspace defined by $(x_1, x_2)$, which is a straight line</span>
<span id="cb72-607"><a href="#cb72-607" aria-hidden="true" tabindex="-1"></a>orthogonal to the plane spanned by $(x_1, x_2)$. Therefore</span>
<span id="cb72-608"><a href="#cb72-608" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon}$ is orthogonal to $x_1$ and to $x_2$, which means that</span>
<span id="cb72-609"><a href="#cb72-609" aria-hidden="true" tabindex="-1"></a>the residuals are uncorrelated with the two covariates. The</span>
<span id="cb72-610"><a href="#cb72-610" aria-hidden="true" tabindex="-1"></a>decomposition of $y$ on the sum of two orthogonal vectors</span>
<span id="cb72-611"><a href="#cb72-611" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon}$ and $\hat{y}$ doesn't depend on the two variables $x_1$</span>
<span id="cb72-612"><a href="#cb72-612" aria-hidden="true" tabindex="-1"></a>and $x_2$ per se, but on the subspace spanned by $x_1$ and $x_2$. This means</span>
<span id="cb72-613"><a href="#cb72-613" aria-hidden="true" tabindex="-1"></a>that any couple of independent linear combination of $x_1$ and $x_2$</span>
<span id="cb72-614"><a href="#cb72-614" aria-hidden="true" tabindex="-1"></a>will leads to the same subspace as the one defined by $x_1$ and $x_2$</span>
<span id="cb72-615"><a href="#cb72-615" aria-hidden="true" tabindex="-1"></a>and therefore to the same residuals and the same fitted values.</span>
<span id="cb72-616"><a href="#cb72-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-617"><a href="#cb72-617" aria-hidden="true" tabindex="-1"></a>More formally, as $\hat{\beta} = (X ^ \top X) ^ {-1} X ^ \top y$, we have</span>
<span id="cb72-618"><a href="#cb72-618" aria-hidden="true" tabindex="-1"></a>$\hat{y} = X \hat{\beta} = X (X ^ \top X) ^ {-1} X ^ \top y = P_X y$. $P$</span>
<span id="cb72-619"><a href="#cb72-619" aria-hidden="true" tabindex="-1"></a>is sometimes called the "hat" matrix, as it "puts a hat" on $y$. This</span>
<span id="cb72-620"><a href="#cb72-620" aria-hidden="true" tabindex="-1"></a>matrix transforms the vector of response on a vector of prediction. As</span>
<span id="cb72-621"><a href="#cb72-621" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon} = y - \hat{y}$, we also have $\hat{\epsilon} = y - P_X y = (I-P_X) y=M_Xy$.</span>
<span id="cb72-622"><a href="#cb72-622" aria-hidden="true" tabindex="-1"></a>We therefore consider two matrices $P_X$ and $M_X$:</span>
<span id="cb72-623"><a href="#cb72-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-624"><a href="#cb72-624" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-625"><a href="#cb72-625" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-626"><a href="#cb72-626" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-627"><a href="#cb72-627" aria-hidden="true" tabindex="-1"></a>P_X &amp;=&amp; X (X ^ \top X) ^ {-1} X ^ \top <span class="sc">\\</span></span>
<span id="cb72-628"><a href="#cb72-628" aria-hidden="true" tabindex="-1"></a>M_X &amp;=&amp; I - X (X ^ \top X) ^ {-1} X ^ \top <span class="sc">\\</span></span>
<span id="cb72-629"><a href="#cb72-629" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-630"><a href="#cb72-630" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-631"><a href="#cb72-631" aria-hidden="true" tabindex="-1"></a>$$ {#eq-projmatrix}</span>
<span id="cb72-632"><a href="#cb72-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-633"><a href="#cb72-633" aria-hidden="true" tabindex="-1"></a>that are square and symmetric of dimension $N \times N$, which means</span>
<span id="cb72-634"><a href="#cb72-634" aria-hidden="true" tabindex="-1"></a>that they are in practice large matrices, and are therefore never</span>
<span id="cb72-635"><a href="#cb72-635" aria-hidden="true" tabindex="-1"></a>computed in practice. However, they have very interesting analytical features. First,</span>
<span id="cb72-636"><a href="#cb72-636" aria-hidden="true" tabindex="-1"></a>they are idempotent, which means that $P_X \times P_X = P_X$ and</span>
<span id="cb72-637"><a href="#cb72-637" aria-hidden="true" tabindex="-1"></a>$M_X \times M_X = M_X$. This means that while premultiplying a vector by such</span>
<span id="cb72-638"><a href="#cb72-638" aria-hidden="true" tabindex="-1"></a>a matrix, this vector is projected in a subspace. For example, premultiplying $y$ by $P_X$ gives $\hat{y}$, the vector of fitted values. It is a linear combination of $x_1$ and $x_2$ and therefore belongs to the subspace spanned by</span>
<span id="cb72-639"><a href="#cb72-639" aria-hidden="true" tabindex="-1"></a>$x_1$ and $x_2$. Therefore, $P_X \hat{y}$ obviously equal $\hat{y}$ and therefore $P_X \times P_X = P_X$. Except for the</span>
<span id="cb72-640"><a href="#cb72-640" aria-hidden="true" tabindex="-1"></a>identity matrix, idempotent matrices are not full rank. Their rank can</span>
<span id="cb72-641"><a href="#cb72-641" aria-hidden="true" tabindex="-1"></a>be easily computed using the fact that the rank of a matrix is equal to</span>
<span id="cb72-642"><a href="#cb72-642" aria-hidden="true" tabindex="-1"></a>its trace (the sum of the diagonal elements) and that the trace of a</span>
<span id="cb72-643"><a href="#cb72-643" aria-hidden="true" tabindex="-1"></a>product of matrices is invariant to any permutation of the matrices:</span>
<span id="cb72-644"><a href="#cb72-644" aria-hidden="true" tabindex="-1"></a>$\mbox{tr} ABC = \mbox{tr} BCA = \mbox{tr} CAB$.</span>
<span id="cb72-645"><a href="#cb72-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-646"><a href="#cb72-646" aria-hidden="true" tabindex="-1"></a>For a regression with an intercept, the model matrix</span>
<span id="cb72-647"><a href="#cb72-647" aria-hidden="true" tabindex="-1"></a>$Z$ has $K + 1$ column, the first one being a column of one. In this case, the rank of $P_Z$ and $M_Z$ are:</span>
<span id="cb72-648"><a href="#cb72-648" aria-hidden="true" tabindex="-1"></a>$\mbox{rank} \,P_Z = \mbox{tr}\, P_Z = \mbox{tr}\, Z (Z ^ \top Z) ^ {-1} Z ^ \top = \mbox{tr}\, (Z ^ \top Z) ^ {-1} Z ^ \top Z = \mbox{tr}\, I_{K+1} = K + 1$</span>
<span id="cb72-649"><a href="#cb72-649" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb72-650"><a href="#cb72-650" aria-hidden="true" tabindex="-1"></a>$\mbox{rank} \,M_Z = \mbox{tr}\, (I_N - P_X) = \mbox{tr}\, I_N - \mbox{tr}\, P_Z = N - K - 1$. </span>
<span id="cb72-651"><a href="#cb72-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-652"><a href="#cb72-652" aria-hidden="true" tabindex="-1"></a>Finally, the two matrices are orthogonal: $P_ZM_Z= P_Z(I - P_Z)=P_Z-P_Z=0$, which</span>
<span id="cb72-653"><a href="#cb72-653" aria-hidden="true" tabindex="-1"></a>means that they perform the projection of a vector on two orthogonal</span>
<span id="cb72-654"><a href="#cb72-654" aria-hidden="true" tabindex="-1"></a>subspaces.</span>
<span id="cb72-655"><a href="#cb72-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-656"><a href="#cb72-656" aria-hidden="true" tabindex="-1"></a>Getting back to @fig-multregmodel, $P_X$ project $y$ on the 2-D</span>
<span id="cb72-657"><a href="#cb72-657" aria-hidden="true" tabindex="-1"></a>subspace (a plane) spanned by $x_1$ and $x_2$ and $M_X$ project $y$ on a 1-D</span>
<span id="cb72-658"><a href="#cb72-658" aria-hidden="true" tabindex="-1"></a>subspace (the straight line orthogonal to the previous plane).</span>
<span id="cb72-659"><a href="#cb72-659" aria-hidden="true" tabindex="-1"></a>$M_X$ and $P_X$ perform therefore an orthogonal decomposition of $y$ in</span>
<span id="cb72-660"><a href="#cb72-660" aria-hidden="true" tabindex="-1"></a>$\hat{y}$ and $\hat{\epsilon}$, which means that</span>
<span id="cb72-661"><a href="#cb72-661" aria-hidden="true" tabindex="-1"></a>$\hat{y} + \hat{\epsilon} = y$ and that</span>
<span id="cb72-662"><a href="#cb72-662" aria-hidden="true" tabindex="-1"></a>$\hat{y}^\top\hat{\epsilon} = 0$.</span>
<span id="cb72-663"><a href="#cb72-663" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{geometry of least squares!multiple linear regression model|)}</span>
<span id="cb72-664"><a href="#cb72-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-665"><a href="#cb72-665" aria-hidden="true" tabindex="-1"></a><span class="fu">### Frisch-Waugh theorem</span></span>
<span id="cb72-666"><a href="#cb72-666" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Frisch-Waugh theorem|(}</span>
<span id="cb72-667"><a href="#cb72-667" aria-hidden="true" tabindex="-1"></a>Consider the regression of $y$ on a set of regressors $X$ which, for some</span>
<span id="cb72-668"><a href="#cb72-668" aria-hidden="true" tabindex="-1"></a>reasons, is separated in two subsets $X_1$ and $X_2$. Suppose that we are</span>
<span id="cb72-669"><a href="#cb72-669" aria-hidden="true" tabindex="-1"></a>only interested in the coefficients $\beta_2$ associated with $X_2$. The</span>
<span id="cb72-670"><a href="#cb72-670" aria-hidden="true" tabindex="-1"></a>Frisch-Waugh theorem states that the same estimator $\hat{\beta}_2$ is</span>
<span id="cb72-671"><a href="#cb72-671" aria-hidden="true" tabindex="-1"></a>obtained:</span>
<span id="cb72-672"><a href="#cb72-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-673"><a href="#cb72-673" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>by regressing $y$ on $X_1$ and $X_2$,</span>
<span id="cb72-674"><a href="#cb72-674" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>by first regressing $y$ and each column of $X_2$ on $X_1$, then</span>
<span id="cb72-675"><a href="#cb72-675" aria-hidden="true" tabindex="-1"></a>    taking the residuals $M_1y$ and $M_1{X}_2$ of these</span>
<span id="cb72-676"><a href="#cb72-676" aria-hidden="true" tabindex="-1"></a>    regressions and finally regressing $M_1y$ on $M_1X_2$.</span>
<span id="cb72-677"><a href="#cb72-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-678"><a href="#cb72-678" aria-hidden="true" tabindex="-1"></a>@fig-Frischwaugh illustrates the Frisch-Waugh theorem.</span>
<span id="cb72-679"><a href="#cb72-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-680"><a href="#cb72-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-681"><a href="#cb72-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-682"><a href="#cb72-682" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-Frischwaugh</span></span>
<span id="cb72-683"><a href="#cb72-683" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Frisch-Waugh theorem"</span></span>
<span id="cb72-684"><a href="#cb72-684" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb72-685"><a href="#cb72-685" aria-hidden="true" tabindex="-1"></a><span class="co">#| out.width: "80%"</span></span>
<span id="cb72-686"><a href="#cb72-686" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"./tikz/fig/frishWaugh.png"</span>, <span class="at">auto_pdf =</span> <span class="cn">TRUE</span>)</span>
<span id="cb72-687"><a href="#cb72-687" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-688"><a href="#cb72-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-689"><a href="#cb72-689" aria-hidden="true" tabindex="-1"></a>The **first regression** is the regression of $y$ on $x_1$ and $x_2$. We then get an orthogonal decomposition of $y$ in the vector of fitted values $\hat{y}=P_{12}y$ and of the residuals $\hat{\epsilon}_{12}=M_{12}y$. We also show in this figure the decomposition of $\hat{y}$ in $x_1$ and $x_2$, which is</span>
<span id="cb72-690"><a href="#cb72-690" aria-hidden="true" tabindex="-1"></a>represented by the sum of the two vectors $\hat{\beta}_1 x_1$ and</span>
<span id="cb72-691"><a href="#cb72-691" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_2 x_2$. $\hat{\beta}_2$ is the estimator of $x_2$ on this first regression and is represented by the ratio between $\hat{\beta}_2 x_2$ and $x_2$.</span>
<span id="cb72-692"><a href="#cb72-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-693"><a href="#cb72-693" aria-hidden="true" tabindex="-1"></a>The **second regression** is the regression of $M_1y$ on $M_1x_2$. $M_1x_2$ is the residual of the regression of $x_2$ on $x_1$. Therefore, this vector lies in the line that is in the plane spanned by $x_1$ and $x_2$ and is orthogonal to $x_1$. $M_1y$ is the residual of $y$ on $x_1$; it is therefore orthogonal to $x_1$. </span>
<span id="cb72-694"><a href="#cb72-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-695"><a href="#cb72-695" aria-hidden="true" tabindex="-1"></a>As both $M_1y$ and $M_{12}y$ are orthogonal to $x_1$, so is the vector that joins those two vectors. Therefore, this vector is parallel to $M_1x_2$, and it is therefore the fitted value of the second regression ($M_1y$ on $M_1x_2$), $P_{M_1x_2}y$. It is also equal to $\tilde{\beta}_2 M_1x_2$, $\tilde{\beta}_2$ being the estimator of $x_2$ on the second regression. Note also that $\hat{\epsilon}_{12} = M_{12}y$ is the residual of the second regression, which is therefore the same as the residuals of the first regression.</span>
<span id="cb72-696"><a href="#cb72-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-697"><a href="#cb72-697" aria-hidden="true" tabindex="-1"></a>Finally, consider the regression of $\hat{\beta}_2 x_2$ on $x_1$. The residual of this regression is $\hat{\beta}_2M_1x_2$. As it lies on the plane spanned by $x_1$ and $x_2$ and is orthogonal to $x_1$, it is parallel to $P_{M_1x_2}y=\tilde{\beta}_2M_1x_2$. Moreover, the Frisch-Waugh theorem states that both vectors have the same length and are therefore identical, which means that $\tilde{\beta}_2 = \hat{\beta}_2$ and that the two regressions give identical estimators.</span>
<span id="cb72-698"><a href="#cb72-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-699"><a href="#cb72-699" aria-hidden="true" tabindex="-1"></a>The Frisch-Waugh is easily demonstrated using some geometric arguments.</span>
<span id="cb72-700"><a href="#cb72-700" aria-hidden="true" tabindex="-1"></a>Consider the regression with all the covariates:</span>
<span id="cb72-701"><a href="#cb72-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-702"><a href="#cb72-702" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-703"><a href="#cb72-703" aria-hidden="true" tabindex="-1"></a>y = X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2 + M_{12}y</span>
<span id="cb72-704"><a href="#cb72-704" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-705"><a href="#cb72-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-706"><a href="#cb72-706" aria-hidden="true" tabindex="-1"></a>Then, premultiply both sides of the model by $M_1$:</span>
<span id="cb72-707"><a href="#cb72-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-708"><a href="#cb72-708" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-709"><a href="#cb72-709" aria-hidden="true" tabindex="-1"></a>M_1 y = M_1 X_1 \hat{\beta}_1 + M_1 X_2 \hat{\beta}_2 + M_1 M_{12}y</span>
<span id="cb72-710"><a href="#cb72-710" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-711"><a href="#cb72-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-712"><a href="#cb72-712" aria-hidden="true" tabindex="-1"></a>$M_1 X_1 \hat{\beta}_1$ is 0 as $X_1 \hat{\beta}_1$ is obviously in the</span>
<span id="cb72-713"><a href="#cb72-713" aria-hidden="true" tabindex="-1"></a>subset spanned by $X_1$ and therefore its projection on the orthogonal</span>
<span id="cb72-714"><a href="#cb72-714" aria-hidden="true" tabindex="-1"></a>complement is 0. $M_{12} y$ is orthogonal to the subset spanned by $X$</span>
<span id="cb72-715"><a href="#cb72-715" aria-hidden="true" tabindex="-1"></a>and is therefore also orthogonal to the subset spanned by $X_1$.</span>
<span id="cb72-716"><a href="#cb72-716" aria-hidden="true" tabindex="-1"></a>Therefore $M_1 M_{12} y = M_{12} y$. We therefore have:</span>
<span id="cb72-717"><a href="#cb72-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-718"><a href="#cb72-718" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-719"><a href="#cb72-719" aria-hidden="true" tabindex="-1"></a>M_1 y = M_1 X_2 \hat{\beta}_2 + M_{12}y</span>
<span id="cb72-720"><a href="#cb72-720" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-721"><a href="#cb72-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-722"><a href="#cb72-722" aria-hidden="true" tabindex="-1"></a>For which the estimation is:</span>
<span id="cb72-723"><a href="#cb72-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-724"><a href="#cb72-724" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-725"><a href="#cb72-725" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2 = (X_2^\top M_1 X_2) ^ {-1} X_2 ^ \top M_1 y</span>
<span id="cb72-726"><a href="#cb72-726" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-727"><a href="#cb72-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-728"><a href="#cb72-728" aria-hidden="true" tabindex="-1"></a>which is exactly the estimation obtained by regressing $M_1 y$ on</span>
<span id="cb72-729"><a href="#cb72-729" aria-hidden="true" tabindex="-1"></a>$M_1 X_2$. We finally note (an important result that will be used in @sec-three_tests), that $\hat{\epsilon}_1=M_1y$, $\hat{\epsilon}_{12}=M_{12}y$ and $P_{M_1x_2}y$ form a right triangle, $\hat{\epsilon}_1$ being the hypotenuse. Therefore, using the Pythagorean theorem, we have:</span>
<span id="cb72-730"><a href="#cb72-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-731"><a href="#cb72-731" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-732"><a href="#cb72-732" aria-hidden="true" tabindex="-1"></a>\mid\mid \hat{\epsilon}_{12}^2\mid\mid + \mid\mid P_{M_1X_2}y\mid\mid = \mid\mid \hat{\epsilon}_{1}^2\mid\mid</span>
<span id="cb72-733"><a href="#cb72-733" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-pyth_ssr}</span>
<span id="cb72-734"><a href="#cb72-734" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Frisch-Waugh theorem|)}</span>
<span id="cb72-735"><a href="#cb72-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-736"><a href="#cb72-736" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb72-737"><a href="#cb72-737" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computation with R {#sec-computation_R_multiple}</span></span>
<span id="cb72-738"><a href="#cb72-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-739"><a href="#cb72-739" aria-hidden="true" tabindex="-1"></a>To estimate the multiple linear model, we use as for the single linear</span>
<span id="cb72-740"><a href="#cb72-740" aria-hidden="true" tabindex="-1"></a>model the <span class="in">`lm`</span> function; the difference being that now, on the right side of</span>
<span id="cb72-741"><a href="#cb72-741" aria-hidden="true" tabindex="-1"></a>the formula, we have several variables (here two), separated by the <span class="in">`+`</span></span>
<span id="cb72-742"><a href="#cb72-742" aria-hidden="true" tabindex="-1"></a>operator. Actually, formulas have a much richer syntax that includes other operators, for example <span class="in">`*`</span> and <span class="in">`:`</span>. This will be discussed in <span class="co">[</span><span class="ot">Chapter -@sec-interpretation_chapter</span><span class="co">]</span>.</span>
<span id="cb72-743"><a href="#cb72-743" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-744"><a href="#cb72-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-745"><a href="#cb72-745" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-746"><a href="#cb72-746" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: estimation_solow_model</span></span>
<span id="cb72-747"><a href="#cb72-747" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-748"><a href="#cb72-748" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(gdp85) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v), growth)</span>
<span id="cb72-749"><a href="#cb72-749" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-750"><a href="#cb72-750" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-751"><a href="#cb72-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-752"><a href="#cb72-752" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation using matrix algebra</span></span>
<span id="cb72-753"><a href="#cb72-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-754"><a href="#cb72-754" aria-hidden="true" tabindex="-1"></a>The estimator can also be computed "by hand", using matrix algebra. To start, we use the <span class="in">`model.frame`</span> function which, as <span class="in">`lm`</span>, has <span class="in">`formula`</span> and  <span class="in">`data`</span> arguments. For pedagogical purposes, we add the <span class="in">`group`</span> variable in the formula.</span>
<span id="cb72-755"><a href="#cb72-755" aria-hidden="true" tabindex="-1"></a>\idxfun{head}{utils}\idxfun{model.frame}{stats}\idxfun{nrow}{base}</span>
<span id="cb72-756"><a href="#cb72-756" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-757"><a href="#cb72-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_frame_growth</span></span>
<span id="cb72-758"><a href="#cb72-758" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-759"><a href="#cb72-759" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(<span class="fu">log</span>(gdp85) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v) <span class="sc">+</span> group, growth)</span>
<span id="cb72-760"><a href="#cb72-760" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mf, <span class="dv">3</span>)</span>
<span id="cb72-761"><a href="#cb72-761" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(mf)</span>
<span id="cb72-762"><a href="#cb72-762" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(growth)</span>
<span id="cb72-763"><a href="#cb72-763" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-764"><a href="#cb72-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-765"><a href="#cb72-765" aria-hidden="true" tabindex="-1"></a><span class="in">`model.frame`</span> returns a data frame that contains the data required for estimating the model described in the formula. More precisely, it performs three tasks:</span>
<span id="cb72-766"><a href="#cb72-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-767"><a href="#cb72-767" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>it selects only the columns of the initial data frame that are required for the estimation,</span>
<span id="cb72-768"><a href="#cb72-768" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>it transforms these variables if required, here <span class="in">`gdp85`</span>, <span class="in">`i`</span> and <span class="in">`v`</span> are transformed in logarithms and the columns are renamed accordingly,</span>
<span id="cb72-769"><a href="#cb72-769" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>it selects only the observations for which there are no missing values for the relevant variables; note here that <span class="in">`growth`</span> has 121 rows and <span class="in">`mf`</span> only 107 rows.</span>
<span id="cb72-770"><a href="#cb72-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-771"><a href="#cb72-771" aria-hidden="true" tabindex="-1"></a>Several interesting elements of the model can be extracted from the</span>
<span id="cb72-772"><a href="#cb72-772" aria-hidden="true" tabindex="-1"></a>model frame. The $Z$ matrix is obtained using the <span class="in">`model.matrix`</span> function, which also uses a </span>
<span id="cb72-773"><a href="#cb72-773" aria-hidden="true" tabindex="-1"></a><span class="in">`formula`</span>/<span class="in">`data`</span> interface, the data being the model frame <span class="in">`mf`</span>:</span>
<span id="cb72-774"><a href="#cb72-774" aria-hidden="true" tabindex="-1"></a>\idxfun{model.matrix}{stats}\idxfun{head}{utils}</span>
<span id="cb72-775"><a href="#cb72-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-776"><a href="#cb72-776" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-777"><a href="#cb72-777" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_matrix_growth</span></span>
<span id="cb72-778"><a href="#cb72-778" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="fu">log</span>(gdp85) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v) <span class="sc">+</span> group, mf)</span>
<span id="cb72-779"><a href="#cb72-779" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Z, <span class="dv">3</span>)</span>
<span id="cb72-780"><a href="#cb72-780" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-781"><a href="#cb72-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-782"><a href="#cb72-782" aria-hidden="true" tabindex="-1"></a>Note that the model matrix includes an intercept^<span class="co">[</span><span class="ot">As seen in @sec-remove_intercept; to remove it, one has to use either `+ 0` or `- 1` in the formula.</span><span class="co">]</span> and the <span class="in">`group`</span> variable, which is a categorical variable is transformed into a set of dummy variables. More precisely, a dummy variable is created for all the modalities except the first one (<span class="in">`"oecd"`</span>).</span>
<span id="cb72-783"><a href="#cb72-783" aria-hidden="true" tabindex="-1"></a>The response is obtained using the <span class="in">`model.response`</span> function:</span>
<span id="cb72-784"><a href="#cb72-784" aria-hidden="true" tabindex="-1"></a>\idxfun{model.response}{stats}\idxfun{head}{utils}</span>
<span id="cb72-785"><a href="#cb72-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-786"><a href="#cb72-786" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-787"><a href="#cb72-787" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_response_growth</span></span>
<span id="cb72-788"><a href="#cb72-788" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-789"><a href="#cb72-789" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">model.response</span>(mf)</span>
<span id="cb72-790"><a href="#cb72-790" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(y, <span class="dv">3</span>)</span>
<span id="cb72-791"><a href="#cb72-791" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-792"><a href="#cb72-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-793"><a href="#cb72-793" aria-hidden="true" tabindex="-1"></a>Once the model matrix and the response vector are created, the estimator can easily be computed using matrix operators provided by <span class="in">`R`</span>. In particular:</span>
<span id="cb72-794"><a href="#cb72-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-795"><a href="#cb72-795" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`%*%`</span> is the matrix product operator (<span class="in">`*`</span> performs an element per</span>
<span id="cb72-796"><a href="#cb72-796" aria-hidden="true" tabindex="-1"></a>    element product),</span>
<span id="cb72-797"><a href="#cb72-797" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`t()`</span> transposes a matrix,</span>
<span id="cb72-798"><a href="#cb72-798" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`solve()`</span> solves a linear system of equation or computes the inverse of a matrix,</span>
<span id="cb72-799"><a href="#cb72-799" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`crossprod`</span> takes the inner products of two matrices (or of one</span>
<span id="cb72-800"><a href="#cb72-800" aria-hidden="true" tabindex="-1"></a>    matrix and a vector).</span>
<span id="cb72-801"><a href="#cb72-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-802"><a href="#cb72-802" aria-hidden="true" tabindex="-1"></a>The most straightforward formula to get the OLS estimator is:</span>
<span id="cb72-803"><a href="#cb72-803" aria-hidden="true" tabindex="-1"></a>\idxfun{solve}{base}</span>
<span id="cb72-804"><a href="#cb72-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-805"><a href="#cb72-805" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-806"><a href="#cb72-806" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ols_straight_formula</span></span>
<span id="cb72-807"><a href="#cb72-807" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y</span>
<span id="cb72-808"><a href="#cb72-808" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-809"><a href="#cb72-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-810"><a href="#cb72-810" aria-hidden="true" tabindex="-1"></a>But <span class="in">`crossprod`</span> is more efficient, $A^\top B$ being obtained using</span>
<span id="cb72-811"><a href="#cb72-811" aria-hidden="true" tabindex="-1"></a><span class="in">`crossprod(A, B)`</span> and $A^\top A$ is either <span class="in">`crossprod(A, A)`</span> or</span>
<span id="cb72-812"><a href="#cb72-812" aria-hidden="true" tabindex="-1"></a><span class="in">`crossprod(A)`</span>. Therefore, $Z^\top Z$ and $Z^\top y$ are respectively obtained using:</span>
<span id="cb72-813"><a href="#cb72-813" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}</span>
<span id="cb72-814"><a href="#cb72-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-815"><a href="#cb72-815" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-816"><a href="#cb72-816" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: hide</span></span>
<span id="cb72-817"><a href="#cb72-817" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: using_crossprod</span></span>
<span id="cb72-818"><a href="#cb72-818" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(Z)</span>
<span id="cb72-819"><a href="#cb72-819" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(Z, y)</span>
<span id="cb72-820"><a href="#cb72-820" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-821"><a href="#cb72-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-822"><a href="#cb72-822" aria-hidden="true" tabindex="-1"></a>Moreover, <span class="in">`solve`</span> can be used to solve a system of linear equations: <span class="in">`solve(A, z)`</span> compute the vector $w$ such that $Aw=z$, which is $w=A^{-1}z$. Therefore, the OLS estimator can be computed using the more efficient and compact following code:</span>
<span id="cb72-823"><a href="#cb72-823" aria-hidden="true" tabindex="-1"></a>\idxfun{solve}{base}\idxfun{crossprod}{base}</span>
<span id="cb72-824"><a href="#cb72-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-827"><a href="#cb72-827" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-828"><a href="#cb72-828" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: hide</span></span>
<span id="cb72-829"><a href="#cb72-829" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: using_crossprod_solve</span></span>
<span id="cb72-830"><a href="#cb72-830" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">crossprod</span>(Z), <span class="fu">crossprod</span>(Z, y))</span>
<span id="cb72-831"><a href="#cb72-831" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-832"><a href="#cb72-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-833"><a href="#cb72-833" aria-hidden="true" tabindex="-1"></a><span class="fu">### Efficient computation: QR decomposition</span></span>
<span id="cb72-834"><a href="#cb72-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-835"><a href="#cb72-835" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{QR decomposition|(}</span>
<span id="cb72-836"><a href="#cb72-836" aria-hidden="true" tabindex="-1"></a>The efficient method used by <span class="in">`lm`</span> to compute the OLS estimate is the QR decomposition:^<span class="co">[</span><span class="ot">See @DAVI:MACK:93\index[author]{Davidson}\index[author]{McKinnon}, section 1.5, pp. 25-31.</span><span class="co">]</span> the matrix of covariates $Z$ can be written as the product of an orthonormal matrix $Q$ of dimension $N \times (K + 1)$ (therefore $Q^\top Q = I$) and an upper triangular matrix $R$ of dimension $(K+1) \times (K+1)$. Then, the linear model can be written as:</span>
<span id="cb72-837"><a href="#cb72-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-838"><a href="#cb72-838" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-839"><a href="#cb72-839" aria-hidden="true" tabindex="-1"></a>y = Z\gamma + \epsilon = QR\gamma + \epsilon = Q \delta + \epsilon</span>
<span id="cb72-840"><a href="#cb72-840" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-841"><a href="#cb72-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-842"><a href="#cb72-842" aria-hidden="true" tabindex="-1"></a>with $\delta = R \gamma$. With $Q$ as the matrix of covariates, the OLS estimator is $\hat{\delta} = (Q ^ \top Q ) ^ {-1} Q ^ \top y = Q^\top y$, which is obtained without matrix inversion. Then $\hat{y} = Q \hat{\gamma} = Q Q ^ \top y$ and $\hat{\epsilon} = (I - Q Q ^ \top) y$. With $Z$ as the matrix of covariates, the OLS estimator is $\hat{\gamma} = R ^ {-1} \delta$. The inverse of a triangular matrix can be very easily obtained with a very high numerical accuracy. Moreover, $\hat{\gamma}$ can be obtained without inverting $R$. With $K = 2$, we have:</span>
<span id="cb72-843"><a href="#cb72-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-844"><a href="#cb72-844" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-845"><a href="#cb72-845" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{c} \delta_0 <span class="sc">\\</span> \delta_1 <span class="sc">\\</span> \delta_2 \end{array}\right) = </span>
<span id="cb72-846"><a href="#cb72-846" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-847"><a href="#cb72-847" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccc}</span>
<span id="cb72-848"><a href="#cb72-848" aria-hidden="true" tabindex="-1"></a>r_{11} &amp; r_{12} &amp; r_{13} <span class="sc">\\</span></span>
<span id="cb72-849"><a href="#cb72-849" aria-hidden="true" tabindex="-1"></a>0 &amp; r_{22} &amp; r_{23} <span class="sc">\\</span></span>
<span id="cb72-850"><a href="#cb72-850" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; r_{33}</span>
<span id="cb72-851"><a href="#cb72-851" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-852"><a href="#cb72-852" aria-hidden="true" tabindex="-1"></a>\right) </span>
<span id="cb72-853"><a href="#cb72-853" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{c} \alpha <span class="sc">\\</span> \beta_1 <span class="sc">\\</span> \beta_2 \end{array}\right)</span>
<span id="cb72-854"><a href="#cb72-854" aria-hidden="true" tabindex="-1"></a>= </span>
<span id="cb72-855"><a href="#cb72-855" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{c} </span>
<span id="cb72-856"><a href="#cb72-856" aria-hidden="true" tabindex="-1"></a>r_{11} \alpha + r_{12} \beta_1 + r_{13} \beta_2 <span class="sc">\\</span></span>
<span id="cb72-857"><a href="#cb72-857" aria-hidden="true" tabindex="-1"></a>r_{22} \beta_1 + r_{23} \beta_2 <span class="sc">\\</span></span>
<span id="cb72-858"><a href="#cb72-858" aria-hidden="true" tabindex="-1"></a>r_{33} \beta_2</span>
<span id="cb72-859"><a href="#cb72-859" aria-hidden="true" tabindex="-1"></a>\end{array}\right)</span>
<span id="cb72-860"><a href="#cb72-860" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-861"><a href="#cb72-861" aria-hidden="true" tabindex="-1"></a>which can be solved recursively:</span>
<span id="cb72-862"><a href="#cb72-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-863"><a href="#cb72-863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_2 = \delta_2 / r_{33}$,</span>
<span id="cb72-864"><a href="#cb72-864" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_1 = (\delta_1 - r_{23}\beta_2) / r_{22}$,</span>
<span id="cb72-865"><a href="#cb72-865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha = (\delta_0 - r_{12} \beta_1 - r_{13} \beta_2) / r_{11}$.</span>
<span id="cb72-866"><a href="#cb72-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-867"><a href="#cb72-867" aria-hidden="true" tabindex="-1"></a>Finally, $(X^\top X) ^ {-1} = (R^\top Q^ \top Q R) ^ {-1} = (R ^ \top R) ^{-1} = R ^ {-1} R ^ {- 1 \top}$.</span>
<span id="cb72-868"><a href="#cb72-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-869"><a href="#cb72-869" aria-hidden="true" tabindex="-1"></a>We illustrate the computation of the OLS estimator using the QR decomposition and using the previously estimated growth model, without the <span class="in">`group`</span> covariate. The QR decomposition is performed using the <span class="in">`qr`</span> function:</span>
<span id="cb72-870"><a href="#cb72-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-871"><a href="#cb72-871" aria-hidden="true" tabindex="-1"></a>\idxfun{model.matrix}{stats}\idxfun{qr}{base}</span>
<span id="cb72-874"><a href="#cb72-874" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-875"><a href="#cb72-875" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: qr_decomposition</span></span>
<span id="cb72-876"><a href="#cb72-876" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="fu">log</span>(gdp85) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v), mf)</span>
<span id="cb72-877"><a href="#cb72-877" aria-hidden="true" tabindex="-1"></a>qrZ <span class="ot">&lt;-</span> <span class="fu">qr</span>(Z)</span>
<span id="cb72-878"><a href="#cb72-878" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-879"><a href="#cb72-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-880"><a href="#cb72-880" aria-hidden="true" tabindex="-1"></a><span class="in">`qr`</span> returns an object of class <span class="in">`qr`</span>, and the <span class="in">`qr.R`</span> and <span class="in">`qr.Q`</span> function can be used to retrieve the two matrices. We check that $R$ is upper triangular and that $Q^\top Q  = I$:</span>
<span id="cb72-881"><a href="#cb72-881" aria-hidden="true" tabindex="-1"></a>\idxfun{qr.R}{base}\idxfun{qr.Q}{base}\idxfun{crossprod}{base}</span>
<span id="cb72-882"><a href="#cb72-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-885"><a href="#cb72-885" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-886"><a href="#cb72-886" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: extracting_Q_R</span></span>
<span id="cb72-887"><a href="#cb72-887" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: false</span></span>
<span id="cb72-888"><a href="#cb72-888" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">qr.R</span>(qrZ)</span>
<span id="cb72-889"><a href="#cb72-889" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="fu">qr.Q</span>(qrZ)</span>
<span id="cb72-890"><a href="#cb72-890" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(Q)</span>
<span id="cb72-891"><a href="#cb72-891" aria-hidden="true" tabindex="-1"></a>R</span>
<span id="cb72-892"><a href="#cb72-892" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-893"><a href="#cb72-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-894"><a href="#cb72-894" aria-hidden="true" tabindex="-1"></a>$\hat{\delta}$ is then obtained as the cross-products of $Q$ and $y$:</span>
<span id="cb72-895"><a href="#cb72-895" aria-hidden="true" tabindex="-1"></a>\idxfun{drop}{base}\idxfun{crossprod}{base}</span>
<span id="cb72-896"><a href="#cb72-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-899"><a href="#cb72-899" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-900"><a href="#cb72-900" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-901"><a href="#cb72-901" aria-hidden="true" tabindex="-1"></a>hdelta <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(Q, y) <span class="sc">%&gt;%</span> drop</span>
<span id="cb72-902"><a href="#cb72-902" aria-hidden="true" tabindex="-1"></a>hdelta</span>
<span id="cb72-903"><a href="#cb72-903" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-904"><a href="#cb72-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-905"><a href="#cb72-905" aria-hidden="true" tabindex="-1"></a>and $\hat{\gamma}$ is obtained by solving recursively $\hat{\delta} = R \hat{\gamma}$:</span>
<span id="cb72-906"><a href="#cb72-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-909"><a href="#cb72-909" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-910"><a href="#cb72-910" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-911"><a href="#cb72-911" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="ot">&lt;-</span> hdelta[<span class="dv">3</span>] <span class="sc">/</span> R[<span class="dv">3</span>, <span class="dv">3</span>]</span>
<span id="cb72-912"><a href="#cb72-912" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> (hdelta[<span class="dv">2</span>] <span class="sc">-</span> R[<span class="dv">2</span>, <span class="dv">3</span>] <span class="sc">*</span> beta_2) <span class="sc">/</span> R[<span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb72-913"><a href="#cb72-913" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> (hdelta[<span class="dv">1</span>] <span class="sc">-</span> R[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">*</span> beta_1  <span class="sc">-</span> R[<span class="dv">1</span>, <span class="dv">3</span>] <span class="sc">*</span> beta_2) <span class="sc">/</span> R[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb72-914"><a href="#cb72-914" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(alpha, beta_1, beta_2)</span>
<span id="cb72-915"><a href="#cb72-915" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-916"><a href="#cb72-916" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{QR decomposition|)}</span>
<span id="cb72-917"><a href="#cb72-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-918"><a href="#cb72-918" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of the estimators {#sec-properties_ols_multiple}</span></span>
<span id="cb72-919"><a href="#cb72-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-920"><a href="#cb72-920" aria-hidden="true" tabindex="-1"></a>In this section, we'll briefly analyze the statistical properties of the OLS estimator with more than one covariate. Most of these properties are similar to the one we have described in @sec-stat_prop_slm. They'll be presented in this section using matrix algebra.</span>
<span id="cb72-921"><a href="#cb72-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-922"><a href="#cb72-922" aria-hidden="true" tabindex="-1"></a><span class="fu">### Unbiasedness of the OLS estimator</span></span>
<span id="cb72-923"><a href="#cb72-923" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{unbiasedness!multiple linear regression model|(}</span>
<span id="cb72-924"><a href="#cb72-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-925"><a href="#cb72-925" aria-hidden="true" tabindex="-1"></a>The vector of slopes can be written as a linear combination of the vector of response</span>
<span id="cb72-926"><a href="#cb72-926" aria-hidden="true" tabindex="-1"></a>and then of vector of the errors:</span>
<span id="cb72-927"><a href="#cb72-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-928"><a href="#cb72-928" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-929"><a href="#cb72-929" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-930"><a href="#cb72-930" aria-hidden="true" tabindex="-1"></a>\hat{\beta}&amp;=&amp;(X^\top\tilde{I}X)^{-1}X^\top\tilde{I}y <span class="sc">\\</span></span>
<span id="cb72-931"><a href="#cb72-931" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;(X^\top\tilde{I}X)^{-1}X^\top\tilde{I}(X\beta+\epsilon)<span class="sc">\\</span></span>
<span id="cb72-932"><a href="#cb72-932" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;\beta+(X^\top\tilde{I}X)^{-1}X^\top\tilde{I}\epsilon <span class="sc">\\</span></span>
<span id="cb72-933"><a href="#cb72-933" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;\beta+(\tilde{X}^\top\tilde{X})^{-1}\tilde{X}^\top\epsilon <span class="sc">\\</span></span>
<span id="cb72-934"><a href="#cb72-934" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-935"><a href="#cb72-935" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hbeta}</span>
<span id="cb72-936"><a href="#cb72-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-937"><a href="#cb72-937" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $\tilde{I} X ^ \top \epsilon = \tilde{X}^\top\epsilon$ is a $K$-length vector containing the product of --&gt;</span></span>
<span id="cb72-938"><a href="#cb72-938" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- every covariates (the column of $X$) in deviation from the sample mean --&gt;</span></span>
<span id="cb72-939"><a href="#cb72-939" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- and the vector of errors: --&gt;</span></span>
<span id="cb72-940"><a href="#cb72-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-941"><a href="#cb72-941" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $$ --&gt;</span></span>
<span id="cb72-942"><a href="#cb72-942" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \tilde{X}^\top\epsilon = --&gt;</span></span>
<span id="cb72-943"><a href="#cb72-943" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \left( --&gt;</span></span>
<span id="cb72-944"><a href="#cb72-944" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{array}{c} --&gt;</span></span>
<span id="cb72-945"><a href="#cb72-945" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n \\ --&gt;</span></span>
<span id="cb72-946"><a href="#cb72-946" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n \\ --&gt;</span></span>
<span id="cb72-947"><a href="#cb72-947" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \vdots \\ --&gt;</span></span>
<span id="cb72-948"><a href="#cb72-948" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \sum_{n=1} ^ N (x_{n1} - \bar{x}_K) \epsilon_n --&gt;</span></span>
<span id="cb72-949"><a href="#cb72-949" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \end{array} --&gt;</span></span>
<span id="cb72-950"><a href="#cb72-950" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \right)  --&gt;</span></span>
<span id="cb72-951"><a href="#cb72-951" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- = \sum_{n = 1} ^ N \psi_n --&gt;</span></span>
<span id="cb72-952"><a href="#cb72-952" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $$ --&gt;</span></span>
<span id="cb72-953"><a href="#cb72-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-954"><a href="#cb72-954" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $\sum_{n = 1} ^ N \psi_n$, evaluated for $\hat{\beta}$ the vector of --&gt;</span></span>
<span id="cb72-955"><a href="#cb72-955" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- slopes estimates is a K-length vector of 0 (i.e., the vector of the --&gt;</span></span>
<span id="cb72-956"><a href="#cb72-956" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- first-order conditions for minimizing the sum of squares residuals, also --&gt;</span></span>
<span id="cb72-957"><a href="#cb72-957" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- called the vector of scores). --&gt;</span></span>
<span id="cb72-958"><a href="#cb72-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-959"><a href="#cb72-959" aria-hidden="true" tabindex="-1"></a>The expected value of $\hat{\beta}$ conditional on $X$ is:</span>
<span id="cb72-960"><a href="#cb72-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-961"><a href="#cb72-961" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-962"><a href="#cb72-962" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\beta}\mid X) = \beta +</span>
<span id="cb72-963"><a href="#cb72-963" aria-hidden="true" tabindex="-1"></a>(\tilde{X}^\top\tilde{X})^{-1}\tilde{X}^\top\mbox{E}(\epsilon \mid X)</span>
<span id="cb72-964"><a href="#cb72-964" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-965"><a href="#cb72-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-966"><a href="#cb72-966" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb72-967"><a href="#cb72-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-968"><a href="#cb72-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-969"><a href="#cb72-969" aria-hidden="true" tabindex="-1"></a>The unbiasedness condition is therefore that</span>
<span id="cb72-970"><a href="#cb72-970" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon \mid X) = 0$, which is a direct generalization of the</span>
<span id="cb72-971"><a href="#cb72-971" aria-hidden="true" tabindex="-1"></a>result obtained for the simple linear regression model, namely $\epsilon$ has a</span>
<span id="cb72-972"><a href="#cb72-972" aria-hidden="true" tabindex="-1"></a>constant expected value (that can be set to 0 without any restriction) whatever the value of the covariates. It implies also that the population covariance between the errors and any of the covariates is 0. </span>
<span id="cb72-973"><a href="#cb72-973" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{unbiasedness!multiple linear regression model|)}</span>
<span id="cb72-974"><a href="#cb72-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-975"><a href="#cb72-975" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variance of the OLS estimator {#sec-variance_ols}</span></span>
<span id="cb72-976"><a href="#cb72-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-977"><a href="#cb72-977" aria-hidden="true" tabindex="-1"></a>The variance of $\hat{\beta}$ is now a matrix of variances and covariances:</span>
<span id="cb72-978"><a href="#cb72-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-979"><a href="#cb72-979" aria-hidden="true" tabindex="-1"></a>$$\mbox{V}(\hat{\beta}\mid X) = </span>
<span id="cb72-980"><a href="#cb72-980" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left<span class="co">[</span><span class="ot">(\hat{\beta}- \beta)(\hat{\beta}- \beta)^\top\mid X\right</span><span class="co">]</span></span>
<span id="cb72-981"><a href="#cb72-981" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-982"><a href="#cb72-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-983"><a href="#cb72-983" aria-hidden="true" tabindex="-1"></a>Using @eq-hbeta:</span>
<span id="cb72-984"><a href="#cb72-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-985"><a href="#cb72-985" aria-hidden="true" tabindex="-1"></a>$$\mbox{V}(\hat{\beta}\mid X) = </span>
<span id="cb72-986"><a href="#cb72-986" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left[(\tilde{X} ^ \top\tilde{X})^{-1}\tilde{X}^\top\epsilon \epsilon ^ \top</span>
<span id="cb72-987"><a href="#cb72-987" aria-hidden="true" tabindex="-1"></a>\tilde{X} (\tilde{X}^\top\tilde{X})^{-1} \mid X\right]</span>
<span id="cb72-988"><a href="#cb72-988" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-989"><a href="#cb72-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-990"><a href="#cb72-990" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-991"><a href="#cb72-991" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}\mid X)=</span>
<span id="cb72-992"><a href="#cb72-992" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}</span>
<span id="cb72-993"><a href="#cb72-993" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N}</span>
<span id="cb72-994"><a href="#cb72-994" aria-hidden="true" tabindex="-1"></a>\tilde{X}^\top\tilde{X}\right)^{-1}\left[\frac{1}{N}\mbox{E}(\tilde{X}^\top \epsilon \epsilon ^</span>
<span id="cb72-995"><a href="#cb72-995" aria-hidden="true" tabindex="-1"></a>\top \tilde{X} \mid X)\right] \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right) ^ {-1}</span>
<span id="cb72-996"><a href="#cb72-996" aria-hidden="true" tabindex="-1"></a>$$ {#eq-general_variance}</span>
<span id="cb72-997"><a href="#cb72-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-998"><a href="#cb72-998" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!multiple linear regression|(}</span>
<span id="cb72-999"><a href="#cb72-999" aria-hidden="true" tabindex="-1"></a>This is a **sandwich** formula, the **meat**:</span>
<span id="cb72-1000"><a href="#cb72-1000" aria-hidden="true" tabindex="-1"></a>$\frac{1}{N}\mbox{E}\left(X'\bar{I}\epsilon \epsilon ^ \top \bar{I} X \mid X\right)$</span>
<span id="cb72-1001"><a href="#cb72-1001" aria-hidden="true" tabindex="-1"></a>being surrounded by two slices of **bread**:</span>
<span id="cb72-1002"><a href="#cb72-1002" aria-hidden="true" tabindex="-1"></a>$\left(\frac{1}{N} \tilde{X}^\top\tilde{X}\right)^{-1}$. Note that the two matrices are</span>
<span id="cb72-1003"><a href="#cb72-1003" aria-hidden="true" tabindex="-1"></a>square and of dimension $K$. The bread is just the inverse of the</span>
<span id="cb72-1004"><a href="#cb72-1004" aria-hidden="true" tabindex="-1"></a>covariance matrix of the covariates.</span>
<span id="cb72-1005"><a href="#cb72-1005" aria-hidden="true" tabindex="-1"></a>The meat is the variance of the score vector, i.e., the vector of the</span>
<span id="cb72-1006"><a href="#cb72-1006" aria-hidden="true" tabindex="-1"></a>first-order conditions. For $K = 2$, it is the expected value of:</span>
<span id="cb72-1007"><a href="#cb72-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1008"><a href="#cb72-1008" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1009"><a href="#cb72-1009" aria-hidden="true" tabindex="-1"></a>\small{</span>
<span id="cb72-1010"><a href="#cb72-1010" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}</span>
<span id="cb72-1011"><a href="#cb72-1011" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1012"><a href="#cb72-1012" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb72-1013"><a href="#cb72-1013" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n)\right) ^ 2 &amp;</span>
<span id="cb72-1014"><a href="#cb72-1014" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n)\right) </span>
<span id="cb72-1015"><a href="#cb72-1015" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n)\right)  <span class="sc">\\</span></span>
<span id="cb72-1016"><a href="#cb72-1016" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{n1} - \bar{x}_1) \epsilon_n)\right) </span>
<span id="cb72-1017"><a href="#cb72-1017" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n)\right) &amp;</span>
<span id="cb72-1018"><a href="#cb72-1018" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{n2} - \bar{x}_2) \epsilon_n)\right) ^ 2</span>
<span id="cb72-1019"><a href="#cb72-1019" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1020"><a href="#cb72-1020" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1021"><a href="#cb72-1021" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-1022"><a href="#cb72-1022" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1023"><a href="#cb72-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1024"><a href="#cb72-1024" aria-hidden="true" tabindex="-1"></a>which is a generalization of the single regression case where the "meat"</span>
<span id="cb72-1025"><a href="#cb72-1025" aria-hidden="true" tabindex="-1"></a>reduces to the scalar</span>
<span id="cb72-1026"><a href="#cb72-1026" aria-hidden="true" tabindex="-1"></a>$\left(\sum_{n=1} ^ N (x_n - \bar{x}) \epsilon_n)\right) ^ 2$.</span>
<span id="cb72-1027"><a href="#cb72-1027" aria-hidden="true" tabindex="-1"></a>As for the simple regression model, the formula of the variance simplifies with the hypothesis that the errors are homoskedastic ($\mbox{E}(\epsilon_n ^ 2 \mid x) = \sigma_\epsilon ^ 2$) and</span>
<span id="cb72-1028"><a href="#cb72-1028" aria-hidden="true" tabindex="-1"></a>uncorrelated ($\mbox{E}(\epsilon_n \epsilon_m \mid x) = 0 \; \forall \; m \neq n$).</span>
<span id="cb72-1029"><a href="#cb72-1029" aria-hidden="true" tabindex="-1"></a>In this case, the meat reduces to</span>
<span id="cb72-1030"><a href="#cb72-1030" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon ^ 2 \frac{1}{N} \tilde{X} ^ \top \tilde{X}$, i.e., up to a scalar to the matrix of covariance of the covariates, and @eq-general_variance becomes:</span>
<span id="cb72-1031"><a href="#cb72-1031" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!multiple linear regression|)}</span>
<span id="cb72-1032"><a href="#cb72-1032" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{homoskedasticity}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{uncorrelation}</span>
<span id="cb72-1033"><a href="#cb72-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1034"><a href="#cb72-1034" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1035"><a href="#cb72-1035" aria-hidden="true" tabindex="-1"></a>V(\hat{\beta})= \sigma_\epsilon^2(\tilde{X}^\top\tilde{X})^{-1}</span>
<span id="cb72-1036"><a href="#cb72-1036" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-vbeta}</span>
<span id="cb72-1037"><a href="#cb72-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1038"><a href="#cb72-1038" aria-hidden="true" tabindex="-1"></a>which can be rewritten, using @eq-XpXm1:</span>
<span id="cb72-1039"><a href="#cb72-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1040"><a href="#cb72-1040" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1041"><a href="#cb72-1041" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta})=</span>
<span id="cb72-1042"><a href="#cb72-1042" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon^2}{N\hat{\sigma}_1\hat{\sigma}_2(1-\hat{\rho}_{12}^2)}</span>
<span id="cb72-1043"><a href="#cb72-1043" aria-hidden="true" tabindex="-1"></a>\left(\begin{array}{cc}</span>
<span id="cb72-1044"><a href="#cb72-1044" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_2}{\hat{\sigma}_1} &amp; -\hat{\rho}_{12}<span class="sc">\\</span></span>
<span id="cb72-1045"><a href="#cb72-1045" aria-hidden="true" tabindex="-1"></a>-\hat{\rho}_{12} &amp; \frac{\hat{\sigma}_1}{\hat{\sigma}_2}<span class="sc">\\</span></span>
<span id="cb72-1046"><a href="#cb72-1046" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1047"><a href="#cb72-1047" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1048"><a href="#cb72-1048" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1049"><a href="#cb72-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1050"><a href="#cb72-1050" aria-hidden="true" tabindex="-1"></a>from which we get:</span>
<span id="cb72-1051"><a href="#cb72-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1052"><a href="#cb72-1052" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1053"><a href="#cb72-1053" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\beta}_k}=</span>
<span id="cb72-1054"><a href="#cb72-1054" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_k\sqrt{1-\hat{\rho}_{12}^2}}</span>
<span id="cb72-1055"><a href="#cb72-1055" aria-hidden="true" tabindex="-1"></a>\mbox{ for }k = 1, 2,\;</span>
<span id="cb72-1056"><a href="#cb72-1056" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\beta}_1\hat{\beta}_2}=</span>
<span id="cb72-1057"><a href="#cb72-1057" aria-hidden="true" tabindex="-1"></a>-\frac{\hat{\rho}_{12}\sigma_\epsilon^2}{N\hat{\sigma}_1\hat{\sigma}_2(1-\hat{\rho}_{12}^2)}</span>
<span id="cb72-1058"><a href="#cb72-1058" aria-hidden="true" tabindex="-1"></a>\mbox{ and } \hat{\rho}_{\hat{\beta}_1\hat{\beta}_2} = -\hat{\rho}_{12}</span>
<span id="cb72-1059"><a href="#cb72-1059" aria-hidden="true" tabindex="-1"></a>$$ {#eq-var_covar_slopes}</span>
<span id="cb72-1060"><a href="#cb72-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1061"><a href="#cb72-1061" aria-hidden="true" tabindex="-1"></a>First remark that if $\hat{\rho}_{12} = 0$, which means that the two</span>
<span id="cb72-1062"><a href="#cb72-1062" aria-hidden="true" tabindex="-1"></a>covariates are uncorrelated, the formula for the standard deviation of a</span>
<span id="cb72-1063"><a href="#cb72-1063" aria-hidden="true" tabindex="-1"></a>slope in the multiple regression model reduces to the formula of the</span>
<span id="cb72-1064"><a href="#cb72-1064" aria-hidden="true" tabindex="-1"></a>single regression model, which means that the standard deviation is</span>
<span id="cb72-1065"><a href="#cb72-1065" aria-hidden="true" tabindex="-1"></a>proportional to:</span>
<span id="cb72-1066"><a href="#cb72-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1067"><a href="#cb72-1067" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the standard deviation of the error,</span>
<span id="cb72-1068"><a href="#cb72-1068" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the inverse of the standard deviation of the corresponding</span>
<span id="cb72-1069"><a href="#cb72-1069" aria-hidden="true" tabindex="-1"></a>    covariate,</span>
<span id="cb72-1070"><a href="#cb72-1070" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the inverse of the square root of the sample size.</span>
<span id="cb72-1071"><a href="#cb72-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1072"><a href="#cb72-1072" aria-hidden="true" tabindex="-1"></a>When the two covariates are correlated, the last term</span>
<span id="cb72-1073"><a href="#cb72-1073" aria-hidden="true" tabindex="-1"></a>$1/\sqrt{1 - \hat{\rho}_{12}^2}$ is added and inflates the standard</span>
<span id="cb72-1074"><a href="#cb72-1074" aria-hidden="true" tabindex="-1"></a>deviation. This means that the more the covariates are correlated</span>
<span id="cb72-1075"><a href="#cb72-1075" aria-hidden="true" tabindex="-1"></a>(whatever the sign of the correlation) the larger is the standard</span>
<span id="cb72-1076"><a href="#cb72-1076" aria-hidden="true" tabindex="-1"></a>deviation of the slope. The intuition is that, if the two covariates are highly correlated, it is difficult to estimate precisely the separate effect of each of them.</span>
<span id="cb72-1077"><a href="#cb72-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1078"><a href="#cb72-1078" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon^2$ being unknown, $\sigma^2_{\hat{\beta}}$ can't be computed. If the error were observed, a natural estimator of $\sigma_\epsilon^2$ would be $\sum_{n=1}^N (\epsilon_n-\bar{\epsilon}) ^ 2/N$. As the errors are unknown, one can use the residuals instead, which are related to the errors by the relation: $\hat{\epsilon} = M_Z y = M_Z (Z\gamma + \epsilon) = M_Z \epsilon$, the last</span>
<span id="cb72-1079"><a href="#cb72-1079" aria-hidden="true" tabindex="-1"></a>equality standing because $Z\gamma$ is a vector of the subspace defined by</span>
<span id="cb72-1080"><a href="#cb72-1080" aria-hidden="true" tabindex="-1"></a>the columns of $Z$ and therefore $M_ZZ\gamma = 0$. Therefore, we have:</span>
<span id="cb72-1081"><a href="#cb72-1081" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon} ^ \top \hat{\epsilon} = \epsilon ^ \top M_Z \epsilon$; as</span>
<span id="cb72-1082"><a href="#cb72-1082" aria-hidden="true" tabindex="-1"></a>it is a scalar, it is equal to its trace. Using the rule of</span>
<span id="cb72-1083"><a href="#cb72-1083" aria-hidden="true" tabindex="-1"></a>permutation, we get:</span>
<span id="cb72-1084"><a href="#cb72-1084" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon} ^ \top \hat{\epsilon} = \epsilon ^ \top M_Z \epsilon = \mbox{tr}\, M_Z \epsilon \epsilon ^ \top$.</span>
<span id="cb72-1085"><a href="#cb72-1085" aria-hidden="true" tabindex="-1"></a>With spherical disturbances, we have</span>
<span id="cb72-1086"><a href="#cb72-1086" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\hat{\epsilon} ^ \top \hat{\epsilon}) = \mbox{tr}\, M_Z \sigma_\epsilon ^ 2 I = \sigma_\epsilon ^ 2 \mbox{tr} M_Z = (N - K - 1) \sigma_\epsilon ^ 2$.</span>
<span id="cb72-1087"><a href="#cb72-1087" aria-hidden="true" tabindex="-1"></a>Therefore, an unbiased estimator of $\sigma_\epsilon^2$ is:</span>
<span id="cb72-1088"><a href="#cb72-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1089"><a href="#cb72-1089" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1090"><a href="#cb72-1090" aria-hidden="true" tabindex="-1"></a>\dot{\sigma}_\epsilon^2 = \frac{\hat{\epsilon}^\top\hat{\epsilon}}{N - K - 1} = </span>
<span id="cb72-1091"><a href="#cb72-1091" aria-hidden="true" tabindex="-1"></a>\frac{\sum_{n=1}^N \hat{\epsilon}_n ^ 2}{N - K - 1}</span>
<span id="cb72-1092"><a href="#cb72-1092" aria-hidden="true" tabindex="-1"></a>$$ {#eq-unbiased_res_se}</span>
<span id="cb72-1093"><a href="#cb72-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1094"><a href="#cb72-1094" aria-hidden="true" tabindex="-1"></a>and replacing $\sigma_\epsilon ^ 2$ by $\dot{\sigma}_\epsilon ^ 2$ in @eq-vbeta, we get an unbiased estimator of the covariance matrix of the estimators:</span>
<span id="cb72-1095"><a href="#cb72-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1096"><a href="#cb72-1096" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1097"><a href="#cb72-1097" aria-hidden="true" tabindex="-1"></a>\hat{V}(\hat{\beta}) = \dot{\sigma}_\epsilon^2(\tilde{X}^\top\tilde{X})^{-1}</span>
<span id="cb72-1098"><a href="#cb72-1098" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-hvbeta}</span>
<span id="cb72-1099"><a href="#cb72-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1100"><a href="#cb72-1100" aria-hidden="true" tabindex="-1"></a>We now go back to the estimation of the growth model. As in @MANK:ROME:WEIL:92\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Mankiw}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Romer}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Weil}, we use a restricted sample by excluding countries for which most of the GDP is linked to oil extraction and those with low quality data. </span>
<span id="cb72-1101"><a href="#cb72-1101" aria-hidden="true" tabindex="-1"></a>\idxdata{growth}{micsr.data}\idxfun{filter}{dplyr}\idxfun{lm}{stats}</span>
<span id="cb72-1102"><a href="#cb72-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1103"><a href="#cb72-1103" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1104"><a href="#cb72-1104" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: growth_subsample</span></span>
<span id="cb72-1105"><a href="#cb72-1105" aria-hidden="true" tabindex="-1"></a>growth_sub <span class="ot">&lt;-</span> growth <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="sc">!</span> group <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"oil"</span>, <span class="st">"lqdata"</span>))</span>
<span id="cb72-1106"><a href="#cb72-1106" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(gdp85) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v), growth_sub)</span>
<span id="cb72-1107"><a href="#cb72-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1108"><a href="#cb72-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1109"><a href="#cb72-1109" aria-hidden="true" tabindex="-1"></a>The covariance matrix of the estimators is obtained using the</span>
<span id="cb72-1110"><a href="#cb72-1110" aria-hidden="true" tabindex="-1"></a><span class="in">`vcov`</span> function:</span>
<span id="cb72-1111"><a href="#cb72-1111" aria-hidden="true" tabindex="-1"></a>\idxfun{vcov}{stats}</span>
<span id="cb72-1112"><a href="#cb72-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1113"><a href="#cb72-1113" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1114"><a href="#cb72-1114" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vcov_function_growth</span></span>
<span id="cb72-1115"><a href="#cb72-1115" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(slw_tot)</span>
<span id="cb72-1116"><a href="#cb72-1116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1117"><a href="#cb72-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1118"><a href="#cb72-1118" aria-hidden="true" tabindex="-1"></a>The <span class="in">`summary`</span> method computes detailed results of the regression, in particular the table of coefficients, a matrix that can be extracted using the <span class="in">`coef`</span> method:</span>
<span id="cb72-1119"><a href="#cb72-1119" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}</span>
<span id="cb72-1120"><a href="#cb72-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1123"><a href="#cb72-1123" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1124"><a href="#cb72-1124" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: summary_growth</span></span>
<span id="cb72-1125"><a href="#cb72-1125" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1126"><a href="#cb72-1126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1127"><a href="#cb72-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1128"><a href="#cb72-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">### The OLS estimator is BLUE</span></span>
<span id="cb72-1129"><a href="#cb72-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1130"><a href="#cb72-1130" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{best linear unbiased estimator!multiple linear regression model|(}</span>
<span id="cb72-1131"><a href="#cb72-1131" aria-hidden="true" tabindex="-1"></a>The demonstration made in @sec-simple_ols_blue for the simple linear model can easily be extended to</span>
<span id="cb72-1132"><a href="#cb72-1132" aria-hidden="true" tabindex="-1"></a>the multiple regression model. The OLS estimator is:</span>
<span id="cb72-1133"><a href="#cb72-1133" aria-hidden="true" tabindex="-1"></a>$\hat{\gamma} = (Z^\top Z)^{-1}Z^\top y = \gamma + (Z^\top Z)^{-1}Z^\top \epsilon$.</span>
<span id="cb72-1134"><a href="#cb72-1134" aria-hidden="true" tabindex="-1"></a>Consider another linear estimator:</span>
<span id="cb72-1135"><a href="#cb72-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1136"><a href="#cb72-1136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1137"><a href="#cb72-1137" aria-hidden="true" tabindex="-1"></a>\tilde{\gamma}=Ay=\left<span class="co">[</span><span class="ot">(Z^\top Z)^{-1}Z^\top + D\right</span><span class="co">]</span>y = (I + DZ)\gamma + \left<span class="co">[</span><span class="ot">(Z^\top Z)^{-1}Z^\top + D\right</span><span class="co">]</span>\epsilon</span>
<span id="cb72-1138"><a href="#cb72-1138" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-1139"><a href="#cb72-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1140"><a href="#cb72-1140" aria-hidden="true" tabindex="-1"></a>The unbiasedness of $\tilde{\gamma}$ implies that $DZ=0$, so that:</span>
<span id="cb72-1141"><a href="#cb72-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1142"><a href="#cb72-1142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1143"><a href="#cb72-1143" aria-hidden="true" tabindex="-1"></a>\tilde{\gamma}-\gamma= \left<span class="co">[</span><span class="ot">(Z^\top Z)^{-1}Z^\top + D\right</span><span class="co">]</span>\epsilon = (\hat{\gamma}- \gamma) + D\epsilon = (\hat{\gamma}- \gamma) + Dy</span>
<span id="cb72-1144"><a href="#cb72-1144" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-1145"><a href="#cb72-1145" aria-hidden="true" tabindex="-1"></a>because $DZ=0$ implies that $D\epsilon=Dy$. Therefore,</span>
<span id="cb72-1146"><a href="#cb72-1146" aria-hidden="true" tabindex="-1"></a>$\tilde{\gamma}- \hat{\gamma}=Dy$. The covariance between the OLS estimator and the vector of differences of the two estimators is:</span>
<span id="cb72-1147"><a href="#cb72-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1148"><a href="#cb72-1148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1149"><a href="#cb72-1149" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left<span class="co">[</span><span class="ot">(\tilde{\gamma}-\hat{\gamma})(\hat{\gamma} - \gamma)^\top\right</span><span class="co">]</span> = \mbox{E}\left<span class="co">[</span><span class="ot">D\epsilon\epsilon^\top Z(Z^\top Z) ^ {-1}\right</span><span class="co">]</span></span>
<span id="cb72-1150"><a href="#cb72-1150" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-1151"><a href="#cb72-1151" aria-hidden="true" tabindex="-1"></a>with spherical disturbances, this reduces to:</span>
<span id="cb72-1152"><a href="#cb72-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1153"><a href="#cb72-1153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1154"><a href="#cb72-1154" aria-hidden="true" tabindex="-1"></a>\mbox{E}\left<span class="co">[</span><span class="ot">(\tilde{\gamma}-\hat{\gamma})(\hat{\gamma} - \gamma)^\top\right</span><span class="co">]</span> = \sigma_\epsilon ^ 2DZ(Z^\top Z) ^ {-1} =0</span>
<span id="cb72-1155"><a href="#cb72-1155" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-1156"><a href="#cb72-1156" aria-hidden="true" tabindex="-1"></a>Therefore, we can write the variance of $\tilde{\gamma}$ as:</span>
<span id="cb72-1157"><a href="#cb72-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1158"><a href="#cb72-1158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1159"><a href="#cb72-1159" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\tilde{\gamma}) = \mbox{V}\left<span class="co">[</span><span class="ot">\hat{\gamma} + (\tilde{\gamma}-\hat{\gamma})\right</span><span class="co">]</span> = \mbox{V}(\hat{\gamma}) + \mbox{V}(Dy)</span>
<span id="cb72-1160"><a href="#cb72-1160" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-1161"><a href="#cb72-1161" aria-hidden="true" tabindex="-1"></a>as $\mbox{V}(Dy)$ is a covariance matrix, it is semi-definite</span>
<span id="cb72-1162"><a href="#cb72-1162" aria-hidden="true" tabindex="-1"></a>positive and, therefore, the difference between the variance matrix of</span>
<span id="cb72-1163"><a href="#cb72-1163" aria-hidden="true" tabindex="-1"></a>any unbiased linear estimator and the OLS estimator is a</span>
<span id="cb72-1164"><a href="#cb72-1164" aria-hidden="true" tabindex="-1"></a>semi-definite positive matrix, which means that the OLS estimator is</span>
<span id="cb72-1165"><a href="#cb72-1165" aria-hidden="true" tabindex="-1"></a>the most efficient linear unbiased estimator.</span>
<span id="cb72-1166"><a href="#cb72-1166" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{best linear unbiased estimator!multiple linear regression model|)}</span>
<span id="cb72-1167"><a href="#cb72-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1168"><a href="#cb72-1168" aria-hidden="true" tabindex="-1"></a><span class="fu">### Asymptotic properties of the OLS estimator</span></span>
<span id="cb72-1169"><a href="#cb72-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1170"><a href="#cb72-1170" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic properties!multiple linear regression model|(}</span>
<span id="cb72-1171"><a href="#cb72-1171" aria-hidden="true" tabindex="-1"></a>Asymptotic properties of the multiple regression model</span>
<span id="cb72-1172"><a href="#cb72-1172" aria-hidden="true" tabindex="-1"></a>are direct extensions of those we have seen for the simple regression</span>
<span id="cb72-1173"><a href="#cb72-1173" aria-hidden="true" tabindex="-1"></a>model. With $\mbox{E}(\hat{\beta}) = \beta$ and </span>
<span id="cb72-1174"><a href="#cb72-1174" aria-hidden="true" tabindex="-1"></a>$\mbox{V}(\hat{\beta}) = \frac{\sigma_\epsilon ^ 2}{N} \left(\frac{1}{N}\tilde{X}^ \top X\right) ^ {- 1}$, the OLS estimator is consistent</span>
<span id="cb72-1175"><a href="#cb72-1175" aria-hidden="true" tabindex="-1"></a>($\mbox{plim} \;\hat{\beta} = \beta$) if the covariance matrix of the</span>
<span id="cb72-1176"><a href="#cb72-1176" aria-hidden="true" tabindex="-1"></a>covariates $\frac{1}{N}\tilde{X} ^  \top \tilde{X}$ converges to a finite</span>
<span id="cb72-1177"><a href="#cb72-1177" aria-hidden="true" tabindex="-1"></a>matrix. The central-limit theorem implies that:</span>
<span id="cb72-1178"><a href="#cb72-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1179"><a href="#cb72-1179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1180"><a href="#cb72-1180" aria-hidden="true" tabindex="-1"></a>\sqrt{N}(\hat{\beta}_N - \beta) \xrightarrow{d} \mathcal{N}\left(0,</span>
<span id="cb72-1181"><a href="#cb72-1181" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon ^ 2 \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right)^ {-1}\right)</span>
<span id="cb72-1182"><a href="#cb72-1182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1183"><a href="#cb72-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1184"><a href="#cb72-1184" aria-hidden="true" tabindex="-1"></a>or</span>
<span id="cb72-1185"><a href="#cb72-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1186"><a href="#cb72-1186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1187"><a href="#cb72-1187" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_N \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_\epsilon ^ 2}{N} \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right)^ {-1}\right)</span>
<span id="cb72-1188"><a href="#cb72-1188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1189"><a href="#cb72-1189" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{asymptotic properties!multiple linear regression model|)}</span>
<span id="cb72-1190"><a href="#cb72-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1191"><a href="#cb72-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">### The coefficient of determination</span></span>
<span id="cb72-1192"><a href="#cb72-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1193"><a href="#cb72-1193" aria-hidden="true" tabindex="-1"></a>From @eq-r2_slrm, one way to write the coefficient of determination for the simple linear regression model is:</span>
<span id="cb72-1194"><a href="#cb72-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1195"><a href="#cb72-1195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1196"><a href="#cb72-1196" aria-hidden="true" tabindex="-1"></a>R^2 = 1 - \frac{\sum_{n=1} ^ N \hat{\epsilon}_n^2}{\sum_{n=1} ^ N (y_n - \hat{y})^2} = 1 - \frac{\hat{\sigma}_\epsilon^2}{\hat{\sigma}_y^2}</span>
<span id="cb72-1197"><a href="#cb72-1197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1198"><a href="#cb72-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1199"><a href="#cb72-1199" aria-hidden="true" tabindex="-1"></a>The last term is obtained by dividing the residual sum of squares and the total variation of $y$ by the sample size $N$. This gives rise to biased estimators of the variances of the errors and of the response. Consider now the unbiased estimators: the numerator should then be divided by $N - K - 1$ and $\dot{\sigma}_\epsilon ^ 2$ is obtained. Similarly, the denominator should be divided by $N-1$ to obtain an unbiased estimation of the variance of the response. We then obtain the **adjusted coefficient of determination**:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{coefficient of determination!adjusted}</span>
<span id="cb72-1200"><a href="#cb72-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1201"><a href="#cb72-1201" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1202"><a href="#cb72-1202" aria-hidden="true" tabindex="-1"></a>\bar{R}^2 = 1 - \frac{\dot{\sigma}_\epsilon^2}{\dot{\sigma}_y^2}= 1 - \frac{\sum_{n=1} ^ N \hat{\epsilon}_n^2 / (N-K-1)}{\sum_{n=1} ^ N (y_n - \hat{y})^2 / (N-1)} =</span>
<span id="cb72-1203"><a href="#cb72-1203" aria-hidden="true" tabindex="-1"></a>1 - \frac{N - 1}{N - K - 1}\frac{\sum_{n=1} ^ N \hat{\epsilon}_n^2}{\sum_{n=1} ^ N (y_n - \hat{y})^2}</span>
<span id="cb72-1204"><a href="#cb72-1204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1205"><a href="#cb72-1205" aria-hidden="true" tabindex="-1"></a>$R^2$ necessarily increases when one more covariate is added to the regression because, even if this covariate is irrelevant, its sample correlation with the response will never be exactly 0 and therefore the sum of square will decrease. This is not the case with $\bar{R}^2$ because $\dot{\sigma}^2_\epsilon$ is the ratio of two terms which both increase when a covariate is added. Note also that $\bar{R}^2$ is not necessarily positive.</span>
<span id="cb72-1206"><a href="#cb72-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1207"><a href="#cb72-1207" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb72-1208"><a href="#cb72-1208" aria-hidden="true" tabindex="-1"></a><span class="fu">## Confidence interval and test {#sec-confint_test_multiple}</span></span>
<span id="cb72-1209"><a href="#cb72-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1210"><a href="#cb72-1210" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!multiple linear regression model|(}</span>
<span id="cb72-1211"><a href="#cb72-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1212"><a href="#cb72-1212" aria-hidden="true" tabindex="-1"></a>In @sec-confint_test_slm, we have seen how to compute a confidence interval for a single parameter and how to perform a test for one hypothesis. The confidence interval was a segment, i.e., a range of values that contains the true value of the parameter with a given probability, and the tests were performed using a normal or a Student distribution. </span>
<span id="cb72-1213"><a href="#cb72-1213" aria-hidden="true" tabindex="-1"></a>Of course, the same kind of analysis can be performed with a multiple regression. But, in this latter case:</span>
<span id="cb72-1214"><a href="#cb72-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1215"><a href="#cb72-1215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a confidence interval can also be computed for several coefficients one at a time,</span>
<span id="cb72-1216"><a href="#cb72-1216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>tests of multiple hypotheses can be performed, using a chi-squared or a Fisher distribution.</span>
<span id="cb72-1217"><a href="#cb72-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1218"><a href="#cb72-1218" aria-hidden="true" tabindex="-1"></a>To illustrate these points, we'll use the Solow model. Remember that the model to estimate is:</span>
<span id="cb72-1219"><a href="#cb72-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1220"><a href="#cb72-1220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1221"><a href="#cb72-1221" aria-hidden="true" tabindex="-1"></a>\ln y = \alpha + \beta_i \ln i + \beta_v \ln v + \epsilon</span>
<span id="cb72-1222"><a href="#cb72-1222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1223"><a href="#cb72-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1224"><a href="#cb72-1224" aria-hidden="true" tabindex="-1"></a>where $y$ is the per capita gdp, $i$ the investment rate and $v$ the sum of the labor force growth rate, the depreciation rate and the technical progress rate. Moreover, the relation between $\beta_i$ and $\beta_v$ and the structural parameter $\kappa$, which is the share of profits in the GDP is $\beta_i = - \beta_v =\kappa / (1 -\kappa)$. </span>
<span id="cb72-1225"><a href="#cb72-1225" aria-hidden="true" tabindex="-1"></a>We'll analyze the confidence interval for the couple of coefficients $(\beta_i, \beta_v)$ and we'll test two hypotheses:</span>
<span id="cb72-1226"><a href="#cb72-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1227"><a href="#cb72-1227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the first is imposed by the model; we must have $\beta_i + \beta_v = 0$,</span>
<span id="cb72-1228"><a href="#cb72-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the second corresponds to a reasonable value of the share of profits, which is approximately one-third; therefore, we'll test the hypothesis that $\kappa = 1/3$, which implies that $\beta_i = 0.5$.</span>
<span id="cb72-1229"><a href="#cb72-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1230"><a href="#cb72-1230" aria-hidden="true" tabindex="-1"></a>Note that these two hypotheses imply that $\beta_v=-0.5$.</span>
<span id="cb72-1231"><a href="#cb72-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1232"><a href="#cb72-1232" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple confidence interval and test</span></span>
<span id="cb72-1233"><a href="#cb72-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1234"><a href="#cb72-1234" aria-hidden="true" tabindex="-1"></a>The asymptotic distribution of the estimators is a multivariate normal</span>
<span id="cb72-1235"><a href="#cb72-1235" aria-hidden="true" tabindex="-1"></a>distribution.^[If the errors are normal, the exact distribution of the</span>
<span id="cb72-1236"><a href="#cb72-1236" aria-hidden="true" tabindex="-1"></a>estimators is normal (see @sec-clt).] The distribution of one</span>
<span id="cb72-1237"><a href="#cb72-1237" aria-hidden="true" tabindex="-1"></a>estimator (say $\hat{\beta}_1$)^[Or of a linear combination of several</span>
<span id="cb72-1238"><a href="#cb72-1238" aria-hidden="true" tabindex="-1"></a>estimators.] is a univariate normal distribution with, for the two covariates case, a standard deviation equal to: $\sigma_{\hat{\beta}_1} = \frac{\sigma_\epsilon}{\sqrt{N}\hat{\sigma}_1\sqrt{1 - \hat{\rho}_{12} ^ 2}}$. Therefore $(\hat{\beta}_1 - \beta_1) / \sigma_{\hat{\beta}_1}$ follows (exactly if $\epsilon$ is normal) a standard normal distribution. </span>
<span id="cb72-1239"><a href="#cb72-1239" aria-hidden="true" tabindex="-1"></a>Confidence interval and tests for a coefficient are therefore computed exactly as for the case of the simple linear regression model. In particular, using @eq-var_covar_slopes and @eq-unbiased_res_se, we get:</span>
<span id="cb72-1240"><a href="#cb72-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1241"><a href="#cb72-1241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1242"><a href="#cb72-1242" aria-hidden="true" tabindex="-1"></a>t_k = \frac{\hat{\beta}_k - \beta_k}{\hat{\sigma}_{\hat{\beta}_k}} = </span>
<span id="cb72-1243"><a href="#cb72-1243" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\beta}_k - \beta_k}{\frac{\dot{\sigma}_\epsilon}{\sqrt{N}\hat{\sigma}_1\sqrt{1 -\hat{\rho}_{12}^2}}}</span>
<span id="cb72-1244"><a href="#cb72-1244" aria-hidden="true" tabindex="-1"></a>$$ {#eq-student_multiple}</span>
<span id="cb72-1245"><a href="#cb72-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1246"><a href="#cb72-1246" aria-hidden="true" tabindex="-1"></a>that follows exactly a Student distribution with $N-K-1$ degrees of freedom if the errors are normal and asymptotically a normal distribution whatever the distribution of the errors. Therefore, $1 - \alpha$ confidence intervals are: $\hat{\beta}_k \pm \mbox{cv}_{1-\alpha / 2} \dot{\sigma}_{\hat{\beta}_k}$ where $\mbox{cv}_{1-\alpha / 2}$ is the critical value of either a Student or a</span>
<span id="cb72-1247"><a href="#cb72-1247" aria-hidden="true" tabindex="-1"></a>normal distribution. For a given hypothesis: $H_0:\beta_k = \beta_{k0}$,</span>
<span id="cb72-1248"><a href="#cb72-1248" aria-hidden="true" tabindex="-1"></a>$t_{k0}=(\hat{\beta}_k - \beta_{k0})/\hat{\sigma}_{\hat{\beta}_k}$</span>
<span id="cb72-1249"><a href="#cb72-1249" aria-hidden="true" tabindex="-1"></a>is a draw on a normal or a Student distribution if $H_0$ is true.</span>
<span id="cb72-1250"><a href="#cb72-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1251"><a href="#cb72-1251" aria-hidden="true" tabindex="-1"></a>Remember that "reasonable" values of the two slopes in our growth model should be $+/-0.5$. We check whether these values are in the confidence intervals, using the <span class="in">`confint`</span> function:</span>
<span id="cb72-1252"><a href="#cb72-1252" aria-hidden="true" tabindex="-1"></a>\idxfun{confint}{stats}</span>
<span id="cb72-1253"><a href="#cb72-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1254"><a href="#cb72-1254" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1255"><a href="#cb72-1255" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confint_growth</span></span>
<span id="cb72-1256"><a href="#cb72-1256" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(slw_tot, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb72-1257"><a href="#cb72-1257" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1258"><a href="#cb72-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1259"><a href="#cb72-1259" aria-hidden="true" tabindex="-1"></a>$0.5$ and $-0.5$ are not in the 95% confidence interval for respectively $\hat{\beta}_i$ and $\hat{\beta}_v$. </span>
<span id="cb72-1260"><a href="#cb72-1260" aria-hidden="true" tabindex="-1"></a>The hypotheses that the true values of the parameters are 0 is very easy to compute, as they are based on the t statistics that are routinely returned by the <span class="in">`summary`</span> method for <span class="in">`lm`</span> objects.</span>
<span id="cb72-1261"><a href="#cb72-1261" aria-hidden="true" tabindex="-1"></a>But, in our example, the hypothesis that $\beta_i = 0$ is the hypothesis that the share of profits is 0, which is of little interest. More interestingly, we could check the hypothesis that the coefficients are equal to $\pm 0.5$. In this case, we can "manually" compute the test statistics by extracting the relevant elements in the matrix returned by <span class="in">`coef(summary(x))`</span>:</span>
<span id="cb72-1262"><a href="#cb72-1262" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}</span>
<span id="cb72-1263"><a href="#cb72-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1264"><a href="#cb72-1264" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1265"><a href="#cb72-1265" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: extraction_summary_coef</span></span>
<span id="cb72-1266"><a href="#cb72-1266" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1267"><a href="#cb72-1267" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> slw_tot <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1268"><a href="#cb72-1268" aria-hidden="true" tabindex="-1"></a>(v[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">1</span>] <span class="sc">-</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="sc">-</span> <span class="fl">0.5</span>)) <span class="sc">/</span> v[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">2</span>]</span>
<span id="cb72-1269"><a href="#cb72-1269" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1270"><a href="#cb72-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1271"><a href="#cb72-1271" aria-hidden="true" tabindex="-1"></a>which confirms that both hypotheses are rejected. The same kind of linear hypothesis on one coefficient can be simpler tested by using a different parametrization of the same model. The trick is to write the model in such a way that a subset of coefficients are 0 if the hypothesis is true. For example, in order to test the hypothesis that $\beta_i = 0.5$, we must have in the model $(\beta_i - 0.5)\ln i$; therefore, the term $-0.5\ln i$ is added on the right side of the formula and should therefore be added also on the left side. Adding also $0.5 \ln v$ on both sides of the formula, we finally get:</span>
<span id="cb72-1272"><a href="#cb72-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1273"><a href="#cb72-1273" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1274"><a href="#cb72-1274" aria-hidden="true" tabindex="-1"></a>(\ln y - 0.5 \ln i + 0.5 \ln v) = \alpha + (\beta_i - 0.5) \ln i + (\beta_v + 0.5) \ln v + \epsilon</span>
<span id="cb72-1275"><a href="#cb72-1275" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1276"><a href="#cb72-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1277"><a href="#cb72-1277" aria-hidden="true" tabindex="-1"></a>and the two slopes are now equal to 0 under $H_0$.</span>
<span id="cb72-1278"><a href="#cb72-1278" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{lm}{stats}</span>
<span id="cb72-1279"><a href="#cb72-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1280"><a href="#cb72-1280" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1281"><a href="#cb72-1281" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: constrained_coefs_pm05</span></span>
<span id="cb72-1282"><a href="#cb72-1282" aria-hidden="true" tabindex="-1"></a>slw_totb <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(gdp85 <span class="sc">*</span> <span class="fu">sqrt</span>(v <span class="sc">/</span> i) ) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v), growth_sub)</span>
<span id="cb72-1283"><a href="#cb72-1283" aria-hidden="true" tabindex="-1"></a>slw_totb <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1284"><a href="#cb72-1284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1285"><a href="#cb72-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1286"><a href="#cb72-1286" aria-hidden="true" tabindex="-1"></a>which gives exactly the same values for the $t$ statistics.</span>
<span id="cb72-1287"><a href="#cb72-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1288"><a href="#cb72-1288" aria-hidden="true" tabindex="-1"></a>A simple hypothesis may also concern a linear combination of several coefficients and not the value of one coefficient. For example, the structural growth model implies that</span>
<span id="cb72-1289"><a href="#cb72-1289" aria-hidden="true" tabindex="-1"></a>$\beta_i + \beta_v = 0$.</span>
<span id="cb72-1290"><a href="#cb72-1290" aria-hidden="true" tabindex="-1"></a>If the hypothesis is true,</span>
<span id="cb72-1291"><a href="#cb72-1291" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\hat{\beta}_i + \hat{\beta}_v) = 0$, the variance being </span>
<span id="cb72-1292"><a href="#cb72-1292" aria-hidden="true" tabindex="-1"></a>$\mbox{V}(\hat{\beta}_i + \hat{\beta}_v) = \hat{\sigma}_{\hat{\beta}_1}^2 + \hat{\sigma}_{\hat{\beta}_2}^2 + 2 \hat{\sigma}_{\hat{\beta}_1\hat{\beta}_2}$, </span>
<span id="cb72-1293"><a href="#cb72-1293" aria-hidden="true" tabindex="-1"></a>the statistic can then be computed by extracting the relevant elements of the covariance matrix of the fitted model:</span>
<span id="cb72-1294"><a href="#cb72-1294" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{vcov}{stats}\idxfun{df.residual}{stats}\idxfun{unname}{base}\idxfun{pt}{stats}\idxfun{sqrt}{base}</span>
<span id="cb72-1295"><a href="#cb72-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1296"><a href="#cb72-1296" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1297"><a href="#cb72-1297" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: test_sum_two_coefficients</span></span>
<span id="cb72-1298"><a href="#cb72-1298" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1299"><a href="#cb72-1299" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">vcov</span>(slw_tot)</span>
<span id="cb72-1300"><a href="#cb72-1300" aria-hidden="true" tabindex="-1"></a>v_sum <span class="ot">&lt;-</span> v[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">+</span> v[<span class="dv">3</span>, <span class="dv">3</span>] <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> v[<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb72-1301"><a href="#cb72-1301" aria-hidden="true" tabindex="-1"></a>stat_sum <span class="ot">&lt;-</span> (<span class="fu">coef</span>(slw_tot)[<span class="dv">2</span>] <span class="sc">+</span> <span class="fu">coef</span>(slw_tot)[<span class="dv">3</span>]) <span class="sc">%&gt;%</span> unname</span>
<span id="cb72-1302"><a href="#cb72-1302" aria-hidden="true" tabindex="-1"></a>t_sum <span class="ot">&lt;-</span> stat_sum <span class="sc">/</span> <span class="fu">sqrt</span>(v_sum) <span class="sc">%&gt;%</span> unname</span>
<span id="cb72-1303"><a href="#cb72-1303" aria-hidden="true" tabindex="-1"></a>pval_sum <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">pt</span>(<span class="fu">abs</span>(t_sum), <span class="at">df =</span> <span class="fu">df.residual</span>(slw_tot), </span>
<span id="cb72-1304"><a href="#cb72-1304" aria-hidden="true" tabindex="-1"></a>                   <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb72-1305"><a href="#cb72-1305" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">stat =</span> stat_sum, <span class="at">t =</span> t_sum, <span class="at">pv =</span> pval_sum)</span>
<span id="cb72-1306"><a href="#cb72-1306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1307"><a href="#cb72-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1308"><a href="#cb72-1308" aria-hidden="true" tabindex="-1"></a>The hypothesis is therefore not rejected, even at the 10% level. Once again, such a linear hypothesis can be more easily tested using a different parametrization. Introducing in the model the term $(\beta_i + \beta_v) \ln i$, subtracting $\beta_v \ln_i$ and rearranging terms:</span>
<span id="cb72-1309"><a href="#cb72-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1310"><a href="#cb72-1310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1311"><a href="#cb72-1311" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-1312"><a href="#cb72-1312" aria-hidden="true" tabindex="-1"></a>\ln y &amp;=&amp; \alpha + (\beta_i + \beta_v) \ln i + \beta_v \ln v -</span>
<span id="cb72-1313"><a href="#cb72-1313" aria-hidden="true" tabindex="-1"></a>\beta_v \ln i+ \epsilon <span class="sc">\\</span></span>
<span id="cb72-1314"><a href="#cb72-1314" aria-hidden="true" tabindex="-1"></a>&amp; = &amp; \alpha + (\beta_i + \beta_v) \ln i + \beta_v</span>
<span id="cb72-1315"><a href="#cb72-1315" aria-hidden="true" tabindex="-1"></a>\ln \frac{v}{i} + \epsilon </span>
<span id="cb72-1316"><a href="#cb72-1316" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1317"><a href="#cb72-1317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1318"><a href="#cb72-1318" aria-hidden="true" tabindex="-1"></a>We then have a model for which the two covariates are now $\ln i$ and $\ln v / i$, the hypothesis being that the coefficient associated to $\ln i$ is equal to 0.</span>
<span id="cb72-1319"><a href="#cb72-1319" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-1320"><a href="#cb72-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1321"><a href="#cb72-1321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1322"><a href="#cb72-1322" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: sum_two_coef_repar</span></span>
<span id="cb72-1323"><a href="#cb72-1323" aria-hidden="true" tabindex="-1"></a>slw_totc <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(gdp85) <span class="sc">~</span> <span class="fu">log</span>(i) <span class="sc">+</span> <span class="fu">log</span>(v <span class="sc">/</span> i), growth_sub)</span>
<span id="cb72-1324"><a href="#cb72-1324" aria-hidden="true" tabindex="-1"></a>slw_totc <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1325"><a href="#cb72-1325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1326"><a href="#cb72-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1327"><a href="#cb72-1327" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1328"><a href="#cb72-1328" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb72-1329"><a href="#cb72-1329" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: confellipse</span></span>
<span id="cb72-1330"><a href="#cb72-1330" aria-hidden="true" tabindex="-1"></a>confellipse <span class="ot">&lt;-</span> <span class="cf">function</span>(object){</span>
<span id="cb72-1331"><a href="#cb72-1331" aria-hidden="true" tabindex="-1"></a>    v <span class="ot">&lt;-</span> car<span class="sc">::</span><span class="fu">confidenceEllipse</span>(object, <span class="at">level =</span> <span class="fu">c</span>(<span class="fl">0.99</span>, <span class="fl">0.95</span>, <span class="fl">0.9</span>), <span class="at">draw =</span> <span class="cn">FALSE</span>)</span>
<span id="cb72-1332"><a href="#cb72-1332" aria-hidden="true" tabindex="-1"></a>    growth_ell <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(<span class="st">"rbind"</span>, <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(x)</span>
<span id="cb72-1333"><a href="#cb72-1333" aria-hidden="true" tabindex="-1"></a>        <span class="fu">cbind</span>(<span class="at">level =</span> <span class="fu">as.character</span>(<span class="fu">names</span>(v)[x]),</span>
<span id="cb72-1334"><a href="#cb72-1334" aria-hidden="true" tabindex="-1"></a>              <span class="fu">data.frame</span>(v[[x]]))</span>
<span id="cb72-1335"><a href="#cb72-1335" aria-hidden="true" tabindex="-1"></a>        )) <span class="sc">%&gt;%</span> as_tibble</span>
<span id="cb72-1336"><a href="#cb72-1336" aria-hidden="true" tabindex="-1"></a>    ci <span class="ot">&lt;-</span> <span class="fu">confint</span>(object, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb72-1337"><a href="#cb72-1337" aria-hidden="true" tabindex="-1"></a>    ebx <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> ci[<span class="dv">2</span>, <span class="dv">1</span>], <span class="at">xend =</span> ci[<span class="dv">2</span>, <span class="dv">2</span>], <span class="at">y =</span> <span class="fu">coef</span>(object)[<span class="dv">3</span>])</span>
<span id="cb72-1338"><a href="#cb72-1338" aria-hidden="true" tabindex="-1"></a>    eby <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> ci[<span class="dv">3</span>, <span class="dv">1</span>], <span class="at">yend =</span> ci[<span class="dv">3</span>, <span class="dv">2</span>], <span class="at">x =</span> <span class="fu">coef</span>(object)[<span class="dv">2</span>])</span>
<span id="cb72-1339"><a href="#cb72-1339" aria-hidden="true" tabindex="-1"></a>    growth_ell <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_path</span>(<span class="fu">aes</span>(<span class="at">linetype =</span> level)) <span class="sc">+</span></span>
<span id="cb72-1340"><a href="#cb72-1340" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_segment</span>(<span class="at">data =</span> ebx, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> xend, <span class="at">y =</span> y, <span class="at">yend =</span> y), <span class="at">lwd =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb72-1341"><a href="#cb72-1341" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_segment</span>(<span class="at">data =</span> eby, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y =</span> y, <span class="at">yend =</span> yend), <span class="at">lwd =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb72-1342"><a href="#cb72-1342" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">coef</span>(object)[<span class="dv">2</span>], <span class="at">y =</span> <span class="fu">coef</span>(object)[<span class="dv">3</span>]), <span class="at">shape =</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="dv">4</span>)</span>
<span id="cb72-1343"><a href="#cb72-1343" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-1344"><a href="#cb72-1344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1345"><a href="#cb72-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1346"><a href="#cb72-1346" aria-hidden="true" tabindex="-1"></a><span class="fu">### Joint confidence interval and test of joint hypothesis</span></span>
<span id="cb72-1347"><a href="#cb72-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1348"><a href="#cb72-1348" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Joint confidence interval</span></span>
<span id="cb72-1349"><a href="#cb72-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1350"><a href="#cb72-1350" aria-hidden="true" tabindex="-1"></a>We now consider the computation of confidence interval for more than one parameter. The distribution of $\hat{\beta}$ is:</span>
<span id="cb72-1351"><a href="#cb72-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1352"><a href="#cb72-1352" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1353"><a href="#cb72-1353" aria-hidden="true" tabindex="-1"></a>\hat{\beta} \stackrel{a}{\sim} \mathcal{N}\left(\beta, \frac{\sigma_\epsilon ^ 2}{N} \left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right)^ {-1}\right)</span>
<span id="cb72-1354"><a href="#cb72-1354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1355"><a href="#cb72-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1356"><a href="#cb72-1356" aria-hidden="true" tabindex="-1"></a>In the simple regression model, subtracting the expected value and dividing by the standard deviation, we get a standard normal deviate. Taking the square, we get a $\chi^2$ with 1 degree of freedom. If the $K$ slopes are uncorrelated, $\sum_{k=1} ^ K(\hat{\beta}_k - \beta_{k0}) ^ 2/\sigma_{\hat{\beta}_k} ^ 2$ is a $\chi^2$ with $K$ degrees of freedom. </span>
<span id="cb72-1357"><a href="#cb72-1357" aria-hidden="true" tabindex="-1"></a>If the slopes are correlated, this correlation should be "corrected"; more precisely, a quadratic form of the vector of slopes in deviation from its expectation with the inverse of its variance should be computed:</span>
<span id="cb72-1358"><a href="#cb72-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1359"><a href="#cb72-1359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1360"><a href="#cb72-1360" aria-hidden="true" tabindex="-1"></a>q_K = (\hat{\beta}-\beta) ^ \top</span>
<span id="cb72-1361"><a href="#cb72-1361" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta})^{-1} (\hat{\beta}-\beta)=</span>
<span id="cb72-1362"><a href="#cb72-1362" aria-hidden="true" tabindex="-1"></a>(\hat{\beta}-\beta) ^ \top \frac{N}{\sigma_\epsilon ^ 2}</span>
<span id="cb72-1363"><a href="#cb72-1363" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N}\tilde{X}^\top\tilde{X}\right) (\hat{\beta}-\beta)</span>
<span id="cb72-1364"><a href="#cb72-1364" aria-hidden="true" tabindex="-1"></a>\sim \chi^2_K</span>
<span id="cb72-1365"><a href="#cb72-1365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1366"><a href="#cb72-1366" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Note that, if $K=1$, $\hat{\beta}- \beta$ and $\mbox{V}(\hat{\beta})$ are scalars and the expression reduce to $(\hat{\beta}- \beta) / \sigma_{\hat{\beta}}^2$, which is a square of the t statistic. --&gt;</span></span>
<span id="cb72-1367"><a href="#cb72-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1368"><a href="#cb72-1368" aria-hidden="true" tabindex="-1"></a>For $K=2$, we have:</span>
<span id="cb72-1369"><a href="#cb72-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1370"><a href="#cb72-1370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1371"><a href="#cb72-1371" aria-hidden="true" tabindex="-1"></a>q_2 = \left(\hat{\beta}_1 - \beta_1, \hat{\beta}_2 - \beta_2 \right)</span>
<span id="cb72-1372"><a href="#cb72-1372" aria-hidden="true" tabindex="-1"></a>\frac{N}{\sigma_\epsilon ^ 2}</span>
<span id="cb72-1373"><a href="#cb72-1373" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1374"><a href="#cb72-1374" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb72-1375"><a href="#cb72-1375" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_1 ^ 2 &amp; \hat{\sigma}_{12} <span class="sc">\\</span> \</span>
<span id="cb72-1376"><a href="#cb72-1376" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{12} &amp; \hat{\sigma}_2 ^ 2</span>
<span id="cb72-1377"><a href="#cb72-1377" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1378"><a href="#cb72-1378" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1379"><a href="#cb72-1379" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1380"><a href="#cb72-1380" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1381"><a href="#cb72-1381" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1 - \beta_1 <span class="sc">\\</span></span>
<span id="cb72-1382"><a href="#cb72-1382" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2 - \beta_2</span>
<span id="cb72-1383"><a href="#cb72-1383" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1384"><a href="#cb72-1384" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1385"><a href="#cb72-1385" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1386"><a href="#cb72-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1387"><a href="#cb72-1387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1388"><a href="#cb72-1388" aria-hidden="true" tabindex="-1"></a>q_2 = \frac{N}{\sigma_\epsilon ^ 2}</span>
<span id="cb72-1389"><a href="#cb72-1389" aria-hidden="true" tabindex="-1"></a>\left[</span>
<span id="cb72-1390"><a href="#cb72-1390" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_1^2(\hat{\beta}_1 - \beta_1) ^ 2+</span>
<span id="cb72-1391"><a href="#cb72-1391" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_2^2(\hat{\beta}_2 - \beta_2) ^ 2+ </span>
<span id="cb72-1392"><a href="#cb72-1392" aria-hidden="true" tabindex="-1"></a>2 \hat{\sigma}_{12}(\hat{\beta}_1 - \beta_1)(\hat{\beta}_2 - \beta_2)</span>
<span id="cb72-1393"><a href="#cb72-1393" aria-hidden="true" tabindex="-1"></a>\right]</span>
<span id="cb72-1394"><a href="#cb72-1394" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ellipsebeta}</span>
<span id="cb72-1395"><a href="#cb72-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1396"><a href="#cb72-1396" aria-hidden="true" tabindex="-1"></a>Using @eq-student_multiple, this expression can also be rewritten in terms of the Student statistics $t_k$:</span>
<span id="cb72-1397"><a href="#cb72-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1398"><a href="#cb72-1398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1399"><a href="#cb72-1399" aria-hidden="true" tabindex="-1"></a>q_2 = \frac{1}{1 - \hat{\rho}_{12} ^ 2}\left(t_1 ^ 2 + t_2 ^ 2 + </span>
<span id="cb72-1400"><a href="#cb72-1400" aria-hidden="true" tabindex="-1"></a>2 \hat{\rho}_{12} t_1 t_2\right) = </span>
<span id="cb72-1401"><a href="#cb72-1401" aria-hidden="true" tabindex="-1"></a>\frac{1}{1 - \hat{\rho}_{\hat{\beta}_1\hat{\beta}_2} ^ 2}\left(t_1 ^ 2 + t_2 ^ 2 - </span>
<span id="cb72-1402"><a href="#cb72-1402" aria-hidden="true" tabindex="-1"></a>2 \hat{\rho}_{\hat{\beta}_1\hat{\beta}_2} t_1 t_2\right)</span>
<span id="cb72-1403"><a href="#cb72-1403" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ellipse}</span>
<span id="cb72-1404"><a href="#cb72-1404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1405"><a href="#cb72-1405" aria-hidden="true" tabindex="-1"></a>the last equality resulting from the fact that the coefficient of correlation of two slopes is the opposite of the coefficient of correlation of the corresponding covariates (see @eq-var_covar_slopes).</span>
<span id="cb72-1406"><a href="#cb72-1406" aria-hidden="true" tabindex="-1"></a>Therefore, a $(1-\alpha)$ confidence interval for a couple of</span>
<span id="cb72-1407"><a href="#cb72-1407" aria-hidden="true" tabindex="-1"></a>coefficients $(\beta_1, \beta_2)$ is the set of values for which</span>
<span id="cb72-1408"><a href="#cb72-1408" aria-hidden="true" tabindex="-1"></a>@eq-ellipse is lower than the critical value for a $\chi ^ 2$ with 2 degrees of freedom, which is, for example, 5.99 for two degrees of freedom and $1-\alpha = 0.95$.</span>
<span id="cb72-1409"><a href="#cb72-1409" aria-hidden="true" tabindex="-1"></a>Equating @eq-ellipse to this critical value, we get the equation of an ellipse, with two particular nested cases:</span>
<span id="cb72-1410"><a href="#cb72-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1411"><a href="#cb72-1411" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>if $\hat{\rho}_{12} = 0$, i.e., if the two covariates and therefore the two</span>
<span id="cb72-1412"><a href="#cb72-1412" aria-hidden="true" tabindex="-1"></a>    coefficients are uncorrelated, the expression reduces to the sum of</span>
<span id="cb72-1413"><a href="#cb72-1413" aria-hidden="true" tabindex="-1"></a>    squares of the t statistics, which is an ellipse with vertical and</span>
<span id="cb72-1414"><a href="#cb72-1414" aria-hidden="true" tabindex="-1"></a>    horizontal tangents,</span>
<span id="cb72-1415"><a href="#cb72-1415" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>if $\hat{\rho}_{12} = 0$ and $\hat{\sigma}_1 = \hat{\sigma}_2$, the expression</span>
<span id="cb72-1416"><a href="#cb72-1416" aria-hidden="true" tabindex="-1"></a>    reduces further to the equation of a circle.</span>
<span id="cb72-1417"><a href="#cb72-1417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1418"><a href="#cb72-1418" aria-hidden="true" tabindex="-1"></a>@eq-ellipse can't be computed as it depends on $\sigma_\epsilon$ which is unknown. Replacing $\sigma_\epsilon$ by $\dot{\sigma}_\epsilon$, we get:</span>
<span id="cb72-1419"><a href="#cb72-1419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1420"><a href="#cb72-1420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1421"><a href="#cb72-1421" aria-hidden="true" tabindex="-1"></a>\hat{q}_2 = \frac{1}{1 - \hat{\rho}_{12} ^ 2}\left(\hat{t}_1 ^ 2 + \hat{t}_2 ^ 2 + </span>
<span id="cb72-1422"><a href="#cb72-1422" aria-hidden="true" tabindex="-1"></a>2 \hat{\rho}_{12} \hat{t}_1 \hat{t}_2\right)</span>
<span id="cb72-1423"><a href="#cb72-1423" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ellipse_est}</span>
<span id="cb72-1424"><a href="#cb72-1424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1425"><a href="#cb72-1425" aria-hidden="true" tabindex="-1"></a>where $\hat{q}_2$ now follows asymptotically a $\chi ^ 2$ distribution with 2 degrees of freedom. If the errors are normal, dividing by $K$ (here 2), we get an exact Fisher $F$ distribution with $2$ and $N - K - 1$ degrees of freedom.</span>
<span id="cb72-1426"><a href="#cb72-1426" aria-hidden="true" tabindex="-1"></a>In the simple case of no correlation between the covariates, the $\chi^2$ and the $F$</span>
<span id="cb72-1427"><a href="#cb72-1427" aria-hidden="true" tabindex="-1"></a>statistics are therefore the sum and the mean of the squares of the t statistics. As</span>
<span id="cb72-1428"><a href="#cb72-1428" aria-hidden="true" tabindex="-1"></a>for the Student distribution, which converges in distribution to a</span>
<span id="cb72-1429"><a href="#cb72-1429" aria-hidden="true" tabindex="-1"></a>normal distribution, $K \times$ the $F$ statistic converges in</span>
<span id="cb72-1430"><a href="#cb72-1430" aria-hidden="true" tabindex="-1"></a>distribution to a $\chi ^ 2$ with $K$ degrees of freedom. For example, with</span>
<span id="cb72-1431"><a href="#cb72-1431" aria-hidden="true" tabindex="-1"></a>$K=2$, the critical value for a $F$ distribution with $2$ and $\infty$</span>
<span id="cb72-1432"><a href="#cb72-1432" aria-hidden="true" tabindex="-1"></a>degrees of freedom is half the corresponding $\chi^2$ value with 2</span>
<span id="cb72-1433"><a href="#cb72-1433" aria-hidden="true" tabindex="-1"></a>degrees of freedom (5.99 for the 95% confidence level), which is</span>
<span id="cb72-1434"><a href="#cb72-1434" aria-hidden="true" tabindex="-1"></a>$2.996$.</span>
<span id="cb72-1435"><a href="#cb72-1435" aria-hidden="true" tabindex="-1"></a>The confidence interval is represented in @fig-ellipse; the circle</span>
<span id="cb72-1436"><a href="#cb72-1436" aria-hidden="true" tabindex="-1"></a>point is the point estimation, the vertical and horizontal segments are</span>
<span id="cb72-1437"><a href="#cb72-1437" aria-hidden="true" tabindex="-1"></a>the separate confidence intervals for both coefficients at the 95% level. We've also added a</span>
<span id="cb72-1438"><a href="#cb72-1438" aria-hidden="true" tabindex="-1"></a>diamond point that corresponds to the hypothesis $\kappa = 1/3$, which</span>
<span id="cb72-1439"><a href="#cb72-1439" aria-hidden="true" tabindex="-1"></a>implies that $\beta_i = - \beta_v = \frac{\kappa}{1-\kappa} = 0.5$. Finally, we add a line with a slope equal</span>
<span id="cb72-1440"><a href="#cb72-1440" aria-hidden="true" tabindex="-1"></a>to $-1$ and an intercept equal to 0 which corresponds to the hypothesis that $\beta_i = - \beta_v$.</span>
<span id="cb72-1441"><a href="#cb72-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1442"><a href="#cb72-1442" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1443"><a href="#cb72-1443" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-ellipse</span></span>
<span id="cb72-1444"><a href="#cb72-1444" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb72-1445"><a href="#cb72-1445" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.width: 4</span></span>
<span id="cb72-1446"><a href="#cb72-1446" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.height: 6</span></span>
<span id="cb72-1447"><a href="#cb72-1447" aria-hidden="true" tabindex="-1"></a><span class="co">#| out.width: "30%"</span></span>
<span id="cb72-1448"><a href="#cb72-1448" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Ellipse of confidence for the two coefficients"</span></span>
<span id="cb72-1449"><a href="#cb72-1449" aria-hidden="true" tabindex="-1"></a><span class="fu">confellipse</span>(slw_tot) <span class="sc">+</span></span>
<span id="cb72-1450"><a href="#cb72-1450" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fl">0.5</span>, <span class="at">y =</span> <span class="sc">-</span> <span class="fl">0.5</span>), <span class="at">shape =</span> <span class="dv">5</span>, <span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb72-1451"><a href="#cb72-1451" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>)  <span class="sc">+</span></span>
<span id="cb72-1452"><a href="#cb72-1452" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="sc">-</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb72-1453"><a href="#cb72-1453" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fl">1.1</span>, <span class="fl">1.25</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="fl">3.1</span>)), <span class="at">shape =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb72-1454"><a href="#cb72-1454" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"ln i"</span>, <span class="at">y =</span> <span class="st">"ln v"</span>, <span class="at">linetype =</span> <span class="cn">NULL</span>)</span>
<span id="cb72-1455"><a href="#cb72-1455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1456"><a href="#cb72-1456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1457"><a href="#cb72-1457" aria-hidden="true" tabindex="-1"></a>The confidence ellipse is a "fat sausage"^<span class="co">[</span><span class="ot">@STOC:WATS:15, p. 235.</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Stock}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Watson} with the long part of the</span>
<span id="cb72-1458"><a href="#cb72-1458" aria-hidden="true" tabindex="-1"></a>sausage slightly oriented in the lower-left/upper-right direction. This</span>
<span id="cb72-1459"><a href="#cb72-1459" aria-hidden="true" tabindex="-1"></a>is because, as we have seen previously (see @fig-invpop), the two covariates exhibit a</span>
<span id="cb72-1460"><a href="#cb72-1460" aria-hidden="true" tabindex="-1"></a>small negative correlation, which implies a small positive correlations</span>
<span id="cb72-1461"><a href="#cb72-1461" aria-hidden="true" tabindex="-1"></a>between $\hat{\beta}_i$ and $\hat{\beta}_v$. The ellipse is also</span>
<span id="cb72-1462"><a href="#cb72-1462" aria-hidden="true" tabindex="-1"></a>"higher" than "wide", because $v$ has a smaller variance than $i$ and therefore  $\hat{\beta}_v$ has a  larger variance than $\hat{\beta}_i$.</span>
<span id="cb72-1463"><a href="#cb72-1463" aria-hidden="true" tabindex="-1"></a>Note the difference between a set of two simple hypotheses and a joint</span>
<span id="cb72-1464"><a href="#cb72-1464" aria-hidden="true" tabindex="-1"></a>hypothesis. Consider for example:</span>
<span id="cb72-1465"><a href="#cb72-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1466"><a href="#cb72-1466" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$(\beta_i = 1.1, \beta_v = -1)$ represented by a square point: both</span>
<span id="cb72-1467"><a href="#cb72-1467" aria-hidden="true" tabindex="-1"></a>    simple hypotheses are not rejected (the two values are in the</span>
<span id="cb72-1468"><a href="#cb72-1468" aria-hidden="true" tabindex="-1"></a>    unidimensional confidence interval), but the joint hypothesis is</span>
<span id="cb72-1469"><a href="#cb72-1469" aria-hidden="true" tabindex="-1"></a>    rejected, as the corresponding square point is outside the 95% confidence</span>
<span id="cb72-1470"><a href="#cb72-1470" aria-hidden="true" tabindex="-1"></a>    interval ellipse,</span>
<span id="cb72-1471"><a href="#cb72-1471" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$(\beta_i = 1.25, \beta_v = -3.1)$ represented by a triangle point:</span>
<span id="cb72-1472"><a href="#cb72-1472" aria-hidden="true" tabindex="-1"></a>    the simple hypothesis $\beta_i = 1.25$ is not rejected, the simple</span>
<span id="cb72-1473"><a href="#cb72-1473" aria-hidden="true" tabindex="-1"></a>    hypothesis $\beta_b = -3.1$ is rejected, but the joint hypothesis</span>
<span id="cb72-1474"><a href="#cb72-1474" aria-hidden="true" tabindex="-1"></a>    is not rejected, the triangle point being inside the 95% confidence interval ellipse.</span>
<span id="cb72-1475"><a href="#cb72-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1476"><a href="#cb72-1476" aria-hidden="true" tabindex="-1"></a>The hypothesis that the two coefficients sum to 0 is not rejected as</span>
<span id="cb72-1477"><a href="#cb72-1477" aria-hidden="true" tabindex="-1"></a>some points of the straight line that figures this hypothesis are in the confidence ellipse.</span>
<span id="cb72-1478"><a href="#cb72-1478" aria-hidden="true" tabindex="-1"></a>Concerning the hypothesis that $\beta_i = 0.5$ and $\beta_v = -0.5$, the two simple hypotheses and the joint hypothesis are rejected at the 95% confidence level; the two estimates are neither in the segments representing the simple confidence interval nor inside the ellipse figuring the joint confidence interval. </span>
<span id="cb72-1479"><a href="#cb72-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1480"><a href="#cb72-1480" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{confidence interval!multiple linear regression model|)}</span>
<span id="cb72-1481"><a href="#cb72-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1482"><a href="#cb72-1482" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Joint hypothesis</span></span>
<span id="cb72-1483"><a href="#cb72-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1484"><a href="#cb72-1484" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!multiple linear regression model|(}</span>
<span id="cb72-1485"><a href="#cb72-1485" aria-hidden="true" tabindex="-1"></a>To test a joint hypothesis for the values of a couple of parameters $(\beta_{10}, \beta_{20})$, we have just seen that we can simply check whether the corresponding point is inside or outside the confidence interval ellipse. We can also compute the statistic given in @eq-ellipse_est for $(\beta_1=\beta_{10}, \beta_2=\beta_{20})$ and compare it to the critical value.</span>
<span id="cb72-1486"><a href="#cb72-1486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1487"><a href="#cb72-1487" aria-hidden="true" tabindex="-1"></a>The statistic is computed using the elements of the matrix returned by</span>
<span id="cb72-1488"><a href="#cb72-1488" aria-hidden="true" tabindex="-1"></a><span class="in">`coef(summary(x))`</span>, which contains in particular the estimations and</span>
<span id="cb72-1489"><a href="#cb72-1489" aria-hidden="true" tabindex="-1"></a>their standard deviations. We first compute the t statistics</span>
<span id="cb72-1490"><a href="#cb72-1490" aria-hidden="true" tabindex="-1"></a>corresponding to the two simple hypothesis $\beta_i = 0.5$ and $\beta_v = -0.5$:</span>
<span id="cb72-1491"><a href="#cb72-1491" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}</span>
<span id="cb72-1492"><a href="#cb72-1492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1493"><a href="#cb72-1493" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1494"><a href="#cb72-1494" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: student_coefs_pm05</span></span>
<span id="cb72-1495"><a href="#cb72-1495" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1496"><a href="#cb72-1496" aria-hidden="true" tabindex="-1"></a>sc <span class="ot">&lt;-</span> slw_tot <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1497"><a href="#cb72-1497" aria-hidden="true" tabindex="-1"></a>t_i <span class="ot">&lt;-</span> (sc[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">-</span> <span class="fl">0.5</span>) <span class="sc">/</span> sc[<span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb72-1498"><a href="#cb72-1498" aria-hidden="true" tabindex="-1"></a>t_v <span class="ot">&lt;-</span> (sc[<span class="dv">3</span>, <span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.5</span>) <span class="sc">/</span> sc[<span class="dv">3</span>, <span class="dv">2</span>]</span>
<span id="cb72-1499"><a href="#cb72-1499" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1500"><a href="#cb72-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1501"><a href="#cb72-1501" aria-hidden="true" tabindex="-1"></a>We then apply the simplified formula, which is the sum or the mean of</span>
<span id="cb72-1502"><a href="#cb72-1502" aria-hidden="true" tabindex="-1"></a>the squares of the t statistics:</span>
<span id="cb72-1503"><a href="#cb72-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1504"><a href="#cb72-1504" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1505"><a href="#cb72-1505" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: chisqf_approx_coef_pm05</span></span>
<span id="cb72-1506"><a href="#cb72-1506" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1507"><a href="#cb72-1507" aria-hidden="true" tabindex="-1"></a>St2 <span class="ot">&lt;-</span> t_i <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> t_v <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb72-1508"><a href="#cb72-1508" aria-hidden="true" tabindex="-1"></a>Mt2 <span class="ot">&lt;-</span> (t_i <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> t_v <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-1509"><a href="#cb72-1509" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(St2, Mt2)</span>
<span id="cb72-1510"><a href="#cb72-1510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1511"><a href="#cb72-1511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1512"><a href="#cb72-1512" aria-hidden="true" tabindex="-1"></a>for which the distributions under $H_0$ are respectively $\chi ^ 2$</span>
<span id="cb72-1513"><a href="#cb72-1513" aria-hidden="true" tabindex="-1"></a>and $F$, with the following critical values:</span>
<span id="cb72-1514"><a href="#cb72-1514" aria-hidden="true" tabindex="-1"></a>\idxfun{qchisq}{stats}\idxfun{qf}{stats}\idxfun{df.residual}{stats}</span>
<span id="cb72-1515"><a href="#cb72-1515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1516"><a href="#cb72-1516" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1517"><a href="#cb72-1517" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1518"><a href="#cb72-1518" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: quantiles_chisq_f</span></span>
<span id="cb72-1519"><a href="#cb72-1519" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">2</span>)</span>
<span id="cb72-1520"><a href="#cb72-1520" aria-hidden="true" tabindex="-1"></a><span class="fu">qf</span>(<span class="fl">0.95</span>, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="fu">df.residual</span>(slw_tot))</span>
<span id="cb72-1521"><a href="#cb72-1521" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1522"><a href="#cb72-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1523"><a href="#cb72-1523" aria-hidden="true" tabindex="-1"></a>The critical values being much smaller than the computed statistics, the</span>
<span id="cb72-1524"><a href="#cb72-1524" aria-hidden="true" tabindex="-1"></a>joint hypothesis is clearly rejected. The exact formula corrects the</span>
<span id="cb72-1525"><a href="#cb72-1525" aria-hidden="true" tabindex="-1"></a>correlation between the two estimators using the coefficient of</span>
<span id="cb72-1526"><a href="#cb72-1526" aria-hidden="true" tabindex="-1"></a>correlation between the two covariates. We compute this coefficient</span>
<span id="cb72-1527"><a href="#cb72-1527" aria-hidden="true" tabindex="-1"></a>using the data frame of the fitted model, which is obtained using the</span>
<span id="cb72-1528"><a href="#cb72-1528" aria-hidden="true" tabindex="-1"></a><span class="in">`model.frame`</span> function. Using the initial data frame <span class="in">`growth`</span> wouldn't</span>
<span id="cb72-1529"><a href="#cb72-1529" aria-hidden="true" tabindex="-1"></a>give the correct value, as the estimation is not performed on the</span>
<span id="cb72-1530"><a href="#cb72-1530" aria-hidden="true" tabindex="-1"></a>full data set because of missing data and because the estimation is performed on a subsample with some groups of countries that are excluded:</span>
<span id="cb72-1531"><a href="#cb72-1531" aria-hidden="true" tabindex="-1"></a>\idxfun{summarise}{dplyr}\idxfun{model.frame}{stats}\idxfun{pull}{dplyr}</span>
<span id="cb72-1532"><a href="#cb72-1532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1533"><a href="#cb72-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1534"><a href="#cb72-1534" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1535"><a href="#cb72-1535" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1536"><a href="#cb72-1536" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: coef_corr_i_v</span></span>
<span id="cb72-1537"><a href="#cb72-1537" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(slw_tot)</span>
<span id="cb72-1538"><a href="#cb72-1538" aria-hidden="true" tabindex="-1"></a>r_iv <span class="ot">&lt;-</span> <span class="fu">summarise</span>(mf, <span class="at">r =</span> <span class="fu">cor</span>(<span class="st">`</span><span class="at">log(i)</span><span class="st">`</span>, <span class="st">`</span><span class="at">log(v)</span><span class="st">`</span>)) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(r)</span>
<span id="cb72-1539"><a href="#cb72-1539" aria-hidden="true" tabindex="-1"></a>r_iv</span>
<span id="cb72-1540"><a href="#cb72-1540" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1541"><a href="#cb72-1541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1542"><a href="#cb72-1542" aria-hidden="true" tabindex="-1"></a>Note that the names of the covariates are not regular names as they</span>
<span id="cb72-1543"><a href="#cb72-1543" aria-hidden="true" tabindex="-1"></a>contain parentheses, therefore they should be surrounded by the \texttt{`} sign. The</span>
<span id="cb72-1544"><a href="#cb72-1544" aria-hidden="true" tabindex="-1"></a>coefficient of correlation between the two covariates is</span>
<span id="cb72-1545"><a href="#cb72-1545" aria-hidden="true" tabindex="-1"></a>$<span class="in">`r round(r_iv, 3)`</span>$. We obtain:</span>
<span id="cb72-1546"><a href="#cb72-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1547"><a href="#cb72-1547" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1548"><a href="#cb72-1548" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fstat_coef_pm05</span></span>
<span id="cb72-1549"><a href="#cb72-1549" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1550"><a href="#cb72-1550" aria-hidden="true" tabindex="-1"></a>Fstat <span class="ot">&lt;-</span> (t_i <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> t_v <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> r_iv <span class="sc">*</span> t_i <span class="sc">*</span> t_v) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> r_iv <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-1551"><a href="#cb72-1551" aria-hidden="true" tabindex="-1"></a>Fstat</span>
<span id="cb72-1552"><a href="#cb72-1552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1553"><a href="#cb72-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1554"><a href="#cb72-1554" aria-hidden="true" tabindex="-1"></a>The statistic (<span class="in">`r round(Fstat, 3)`</span>) is slightly greater than the</span>
<span id="cb72-1555"><a href="#cb72-1555" aria-hidden="true" tabindex="-1"></a>approximative value (<span class="in">`r round(Mt2, 3)`</span>) which was previously computed; we</span>
<span id="cb72-1556"><a href="#cb72-1556" aria-hidden="true" tabindex="-1"></a>therefore reject once again the null hypothesis. </span>
<span id="cb72-1557"><a href="#cb72-1557" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!multiple linear regression model|)}</span>
<span id="cb72-1558"><a href="#cb72-1558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1559"><a href="#cb72-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1560"><a href="#cb72-1560" aria-hidden="true" tabindex="-1"></a><span class="fu">### The three tests {#sec-three_tests}</span></span>
<span id="cb72-1561"><a href="#cb72-1561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1562"><a href="#cb72-1562" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!three test principles!linear regression models|(}</span>
<span id="cb72-1563"><a href="#cb72-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1564"><a href="#cb72-1564" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- !!! clarifier les notations Z vs X --&gt;</span></span>
<span id="cb72-1565"><a href="#cb72-1565" aria-hidden="true" tabindex="-1"></a>In the previous subsection, we started with a general (unconstrained) model, we constructed a confidence ellipse for the two parameters, and we were able to test a set of hypotheses, either by checking whether the values of the parameters corresponding to the hypotheses were inside the confidence ellipse, or by computing the value of the statistic for the tested values of the parameters. Actually, this testing principle, based on the unconstrained model, is just one way of testing a hypothesis. The geometry of least squares and the Frisch-Waugh theorem highlights the fact that any set of hypotheses can be tested using the fact that this set of hypotheses gives rise to two models:</span>
<span id="cb72-1566"><a href="#cb72-1566" aria-hidden="true" tabindex="-1"></a>a **constrained model**, which imposes the hypotheses and an **unconstrained model**, which doesn't impose the hypotheses.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{constrained model}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{unconstrained model}</span>
<span id="cb72-1567"><a href="#cb72-1567" aria-hidden="true" tabindex="-1"></a>The same test can be performed using the constrained model, the</span>
<span id="cb72-1568"><a href="#cb72-1568" aria-hidden="true" tabindex="-1"></a>unconstrained model or both, which give rise to three test principles:^[This section is largely based on @DAVI:MACK:93, section 3.6, pp. 88-94. The three "classical" tests are often understood as tests suitable for models estimated by maximum likelihood (see @sec-three_tests_ml). @DAVI:MACK:93</span>
<span id="cb72-1569"><a href="#cb72-1569" aria-hidden="true" tabindex="-1"></a>advocate the presentation of the three test principles for other estimators, including the linear regression model.]</span>
<span id="cb72-1570"><a href="#cb72-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1571"><a href="#cb72-1571" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Wald** test is based only on the unconstrained model,</span>
<span id="cb72-1572"><a href="#cb72-1572" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**lagrange multiplier** or **score** test is based on the constrained</span>
<span id="cb72-1573"><a href="#cb72-1573" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb72-1574"><a href="#cb72-1574" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**likelihood ratio** test is based on the comparison between the two models.</span>
<span id="cb72-1575"><a href="#cb72-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1576"><a href="#cb72-1576" aria-hidden="true" tabindex="-1"></a>A set of $J$ linear hypotheses is written as:</span>
<span id="cb72-1577"><a href="#cb72-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1578"><a href="#cb72-1578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1579"><a href="#cb72-1579" aria-hidden="true" tabindex="-1"></a>R \gamma = q</span>
<span id="cb72-1580"><a href="#cb72-1580" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1581"><a href="#cb72-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1582"><a href="#cb72-1582" aria-hidden="true" tabindex="-1"></a>where $R$ is a matrix of dimension $J \times (K + 1)$ and $q$ is a vector of length</span>
<span id="cb72-1583"><a href="#cb72-1583" aria-hidden="true" tabindex="-1"></a>$J$. $J$, the number of hypotheses, is necessarily lower or equal to $K$.</span>
<span id="cb72-1584"><a href="#cb72-1584" aria-hidden="true" tabindex="-1"></a>Actually, a set of $J$ hypotheses can always be rewritten as a model of</span>
<span id="cb72-1585"><a href="#cb72-1585" aria-hidden="true" tabindex="-1"></a>the form $y = X_1\beta_1 + X_2\beta_2 + \epsilon$, the hypothesis being</span>
<span id="cb72-1586"><a href="#cb72-1586" aria-hidden="true" tabindex="-1"></a>$\mbox{H}_0: \beta_2 = 0$. In this setting, the three tests are easily constructed using the Frisch-Waugh theorem, with $H_0: \beta_2 = 0$.</span>
<span id="cb72-1587"><a href="#cb72-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1588"><a href="#cb72-1588" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Wald test</span></span>
<span id="cb72-1589"><a href="#cb72-1589" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test|(}</span>
<span id="cb72-1590"><a href="#cb72-1590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1591"><a href="#cb72-1591" aria-hidden="true" tabindex="-1"></a>The Wald test is based on the unconstrained model, for which a vector of</span>
<span id="cb72-1592"><a href="#cb72-1592" aria-hidden="true" tabindex="-1"></a>slopes $\hat{\beta}_2$ is estimated. Using the Frisch-Waugh theorem, this vector can be obtained as the regression of the residuals of $y$ on $X_1$ ($M_1y$) on the residuals of every column of $X_2$ on $X_1$ ($M_1X_2$). Then, $\hat{\beta}_2 = (X_2^\top M_1 X_2) ^ {-1} X_2^\top M_1 y$, with expected value and variance equal to $\beta_2$ (0 under $H_0$) and $\mbox{V}(\hat{\beta}_2) = \sigma_\epsilon^2 (X_2^\top M_1 X_2) ^ {-1}$. Convergence in distribution implies that:</span>
<span id="cb72-1593"><a href="#cb72-1593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1594"><a href="#cb72-1594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1595"><a href="#cb72-1595" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2 \stackrel{a}{\sim} \mathcal{N} \left(0, \mbox{V}(\hat{\beta}_2)\right)</span>
<span id="cb72-1596"><a href="#cb72-1596" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1597"><a href="#cb72-1597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1598"><a href="#cb72-1598" aria-hidden="true" tabindex="-1"></a>The distribution of the quadratic form of a centered vector of normal</span>
<span id="cb72-1599"><a href="#cb72-1599" aria-hidden="true" tabindex="-1"></a>random variables of length $J$ with the inverse of its covariance matrix is a $\chi^2$</span>
<span id="cb72-1600"><a href="#cb72-1600" aria-hidden="true" tabindex="-1"></a>with $J$ degrees of freedom:</span>
<span id="cb72-1601"><a href="#cb72-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1602"><a href="#cb72-1602" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1603"><a href="#cb72-1603" aria-hidden="true" tabindex="-1"></a>\mbox{wald} = \hat{\beta}_2 ^ \top \mbox{V}(\hat{\beta}_2) ^ {-1}</span>
<span id="cb72-1604"><a href="#cb72-1604" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2</span>
<span id="cb72-1605"><a href="#cb72-1605" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-1606"><a href="#cb72-1606" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\beta}_2 ^ \top</span>
<span id="cb72-1607"><a href="#cb72-1607" aria-hidden="true" tabindex="-1"></a>(X_2^\top M_1 X_2)</span>
<span id="cb72-1608"><a href="#cb72-1608" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_2}</span>
<span id="cb72-1609"><a href="#cb72-1609" aria-hidden="true" tabindex="-1"></a>{\sigma_\epsilon^2}</span>
<span id="cb72-1610"><a href="#cb72-1610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1611"><a href="#cb72-1611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1612"><a href="#cb72-1612" aria-hidden="true" tabindex="-1"></a>which is also, replacing $\hat{\beta}_2$ by its expression:</span>
<span id="cb72-1613"><a href="#cb72-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1614"><a href="#cb72-1614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1615"><a href="#cb72-1615" aria-hidden="true" tabindex="-1"></a>\mbox{wald} = </span>
<span id="cb72-1616"><a href="#cb72-1616" aria-hidden="true" tabindex="-1"></a>\frac{y ^ \top M_1 X_2 (X_2^\top M_1 X_2) ^ {-1} X_2^\top M_1 y}</span>
<span id="cb72-1617"><a href="#cb72-1617" aria-hidden="true" tabindex="-1"></a>{\sigma_\epsilon^2} = </span>
<span id="cb72-1618"><a href="#cb72-1618" aria-hidden="true" tabindex="-1"></a>\frac{y ^ \top P_{M_1X_2} y}</span>
<span id="cb72-1619"><a href="#cb72-1619" aria-hidden="true" tabindex="-1"></a>{\sigma_\epsilon^2}</span>
<span id="cb72-1620"><a href="#cb72-1620" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1621"><a href="#cb72-1621" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test|)}</span>
<span id="cb72-1622"><a href="#cb72-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1623"><a href="#cb72-1623" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Lagrange multiplier or score test</span></span>
<span id="cb72-1624"><a href="#cb72-1624" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test|(}</span>
<span id="cb72-1625"><a href="#cb72-1625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1626"><a href="#cb72-1626" aria-hidden="true" tabindex="-1"></a>Consider the constrained model, which imposes $\beta_2 = 0$. It is therefore obtained by regressing $y$ on $X_1$ only, and the vector of residuals is $\hat{\epsilon}_1 = M_1 y$. The idea of this test is that, if $H_0$ is true, $X_2 ^ \top \hat{\epsilon}_1$</span>
<span id="cb72-1627"><a href="#cb72-1627" aria-hidden="true" tabindex="-1"></a>should be close to zero, $\hat{\epsilon}_1$ being "almost" orthogonal to</span>
<span id="cb72-1628"><a href="#cb72-1628" aria-hidden="true" tabindex="-1"></a>the subspace spanned by $X_2$. Therefore, we consider the vector</span>
<span id="cb72-1629"><a href="#cb72-1629" aria-hidden="true" tabindex="-1"></a>$X_2 ^ \top \hat{\epsilon}_1 = X_2 ^ \top M_1 y$, which, under $H_0$, should have a 0 expected value. The variance of this vector is: $\sigma_\epsilon ^ 2 (X_2^\top M_1X_2)^{-1}$ so that, applying the central-limit theorem:</span>
<span id="cb72-1630"><a href="#cb72-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1631"><a href="#cb72-1631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1632"><a href="#cb72-1632" aria-hidden="true" tabindex="-1"></a>X_2 ^ \top \hat{\epsilon}_1 \equiv X_2 ^ \top M_1 y \stackrel{a}{\sim} \mathcal{N} </span>
<span id="cb72-1633"><a href="#cb72-1633" aria-hidden="true" tabindex="-1"></a>\left(0, \sigma_\epsilon ^ 2 X_2 ^ \top M_1 X_2\right)</span>
<span id="cb72-1634"><a href="#cb72-1634" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1635"><a href="#cb72-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1636"><a href="#cb72-1636" aria-hidden="true" tabindex="-1"></a>The statistic is obtained, as previously, by computing the quadratic form:</span>
<span id="cb72-1637"><a href="#cb72-1637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1638"><a href="#cb72-1638" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1639"><a href="#cb72-1639" aria-hidden="true" tabindex="-1"></a>\mbox{score}=y ^ \top M_1 X_2  \left(\sigma_\epsilon ^ 2 X_2 ^ \top M_1</span>
<span id="cb72-1640"><a href="#cb72-1640" aria-hidden="true" tabindex="-1"></a>X_2\right) ^ {-1} X_2 ^ \top M_1 y=</span>
<span id="cb72-1641"><a href="#cb72-1641" aria-hidden="true" tabindex="-1"></a>\frac{y ^ \top M_1 X_2 (X_2^\top M_1 X_2) ^ {-1} X_2^\top M_1 y}</span>
<span id="cb72-1642"><a href="#cb72-1642" aria-hidden="true" tabindex="-1"></a>{\sigma_\epsilon^2} = </span>
<span id="cb72-1643"><a href="#cb72-1643" aria-hidden="true" tabindex="-1"></a>\frac{y ^ \top P_{M_1X_2} y}</span>
<span id="cb72-1644"><a href="#cb72-1644" aria-hidden="true" tabindex="-1"></a>{\sigma_\epsilon^2}</span>
<span id="cb72-1645"><a href="#cb72-1645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1646"><a href="#cb72-1646" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test|)}</span>
<span id="cb72-1647"><a href="#cb72-1647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1648"><a href="#cb72-1648" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Likelihood ratio test</span></span>
<span id="cb72-1649"><a href="#cb72-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1650"><a href="#cb72-1650" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test|(}</span>
<span id="cb72-1651"><a href="#cb72-1651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1652"><a href="#cb72-1652" aria-hidden="true" tabindex="-1"></a>The likelihood ratio test is based on the comparison of the</span>
<span id="cb72-1653"><a href="#cb72-1653" aria-hidden="true" tabindex="-1"></a>objective function (the sum of square residuals) for the constrained and</span>
<span id="cb72-1654"><a href="#cb72-1654" aria-hidden="true" tabindex="-1"></a>the unconstrained model. Remember, from @eq-pyth_ssr, that:</span>
<span id="cb72-1655"><a href="#cb72-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1656"><a href="#cb72-1656" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1657"><a href="#cb72-1657" aria-hidden="true" tabindex="-1"></a>\mid\mid \hat{\epsilon}_{12}^2\mid\mid + \mid\mid P_{M_1X_2}y\mid\mid = \mid\mid \hat{\epsilon}_{1}^2\mid\mid</span>
<span id="cb72-1658"><a href="#cb72-1658" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1659"><a href="#cb72-1659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1660"><a href="#cb72-1660" aria-hidden="true" tabindex="-1"></a>The first term on the left and the term on the right are residual sums of squares (respectively for the unconstrained and the constrained model). Therefore, the likelihood ratio test is based on:</span>
<span id="cb72-1661"><a href="#cb72-1661" aria-hidden="true" tabindex="-1"></a>$\mbox{SSR}_c - \mbox{SSR}_{nc} = ||P_{M_1X_2}y|| ^ 2 = y ^ \top P_{M_1X_2} y$. Dividing by $\sigma_\epsilon^2$, we get exactly the same statistic as previously.</span>
<span id="cb72-1662"><a href="#cb72-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1663"><a href="#cb72-1663" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test|)}</span>
<span id="cb72-1664"><a href="#cb72-1664" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!three test principles!linear regression models|)}</span>
<span id="cb72-1665"><a href="#cb72-1665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1666"><a href="#cb72-1666" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the three tests</span></span>
<span id="cb72-1667"><a href="#cb72-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1668"><a href="#cb72-1668" aria-hidden="true" tabindex="-1"></a>Consider the augmented Solow model:</span>
<span id="cb72-1669"><a href="#cb72-1669" aria-hidden="true" tabindex="-1"></a>$y = \alpha + \beta_i i + \beta_v v + \beta_e e + \epsilon$,</span>
<span id="cb72-1670"><a href="#cb72-1670" aria-hidden="true" tabindex="-1"></a>with $\beta_i = \frac{\kappa}{1 - \kappa - \lambda}$, $\beta_v = -\frac{\kappa + \lambda}{1 - \kappa - \lambda}$ and $\beta_e = \frac{\lambda}{1 - \kappa - \lambda}$.</span>
<span id="cb72-1671"><a href="#cb72-1671" aria-hidden="true" tabindex="-1"></a>For convenience, we compute the logarithm of the variables before using <span class="in">`lm`</span>:</span>
<span id="cb72-1672"><a href="#cb72-1672" aria-hidden="true" tabindex="-1"></a>\idxfun{mutate}{dplyr}\idxfun{lm}{stats}</span>
<span id="cb72-1673"><a href="#cb72-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1674"><a href="#cb72-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1677"><a href="#cb72-1677" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1678"><a href="#cb72-1678" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: compute_logs</span></span>
<span id="cb72-1679"><a href="#cb72-1679" aria-hidden="true" tabindex="-1"></a>growth_sub2 <span class="ot">&lt;-</span> growth_sub <span class="sc">%&gt;%</span> </span>
<span id="cb72-1680"><a href="#cb72-1680" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">log</span>(gdp85), <span class="at">i =</span> <span class="fu">log</span>(i), <span class="at">v =</span> <span class="fu">log</span>(v), <span class="at">e =</span> <span class="fu">log</span>(e))</span>
<span id="cb72-1681"><a href="#cb72-1681" aria-hidden="true" tabindex="-1"></a>mrw2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> i <span class="sc">+</span> v <span class="sc">+</span> e, growth_sub2)</span>
<span id="cb72-1682"><a href="#cb72-1682" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1683"><a href="#cb72-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1684"><a href="#cb72-1684" aria-hidden="true" tabindex="-1"></a>We consider two hypotheses:</span>
<span id="cb72-1685"><a href="#cb72-1685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1686"><a href="#cb72-1686" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_i + \beta_v + \beta_e = 0$, this hypothesis is directly implied by the structural model,</span>
<span id="cb72-1687"><a href="#cb72-1687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\kappa = 1 /3$, the share of profits has a reasonable value.</span>
<span id="cb72-1688"><a href="#cb72-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1689"><a href="#cb72-1689" aria-hidden="true" tabindex="-1"></a>The second hypothesis implies that $\beta_i = \frac{1/3}{2/3 - \lambda}$ and $\beta_e = \frac{\lambda}{2/3 - \lambda}$, and therefore that $\beta_e = 2 \beta_i - 1$. The model can be reparametrized in such way that two slopes are 0 if the two hypotheses are satisfied:</span>
<span id="cb72-1690"><a href="#cb72-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1691"><a href="#cb72-1691" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1692"><a href="#cb72-1692" aria-hidden="true" tabindex="-1"></a>y + e - v = \alpha + \beta_i (i + 2e - 3v) + (\beta_e-2\beta_i+1)(e-v)+(\beta_i + \beta_e+\beta_v)v</span>
<span id="cb72-1693"><a href="#cb72-1693" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1694"><a href="#cb72-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1695"><a href="#cb72-1695" aria-hidden="true" tabindex="-1"></a>Therefore, the unconstrained model can be written as a model with $y + e - v$ as the response and $i + 2 e - 3 v$, $e-v$ and $v$ as the three covariates and the constrained model as a model with the same response but with $i + 2 e - 3$ as the unique covariate.</span>
<span id="cb72-1696"><a href="#cb72-1696" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-1697"><a href="#cb72-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1698"><a href="#cb72-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1701"><a href="#cb72-1701" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1702"><a href="#cb72-1702" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: constrained_unconstr_models_growth</span></span>
<span id="cb72-1703"><a href="#cb72-1703" aria-hidden="true" tabindex="-1"></a>growth_sub2 <span class="ot">&lt;-</span> growth_sub2 <span class="sc">%&gt;%</span> </span>
<span id="cb72-1704"><a href="#cb72-1704" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y2 =</span> y <span class="sc">+</span> e <span class="sc">-</span> v, <span class="at">i2 =</span> i <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> e <span class="sc">-</span> <span class="dv">3</span> <span class="sc">*</span> v, <span class="at">e2 =</span> e <span class="sc">-</span> v)</span>
<span id="cb72-1705"><a href="#cb72-1705" aria-hidden="true" tabindex="-1"></a>mrw_c <span class="ot">&lt;-</span>  <span class="fu">lm</span>(y2 <span class="sc">~</span> i2,         growth_sub2)</span>
<span id="cb72-1706"><a href="#cb72-1706" aria-hidden="true" tabindex="-1"></a>mrw_nc <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> i2 <span class="sc">+</span> e2 <span class="sc">+</span> v, growth_sub2)</span>
<span id="cb72-1707"><a href="#cb72-1707" aria-hidden="true" tabindex="-1"></a>coef_nc <span class="ot">&lt;-</span> mrw_nc <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1708"><a href="#cb72-1708" aria-hidden="true" tabindex="-1"></a>coef_nc</span>
<span id="cb72-1709"><a href="#cb72-1709" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1710"><a href="#cb72-1710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1711"><a href="#cb72-1711" aria-hidden="true" tabindex="-1"></a>The two hypotheses can be tested one by one, as the test is that:</span>
<span id="cb72-1712"><a href="#cb72-1712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1713"><a href="#cb72-1713" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the slope of $e2 = e-v$ equals 0 for the hypothesis that $\kappa = 1/3$,</span>
<span id="cb72-1714"><a href="#cb72-1714" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the slope of $v$ equals 0 for the hypothesis that $\beta_i+\beta_e+\beta_v=0$.</span>
<span id="cb72-1715"><a href="#cb72-1715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1716"><a href="#cb72-1716" aria-hidden="true" tabindex="-1"></a>Both hypotheses are not rejected, even at the 10% level. To test the joint hypothesis, we can first use the approximate formula which is the sum or the mean of the squares of the t statistics (respectively $\chi^2$ with 2 degrees of freedom and F with 2 and 71 degrees of freedom):</span>
<span id="cb72-1717"><a href="#cb72-1717" aria-hidden="true" tabindex="-1"></a>\idxfun{pf}{stats}\idxfun{pchisq}{stats}</span>
<span id="cb72-1718"><a href="#cb72-1718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1721"><a href="#cb72-1721" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1722"><a href="#cb72-1722" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: approx_results_joint_hyp</span></span>
<span id="cb72-1723"><a href="#cb72-1723" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: TRUE</span></span>
<span id="cb72-1724"><a href="#cb72-1724" aria-hidden="true" tabindex="-1"></a>appr_chisq <span class="ot">&lt;-</span> coef_nc[<span class="dv">3</span>, <span class="dv">3</span>] <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> coef_nc[<span class="dv">4</span>, <span class="dv">3</span>] <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb72-1725"><a href="#cb72-1725" aria-hidden="true" tabindex="-1"></a>appr_f <span class="ot">&lt;-</span> appr_chisq <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-1726"><a href="#cb72-1726" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(appr_chisq, appr_f)</span>
<span id="cb72-1727"><a href="#cb72-1727" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(appr_chisq, <span class="at">df =</span> <span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb72-1728"><a href="#cb72-1728" aria-hidden="true" tabindex="-1"></a><span class="fu">pf</span>(appr_f, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="dv">71</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb72-1729"><a href="#cb72-1729" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1730"><a href="#cb72-1730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1731"><a href="#cb72-1731" aria-hidden="true" tabindex="-1"></a>Based on this approximation, the joint hypothesis is clearly not rejected. We now turn to the computation of the statistics, using the three test principles.</span>
<span id="cb72-1732"><a href="#cb72-1732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1733"><a href="#cb72-1733" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Wald test {#sec-wald_test_example}</span></span>
<span id="cb72-1734"><a href="#cb72-1734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1735"><a href="#cb72-1735" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test|(}</span>
<span id="cb72-1736"><a href="#cb72-1736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1737"><a href="#cb72-1737" aria-hidden="true" tabindex="-1"></a>Considering the initial unconstrained model, for which the formula is $y \sim i + v + e$, the set of the two hypotheses can be written in matrix form as:</span>
<span id="cb72-1738"><a href="#cb72-1738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1739"><a href="#cb72-1739" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1740"><a href="#cb72-1740" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1741"><a href="#cb72-1741" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb72-1742"><a href="#cb72-1742" aria-hidden="true" tabindex="-1"></a>0 &amp; 2 &amp; 0 &amp; - 1 <span class="sc">\\</span></span>
<span id="cb72-1743"><a href="#cb72-1743" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 1 &amp;   1 <span class="sc">\\</span></span>
<span id="cb72-1744"><a href="#cb72-1744" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1745"><a href="#cb72-1745" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1746"><a href="#cb72-1746" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1747"><a href="#cb72-1747" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1748"><a href="#cb72-1748" aria-hidden="true" tabindex="-1"></a>\alpha <span class="sc">\\</span> \beta_i <span class="sc">\\</span> \beta_v <span class="sc">\\</span> \beta_e</span>
<span id="cb72-1749"><a href="#cb72-1749" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1750"><a href="#cb72-1750" aria-hidden="true" tabindex="-1"></a>\right) =</span>
<span id="cb72-1751"><a href="#cb72-1751" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1752"><a href="#cb72-1752" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1753"><a href="#cb72-1753" aria-hidden="true" tabindex="-1"></a>1 <span class="sc">\\</span> 0</span>
<span id="cb72-1754"><a href="#cb72-1754" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1755"><a href="#cb72-1755" aria-hidden="true" tabindex="-1"></a>\right) </span>
<span id="cb72-1756"><a href="#cb72-1756" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1757"><a href="#cb72-1757" aria-hidden="true" tabindex="-1"></a>The matrix $R$ and the vector $q$ are created in **R**:</span>
<span id="cb72-1758"><a href="#cb72-1758" aria-hidden="true" tabindex="-1"></a>\idxfun{matrix}{base}</span>
<span id="cb72-1759"><a href="#cb72-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1762"><a href="#cb72-1762" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1763"><a href="#cb72-1763" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: R_constraints_growth</span></span>
<span id="cb72-1764"><a href="#cb72-1764" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>) ; q <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb72-1765"><a href="#cb72-1765" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">1</span>, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">2</span> ; R[<span class="dv">1</span>, <span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="dv">1</span> ; R[<span class="dv">2</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb72-1766"><a href="#cb72-1766" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1767"><a href="#cb72-1767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1768"><a href="#cb72-1768" aria-hidden="true" tabindex="-1"></a>We then have $H_0: R\gamma - q = 0$ and, for the fitted unconstrained model, we get: $R\hat{\gamma} - q$. Under $H_0$ the expected value of this vector is 0 and its estimated variance is $R^\top \hat{V}_{\hat{\gamma}}R=\hat{\sigma}_\epsilon ^ 2R^\top(Z^\top Z)^{-1}R$. Then:</span>
<span id="cb72-1769"><a href="#cb72-1769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1770"><a href="#cb72-1770" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1771"><a href="#cb72-1771" aria-hidden="true" tabindex="-1"></a>(R\hat{\gamma} - q)^\top \left<span class="co">[</span><span class="ot">R \hat{V}_{\hat{\gamma}}R ^ \top\right</span><span class="co">]</span>^{-1}(R\hat{\gamma} - q)</span>
<span id="cb72-1772"><a href="#cb72-1772" aria-hidden="true" tabindex="-1"></a>$$ {#eq-wald_test_formula}</span>
<span id="cb72-1773"><a href="#cb72-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1774"><a href="#cb72-1774" aria-hidden="true" tabindex="-1"></a>is asymptotically a $\chi^2$ with $J=2$ degrees of freedom. Dividing by $J$, we get a F statistic with $2$ and $71$ degrees of freedom.</span>
<span id="cb72-1775"><a href="#cb72-1775" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{solve}{base}\idxfun{vcov}{stats}</span>
<span id="cb72-1776"><a href="#cb72-1776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1779"><a href="#cb72-1779" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1780"><a href="#cb72-1780" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: wald_test_growth</span></span>
<span id="cb72-1781"><a href="#cb72-1781" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1782"><a href="#cb72-1782" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(R <span class="sc">%*%</span> <span class="fu">coef</span>(mrw2) <span class="sc">-</span> q) <span class="sc">%*%</span> </span>
<span id="cb72-1783"><a href="#cb72-1783" aria-hidden="true" tabindex="-1"></a>  <span class="fu">solve</span>(R <span class="sc">%*%</span> <span class="fu">vcov</span>(mrw2) <span class="sc">%*%</span> <span class="fu">t</span>(R)) <span class="sc">%*%</span>  </span>
<span id="cb72-1784"><a href="#cb72-1784" aria-hidden="true" tabindex="-1"></a>  (R <span class="sc">%*%</span> <span class="fu">coef</span>(mrw2) <span class="sc">-</span> q) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-1785"><a href="#cb72-1785" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1786"><a href="#cb72-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1787"><a href="#cb72-1787" aria-hidden="true" tabindex="-1"></a><span class="in">`car::linearHypothesis`</span> performs Wald tests with a nice syntax: the first argument is a fitted model, and the second one is a vector of characters which contains the character representation of the hypothesis:</span>
<span id="cb72-1788"><a href="#cb72-1788" aria-hidden="true" tabindex="-1"></a>\idxfun{linearHypothesis}{car}\idxfun{gaze}{micsr}</span>
<span id="cb72-1789"><a href="#cb72-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1792"><a href="#cb72-1792" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1793"><a href="#cb72-1793" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: linear_hypothesis</span></span>
<span id="cb72-1794"><a href="#cb72-1794" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1795"><a href="#cb72-1795" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">linearHypothesis</span>(mrw2, <span class="fu">c</span>(<span class="st">"i + v + e = 0"</span>, <span class="st">"e = 2 * i - 1"</span>)) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-1796"><a href="#cb72-1796" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1797"><a href="#cb72-1797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1798"><a href="#cb72-1798" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald test|)}</span>
<span id="cb72-1799"><a href="#cb72-1799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1800"><a href="#cb72-1800" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Likelihood ratio test</span></span>
<span id="cb72-1801"><a href="#cb72-1801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1802"><a href="#cb72-1802" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test|(}</span>
<span id="cb72-1803"><a href="#cb72-1803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1804"><a href="#cb72-1804" aria-hidden="true" tabindex="-1"></a>The computation of the likelihood ratio statistic is very simple once the two models have been estimated. The residual sums of squares of the two models are extracted using the <span class="in">`deviance`</span> method, and we divide the difference of the two sums of squares by $\dot{\sigma}_\epsilon^2$ (the <span class="in">`sigma`</span> method is used to extract the residual standard error) and by 2 to get an F statistic.</span>
<span id="cb72-1805"><a href="#cb72-1805" aria-hidden="true" tabindex="-1"></a>\idxfun{deviance}{stats}\idxfun{sigma}{stats}</span>
<span id="cb72-1806"><a href="#cb72-1806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1809"><a href="#cb72-1809" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1810"><a href="#cb72-1810" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lr_test_growth</span></span>
<span id="cb72-1811"><a href="#cb72-1811" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1812"><a href="#cb72-1812" aria-hidden="true" tabindex="-1"></a>(<span class="fu">deviance</span>(mrw_c) <span class="sc">-</span> <span class="fu">deviance</span>(mrw2))<span class="sc">/</span> <span class="fu">sigma</span>(mrw2) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-1813"><a href="#cb72-1813" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1814"><a href="#cb72-1814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1815"><a href="#cb72-1815" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{likelihood ratio test|)}</span>
<span id="cb72-1816"><a href="#cb72-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1817"><a href="#cb72-1817" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Score test</span></span>
<span id="cb72-1818"><a href="#cb72-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1819"><a href="#cb72-1819" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test|(}</span>
<span id="cb72-1820"><a href="#cb72-1820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1821"><a href="#cb72-1821" aria-hidden="true" tabindex="-1"></a>For the score test, we consider the reparametrized model and we define $Z_1$ as a matrix containing a column of one and $i + 2  e - 3  v$ and $X_2$ as a matrix containing $e-v$ and $v$.</span>
<span id="cb72-1822"><a href="#cb72-1822" aria-hidden="true" tabindex="-1"></a>\idxfun{model.matrix}{stats}\idxfun{model.frame}{stats}</span>
<span id="cb72-1823"><a href="#cb72-1823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1826"><a href="#cb72-1826" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1827"><a href="#cb72-1827" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: score_test_model_matrix</span></span>
<span id="cb72-1828"><a href="#cb72-1828" aria-hidden="true" tabindex="-1"></a>Z1 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span> i2, <span class="fu">model.frame</span>(mrw_nc))</span>
<span id="cb72-1829"><a href="#cb72-1829" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span> e2 <span class="sc">+</span> v <span class="sc">-</span> <span class="dv">1</span>, <span class="fu">model.frame</span>(mrw_nc))</span>
<span id="cb72-1830"><a href="#cb72-1830" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1831"><a href="#cb72-1831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1832"><a href="#cb72-1832" aria-hidden="true" tabindex="-1"></a>The test is based on the vector $X_2 ^ \top \hat{\epsilon}_c$, where $\hat{\epsilon}_c$ is the vector of the residuals for the constrained model. Under $H_0$, the expected value of $X_2 ^ \top \hat{\epsilon}_c$ is 0 and its variance is $\sigma_\epsilon ^ 2 X_2^ \top M_1 X_2$. $M_1 X_2$ is a matrix of residuals of all the columns of $X_2$ on $Z_1$.</span>
<span id="cb72-1833"><a href="#cb72-1833" aria-hidden="true" tabindex="-1"></a>\idxfun{resid}{stats}\idxfun{lm}{stats}</span>
<span id="cb72-1834"><a href="#cb72-1834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1837"><a href="#cb72-1837" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1838"><a href="#cb72-1838" aria-hidden="true" tabindex="-1"></a>ec <span class="ot">&lt;-</span> <span class="fu">resid</span>(mrw_c)</span>
<span id="cb72-1839"><a href="#cb72-1839" aria-hidden="true" tabindex="-1"></a>M1X2 <span class="ot">&lt;-</span> <span class="fu">resid</span>(<span class="fu">lm</span>(X2 <span class="sc">~</span> Z1))</span>
<span id="cb72-1840"><a href="#cb72-1840" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1841"><a href="#cb72-1841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1842"><a href="#cb72-1842" aria-hidden="true" tabindex="-1"></a>Note that we've used <span class="in">`lm`</span> with only a formula argument. In this case, the response and the covariates are vectors or matrices and not columns of a tibble. Note also that the left-hand side of a formula is a matrix and not a vector; in this case, each column is supposed to be a response and <span class="in">`lm`</span> fit as many models than there are columns in this matrix. Then, the <span class="in">`resid`</span> method no longer returns a vector, but a matrix, each column being a vector of residuals for one of the fitted models.</span>
<span id="cb72-1843"><a href="#cb72-1843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1844"><a href="#cb72-1844" aria-hidden="true" tabindex="-1"></a>The statistic is then $\hat{\epsilon}_c^\top X_2 \left[X_2^ \top M_1 X_2\right]^{-1} X_2 \hat{\epsilon}_c / \sigma_\epsilon^2$ and it is computed using an estimator of $\sigma_\epsilon^2$, and dividing by $J=2$ to get an F statistic.</span>
<span id="cb72-1845"><a href="#cb72-1845" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}\idxfun{sigma}{stats}\idxfun{solve}{base}</span>
<span id="cb72-1846"><a href="#cb72-1846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1849"><a href="#cb72-1849" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1850"><a href="#cb72-1850" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1851"><a href="#cb72-1851" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(<span class="fu">crossprod</span>(X2, ec)) <span class="sc">%*%</span> </span>
<span id="cb72-1852"><a href="#cb72-1852" aria-hidden="true" tabindex="-1"></a>  <span class="fu">solve</span>(<span class="fu">crossprod</span>(M1X2)) <span class="sc">%*%</span> </span>
<span id="cb72-1853"><a href="#cb72-1853" aria-hidden="true" tabindex="-1"></a>  <span class="fu">crossprod</span>(X2, ec) <span class="sc">/</span> <span class="fu">sigma</span>(mrw_c) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-1854"><a href="#cb72-1854" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1855"><a href="#cb72-1855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1856"><a href="#cb72-1856" aria-hidden="true" tabindex="-1"></a>The statistic is slightly different from the one computed previously, the difference being only due to the fact that the estimation of $\sigma_\epsilon$ is based, for the score test, on the constrained model.</span>
<span id="cb72-1857"><a href="#cb72-1857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1858"><a href="#cb72-1858" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{score test|)}</span>
<span id="cb72-1859"><a href="#cb72-1859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1860"><a href="#cb72-1860" aria-hidden="true" tabindex="-1"></a><span class="fu">### Testing that all the slopes are 0</span></span>
<span id="cb72-1861"><a href="#cb72-1861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1862"><a href="#cb72-1862" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{F statistic|(}</span>
<span id="cb72-1863"><a href="#cb72-1863" aria-hidden="true" tabindex="-1"></a>The test that all the slopes are 0 is routinely reported by software performing OLS estimation. It can be computed using any of the three test principles, but the likelihood ratio test is particularly appealing in this context, the constrained model being a model with only an intercept: $y_n = \alpha + \epsilon_n$. In this case, $\hat{\alpha} = \bar{y}$ and $\hat{\epsilon} = (y_n - \bar{y})$. Therefore, the residual sum of squares for the constrained model is $S_{yy} = \sum_n (y_n - \bar{y}) ^ 2$ and is also denoted by $\mbox{TSS}$ for total sum of squares. The statistic is then:</span>
<span id="cb72-1864"><a href="#cb72-1864" aria-hidden="true" tabindex="-1"></a>$(\mbox{TSS} - \mbox{RSS})/\sigma_\epsilon ^ 2 \sim \chi ^ 2_K$, </span>
<span id="cb72-1865"><a href="#cb72-1865" aria-hidden="true" tabindex="-1"></a>which is a $\chi ^ 2$ with $K$ degrees of freedom if the hypothesis that all the slopes are 0 is true. To compute this statistic, $\sigma_\epsilon^2$ has to be estimated. A natural estimator is $\mbox{RSS} / (N - K - 1)$ but, if $H_0$ is true, $\mbox{TSS} / (N - 1)$ is also an unbiased estimator. Moreover, dividing by the sample size ($N$) and not by the number of degrees of freedom leads to biased but consistent estimators. Using the first estimator of $\sigma_\epsilon^2$ and dividing by $K$, we get the $F$ statistic with $K$ and $N-K-1$ degrees of freedom:</span>
<span id="cb72-1866"><a href="#cb72-1866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1867"><a href="#cb72-1867" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1868"><a href="#cb72-1868" aria-hidden="true" tabindex="-1"></a>\frac{\mbox{TSS} -\mbox{RSS}}{\mbox{RSS}}\frac{N - K - 1}{K} \sim F_{K, N-K-1}</span>
<span id="cb72-1869"><a href="#cb72-1869" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1870"><a href="#cb72-1870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1871"><a href="#cb72-1871" aria-hidden="true" tabindex="-1"></a>Using $\mbox{TSS} / N$ as an estimator of $\sigma_\epsilon^2$, we get a very simple statistic that is asymptotically a $\chi^2$ with $K$ degrees of freedom:</span>
<span id="cb72-1872"><a href="#cb72-1872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1873"><a href="#cb72-1873" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1874"><a href="#cb72-1874" aria-hidden="true" tabindex="-1"></a>N \frac{\mbox{TSS} -\mbox{RSS}}{\mbox{TSS}}  \stackrel{a}{\sim} \chi^2_{K}</span>
<span id="cb72-1875"><a href="#cb72-1875" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1876"><a href="#cb72-1876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1877"><a href="#cb72-1877" aria-hidden="true" tabindex="-1"></a>These two statistics are closely related to the $R^2$ which is, using this notation, equal to $1 - \mbox{RSS} / \mbox{TSS} = (\mbox{TSS} - \mbox{RSS}) / \mbox{TSS}$. We can then write the $F$ statistic as: </span>
<span id="cb72-1878"><a href="#cb72-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1879"><a href="#cb72-1879" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1880"><a href="#cb72-1880" aria-hidden="true" tabindex="-1"></a>\frac{R^2}{1 - R ^ 2}\frac{N - K - 1}{K}</span>
<span id="cb72-1881"><a href="#cb72-1881" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1882"><a href="#cb72-1882" aria-hidden="true" tabindex="-1"></a>and the asymptotic $\chi^2$ statistic as $N R^2$.</span>
<span id="cb72-1883"><a href="#cb72-1883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1884"><a href="#cb72-1884" aria-hidden="true" tabindex="-1"></a>There is no easy way to extract the $R^2$ and the $F$ statistic with **R**. Both are computed by the <span class="in">`summary`</span> method of <span class="in">`lm`</span>:</span>
<span id="cb72-1885"><a href="#cb72-1885" aria-hidden="true" tabindex="-1"></a>\idxfun{names}{base}</span>
<span id="cb72-1886"><a href="#cb72-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1889"><a href="#cb72-1889" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1890"><a href="#cb72-1890" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: summary_lm_names</span></span>
<span id="cb72-1891"><a href="#cb72-1891" aria-hidden="true" tabindex="-1"></a>slm <span class="ot">&lt;-</span> slw_tot <span class="sc">%&gt;%</span> summary</span>
<span id="cb72-1892"><a href="#cb72-1892" aria-hidden="true" tabindex="-1"></a>slm <span class="sc">%&gt;%</span> names</span>
<span id="cb72-1893"><a href="#cb72-1893" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1894"><a href="#cb72-1894" aria-hidden="true" tabindex="-1"></a>and can be extracted "manually" from the list returned by <span class="in">`summary`</span> using the <span class="in">`$`</span> operator:</span>
<span id="cb72-1895"><a href="#cb72-1895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1898"><a href="#cb72-1898" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1899"><a href="#cb72-1899" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: extracting_r2_f</span></span>
<span id="cb72-1900"><a href="#cb72-1900" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1901"><a href="#cb72-1901" aria-hidden="true" tabindex="-1"></a>slm<span class="sc">$</span>r.squared</span>
<span id="cb72-1902"><a href="#cb72-1902" aria-hidden="true" tabindex="-1"></a>slm<span class="sc">$</span>adj.r.squared</span>
<span id="cb72-1903"><a href="#cb72-1903" aria-hidden="true" tabindex="-1"></a>slm<span class="sc">$</span>fstatistic</span>
<span id="cb72-1904"><a href="#cb72-1904" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1905"><a href="#cb72-1905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1906"><a href="#cb72-1906" aria-hidden="true" tabindex="-1"></a>More simply, <span class="in">`micsr:rsq`</span> and <span class="in">`micsr:ftest`</span> can be used:</span>
<span id="cb72-1907"><a href="#cb72-1907" aria-hidden="true" tabindex="-1"></a>\idxfun{rsq}{micsr}\idxfun{ftest}{micsr}\idxfun{gaze}{micsr}</span>
<span id="cb72-1908"><a href="#cb72-1908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1911"><a href="#cb72-1911" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1912"><a href="#cb72-1912" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: extracting_r2_f_micsr</span></span>
<span id="cb72-1913"><a href="#cb72-1913" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1914"><a href="#cb72-1914" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="sc">%&gt;%</span> rsq</span>
<span id="cb72-1915"><a href="#cb72-1915" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="sc">%&gt;%</span> <span class="fu">rsq</span>(<span class="at">type =</span> <span class="st">"adj"</span>)</span>
<span id="cb72-1916"><a href="#cb72-1916" aria-hidden="true" tabindex="-1"></a>slw_tot <span class="sc">%&gt;%</span> ftest <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-1917"><a href="#cb72-1917" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1918"><a href="#cb72-1918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1919"><a href="#cb72-1919" aria-hidden="true" tabindex="-1"></a>The interest of this testing strategy is not limited to the test that all the slopes of a real model are 0. It can also be used to test any set of hypotheses, using reparametrization and the Frisch-Waugh theorem. Consider for example the hypothesis that $\kappa = 0.5$ and $\beta_i + \beta_e + \beta_v=0$. We have seen previously that, after reparametrization, this corresponds to the model:</span>
<span id="cb72-1920"><a href="#cb72-1920" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-1921"><a href="#cb72-1921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1924"><a href="#cb72-1924" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1925"><a href="#cb72-1925" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: constrained_reparam_growth</span></span>
<span id="cb72-1926"><a href="#cb72-1926" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1927"><a href="#cb72-1927" aria-hidden="true" tabindex="-1"></a>mrw_nc <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> i2 <span class="sc">+</span> e2 <span class="sc">+</span> v, growth_sub2)</span>
<span id="cb72-1928"><a href="#cb72-1928" aria-hidden="true" tabindex="-1"></a>mrw_nc <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1929"><a href="#cb72-1929" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1930"><a href="#cb72-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1931"><a href="#cb72-1931" aria-hidden="true" tabindex="-1"></a>with, if the two hypotheses are true, the two slopes associated with <span class="in">`e2`</span> and <span class="in">`v`</span> equal to 0. Now, using the Frisch-Waugh theorem, and denoting $Z_1 = (j, i_2)$ and $X_2 = (e_2, v)$:</span>
<span id="cb72-1932"><a href="#cb72-1932" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}\idxfun{resid}{stats}</span>
<span id="cb72-1933"><a href="#cb72-1933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1936"><a href="#cb72-1936" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1937"><a href="#cb72-1937" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: frisch_waugh_growth</span></span>
<span id="cb72-1938"><a href="#cb72-1938" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1939"><a href="#cb72-1939" aria-hidden="true" tabindex="-1"></a>y2b <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> i2, growth_sub2) <span class="sc">%&gt;%</span> resid</span>
<span id="cb72-1940"><a href="#cb72-1940" aria-hidden="true" tabindex="-1"></a>e2b <span class="ot">&lt;-</span> <span class="fu">lm</span>(e2 <span class="sc">~</span> i2, growth_sub2) <span class="sc">%&gt;%</span> resid</span>
<span id="cb72-1941"><a href="#cb72-1941" aria-hidden="true" tabindex="-1"></a>vb <span class="ot">&lt;-</span> <span class="fu">lm</span>(v <span class="sc">~</span> i2, growth_sub2) <span class="sc">%&gt;%</span> resid</span>
<span id="cb72-1942"><a href="#cb72-1942" aria-hidden="true" tabindex="-1"></a>mrw_ncb <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2b <span class="sc">~</span> e2b <span class="sc">+</span> vb <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb72-1943"><a href="#cb72-1943" aria-hidden="true" tabindex="-1"></a>mrw_ncb <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1944"><a href="#cb72-1944" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1945"><a href="#cb72-1945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1946"><a href="#cb72-1946" aria-hidden="true" tabindex="-1"></a>We get exactly the same estimators as previously for <span class="in">`e2`</span> and <span class="in">`v`</span>, but now the joint hypothesis is that all the slopes of the second model are 0. Therefore, the test is based on the F statistic that is returned by <span class="in">`summary(lm(x))`</span> and doesn't require any further calculus:</span>
<span id="cb72-1947"><a href="#cb72-1947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1948"><a href="#cb72-1948" aria-hidden="true" tabindex="-1"></a>\idxfun{ftest}{micsr}\idxfun{gaze}{micsr}</span>
<span id="cb72-1951"><a href="#cb72-1951" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1952"><a href="#cb72-1952" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: frisch_waugh_growth_fstat</span></span>
<span id="cb72-1953"><a href="#cb72-1953" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1954"><a href="#cb72-1954" aria-hidden="true" tabindex="-1"></a>mrw_ncb <span class="sc">%&gt;%</span> ftest <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-1955"><a href="#cb72-1955" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1956"><a href="#cb72-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1957"><a href="#cb72-1957" aria-hidden="true" tabindex="-1"></a>Actually, a degrees of freedom correction should be performed to get exactly the same results because <span class="in">`mrw_ncb`</span> has $75 - 2 = 73$ degrees of freedom, as the real number of degrees of freedom is $75 - 4  = 71$.</span>
<span id="cb72-1958"><a href="#cb72-1958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1959"><a href="#cb72-1959" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{F statistic|)}</span>
<span id="cb72-1960"><a href="#cb72-1960" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!three test principles!linear regression models|)}</span>
<span id="cb72-1961"><a href="#cb72-1961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1962"><a href="#cb72-1962" aria-hidden="true" tabindex="-1"></a><span class="fu">## System estimation and constrained least squares {#sec-system_equation}</span></span>
<span id="cb72-1963"><a href="#cb72-1963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1964"><a href="#cb72-1964" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation|(}</span>
<span id="cb72-1965"><a href="#cb72-1965" aria-hidden="true" tabindex="-1"></a>Very often in economics, the phenomenon under investigation is not</span>
<span id="cb72-1966"><a href="#cb72-1966" aria-hidden="true" tabindex="-1"></a>well described by a single equation, but by a system of</span>
<span id="cb72-1967"><a href="#cb72-1967" aria-hidden="true" tabindex="-1"></a>equations. Moreover, there may be inter-equations constraints on the</span>
<span id="cb72-1968"><a href="#cb72-1968" aria-hidden="true" tabindex="-1"></a>coefficients. It is particularly the case in the field of the</span>
<span id="cb72-1969"><a href="#cb72-1969" aria-hidden="true" tabindex="-1"></a>microeconometrics of consumption or production. For example, the</span>
<span id="cb72-1970"><a href="#cb72-1970" aria-hidden="true" tabindex="-1"></a>behavior of a producer is described by a minimum cost equation along</span>
<span id="cb72-1971"><a href="#cb72-1971" aria-hidden="true" tabindex="-1"></a>with equations of factor demand and the behavior of a consumer is</span>
<span id="cb72-1972"><a href="#cb72-1972" aria-hidden="true" tabindex="-1"></a>described by a set of demand equations. </span>
<span id="cb72-1973"><a href="#cb72-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1974"><a href="#cb72-1974" aria-hidden="true" tabindex="-1"></a><span class="fu">### System of equations {#sec-sys_eq_ols}</span></span>
<span id="cb72-1975"><a href="#cb72-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1976"><a href="#cb72-1976" aria-hidden="true" tabindex="-1"></a>We consider therefore a system of $L$ equations denoted by</span>
<span id="cb72-1977"><a href="#cb72-1977" aria-hidden="true" tabindex="-1"></a>$y_l=Z_l\beta_l+\epsilon_l$, with $l=1\ldots L$. In matrix form, the</span>
<span id="cb72-1978"><a href="#cb72-1978" aria-hidden="true" tabindex="-1"></a>system can be written as follows:</span>
<span id="cb72-1979"><a href="#cb72-1979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1980"><a href="#cb72-1980" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1981"><a href="#cb72-1981" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1982"><a href="#cb72-1982" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb72-1983"><a href="#cb72-1983" aria-hidden="true" tabindex="-1"></a>    y_1 <span class="sc">\\</span> y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> y_L</span>
<span id="cb72-1984"><a href="#cb72-1984" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1985"><a href="#cb72-1985" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1986"><a href="#cb72-1986" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-1987"><a href="#cb72-1987" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1988"><a href="#cb72-1988" aria-hidden="true" tabindex="-1"></a>  \begin{array}{ccccc}</span>
<span id="cb72-1989"><a href="#cb72-1989" aria-hidden="true" tabindex="-1"></a>    Z_1 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1990"><a href="#cb72-1990" aria-hidden="true" tabindex="-1"></a>    0 &amp; Z_2 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1991"><a href="#cb72-1991" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1992"><a href="#cb72-1992" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; \ldots &amp; Z_L</span>
<span id="cb72-1993"><a href="#cb72-1993" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1994"><a href="#cb72-1994" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1995"><a href="#cb72-1995" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1996"><a href="#cb72-1996" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb72-1997"><a href="#cb72-1997" aria-hidden="true" tabindex="-1"></a>    \gamma_1 <span class="sc">\\</span> \gamma_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \gamma_L</span>
<span id="cb72-1998"><a href="#cb72-1998" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1999"><a href="#cb72-1999" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2000"><a href="#cb72-2000" aria-hidden="true" tabindex="-1"></a>+</span>
<span id="cb72-2001"><a href="#cb72-2001" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2002"><a href="#cb72-2002" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb72-2003"><a href="#cb72-2003" aria-hidden="true" tabindex="-1"></a>    \epsilon_1 <span class="sc">\\</span> \epsilon_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \epsilon_L</span>
<span id="cb72-2004"><a href="#cb72-2004" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-2005"><a href="#cb72-2005" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2006"><a href="#cb72-2006" aria-hidden="true" tabindex="-1"></a>$$ {#eq-system_equation}</span>
<span id="cb72-2007"><a href="#cb72-2007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2008"><a href="#cb72-2008" aria-hidden="true" tabindex="-1"></a>Therefore, the whole system can be estimated directly by stacking the</span>
<span id="cb72-2009"><a href="#cb72-2009" aria-hidden="true" tabindex="-1"></a>vector of responses and by constructing a block-diagonal matrix of</span>
<span id="cb72-2010"><a href="#cb72-2010" aria-hidden="true" tabindex="-1"></a>covariates, each block being the matrix of covariates for one equation. </span>
<span id="cb72-2011"><a href="#cb72-2011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2012"><a href="#cb72-2012" aria-hidden="true" tabindex="-1"></a>As an example, consider the analysis of production characteristics (returns to scale, elasticities</span>
<span id="cb72-2013"><a href="#cb72-2013" aria-hidden="true" tabindex="-1"></a>of substitution between pairs of inputs). The modern approach of production analysis consists of first considering the minimum cost function, which depends on the level of</span>
<span id="cb72-2014"><a href="#cb72-2014" aria-hidden="true" tabindex="-1"></a>production and on input unit prices $C(y, p_1, \ldots p_J)$ and then of deriving the demands for input using Shepard Lemma:</span>
<span id="cb72-2015"><a href="#cb72-2015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2016"><a href="#cb72-2016" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2017"><a href="#cb72-2017" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial p_j} = x_j(y, p_1, \ldots, p_J)</span>
<span id="cb72-2018"><a href="#cb72-2018" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2019"><a href="#cb72-2019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2020"><a href="#cb72-2020" aria-hidden="true" tabindex="-1"></a>The cost function is obviously homogeneous of degree 1 in input unit prices,</span>
<span id="cb72-2021"><a href="#cb72-2021" aria-hidden="true" tabindex="-1"></a>which means that, for a given level of input, if all the prices</span>
<span id="cb72-2022"><a href="#cb72-2022" aria-hidden="true" tabindex="-1"></a>increase proportionally, the quantity of the different inputs are the</span>
<span id="cb72-2023"><a href="#cb72-2023" aria-hidden="true" tabindex="-1"></a>same and therefore the cost function increases by the same</span>
<span id="cb72-2024"><a href="#cb72-2024" aria-hidden="true" tabindex="-1"></a>percentage. This writes: $C(y, \lambda p_1, \ldots \lambda p_J) =</span>
<span id="cb72-2025"><a href="#cb72-2025" aria-hidden="true" tabindex="-1"></a>\lambda C(y, p_1, \ldots, p_J)$ and $x_j(y, \lambda p_1, \ldots,</span>
<span id="cb72-2026"><a href="#cb72-2026" aria-hidden="true" tabindex="-1"></a>\lambda p _J) = x_j(y, p_1, \ldots p_J)$; the latter relation</span>
<span id="cb72-2027"><a href="#cb72-2027" aria-hidden="true" tabindex="-1"></a>indicating that the demands for input are homogeneous of degree 0 in</span>
<span id="cb72-2028"><a href="#cb72-2028" aria-hidden="true" tabindex="-1"></a>input unit prices. Among the different functional forms that have been proposed to</span>
<span id="cb72-2029"><a href="#cb72-2029" aria-hidden="true" tabindex="-1"></a>estimate the cost function, the translog specification is the most</span>
<span id="cb72-2030"><a href="#cb72-2030" aria-hidden="true" tabindex="-1"></a>popular. It can be considered as the second-order approximation of a</span>
<span id="cb72-2031"><a href="#cb72-2031" aria-hidden="true" tabindex="-1"></a>general cost function:</span>
<span id="cb72-2032"><a href="#cb72-2032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2033"><a href="#cb72-2033" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2034"><a href="#cb72-2034" aria-hidden="true" tabindex="-1"></a>\ln C = \alpha + \beta_y \ln y + \frac{1}{2}\beta_{yy} \ln^2 y +\sum_i \beta_i \ln p_i + \frac{1}{2}</span>
<span id="cb72-2035"><a href="#cb72-2035" aria-hidden="true" tabindex="-1"></a>\sum_i\sum_j \beta_{ij} \ln p_i \ln p_j</span>
<span id="cb72-2036"><a href="#cb72-2036" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2037"><a href="#cb72-2037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2038"><a href="#cb72-2038" aria-hidden="true" tabindex="-1"></a>Using Shephard Lemma, the cost share of input $i$ is the derivative of</span>
<span id="cb72-2039"><a href="#cb72-2039" aria-hidden="true" tabindex="-1"></a>$\ln C$ with $\ln p_i$. </span>
<span id="cb72-2040"><a href="#cb72-2040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2041"><a href="#cb72-2041" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2042"><a href="#cb72-2042" aria-hidden="true" tabindex="-1"></a>s_i = \frac{\ln C}{\ln p_i} = \frac{p_i x_i}{C} = \beta_i + \beta_{ii} \ln p_i + \frac{1}{2}\sum_{j\neq i} (\beta_{ij} + \beta_{ji}) \ln p_j</span>
<span id="cb72-2043"><a href="#cb72-2043" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2044"><a href="#cb72-2044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2045"><a href="#cb72-2045" aria-hidden="true" tabindex="-1"></a>Homogeneity of degree 1 in input prices implies that the cost shares don't</span>
<span id="cb72-2046"><a href="#cb72-2046" aria-hidden="true" tabindex="-1"></a>depend on the level of prices. Therefore, $\sum_j^I \beta_{ij} = 0$,</span>
<span id="cb72-2047"><a href="#cb72-2047" aria-hidden="true" tabindex="-1"></a>or $\beta_{iI} = - \sum_j^{I-1} \beta_{ij}$ and:</span>
<span id="cb72-2048"><a href="#cb72-2048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2049"><a href="#cb72-2049" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2050"><a href="#cb72-2050" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-2051"><a href="#cb72-2051" aria-hidden="true" tabindex="-1"></a>\sum_i^I\sum_j^I \beta_{ij} \ln p_i \ln p_j &amp;=&amp;</span>
<span id="cb72-2052"><a href="#cb72-2052" aria-hidden="true" tabindex="-1"></a>\sum_i^I \ln p_i \left[\sum_j^{I-1} \beta_{ij}\ln p_j  + \beta_{iJ}\ln</span>
<span id="cb72-2053"><a href="#cb72-2053" aria-hidden="true" tabindex="-1"></a>p_I\right] <span class="sc">\\</span></span>
<span id="cb72-2054"><a href="#cb72-2054" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \sum_i^I \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} <span class="sc">\\</span></span>
<span id="cb72-2055"><a href="#cb72-2055" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \sum_i^{I-1} \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} + </span>
<span id="cb72-2056"><a href="#cb72-2056" aria-hidden="true" tabindex="-1"></a>\ln p_I \sum_j^{I-1} \beta_{Ij} \ln\frac{p_j}{p_I} <span class="sc">\\</span></span>
<span id="cb72-2057"><a href="#cb72-2057" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \sum_i^{I-1} \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} + </span>
<span id="cb72-2058"><a href="#cb72-2058" aria-hidden="true" tabindex="-1"></a>\ln p_I \sum_j^{I-1} \beta_{jI} \ln\frac{p_j}{p_I} <span class="sc">\\</span></span>
<span id="cb72-2059"><a href="#cb72-2059" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \sum_i^{I-1} \ln p_i \sum_j^{I-1} \beta_{ij} \ln\frac{p_j}{p_I} -</span>
<span id="cb72-2060"><a href="#cb72-2060" aria-hidden="true" tabindex="-1"></a>\ln p_I \sum_i^{I-1} \sum_j^{I-1}\beta_{ji} \ln\frac{p_j}{p_I} <span class="sc">\\</span></span>
<span id="cb72-2061"><a href="#cb72-2061" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \sum_i^{I-1} \sum_j^{I-1} \beta_{ij} \ln \frac{p_i}{p_I}\ln\frac{p_j}{p_I}</span>
<span id="cb72-2062"><a href="#cb72-2062" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-2063"><a href="#cb72-2063" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2064"><a href="#cb72-2064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2065"><a href="#cb72-2065" aria-hidden="true" tabindex="-1"></a>Moreover, the cost shares sum to 1 whatever the value of the prices, so that</span>
<span id="cb72-2066"><a href="#cb72-2066" aria-hidden="true" tabindex="-1"></a>$\sum_i \beta_i = 1$. Therefore, the cost function can be rewritten as:</span>
<span id="cb72-2067"><a href="#cb72-2067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2068"><a href="#cb72-2068" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2069"><a href="#cb72-2069" aria-hidden="true" tabindex="-1"></a>C^* = \alpha + \beta_y \ln y + \frac{1}{2}\beta_{yy} \ln^2 y + \sum_i ^{I-1}\beta_i p_i^* + </span>
<span id="cb72-2070"><a href="#cb72-2070" aria-hidden="true" tabindex="-1"></a>\frac{1}{2} \sum_{i=1} ^ {I - 1} \beta_{ii} {p_i^*} ^ 2 +  </span>
<span id="cb72-2071"><a href="#cb72-2071" aria-hidden="true" tabindex="-1"></a>\sum_{i=1} ^ {I - 1} \sum_{j&gt;i} ^ {I - 1} \beta_{ij} p_i^* p_j^* </span>
<span id="cb72-2072"><a href="#cb72-2072" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2073"><a href="#cb72-2073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2074"><a href="#cb72-2074" aria-hidden="true" tabindex="-1"></a>where $z^* = \ln (z / p_I)$  and the cost shares are:</span>
<span id="cb72-2075"><a href="#cb72-2075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2076"><a href="#cb72-2076" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2077"><a href="#cb72-2077" aria-hidden="true" tabindex="-1"></a>s_i = \beta_i + \sum_{j=1} ^ {I-1} \beta_{ij} p_j ^ *</span>
<span id="cb72-2078"><a href="#cb72-2078" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2079"><a href="#cb72-2079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2080"><a href="#cb72-2080" aria-hidden="true" tabindex="-1"></a>Consider the case where $I=3$. In this case, the complete system of</span>
<span id="cb72-2081"><a href="#cb72-2081" aria-hidden="true" tabindex="-1"></a>equations is:</span>
<span id="cb72-2082"><a href="#cb72-2082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2083"><a href="#cb72-2083" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2084"><a href="#cb72-2084" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-2085"><a href="#cb72-2085" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-2086"><a href="#cb72-2086" aria-hidden="true" tabindex="-1"></a>C^* &amp;=&amp; \alpha + \beta_y \ln y + \frac{1}{2}\beta_{yy} \ln^2 y + \beta_1 p_1 ^ * + \beta_2 p_2 ^ *</span>
<span id="cb72-2087"><a href="#cb72-2087" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\frac{1}{2} \beta_{11} {p_1^*} ^ 2 + \beta_{12} p_1^* p_2^* +</span>
<span id="cb72-2088"><a href="#cb72-2088" aria-hidden="true" tabindex="-1"></a>  \frac{1}{2} \beta_{22} {p_2^*} ^ 2 <span class="sc">\\</span></span>
<span id="cb72-2089"><a href="#cb72-2089" aria-hidden="true" tabindex="-1"></a>s_1 &amp;=&amp; \beta_1 + \beta_{11} p_1 ^ * + \beta_{12} p_2 ^ * <span class="sc">\\</span></span>
<span id="cb72-2090"><a href="#cb72-2090" aria-hidden="true" tabindex="-1"></a>s_2 &amp;=&amp; \beta_2 + \beta_{12} p_1 ^ * + \beta_{22} p_2 ^* <span class="sc">\\</span></span>
<span id="cb72-2091"><a href="#cb72-2091" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-2092"><a href="#cb72-2092" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-2093"><a href="#cb72-2093" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2094"><a href="#cb72-2094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2095"><a href="#cb72-2095" aria-hidden="true" tabindex="-1"></a>There are 14 parameters to estimate in total (8 in the cost function</span>
<span id="cb72-2096"><a href="#cb72-2096" aria-hidden="true" tabindex="-1"></a>and 3 in each of the cost share equations), but there are 6 linear</span>
<span id="cb72-2097"><a href="#cb72-2097" aria-hidden="true" tabindex="-1"></a>restrictions; for example, the coefficient of $p_1^*$ in the cost</span>
<span id="cb72-2098"><a href="#cb72-2098" aria-hidden="true" tabindex="-1"></a>equation should be equal to the intercept of the cost share for the</span>
<span id="cb72-2099"><a href="#cb72-2099" aria-hidden="true" tabindex="-1"></a>first factor. </span>
<span id="cb72-2100"><a href="#cb72-2100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2101"><a href="#cb72-2101" aria-hidden="true" tabindex="-1"></a>We estimate the translog cost function with the <span class="in">`apples`</span> data set \idxdata{apples}{micsr} of</span>
<span id="cb72-2102"><a href="#cb72-2102" aria-hidden="true" tabindex="-1"></a>@IVAL:LADO:OSSA:SIMI:96\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Ivaldi}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Ladoux}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Ossard}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Simioni} who studied the production cost of apple</span>
<span id="cb72-2103"><a href="#cb72-2103" aria-hidden="true" tabindex="-1"></a>producers. Farms in this sample produce apples and other fruits</span>
<span id="cb72-2104"><a href="#cb72-2104" aria-hidden="true" tabindex="-1"></a>(respectively <span class="in">`apples`</span> and <span class="in">`otherprod`</span>). The authors observe the sales</span>
<span id="cb72-2105"><a href="#cb72-2105" aria-hidden="true" tabindex="-1"></a>of apples and other fruits as well as the quantity of apple</span>
<span id="cb72-2106"><a href="#cb72-2106" aria-hidden="true" tabindex="-1"></a>produced. Therefore, they are able to compute the unit price of</span>
<span id="cb72-2107"><a href="#cb72-2107" aria-hidden="true" tabindex="-1"></a>apples. Both sales are divided by this unit price, so that <span class="in">`apples`</span> is</span>
<span id="cb72-2108"><a href="#cb72-2108" aria-hidden="true" tabindex="-1"></a>measured in apple quantity, and <span class="in">`otherprod`</span> is measured in "equivalent"</span>
<span id="cb72-2109"><a href="#cb72-2109" aria-hidden="true" tabindex="-1"></a>apple quantities. Therefore, they can be summed in order to have a</span>
<span id="cb72-2110"><a href="#cb72-2110" aria-hidden="true" tabindex="-1"></a>unique output variable <span class="in">`y`</span>. The expenses in the tree factors are given</span>
<span id="cb72-2111"><a href="#cb72-2111" aria-hidden="true" tabindex="-1"></a>by <span class="in">`capital`</span>, <span class="in">`labor`</span> and <span class="in">`materials`</span> and the corresponding unit</span>
<span id="cb72-2112"><a href="#cb72-2112" aria-hidden="true" tabindex="-1"></a>prices are <span class="in">`pc`</span>, <span class="in">`pl`</span> and <span class="in">`pm`</span>. The data set is an unbalanced panel of</span>
<span id="cb72-2113"><a href="#cb72-2113" aria-hidden="true" tabindex="-1"></a>173 farms observed for three years (1984, 1985 and 1986). We consider</span>
<span id="cb72-2114"><a href="#cb72-2114" aria-hidden="true" tabindex="-1"></a>only one year (1985) and, for a reason that will be clear later, we</span>
<span id="cb72-2115"><a href="#cb72-2115" aria-hidden="true" tabindex="-1"></a>divide all the variables by their sample mean:</span>
<span id="cb72-2116"><a href="#cb72-2116" aria-hidden="true" tabindex="-1"></a>\idxfun{filter}{dplyr}\idxfun{transmute}{dplyr}</span>
<span id="cb72-2117"><a href="#cb72-2117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2118"><a href="#cb72-2118" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2119"><a href="#cb72-2119" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: apples</span></span>
<span id="cb72-2120"><a href="#cb72-2120" aria-hidden="true" tabindex="-1"></a>ap <span class="ot">&lt;-</span> apples <span class="sc">%&gt;%</span> <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">1985</span>) <span class="sc">%&gt;%</span></span>
<span id="cb72-2121"><a href="#cb72-2121" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(<span class="at">y =</span> otherprod <span class="sc">+</span> apples,</span>
<span id="cb72-2122"><a href="#cb72-2122" aria-hidden="true" tabindex="-1"></a>              <span class="at">ct =</span> capital <span class="sc">+</span> labor <span class="sc">+</span> materials,</span>
<span id="cb72-2123"><a href="#cb72-2123" aria-hidden="true" tabindex="-1"></a>              <span class="at">sl =</span> labor <span class="sc">/</span> ct, <span class="at">sm =</span> materials <span class="sc">/</span> ct,</span>
<span id="cb72-2124"><a href="#cb72-2124" aria-hidden="true" tabindex="-1"></a>              <span class="at">pk =</span> <span class="fu">log</span>(pc <span class="sc">/</span> <span class="fu">mean</span>(pc)), <span class="at">pl =</span> <span class="fu">log</span>(pl <span class="sc">/</span> <span class="fu">mean</span>(pl)) <span class="sc">-</span> pk,</span>
<span id="cb72-2125"><a href="#cb72-2125" aria-hidden="true" tabindex="-1"></a>              <span class="at">pm =</span> <span class="fu">log</span>(pm <span class="sc">/</span> <span class="fu">mean</span>(pm)) <span class="sc">-</span> pk, <span class="at">ct =</span> <span class="fu">log</span>(ct <span class="sc">/</span> <span class="fu">mean</span>(ct)) <span class="sc">-</span> pk,</span>
<span id="cb72-2126"><a href="#cb72-2126" aria-hidden="true" tabindex="-1"></a>              <span class="at">y =</span> <span class="fu">log</span>(y <span class="sc">/</span> <span class="fu">mean</span>(y)), <span class="at">y2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> y <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb72-2127"><a href="#cb72-2127" aria-hidden="true" tabindex="-1"></a>              <span class="at">ct =</span> ct, <span class="at">pl2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> pl <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb72-2128"><a href="#cb72-2128" aria-hidden="true" tabindex="-1"></a>              <span class="at">pm2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> pm <span class="sc">^</span> <span class="dv">2</span>, <span class="at">plm =</span> pl <span class="sc">*</span> pm)</span>
<span id="cb72-2129"><a href="#cb72-2129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2130"><a href="#cb72-2130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2131"><a href="#cb72-2131" aria-hidden="true" tabindex="-1"></a>We then create three formulas corresponding to the system of three</span>
<span id="cb72-2132"><a href="#cb72-2132" aria-hidden="true" tabindex="-1"></a>equations (the cost function and the two factor shares):</span>
<span id="cb72-2133"><a href="#cb72-2133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2134"><a href="#cb72-2134" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2135"><a href="#cb72-2135" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: apples_equations</span></span>
<span id="cb72-2136"><a href="#cb72-2136" aria-hidden="true" tabindex="-1"></a>eq_ct <span class="ot">&lt;-</span> ct <span class="sc">~</span> y <span class="sc">+</span> y2 <span class="sc">+</span> pl <span class="sc">+</span> pm <span class="sc">+</span> pl2 <span class="sc">+</span> plm <span class="sc">+</span> pm2</span>
<span id="cb72-2137"><a href="#cb72-2137" aria-hidden="true" tabindex="-1"></a>eq_sl <span class="ot">&lt;-</span> sl <span class="sc">~</span> pl <span class="sc">+</span> pm</span>
<span id="cb72-2138"><a href="#cb72-2138" aria-hidden="true" tabindex="-1"></a>eq_sm <span class="ot">&lt;-</span> sm <span class="sc">~</span> pl <span class="sc">+</span> pm</span>
<span id="cb72-2139"><a href="#cb72-2139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2140"><a href="#cb72-2140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2141"><a href="#cb72-2141" aria-hidden="true" tabindex="-1"></a>These equations can be estimated one by one using OLS, but in this</span>
<span id="cb72-2142"><a href="#cb72-2142" aria-hidden="true" tabindex="-1"></a>case, the trans-equations restrictions are ignored. The whole system can also be</span>
<span id="cb72-2143"><a href="#cb72-2143" aria-hidden="true" tabindex="-1"></a>estimated directly by stacking the three vectors of responses and</span>
<span id="cb72-2144"><a href="#cb72-2144" aria-hidden="true" tabindex="-1"></a>constructing a block diagonal matrix of covariates, each block being</span>
<span id="cb72-2145"><a href="#cb72-2145" aria-hidden="true" tabindex="-1"></a>the relevant set of covariates for one equation. We use for this</span>
<span id="cb72-2146"><a href="#cb72-2146" aria-hidden="true" tabindex="-1"></a>purpose the **Formula** package <span class="co">[</span><span class="ot">@ZEIL:CROI:10</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Zeileis}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Croissant}. This package extends usual formulas in two directions: first, several covariates can be indicated on the left- hand side of the formula (using the <span class="in">`+`</span> operator), and several parts can be defined on both sides of the formula, using the <span class="in">`|`</span> sign. For example:</span>
<span id="cb72-2147"><a href="#cb72-2147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2150"><a href="#cb72-2150" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-2151"><a href="#cb72-2151" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb72-2152"><a href="#cb72-2152" aria-hidden="true" tabindex="-1"></a>y_1 <span class="sc">+</span> y_2 <span class="sc">|</span> y_3 <span class="sc">~</span> x_1 <span class="sc">+</span> x_2 <span class="sc">|</span> x_3 <span class="sc">|</span> x_4 <span class="sc">+</span> x_5</span>
<span id="cb72-2153"><a href="#cb72-2153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2154"><a href="#cb72-2154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2155"><a href="#cb72-2155" aria-hidden="true" tabindex="-1"></a>This formula has two sets of responses, the first containing $y_1$ and $y_2$, and the second $y_3$. Three sets of covariates are defined on the right-hand side of the formula. </span>
<span id="cb72-2156"><a href="#cb72-2156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2157"><a href="#cb72-2157" aria-hidden="true" tabindex="-1"></a>For our production analysis, we first create a "meta" formula</span>
<span id="cb72-2158"><a href="#cb72-2158" aria-hidden="true" tabindex="-1"></a>which contains the three responses on the left side and the whole set</span>
<span id="cb72-2159"><a href="#cb72-2159" aria-hidden="true" tabindex="-1"></a>of covariates on the right side. Then, we extract, using</span>
<span id="cb72-2160"><a href="#cb72-2160" aria-hidden="true" tabindex="-1"></a><span class="in">`model.matrix`</span>, the three model matrices for the three</span>
<span id="cb72-2161"><a href="#cb72-2161" aria-hidden="true" tabindex="-1"></a>equations (<span class="in">`Z_c`</span>, <span class="in">`Z_l`</span> and <span class="in">`Z_m`</span> respectively for the cost, labor</span>
<span id="cb72-2162"><a href="#cb72-2162" aria-hidden="true" tabindex="-1"></a>share and material share equations). The column names of these</span>
<span id="cb72-2163"><a href="#cb72-2163" aria-hidden="true" tabindex="-1"></a>matrices are customized using the <span class="in">`nms_cols`</span> function which, for</span>
<span id="cb72-2164"><a href="#cb72-2164" aria-hidden="true" tabindex="-1"></a>example, turns the original column names of <span class="in">`Z_l`</span> (<span class="in">`(Intercept)`</span>, <span class="in">`pl`</span></span>
<span id="cb72-2165"><a href="#cb72-2165" aria-hidden="true" tabindex="-1"></a>and <span class="in">`pm`</span>) to <span class="in">`sl_cst`</span>, <span class="in">`sl_pl`</span> and <span class="in">`sl_pm`</span>. We then construct the</span>
<span id="cb72-2166"><a href="#cb72-2166" aria-hidden="true" tabindex="-1"></a>block diagonal matrix (using the <span class="in">`Matrix::bdiag`</span> function) and use our</span>
<span id="cb72-2167"><a href="#cb72-2167" aria-hidden="true" tabindex="-1"></a>customized names:</span>
<span id="cb72-2168"><a href="#cb72-2168" aria-hidden="true" tabindex="-1"></a>\idxfun{Formula}{Formula}\idxfun{model.frame}{stats}\idxfun{model.matrix}{stats}\idxfun{paste}{base}\idxfun{bdiag}{Matrix}\idxfun{head}{utils}</span>
<span id="cb72-2169"><a href="#cb72-2169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2170"><a href="#cb72-2170" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2171"><a href="#cb72-2171" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bdiag_covariates</span></span>
<span id="cb72-2172"><a href="#cb72-2172" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Formula)</span>
<span id="cb72-2173"><a href="#cb72-2173" aria-hidden="true" tabindex="-1"></a>eq_sys <span class="ot">&lt;-</span> <span class="fu">Formula</span>(ct <span class="sc">+</span> sl <span class="sc">+</span> sm <span class="sc">~</span> y <span class="sc">+</span> y2 <span class="sc">+</span> pl <span class="sc">+</span> pm <span class="sc">+</span> pl2 <span class="sc">+</span> plm <span class="sc">+</span> pm2)</span>
<span id="cb72-2174"><a href="#cb72-2174" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(eq_sys, ap)  ; Z_c <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_ct, mf) </span>
<span id="cb72-2175"><a href="#cb72-2175" aria-hidden="true" tabindex="-1"></a>Z_l <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_sl, mf) ; Z_m <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_sm, mf)</span>
<span id="cb72-2176"><a href="#cb72-2176" aria-hidden="true" tabindex="-1"></a>nms_cols <span class="ot">&lt;-</span> <span class="cf">function</span>(x, label)</span>
<span id="cb72-2177"><a href="#cb72-2177" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(label, <span class="fu">c</span>(<span class="st">"cst"</span>, <span class="fu">colnames</span>(x)[<span class="sc">-</span><span class="dv">1</span>]), <span class="at">sep =</span> <span class="st">"_"</span>)</span>
<span id="cb72-2178"><a href="#cb72-2178" aria-hidden="true" tabindex="-1"></a>nms_c <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_c, <span class="st">"cost"</span>) ; nms_l <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_l, <span class="st">"sl"</span>)</span>
<span id="cb72-2179"><a href="#cb72-2179" aria-hidden="true" tabindex="-1"></a>nms_m <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_m, <span class="st">"sm"</span>)</span>
<span id="cb72-2180"><a href="#cb72-2180" aria-hidden="true" tabindex="-1"></a>Zs <span class="ot">&lt;-</span> Matrix<span class="sc">::</span><span class="fu">bdiag</span>(Z_c, Z_l, Z_m) <span class="sc">%&gt;%</span> as.matrix</span>
<span id="cb72-2181"><a href="#cb72-2181" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Zs) <span class="ot">&lt;-</span> <span class="fu">c</span>(nms_c, nms_l, nms_m)</span>
<span id="cb72-2182"><a href="#cb72-2182" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Zs, <span class="dv">2</span>)</span>
<span id="cb72-2183"><a href="#cb72-2183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2184"><a href="#cb72-2184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2185"><a href="#cb72-2185" aria-hidden="true" tabindex="-1"></a><span class="in">`Formula::model.part`</span> enables to retrieve any part of the model. Here, we want to extract the three responses which are on the only left side of the formula. Therefore, we set <span class="in">`lrs`</span> and <span class="in">`rhs`</span> respectively to 1 and 0. The result is a data frame with three variables <span class="in">`ct`</span>, <span class="in">`sl`</span> and <span class="in">`sm`</span>. We then use <span class="in">`dplyr::pivot_longer`</span>, to stack the three responses in one column. Note the use of the optional argument <span class="in">`cols_vary`</span> that is set to <span class="in">`"slowest"`</span>, so that the elements are stacked columnwise:</span>
<span id="cb72-2186"><a href="#cb72-2186" aria-hidden="true" tabindex="-1"></a>\idxfun{model.part}{Formula}\idxfun{pivot<span class="sc">\_</span>longer}{tidyr}</span>
<span id="cb72-2187"><a href="#cb72-2187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2188"><a href="#cb72-2188" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2189"><a href="#cb72-2189" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: stack_responses</span></span>
<span id="cb72-2190"><a href="#cb72-2190" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">model.part</span>(eq_sys, mf, <span class="at">rhs =</span> <span class="dv">0</span>, <span class="at">lhs =</span> <span class="dv">1</span>)</span>
<span id="cb72-2191"><a href="#cb72-2191" aria-hidden="true" tabindex="-1"></a>ys <span class="ot">&lt;-</span> Y <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">cols_vary =</span> <span class="st">"slowest"</span>, </span>
<span id="cb72-2192"><a href="#cb72-2192" aria-hidden="true" tabindex="-1"></a>                         <span class="at">names_to =</span> <span class="st">"equation"</span>, <span class="at">values_to =</span> <span class="st">"response"</span>)</span>
<span id="cb72-2193"><a href="#cb72-2193" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(ys, <span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-2194"><a href="#cb72-2194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2195"><a href="#cb72-2195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2196"><a href="#cb72-2196" aria-hidden="true" tabindex="-1"></a>The estimation can be performed using the response vector and the</span>
<span id="cb72-2197"><a href="#cb72-2197" aria-hidden="true" tabindex="-1"></a>matrix of covariates:</span>
<span id="cb72-2198"><a href="#cb72-2198" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}</span>
<span id="cb72-2199"><a href="#cb72-2199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2200"><a href="#cb72-2200" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2201"><a href="#cb72-2201" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb72-2202"><a href="#cb72-2202" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: system_estimation_matrix</span></span>
<span id="cb72-2203"><a href="#cb72-2203" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(ys<span class="sc">$</span>response <span class="sc">~</span> Zs <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb72-2204"><a href="#cb72-2204" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2205"><a href="#cb72-2205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2206"><a href="#cb72-2206" aria-hidden="true" tabindex="-1"></a>However, nicer input is obtained by constructing a tibble by binding</span>
<span id="cb72-2207"><a href="#cb72-2207" aria-hidden="true" tabindex="-1"></a>the columns of <span class="in">`ys`</span> and <span class="in">`Zs`</span> and then using the usual formula-data</span>
<span id="cb72-2208"><a href="#cb72-2208" aria-hidden="true" tabindex="-1"></a>interface. </span>
<span id="cb72-2209"><a href="#cb72-2209" aria-hidden="true" tabindex="-1"></a>\idxfun{bind<span class="sc">\_</span>cols}{dplyr}</span>
<span id="cb72-2210"><a href="#cb72-2210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2213"><a href="#cb72-2213" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-2214"><a href="#cb72-2214" aria-hidden="true" tabindex="-1"></a>stack_data <span class="ot">&lt;-</span> ys <span class="sc">%&gt;%</span> <span class="fu">bind_cols</span>(Zs)</span>
<span id="cb72-2215"><a href="#cb72-2215" aria-hidden="true" tabindex="-1"></a>stack_data <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-2216"><a href="#cb72-2216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2217"><a href="#cb72-2217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2218"><a href="#cb72-2218" aria-hidden="true" tabindex="-1"></a>In order to avoid having to write the whole long list of</span>
<span id="cb72-2219"><a href="#cb72-2219" aria-hidden="true" tabindex="-1"></a>covariates, the dot can be used on the right-hand side of the formula,</span>
<span id="cb72-2220"><a href="#cb72-2220" aria-hidden="true" tabindex="-1"></a>which means in this context all the variables (except the response</span>
<span id="cb72-2221"><a href="#cb72-2221" aria-hidden="true" tabindex="-1"></a>which is on the left-hand side of the formula). The intercept and the</span>
<span id="cb72-2222"><a href="#cb72-2222" aria-hidden="true" tabindex="-1"></a><span class="in">`equation`</span> variable should be omitted from the regression.</span>
<span id="cb72-2223"><a href="#cb72-2223" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-2224"><a href="#cb72-2224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2225"><a href="#cb72-2225" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2226"><a href="#cb72-2226" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: system_estimation_tibble</span></span>
<span id="cb72-2227"><a href="#cb72-2227" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-2228"><a href="#cb72-2228" aria-hidden="true" tabindex="-1"></a>ols_unconst <span class="ot">&lt;-</span> <span class="fu">lm</span>(response <span class="sc">~</span> . <span class="sc">-</span> <span class="dv">1</span> <span class="sc">-</span> equation, stack_data)</span>
<span id="cb72-2229"><a href="#cb72-2229" aria-hidden="true" tabindex="-1"></a>ols_unconst <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-2230"><a href="#cb72-2230" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2231"><a href="#cb72-2231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2232"><a href="#cb72-2232" aria-hidden="true" tabindex="-1"></a><span class="fu">### Constrained least squares {#sec-constrained_ls}</span></span>
<span id="cb72-2233"><a href="#cb72-2233" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{constrained least squares|(}</span>
<span id="cb72-2234"><a href="#cb72-2234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2235"><a href="#cb72-2235" aria-hidden="true" tabindex="-1"></a>Linear restrictions on the vector of coefficients to be estimated can be</span>
<span id="cb72-2236"><a href="#cb72-2236" aria-hidden="true" tabindex="-1"></a>represented using a matrix $R$ and a numeric vector $q$: $R\gamma = q$,^<span class="co">[</span><span class="ot">See @sec-wald_test_example.</span><span class="co">]</span> where $\gamma ^ \top = (\gamma_1 ^ \top, \ldots, \gamma_L ^ \top)$ is the</span>
<span id="cb72-2237"><a href="#cb72-2237" aria-hidden="true" tabindex="-1"></a>stacked vector of the coefficients for the whole system of equations. The OLS estimator is now the solution of a constrained optimization problem. Denoting $\lambda$ a vector of Lagrange multipliers,^<span class="co">[</span><span class="ot">These multipliers are multiplied by 2 in order to simplify the first-order conditions.</span><span class="co">]</span> and $\epsilon$ the errors, the objective function is:</span>
<span id="cb72-2238"><a href="#cb72-2238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2239"><a href="#cb72-2239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2240"><a href="#cb72-2240" aria-hidden="true" tabindex="-1"></a>L = \epsilon^\top \epsilon + 2\lambda^\top(R\gamma-q)</span>
<span id="cb72-2241"><a href="#cb72-2241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2242"><a href="#cb72-2242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2243"><a href="#cb72-2243" aria-hidden="true" tabindex="-1"></a>The Lagrangian can also be written as:</span>
<span id="cb72-2244"><a href="#cb72-2244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2245"><a href="#cb72-2245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2246"><a href="#cb72-2246" aria-hidden="true" tabindex="-1"></a>L = y^\top y - 2 \gamma^\top Z^\top y + \gamma^\top Z^\top Z \gamma +</span>
<span id="cb72-2247"><a href="#cb72-2247" aria-hidden="true" tabindex="-1"></a>2\lambda (R\gamma-q)</span>
<span id="cb72-2248"><a href="#cb72-2248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2249"><a href="#cb72-2249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2250"><a href="#cb72-2250" aria-hidden="true" tabindex="-1"></a>The first-order conditions are:</span>
<span id="cb72-2251"><a href="#cb72-2251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2252"><a href="#cb72-2252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2253"><a href="#cb72-2253" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-2254"><a href="#cb72-2254" aria-hidden="true" tabindex="-1"></a>  \begin{array}{rcl}</span>
<span id="cb72-2255"><a href="#cb72-2255" aria-hidden="true" tabindex="-1"></a>    \frac{\partial L}{\partial \gamma}&amp;=&amp;-2Z^\top y + 2 Z^\top Z \gamma + 2R^\top \lambda =0<span class="sc">\\</span></span>
<span id="cb72-2256"><a href="#cb72-2256" aria-hidden="true" tabindex="-1"></a>    \frac{\partial L}{\partial \lambda}&amp;=&amp;2(R\gamma-q)=0</span>
<span id="cb72-2257"><a href="#cb72-2257" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-2258"><a href="#cb72-2258" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-2259"><a href="#cb72-2259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2260"><a href="#cb72-2260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2261"><a href="#cb72-2261" aria-hidden="true" tabindex="-1"></a>which can also be written in matrix form as:</span>
<span id="cb72-2262"><a href="#cb72-2262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2263"><a href="#cb72-2263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2264"><a href="#cb72-2264" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2265"><a href="#cb72-2265" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb72-2266"><a href="#cb72-2266" aria-hidden="true" tabindex="-1"></a>  Z^\top Z &amp; R^\top <span class="sc">\\</span></span>
<span id="cb72-2267"><a href="#cb72-2267" aria-hidden="true" tabindex="-1"></a>  R &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-2268"><a href="#cb72-2268" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-2269"><a href="#cb72-2269" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2270"><a href="#cb72-2270" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2271"><a href="#cb72-2271" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-2272"><a href="#cb72-2272" aria-hidden="true" tabindex="-1"></a>  \gamma <span class="sc">\\</span> \lambda </span>
<span id="cb72-2273"><a href="#cb72-2273" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-2274"><a href="#cb72-2274" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2275"><a href="#cb72-2275" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-2276"><a href="#cb72-2276" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2277"><a href="#cb72-2277" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-2278"><a href="#cb72-2278" aria-hidden="true" tabindex="-1"></a>  Z^\top y<span class="sc">\\</span> q</span>
<span id="cb72-2279"><a href="#cb72-2279" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-2280"><a href="#cb72-2280" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2281"><a href="#cb72-2281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2282"><a href="#cb72-2282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2283"><a href="#cb72-2283" aria-hidden="true" tabindex="-1"></a>The constrained OLS estimator can be obtained using the formula for</span>
<span id="cb72-2284"><a href="#cb72-2284" aria-hidden="true" tabindex="-1"></a>the inverse of a partitioned matrix:^<span class="co">[</span><span class="ot">See @GREE:18\index[author]{Greene}, online appendix p. 1076.</span><span class="co">]</span></span>
<span id="cb72-2285"><a href="#cb72-2285" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{inverse of a partitioned matrix}</span>
<span id="cb72-2286"><a href="#cb72-2286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2287"><a href="#cb72-2287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2288"><a href="#cb72-2288" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2289"><a href="#cb72-2289" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cc}</span>
<span id="cb72-2290"><a href="#cb72-2290" aria-hidden="true" tabindex="-1"></a>    A_{11} &amp; A_{12} <span class="sc">\\</span></span>
<span id="cb72-2291"><a href="#cb72-2291" aria-hidden="true" tabindex="-1"></a>    A_{21} &amp; A_{22}</span>
<span id="cb72-2292"><a href="#cb72-2292" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-2293"><a href="#cb72-2293" aria-hidden="true" tabindex="-1"></a>\right)^{-1}</span>
<span id="cb72-2294"><a href="#cb72-2294" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-2295"><a href="#cb72-2295" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2296"><a href="#cb72-2296" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cc}</span>
<span id="cb72-2297"><a href="#cb72-2297" aria-hidden="true" tabindex="-1"></a>    B_{11} &amp; B_{12} <span class="sc">\\</span></span>
<span id="cb72-2298"><a href="#cb72-2298" aria-hidden="true" tabindex="-1"></a>    B_{21} &amp; B_{22}</span>
<span id="cb72-2299"><a href="#cb72-2299" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-2300"><a href="#cb72-2300" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2301"><a href="#cb72-2301" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-2302"><a href="#cb72-2302" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-2303"><a href="#cb72-2303" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cc}</span>
<span id="cb72-2304"><a href="#cb72-2304" aria-hidden="true" tabindex="-1"></a>    A_{11}^{-1}(I+A_{12}F_2A_{21}A_{11}^{-1}) &amp; - A_{11}^{-1}A_{12}F_2 <span class="sc">\\</span></span>
<span id="cb72-2305"><a href="#cb72-2305" aria-hidden="true" tabindex="-1"></a>    -F_2A_{21}A_{11}^{-1} &amp; F_2</span>
<span id="cb72-2306"><a href="#cb72-2306" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-2307"><a href="#cb72-2307" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-2308"><a href="#cb72-2308" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2309"><a href="#cb72-2309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2310"><a href="#cb72-2310" aria-hidden="true" tabindex="-1"></a>with $F_2=\left(A_{22}-A_{21}A_{11}^{-1}A_{12}\right)^{-1}$.</span>
<span id="cb72-2311"><a href="#cb72-2311" aria-hidden="true" tabindex="-1"></a>We have here $F_2=-\left(R(Z^\top Z)^{-1}R^\top\right)^{-1}$. The</span>
<span id="cb72-2312"><a href="#cb72-2312" aria-hidden="true" tabindex="-1"></a>constrained estimator is then: $\hat{\gamma}_c=B_{11}Z^\top y+ B_{12}q$,</span>
<span id="cb72-2313"><a href="#cb72-2313" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb72-2314"><a href="#cb72-2314" aria-hidden="true" tabindex="-1"></a>$B_{11} = (Z^\top Z)^{-1}\left(I-R^\top(R(Z^\top Z)^{-1}R^\top)^{-1}R(Z^\top Z)^{-1}\right)$</span>
<span id="cb72-2315"><a href="#cb72-2315" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb72-2316"><a href="#cb72-2316" aria-hidden="true" tabindex="-1"></a>$B_{12}=(Z^\top Z)^{-1}R^\top\left(R(Z^\top Z)^{-1}R^\top\right)^{-1}$</span>
<span id="cb72-2317"><a href="#cb72-2317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2318"><a href="#cb72-2318" aria-hidden="true" tabindex="-1"></a>The unconstrained estimator being</span>
<span id="cb72-2319"><a href="#cb72-2319" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_{nc}=\left(Z^\top Z\right)^{-1}Z^\top y$, we finally get:</span>
<span id="cb72-2320"><a href="#cb72-2320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2321"><a href="#cb72-2321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2322"><a href="#cb72-2322" aria-hidden="true" tabindex="-1"></a>\hat{\gamma}_c=\hat{\gamma}_{nc} - (Z^\top Z)^{-1}R^\top(R(Z^\top</span>
<span id="cb72-2323"><a href="#cb72-2323" aria-hidden="true" tabindex="-1"></a>Z)^{-1}R^\top)^{-1}(R\hat{\gamma}_{nc}-q)</span>
<span id="cb72-2324"><a href="#cb72-2324" aria-hidden="true" tabindex="-1"></a>$$ {#eq-const_lm}</span>
<span id="cb72-2325"><a href="#cb72-2325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2326"><a href="#cb72-2326" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{constrained least squares|)}</span>
<span id="cb72-2327"><a href="#cb72-2327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2328"><a href="#cb72-2328" aria-hidden="true" tabindex="-1"></a>The difference between the constrained and unconstrained</span>
<span id="cb72-2329"><a href="#cb72-2329" aria-hidden="true" tabindex="-1"></a>estimators is then a linear combination of the excess of the linear</span>
<span id="cb72-2330"><a href="#cb72-2330" aria-hidden="true" tabindex="-1"></a>constraints of the model evaluated for the unconstrained model.</span>
<span id="cb72-2331"><a href="#cb72-2331" aria-hidden="true" tabindex="-1"></a>For the system of cost and factor shares for apple production</span>
<span id="cb72-2332"><a href="#cb72-2332" aria-hidden="true" tabindex="-1"></a>previously described, we have 14 coefficients and 6 restrictions:</span>
<span id="cb72-2333"><a href="#cb72-2333" aria-hidden="true" tabindex="-1"></a>\idxfun{matrix}{base}</span>
<span id="cb72-2334"><a href="#cb72-2334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2335"><a href="#cb72-2335" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2336"><a href="#cb72-2336" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: restrict_matrix_apples</span></span>
<span id="cb72-2337"><a href="#cb72-2337" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">14</span>)</span>
<span id="cb72-2338"><a href="#cb72-2338" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">1</span>, <span class="fu">c</span>(<span class="dv">4</span>,  <span class="dv">9</span>)] <span class="ot">&lt;-</span> R[<span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">12</span>)] <span class="ot">&lt;-</span> R[<span class="dv">3</span>, <span class="fu">c</span>(<span class="dv">6</span>, <span class="dv">10</span>)] <span class="ot">&lt;-</span> </span>
<span id="cb72-2339"><a href="#cb72-2339" aria-hidden="true" tabindex="-1"></a>  R[<span class="dv">4</span>, <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">11</span>)] <span class="ot">&lt;-</span> R[<span class="dv">5</span>, <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">13</span>)] <span class="ot">&lt;-</span> </span>
<span id="cb72-2340"><a href="#cb72-2340" aria-hidden="true" tabindex="-1"></a>  R[<span class="dv">6</span>, <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">14</span>)] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb72-2341"><a href="#cb72-2341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2342"><a href="#cb72-2342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2343"><a href="#cb72-2343" aria-hidden="true" tabindex="-1"></a>For example, the first line of <span class="in">`R`</span> returns the difference between the fourth coefficient (<span class="in">`cost_pl`</span>) and the ninth coefficient (<span class="in">`sl_cst`</span>). As the vector $q$ is 0 in our example, this means that the coefficient of $p_l^*$ in the cost equation should equal the intercept in the labor share equation.</span>
<span id="cb72-2344"><a href="#cb72-2344" aria-hidden="true" tabindex="-1"></a>Applying @eq-const_lm, we get:</span>
<span id="cb72-2345"><a href="#cb72-2345" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{crossprod}{base}\idxfun{solve}{base}\idxfun{drop}{base}\idxfun{model.matrix}{stats}</span>
<span id="cb72-2346"><a href="#cb72-2346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2347"><a href="#cb72-2347" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2348"><a href="#cb72-2348" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: constrained_ls_manual</span></span>
<span id="cb72-2349"><a href="#cb72-2349" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-2350"><a href="#cb72-2350" aria-hidden="true" tabindex="-1"></a>excess <span class="ot">&lt;-</span> R <span class="sc">%*%</span> <span class="fu">coef</span>(ols_unconst)</span>
<span id="cb72-2351"><a href="#cb72-2351" aria-hidden="true" tabindex="-1"></a>XpX <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">model.matrix</span>(ols_unconst))</span>
<span id="cb72-2352"><a href="#cb72-2352" aria-hidden="true" tabindex="-1"></a>beta_c <span class="ot">&lt;-</span> <span class="fu">coef</span>(ols_unconst) <span class="sc">-</span></span>
<span id="cb72-2353"><a href="#cb72-2353" aria-hidden="true" tabindex="-1"></a>    <span class="fu">drop</span>(<span class="fu">solve</span>(XpX) <span class="sc">%*%</span> <span class="fu">t</span>(R) <span class="sc">%*%</span></span>
<span id="cb72-2354"><a href="#cb72-2354" aria-hidden="true" tabindex="-1"></a>         <span class="fu">solve</span>(R <span class="sc">%*%</span> <span class="fu">solve</span>(XpX) <span class="sc">%*%</span> <span class="fu">t</span>(R)) <span class="sc">%*%</span> excess)</span>
<span id="cb72-2355"><a href="#cb72-2355" aria-hidden="true" tabindex="-1"></a>beta_c</span>
<span id="cb72-2356"><a href="#cb72-2356" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2357"><a href="#cb72-2357" aria-hidden="true" tabindex="-1"></a>More simply, <span class="in">`micsr::clm`</span> \idxfun{clm}{micsr} can be used, which computes the constrained</span>
<span id="cb72-2358"><a href="#cb72-2358" aria-hidden="true" tabindex="-1"></a>least squares estimator with, as arguments, a <span class="in">`lm`</span> object (the</span>
<span id="cb72-2359"><a href="#cb72-2359" aria-hidden="true" tabindex="-1"></a>unconstrained model) and <span class="in">`R`</span>, the matrix of restrictions (and</span>
<span id="cb72-2360"><a href="#cb72-2360" aria-hidden="true" tabindex="-1"></a>optionally a <span class="in">`q`</span> vector):</span>
<span id="cb72-2361"><a href="#cb72-2361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2362"><a href="#cb72-2362" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2363"><a href="#cb72-2363" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: constrained_ls_clm</span></span>
<span id="cb72-2364"><a href="#cb72-2364" aria-hidden="true" tabindex="-1"></a>ols_const <span class="ot">&lt;-</span> <span class="fu">clm</span>(ols_unconst, R)</span>
<span id="cb72-2365"><a href="#cb72-2365" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2366"><a href="#cb72-2366" aria-hidden="true" tabindex="-1"></a>and returns a <span class="in">`lm`</span> object.</span>
<span id="cb72-2367"><a href="#cb72-2367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2368"><a href="#cb72-2368" aria-hidden="true" tabindex="-1"></a>Finally, the **systemfit** package <span class="co">[</span><span class="ot">@HENN:HAMA:07</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Henningsen}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Hamann} is devoted to system</span>
<span id="cb72-2369"><a href="#cb72-2369" aria-hidden="true" tabindex="-1"></a>estimation and provides a <span class="in">`systemfit`</span>\idxfun{systemfit}{systemfit} function. Its main arguments are a list of equations and a data</span>
<span id="cb72-2370"><a href="#cb72-2370" aria-hidden="true" tabindex="-1"></a>frame, but it also has <span class="in">`restrict.matrix`</span> and <span class="in">`restrict.rhs`</span></span>
<span id="cb72-2371"><a href="#cb72-2371" aria-hidden="true" tabindex="-1"></a>arguments to provide respectively $R$ and $q$.</span>
<span id="cb72-2372"><a href="#cb72-2372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2373"><a href="#cb72-2373" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2374"><a href="#cb72-2374" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: hide</span></span>
<span id="cb72-2375"><a href="#cb72-2375" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: system_est_systemfit</span></span>
<span id="cb72-2376"><a href="#cb72-2376" aria-hidden="true" tabindex="-1"></a>systemfit<span class="sc">::</span><span class="fu">systemfit</span>(<span class="fu">list</span>(<span class="at">cost =</span> eq_ct, <span class="at">labor =</span> eq_sl, </span>
<span id="cb72-2377"><a href="#cb72-2377" aria-hidden="true" tabindex="-1"></a>                          <span class="at">materials =</span> eq_sm),</span>
<span id="cb72-2378"><a href="#cb72-2378" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> ap, <span class="at">restrict.matrix =</span> R)</span>
<span id="cb72-2379"><a href="#cb72-2379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2380"><a href="#cb72-2380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2381"><a href="#cb72-2381" aria-hidden="true" tabindex="-1"></a>The output of <span class="in">`systemfit`</span> is large and is not reproduced here. The full strength of this package will be presented in later chapters, while describing the seemingly unrelated regression (@sec-sur) and the three-stage least squares (@sec-three_sls) estimators.</span>
<span id="cb72-2382"><a href="#cb72-2382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2383"><a href="#cb72-2383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2384"><a href="#cb72-2384" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Once the constrained least squares estimator has been computed, one can test the validity of the constraints, using @eq-wald_test_formula: --&gt;</span></span>
<span id="cb72-2385"><a href="#cb72-2385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2386"><a href="#cb72-2386" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r} --&gt;</span></span>
<span id="cb72-2387"><a href="#cb72-2387" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- R %*% vcov(ols_const) %*% t(R) --&gt;</span></span>
<span id="cb72-2388"><a href="#cb72-2388" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb72-2389"><a href="#cb72-2389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2390"><a href="#cb72-2390" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation|)}</span>
<span id="cb72-2391"><a href="#cb72-2391" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>