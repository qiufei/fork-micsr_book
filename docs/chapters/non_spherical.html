<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Microeconometrics with R - 6&nbsp; Non-spherical disturbances</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/endogeneity.html" rel="next">
<link href="../chapters/maximum_likelihood.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Microeconometrics with R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../OLS.html" class="sidebar-item-text sidebar-link">Ordinary least squares estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/multiple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coefficients.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../beyond_OLS.html" class="sidebar-item-text sidebar-link">Beyond the OLS estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/maximum_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/non_spherical.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/endogeneity.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/treateff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/spatial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Spatial econometrics</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../special_responses.html" class="sidebar-item-text sidebar-link">Special responses</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/binomial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Binomial models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/tobit.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Censored and truncated models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/count.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Count data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/duration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Duration models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discrete choice models</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-situations_non_spher" id="toc-sec-situations_non_spher" class="nav-link active" data-scroll-target="#sec-situations_non_spher"><span class="toc-section-number">6.1</span>  Situations where the errors are non-spherical</a>
  <ul class="collapse">
<li><a href="#heteroskedasticity" id="toc-heteroskedasticity" class="nav-link" data-scroll-target="#heteroskedasticity">Heteroskedasticity</a></li>
  <li><a href="#sec-error_component" id="toc-sec-error_component" class="nav-link" data-scroll-target="#sec-error_component">Correlation of the errors</a></li>
  <li><a href="#system-of-equations" id="toc-system-of-equations" class="nav-link" data-scroll-target="#system-of-equations">System of equations</a></li>
  </ul>
</li>
  <li>
<a href="#sec-test_non_spher" id="toc-sec-test_non_spher" class="nav-link" data-scroll-target="#sec-test_non_spher"><span class="toc-section-number">6.2</span>  Testing for non-spherical disturbances</a>
  <ul class="collapse">
<li><a href="#testing-for-heteroskedasticity" id="toc-testing-for-heteroskedasticity" class="nav-link" data-scroll-target="#testing-for-heteroskedasticity">Testing for heteroskedasticity</a></li>
  <li><a href="#testing-for-individual-effects" id="toc-testing-for-individual-effects" class="nav-link" data-scroll-target="#testing-for-individual-effects">Testing for individual effects</a></li>
  <li><a href="#sec-bptest_system" id="toc-sec-bptest_system" class="nav-link" data-scroll-target="#sec-bptest_system">System of equations</a></li>
  </ul>
</li>
  <li>
<a href="#sec-sandwich" id="toc-sec-sandwich" class="nav-link" data-scroll-target="#sec-sandwich"><span class="toc-section-number">6.3</span>  Robust inference</a>
  <ul class="collapse">
<li><a href="#simple-linear-model" id="toc-simple-linear-model" class="nav-link" data-scroll-target="#simple-linear-model">Simple linear model</a></li>
  <li><a href="#multiple-linear-model" id="toc-multiple-linear-model" class="nav-link" data-scroll-target="#multiple-linear-model">Multiple linear model</a></li>
  </ul>
</li>
  <li>
<a href="#sec-gls" id="toc-sec-gls" class="nav-link" data-scroll-target="#sec-gls"><span class="toc-section-number">6.4</span>  Generalized least squares estimator</a>
  <ul class="collapse">
<li><a href="#general-formulation-of-the-gls-estimator" id="toc-general-formulation-of-the-gls-estimator" class="nav-link" data-scroll-target="#general-formulation-of-the-gls-estimator">General formulation of the GLS estimator</a></li>
  <li><a href="#weighted-least-squares" id="toc-weighted-least-squares" class="nav-link" data-scroll-target="#weighted-least-squares">Weighted least squares</a></li>
  <li><a href="#sec-error_component_gls" id="toc-sec-error_component_gls" class="nav-link" data-scroll-target="#sec-error_component_gls">Error component model</a></li>
  <li><a href="#sec-sur" id="toc-sec-sur" class="nav-link" data-scroll-target="#sec-sur">Seemingly unrelated regression</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-non_spherical" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>In the first part of the book, we considered a model of the form: <span class="math inline">\(y_n = \alpha + \beta x_n + \epsilon_n\)</span> with the hypothesis that the errors were homoskedastic: <span class="math inline">\(\mbox{V}(\epsilon_n) = \sigma_\epsilon ^ 2\)</span> and uncorrelated: <span class="math inline">\(\mbox{E}(\epsilon_n \epsilon_m) = 0 \; \forall n \neq m\)</span>. In this case, the errors (or disturbances) are <strong>spherical</strong> and the covariance matrix of the errors <span class="math inline">\(\Omega\)</span> is, up to a multiplicative constant <span class="math inline">\(\sigma_\epsilon^2\)</span>, the identity matrix:</p>
<p><span class="math display">\[
\Omega = \mbox{V}(\epsilon) = \mbox{E}(\epsilon\epsilon^\top) = \sigma_\epsilon ^ 2 I
\]</span></p>
<p>In this chapter, we analyze cases where these hypothesis are violated. This has the following consequences concerning the results established in the first part of the book:</p>
<ul>
<li>the OLS estimator is still consistent: this means that <span class="math inline">\(\hat{\beta}\)</span> estimates consistently <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\hat{\epsilon}\)</span> estimates also consistently <span class="math inline">\(\epsilon\)</span>; this is an important result, as the residuals of the OLS estimator can therefore be used to test whether the errors are spherical or not,</li>
<li>the OLS estimator is no longer BLUE, i.e., it is no longer the best linear unbiased estimator,</li>
<li>there is another linear unbiased estimator (the <strong>generalized least squares</strong>, or <strong>GLS</strong>) which is more efficient (which means that it has a smaller variance) than the OLS estimator and which is the BLUE estimator when the errors are non-spherical,</li>
<li>the simple formula for the variance of the OLS estimator, <span class="math inline">\(\mbox{V}(\hat{\beta}) = \sigma_\epsilon ^ 2 \left(\tilde{X}^\top \tilde{X}\right)^{-1}\)</span> is no longer an unbiased estimator of the true covariance matrix of the OLS estimator. However, as we’ll see in this chapter, <strong>sandwich</strong> estimators, which are consistent, can be used.</li>
</ul>
<p><a href="#sec-situations_non_spher"><span>Section&nbsp;6.1</span></a> reviews some important cases where the errors are non-spherical. <a href="#sec-test_non_spher"><span>Section&nbsp;6.2</span></a> presents tests that enable to detect whether the errors are spherical or not. <a href="#sec-sandwich"><span>Section&nbsp;6.3</span></a> presents robust estimators of the variance of the OLS estimators. Finally, <a href="#sec-gls"><span>Section&nbsp;6.4</span></a> is devoted to the GLS estimator.</p>
<section id="sec-situations_non_spher" class="level2" data-number="6.1"><h2 data-number="6.1" class="anchored" data-anchor-id="sec-situations_non_spher">
<span class="header-section-number">6.1</span> Situations where the errors are non-spherical</h2>
<p>As stated previously, the hypothesis of spherical disturbances implies that the errors are homoskedastic and uncorrelated. We’ll describe in the next three subsections important situations where this hypothesis is violated. In each case, we’ll establish the expression of the matrix of covariance of the errors <span class="math inline">\(\Omega\)</span> and, for a reason that will be clear in the subsequent sections, we’ll also compute the inverse of this matrix.</p>
<section id="heteroskedasticity" class="level3"><h3 class="anchored" data-anchor-id="heteroskedasticity">Heteroskedasticity</h3>
<p> In a linear model: <span class="math inline">\(y_n = \alpha + \beta x_n + \epsilon_n\)</span>, heteroskedasticity occurs when the conditional variance of the response <span class="math inline">\(y_n\)</span> (the variance of <span class="math inline">\(\epsilon_n\)</span>) is not a constant. As an example, <span class="citation" data-cites="HOUT:51">Houthakker (<a href="#ref-HOUT:51" role="doc-biblioref">1951</a>)</span> analyzed electric consumption in the United Kingdom, and his data set (called <code>uk_elec</code>) is a sample of 42 British cities.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The response is the per capita consumption in kilowatt hours. Denoting <span class="math inline">\(c_{ni}\)</span> the consumption of an individual <span class="math inline">\(i\)</span> in city <span class="math inline">\(n\)</span>, the response is then <span class="math inline">\(y_n =\frac{1}{I_n}\sum_i^{I_n} c_{ni}\)</span> where <span class="math inline">\(I_n\)</span> is the total number of consumers in city <span class="math inline">\(n\)</span>. Then, if the standard deviation of the individual consumption is <span class="math inline">\(\sigma_c\)</span> (the same in every city), the variance of <span class="math inline">\(y_n\)</span> is <span class="math inline">\(\sigma_c ^ 2 / I_n\)</span> and therefore depends on the number of consumer units of the city. Even if covariates are taken into account, it is doubtful that the conditional variance of <span class="math inline">\(y\)</span> will be the same for every city, and a more reasonable hypothesis is that, as the unconditional variance, it is inversely proportional to the number of consumer units. With heteroskedastic but uncorrelated errors, the matrix of covariance of the errors is a diagonal matrix with non-constant diagonal terms:</p>
<p><span id="eq-matheterosc"><span class="math display">\[
\Omega=
\left(
\begin{array}{cccc}
\sigma_{1} ^ 2 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \sigma_{2} ^ 2 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; \sigma_N ^ 2
\end{array}
\right)
\tag{6.1}\]</span></span></p>
<p>The inverse of <span class="math inline">\(\Omega\)</span> is easily obtained:</p>
<p><span class="math display">\[
\Omega ^ {-1}=
\left(
\begin{array}{cccc}
1 / \sigma_{1} ^ 2 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 / \sigma_{2} ^ 2 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; 1/ \sigma_N ^ 2
\end{array}
\right)
\]</span></p>
<p>and, more generally, <span class="math inline">\(\Omega^{r}\)</span> (<span class="math inline">\(r\)</span> being any integer or rational number) is a diagonal matrix with typical element <span class="math inline">\(\left(\sigma_n^2\right)^r\)</span>. </p>
</section><section id="sec-error_component" class="level3"><h3 class="anchored" data-anchor-id="sec-error_component">Correlation of the errors</h3>
<p></p>
<p>Consider now the case where we have several observations from the same <strong>entity</strong>. An example is the case where the unit of observation is the individual, but siblings are observed. In this case, each observation is doubly indexed, the first index being the family and the second one the rank of the sibling’s birth. Another very important case is when the same individual (in a wide sense, it can be a household, firm, country, etc.) is observed several times, for example for different periods like years or months. Such a data set is called a <strong>panel data</strong>. In the subsequent sections, we’ll use two data sets. The first data set, called <code>twins</code>, is from <span class="citation" data-cites="BONJ:CHERK:KASK:03">Bonjour et al. (<a href="#ref-BONJ:CHERK:KASK:03" role="doc-biblioref">2003</a>)</span> who studied the return to education with a sample of twins. The sample contains 428 observations (214 pairs of twins), and it is reasonable to assume that, for a given pair of twins, the two errors are correlated as they partly contain unobserved characteristics that are common to both twins. The second data set, called <code>tobinq</code>, is from <span class="citation" data-cites="SCHA:90">Schaller (<a href="#ref-SCHA:90" role="doc-biblioref">1990</a>)</span> who tests the relevance of Tobins’ Q theory of investment by regressing the investment rate (the ratio of the investment and the stock of capital) to Tobin’s Q, which is the ratio of the value of the firm and the stock of capital. The data set is a panel of 188 firms observed for 35 years (from 1951 to 1985).</p>
<p>For such data, we’ll denote <span class="math inline">\(n = 1, 2, \ldots N\)</span> the entity / individual index and <span class="math inline">\(t = 1, 2, \ldots T\)</span> the index of the observation in the entity / the time period. Then, the simple linear model can be written: <span class="math inline">\(y_{nt} = \alpha + \beta x_{nt} + \epsilon_{nt}\)</span> and it is useful to write the error term as the sum of two components:</p>
<ul>
<li>an entity / individual effect <span class="math inline">\(\eta_n\)</span>,</li>
<li>an idiosyncratic effect <span class="math inline">\(\nu_{nt}\)</span>.</li>
</ul>
<p>We therefore have <span class="math inline">\(\epsilon_{nt} = \eta_n + \nu_{nt}\)</span>. This leads to the so-called <strong>error-component</strong> model, which can be easily analyzed with the following hypothesis:</p>
<ul>
<li>the two components are homoskedastic and uncorrelated: <span class="math inline">\(\mbox{V}(\eta_n) = \sigma_{\eta} ^ 2, \forall n\)</span>, <span class="math inline">\(\mbox{V}(\nu_{nt}) = \sigma_{\nu} ^ 2, \forall n, t\)</span> and <span class="math inline">\(\mbox{cov}(\eta_n, \nu_{nt}) = 0, \forall n, t\)</span>,</li>
<li>the idiosyncratic terms for the same entity are uncorrelated: <span class="math inline">\(\mbox{E}(\nu_{nt}\nu_{ns})=0\; \forall \; n, t \neq s\)</span>,</li>
<li>the two components of the errors are uncorrelated for two observations of different entities <span class="math inline">\(\mbox{cov}(\nu_{nt}, \nu_{ms})=\mbox{cov}(\eta_n, \eta_m)=0\; \forall \; n \neq m, t, s\)</span>.</li>
</ul>
<p>With these hypotheses, we have:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\mbox{E}(\epsilon_{nt}^2) &amp;=&amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 \\
\mbox{E}(\epsilon_{nt}\epsilon_{mt}) &amp;=&amp; \sigma_\eta ^ 2 \\
\mbox{E}(\epsilon_{nt}\epsilon_{ms}) &amp;=&amp; 0;\ \forall \; n \neq m\\
\end{array}
\right.
\]</span></p>
<p>and <span class="math inline">\(\Omega\)</span> is a block-diagonal matrix with identical blocks. For example, with <span class="math inline">\(N = 2\)</span> and <span class="math inline">\(T = 3\)</span>:</p>
<p><span class="math display">\[
\Omega =
\left(
\begin{array}{cccccc}
\sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2 &amp; \sigma_\eta ^ 2 &amp; 0 &amp; 0 &amp; 0 \\
\sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2 &amp; 0 &amp; 0 &amp; 0 \\
\sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2 &amp; \sigma_\eta ^ 2 \\
0 &amp; 0 &amp; 0 &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2  \\
0 &amp; 0 &amp; 0 &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2\\
\end{array}
\right)
\]</span></p>
<p>The blocks of this matrix can be written as, denoting <span class="math inline">\(j\)</span> a vector of ones and <span class="math inline">\(J=jj^\top\)</span> a square matrix of 1:</p>
<p><span class="math display">\[
\sigma^2_\nu I_T + \sigma ^ 2_\eta J_T
\]</span></p>
<p>and, using Kronecker product, <span class="math inline">\(\Omega\)</span> is:</p>
<p><span class="math display">\[
\Omega = I_N \otimes \left(\sigma^2_\nu I_T + \sigma ^ 2_\eta J_T\right) = \sigma ^ 2_\nu I_{NT} + \sigma_\eta ^ 2 I_N \otimes J_T
\]</span></p>
<p>Another equivalent expression which will prove to be particularly useful is:</p>
<p><span id="eq-omega_panel"><span class="math display">\[
\Omega = \sigma ^ 2_\nu \left(I_N - I_N \otimes J_T / T\right)+ (T \sigma_\eta ^ 2 + \sigma ^ 2_\nu) \left(I_N \otimes J_T / T\right) = \sigma ^ 2_\nu W + \sigma_\iota^2 B
\tag{6.2}\]</span></span></p>
<p>where <span class="math inline">\(\sigma_\iota ^ 2 = T \sigma_\eta ^ 2 + \sigma ^ 2_\nu\)</span>. With our <span class="math inline">\(N=2\)</span> and <span class="math inline">\(T=3\)</span> simple case, the two matrices are:</p>
<p><span class="math display">\[
W =
\left(
\begin{array}{cccccc}
2/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 \\
- 1/3 &amp; 2/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 \\
- 1/3 &amp; - 1/3 &amp; 2/3 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2/3 &amp; - 1/3 &amp; - 1/3 \\
0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; 2/3 &amp; - 1/3 \\
0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; 2/3 \\
\end{array}
\right)
\]</span></p>
<p>and:</p>
<p><span class="math display">\[
B =
\left(
\begin{array}{cccccc}
- 1/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 \\
- 1/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 \\
- 1/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; - 1/3 \\
0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; - 1/3 \\
0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; - 1/3 \\
\end{array}
\right)
\]</span></p>
<p>The first matrix is called the <strong>within</strong> matrix. Premultiplying a vector by <span class="math inline">\(W\)</span> transforms it as deviations from the individual means. The second matrix is called the <strong>between</strong> matrix, and premultiplying a vector by <span class="math inline">\(B\)</span> transforms it as a vector of individual means. These two symmetric matrices have interesting properties: </p>
<ul>
<li>they are <strong>idempotent</strong>, which means that <span class="math inline">\(B B = B\)</span> and <span class="math inline">\(W W =W\)</span>. For example <span class="math inline">\(W (Wz) = Wz\)</span>, as taking the deviations from the individual means of a vector of deviations from the individual means leaves this vector unchanged,</li>
<li>they are <strong>orthogonal</strong>, which means that <span class="math inline">\(W B = B W = 0\)</span>. For example <span class="math inline">\(W (Bz) = 0\)</span> because the deviations from the individual means of a vector of individual means are zero,</li>
<li>they sum to the <strong>identity matrix</strong>, <span class="math inline">\(W + B = I\)</span>. <span class="math inline">\(Wz + Bz=z\)</span>, because the sum of the deviations from the individual means of a vector and its individual means is the vector itself.</li>
</ul>
<p><span class="math inline">\(W\)</span> and <span class="math inline">\(B\)</span> therefore perform an <strong>orthogonal decomposition</strong> of a vector. One advantage of this decomposition is that it is very easy to obtain powers of <span class="math inline">\(\Omega\)</span>. For example, the inverse of <span class="math inline">\(\Omega\)</span> is:</p>
<p><span class="math display">\[
\Omega ^ {-1} = \frac{1}{\sigma_\nu ^ 2} W + \frac{1}{\sigma_\iota ^ 2}B
\]</span></p>
<p>and, more generally, for any power <span class="math inline">\(r\)</span> (either an integer or a rational):</p>
<p><span id="eq-poweromegaec"><span class="math display">\[
\Omega ^ {r} = {\sigma_\nu ^ 2} ^ r W + {\sigma_\iota ^ 2} ^ rB
\tag{6.3}\]</span></span> </p>
</section><section id="system-of-equations" class="level3"><h3 class="anchored" data-anchor-id="system-of-equations">System of equations</h3>
<p> We have seen in <a href="multiple_regression.html#sec-system_equation"><span>Section&nbsp;3.7</span></a> that, for example in fields such as consumption or production analysis, it is more relevant to consider the estimation of system of equations instead of the estimation of a single equation. In matrix form, the model corresponding to the whole system was presented in <a href="multiple_regression.html#eq-system_equation">Equation&nbsp;<span>3.26</span></a>. We’ve seen in <a href="multiple_regression.html#sec-system_equation"><span>Section&nbsp;3.7</span></a> that a first advantage of considering the whole system of equations, and not an equation in isolation, is that restrictions on coefficients that concern different equations can be taken into account using the constrained least squares estimator.</p>
<p>A second advantage is that, if the errors of the different equations for the same observation are correlated, these correlations can be taken into account if the whole system of equations is considered. Denoting <span class="math inline">\(\epsilon_l\)</span> the vector of length <span class="math inline">\(N\)</span> containing the errors for the <span class="math inline">\(l\)</span><sup>th</sup> equation and <span class="math inline">\(\Xi = (\epsilon_1, \epsilon_2, \ldots \epsilon_L)\)</span> the <span class="math inline">\(N\times L\)</span> matrix containing errors for the whole system, the covariance matrix of the errors for the system is:</p>
<p><span class="math display">\[
\Omega =
\mbox{E}(\Xi \Xi^\top)
=\mbox{E}
\left(
  \begin{array}{cccc}
    \epsilon_1\epsilon_1^\top &amp; \epsilon_1 \epsilon_2^\top &amp; \ldots &amp; \epsilon_1 \epsilon_L^\top \\
    \epsilon_2\epsilon_1^\top &amp; \epsilon_2 \epsilon_2^\top &amp; \ldots &amp; \epsilon_2 \epsilon_L^\top \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \epsilon_L\epsilon_1^\top &amp; \epsilon_L \epsilon_2^\top &amp; \ldots &amp; \epsilon_L \epsilon_L^\top \\
  \end{array}
\right)
\]</span> Assume that the errors of two equations <span class="math inline">\(l\)</span> and <span class="math inline">\(m\)</span> for the same observation are correlated and that the covariance, denoted <span class="math inline">\(\sigma_{lm}\)</span>, is constant. The variance of errors for each equation <span class="math inline">\(l\)</span> is denoted <span class="math inline">\(\sigma_{ll}\)</span> and may be different from one equation to another. Moreover, we assume that errors for different individuals are uncorrelated. With these hypotheses, the covariance matrix is, denoting <span class="math inline">\(I\)</span> the identity matrix of dimension <span class="math inline">\(N\)</span>:</p>
<p><span class="math display">\[
\Omega=
\left(
\begin{array}{cccc}
  \sigma_1^2 I &amp; \sigma_{12} I &amp; \ldots &amp;\sigma_{1L} I \\
  \sigma_{12} I &amp; \sigma_2^2 I &amp; \ldots &amp;\sigma_{2L} I \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \sigma_{1L} I &amp; \sigma_{2L} I &amp; \ldots &amp; \sigma_L^2 I
  \end{array}
\right)
\]</span></p>
<p>Denoting <span class="math inline">\(\Sigma\)</span> the <span class="math inline">\(L\times L\)</span> matrix of inter-equation variances and covariances, we have:</p>
<p><span class="math display">\[
\Sigma=
\left(
  \begin{array}{cccc}
  \sigma_1^2 &amp; \sigma_{12} &amp; \ldots &amp;\sigma_{1L} \\
  \sigma_{12} &amp; \sigma_2^2 &amp; \ldots &amp;\sigma_{2L} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \sigma_{1L} &amp; \sigma_{2L} &amp; \ldots &amp; \sigma_L^2
  \end{array}
\right)
\]</span></p>
<p>and <span class="math inline">\(\Omega=\Sigma \otimes I\)</span>. The inverse of the covariance matrix of the errors is easily obtained, as it requires only to compute the inverse of <span class="math inline">\(\Sigma\)</span>: <span class="math inline">\(\Omega ^ {-1} = \Sigma ^ {-1} \otimes I\)</span>.</p>
</section></section><section id="sec-test_non_spher" class="level2" data-number="6.2"><h2 data-number="6.2" class="anchored" data-anchor-id="sec-test_non_spher">
<span class="header-section-number">6.2</span> Testing for non-spherical disturbances</h2>
<p>Numerous tests have been proposed to investigate whether, in different contexts, the disturbances are spherical or not. Among them, we’ll present a family of tests that are based on OLS residuals. Even if the disturbances are non-spherical, OLS is a consistent estimator and therefore OLS’s residuals are a consistent estimate of the errors of the model. Therefore, one can use these residuals to analyze the unknown features of the errors.</p>
<section id="testing-for-heteroskedasticity" class="level3"><h3 class="anchored" data-anchor-id="testing-for-heteroskedasticity">Testing for heteroskedasticity</h3>
<p> </p>
<p><span class="citation" data-cites="BREU:PAGA:79">Breusch and Pagan (<a href="#ref-BREU:PAGA:79" role="doc-biblioref">1979</a>)</span> consider the following heteroskedastic model: <span class="math inline">\(y_n = \gamma ^ \top z_n + \epsilon_n\)</span> with <span class="math inline">\(\epsilon_n \sim \mathcal{N}(0, \sigma_n ^ 2)\)</span>. Assume that <span class="math inline">\(\sigma_n^2\)</span> is a function of a set of <span class="math inline">\(J\)</span> covariates denoted by <span class="math inline">\(w_n\)</span>:</p>
<p><span class="math display">\[
\sigma_n ^ 2 = h(\delta ^ \top w_n)
\]</span></p>
<p>The first element of <span class="math inline">\(w\)</span> is 1, so that the homoskedasticity hypothesis is that all the elements of <span class="math inline">\(\delta\)</span> except the first one are 0: <span class="math inline">\(\delta_0^\top = (\alpha, 0, \ldots, 0)\)</span>, so that <span class="math inline">\(\sigma ^ 2_n = h(\delta_0^\top w_n) = h_0\)</span>. The log-likelihood function is:</p>
<p><span class="math display">\[
\ln L = -\frac{N}{2}\ln 2\pi - \frac{1}{2} \sum_n \ln \sigma_n ^ 2 - \frac{1}{2}\sum_n \frac{(y_n - \gamma z_n)^2}{\sigma_n ^ 2}
\]</span> The derivative of <span class="math inline">\(\ln L\)</span> with <span class="math inline">\(\delta\)</span> is, denoting <span class="math inline">\(h'_n = \frac{\partial h}{\partial \delta}(w_n)\)</span>:</p>
<p><span id="eq-gen_score_bp_heter"><span class="math display">\[
\frac{\partial \ln L}{\partial \delta} = \frac{1}{2}\sum_n \left(\frac{\epsilon ^ 2}{\sigma_n ^ 4} - \frac{1}{\sigma_n ^ 2}\right) h'_n w_n
\tag{6.4}\]</span></span></p>
<p>With the homoskedasticity hypothesis, <span class="math inline">\(\sigma_n = \sigma\)</span> and <span class="math inline">\(h'_n = h_n'(\delta_0) = h_0'\)</span> and <a href="#eq-gen_score_bp_heter">Equation&nbsp;<span>6.4</span></a> simplifies to:</p>
<p><span class="math display">\[
d = \frac{\partial \ln L}{\partial \delta}(\delta_0) = \frac{h'_0}{2\sigma ^ 2}\sum_n \left(\frac{\epsilon_n ^ 2}{\sigma ^ 2} - 1\right) w_n
\]</span> The second derivatives are:</p>
<p><span class="math display">\[
\frac{\partial \ln ^ 2 L}{\partial \delta \partial\delta ^ \top} =
\frac{1}{2}\sum_n\left[ h_n''\left(\frac{\epsilon_n ^ 2}{\sigma_n ^ 4}-
\frac{1}{\sigma_n ^ 2} \right)- h_n^{'2} \left(\frac{2 \epsilon_n ^
2}{\sigma_n ^ 6}- \frac{1}{\sigma_n ^ 4} \right)\right]w_n w_n'
\]</span></p>
<p>To get the information matrix, we take the expectation of the opposite of this matrix. If the errors are homoskedastic, the first term disappears and <span class="math inline">\(h'_n = h'_0\)</span> so that:</p>
<p><span id="eq-info"><span class="math display">\[
I_0 = \mbox{E}\left(- \frac{\partial \ln ^ 2 L}{\partial \delta \partial\delta ^ \top}(\delta_0)\right) = \frac{h_0^{'2}}{2\sigma ^ 4}\sum_nw_n w_n'
\tag{6.5}\]</span></span></p>
<p>Denoting <span class="math inline">\(\hat{\epsilon}\)</span> the vector of OLS residuals and <span class="math inline">\(\hat{\sigma} ^ 2 = \hat{\epsilon} ^ \top \hat{\epsilon} / N\)</span> the estimate of <span class="math inline">\(\sigma ^ 2\)</span>, the estimated score is:</p>
<p><span class="math display">\[
\hat{d} = \frac{h'_0}{2\hat{\sigma} ^ 2}\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n
\]</span></p>
<p>and the test statistic is the quadratic form of <span class="math inline">\(\hat{d}\)</span> with the inverse of its variance given by <a href="#eq-info">Equation&nbsp;<span>6.5</span></a>:</p>
<p><span class="math display">\[
LM = \frac{1}{2} \left[\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n^\top\right] \left(\sum_n w_n w_n ^ \top\right)^{-1}
\left[\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n\right]
\]</span> or, in matrix form, denoting <span class="math inline">\(f\)</span> the <span class="math inline">\(N\)</span>-length vector with typical element <span class="math inline">\(\left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right)\)</span> and <span class="math inline">\(W\)</span> the matrix of covariates:</p>
<p><span id="eq-bp_heteros_ESS"><span class="math display">\[
LM =\frac{1}{2}f^\top W (W^\top W)^{-1} W f = \frac{1}{2}f ^ \top P_W f
\tag{6.6}\]</span></span></p>
<p>which is half the explained sum of squares of a regression of <span class="math inline">\(f_n\)</span> on <span class="math inline">\(w_n\)</span> and is a <span class="math inline">\(\chi ^ 2\)</span> with <span class="math inline">\(J\)</span> degrees of freedom in case of homoskedasticity. Note also that <span class="math inline">\(f^\top f / N = \sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) ^ 2 / N= \sum_n\left(\frac{\hat{\epsilon}_n ^ 4}{\hat{\sigma} ^ 4} + 1 - 2 \frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2}\right) / N\)</span> is the total sum of squares divided by <span class="math inline">\(N\)</span>. It converges to 2 as the first term is the fourth center moment of a normal variable, which is 3. Therefore, a second version of the statistic can be computed as <span class="math inline">\(N\)</span> times the R<sup>2</sup> of a regression of the first-step residuals on <span class="math inline">\(w\)</span>:</p>
<p><span class="math display">\[
N R^2 = \frac{f^\top P_W f}{f'f / N}
\]</span></p>
<p><span class="citation" data-cites="WHIT:80">White (<a href="#ref-WHIT:80" role="doc-biblioref">1980</a>)</span> proposed a test that is directly linked to its proposition of the heteroskedasticity-robust matrix of covariance of the OLS estimates.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This matrix depends on the squares and on the cross-products of the covariates. Therefore, he proposed to run a regression of the squares of the first-step residuals on the covariates, their squares and their cross-product. <span class="math inline">\(N R^2\)</span> of this regression is asymptotically distributed as a <span class="math inline">\(\chi ^ 2\)</span> with <span class="math inline">\(K(K+1)/2\)</span> degrees of freedom. Therefore, White’s test can be viewed as a special case of Breusch and Pagan’s test.</p>
<p>In his electric consumption regression, <span class="citation" data-cites="HOUT:51">Houthakker (<a href="#ref-HOUT:51" role="doc-biblioref">1951</a>)</span> used as covariates <code>inc</code> (average yearly income in pounds), the inverse of <code>mc6</code> (the marginal cost of electricity), <code>gas6</code> (the marginal price of gas) and <code>cap</code> (the average holdings of heavy electric equipment). The OLS estimation is: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_elec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">kwh</span> <span class="op">~</span> <span class="va">inc</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">mc6</span><span class="op">)</span> <span class="op">+</span>  <span class="va">gas6</span> <span class="op">+</span> <span class="va">cap</span>, <span class="va">uk_elec</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <span class="math inline">\(f\)</span> vector in <a href="#eq-bp_heteros_ESS">Equation&nbsp;<span>6.6</span></a> is: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">f</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then regress <span class="math inline">\(f\)</span> on <span class="math inline">\(W\)</span> (which is here a constant and the inverse of <code>cust</code>) and get half the explained sum of squares: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_elec_resid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">f</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">cust</span><span class="op">)</span>, <span class="va">uk_elec</span><span class="op">)</span></span>
<span><span class="va">bp1_elec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">lm_elec_resid</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For the second version of the test, we compute <span class="math inline">\(N\)</span> times the <span class="math inline">\(R ^ 2\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bp2_elec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">lm_elec_resid</span><span class="op">)</span> <span class="op">*</span> <span class="fu">rsq</span><span class="op">(</span><span class="va">lm_elec_resid</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The values and the probability values for the two versions of the Breusch-Pagan test are: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bp1_elec</span>, <span class="va">bp2_elec</span><span class="op">)</span></span>
<span><span class="co">## [1] 16.49 11.38</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bp1_elec</span>, <span class="va">bp2_elec</span><span class="op">)</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span>, df <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">## [1] 4.887e-05 7.441e-04</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The homoskedasticity hypothesis is therefore highly rejected. The <code><a href="https://rdrr.io/pkg/lmtest/man/bptest.html">lmtest::bptest</a></code> computes automatically the Breusch-Pagan test for heteroskedasticity, with two formulas: the first being the formula of the model and the second being a one-side formula for the skedasticity equation. An alternative syntax is to provide a <code>lm</code> model as the first argument: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/bptest.html">bptest</a></span><span class="op">(</span><span class="va">kwh</span> <span class="op">~</span> <span class="va">inc</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">mc6</span><span class="op">)</span> <span class="op">+</span> <span class="va">gas6</span> <span class="op">+</span> <span class="va">cap</span>, <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">cust</span><span class="op">)</span>, </span>
<span>               data <span class="op">=</span> <span class="va">uk_elec</span>, studentize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/bptest.html">bptest</a></span><span class="op">(</span><span class="va">lm_elec</span>, <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">cust</span><span class="op">)</span>,</span>
<span>               data <span class="op">=</span> <span class="va">uk_elec</span>, studentize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we set the <code>studentize</code> argument to <code>FALSE</code>. The default value is <code>TRUE</code> and in this case, a modified version of the test due to <span class="citation" data-cites="KOEN:81">Koenker (<a href="#ref-KOEN:81" role="doc-biblioref">1981</a>)</span> is used. </p>
</section><section id="testing-for-individual-effects" class="level3"><h3 class="anchored" data-anchor-id="testing-for-individual-effects">Testing for individual effects</h3>
<p> </p>
<p><span class="citation" data-cites="BREU:PAGA:80">Breusch and Pagan (<a href="#ref-BREU:PAGA:80" role="doc-biblioref">1980</a>)</span> extend their Lagrange multiplier test to the problem of individual (or entity) effects in a panel (or in a pseudo-panel) setting. Assuming a normal distribution, the joint density for the whole sample is:</p>
<p><span id="eq-joint_density_panel"><span class="math display">\[
f(y\mid X) = \frac{1}{(2\pi) ^ {NT /2}\mid\Omega\mid}e^{-\frac{1}{2}\epsilon ^ \top \Omega ^ {-1} \epsilon}
\tag{6.7}\]</span></span></p>
<p>We have seen (<a href="#eq-omega_panel">Equation&nbsp;<span>6.2</span></a>) that <span class="math inline">\(\Omega = \sigma_\nu ^ 2 W + \sigma_\iota ^ 2B\)</span>, with <span class="math inline">\(\sigma_\iota ^ 2 = \sigma_\nu + T \sigma_\eta\)</span>. Then,</p>
<p><span class="math display">\[\epsilon^ \top \Omega ^ {-1} \epsilon =
\frac{1}{\sigma_\nu ^ 2}\epsilon ^ \top W \epsilon + \frac{1}{\sigma_\iota ^ 2}\epsilon ^ \top B \epsilon\]</span> The determinant of <span class="math inline">\(\Omega\)</span> is the product of its eigenvalues, which are <span class="math inline">\(\sigma_\nu ^ 2\)</span> with periodicity <span class="math inline">\(N(T-1)\)</span> and <span class="math inline">\(\sigma_\iota^2\)</span> with periodicity <span class="math inline">\(N\)</span>. Then, taking the logarithm of <a href="#eq-joint_density_panel">Equation&nbsp;<span>6.7</span></a> and denoting <span class="math inline">\(\theta^ \top = (\sigma_\nu ^ 2, \sigma_\eta ^ 2)\)</span>, we get the following log-likelihood function:</p>
<p><span class="math display">\[
\ln L (\theta)= \frac{NT}{2} \ln 2\pi - \frac{N(T-1)}{2}\ln \sigma_\nu ^ 2 - \frac{N}{2}\ln(T\sigma_\eta ^ 2 + \sigma_\nu ^ 2) - \frac{\epsilon ^ \top W \epsilon}{2\sigma_\nu ^ 2}- \frac{\epsilon ^ \top W \epsilon}{2(\sigma_\nu ^ 2 + T \sigma_\eta ^ 2)}
\]</span></p>
<p>The gradient and the hessian are respectively:</p>
<p><span class="math display">\[
g(\theta)=
\left(
  \begin{array}{cc}
    \frac{\partial \ln L}{\partial \sigma_\nu^2} \\ \frac{\partial
      \ln L}{\partial \sigma_\eta^2} \\
  \end{array}
  \right)
=
\left(
  \begin{array}{cc}
    -\frac{N(T-1)}{2\sigma_\nu^2}-\frac{N}{2\sigma_\iota^2}+
    \frac{\epsilon^\top W\epsilon}{2\sigma_\nu^4}+\frac{\epsilon^\top B_\eta\epsilon}{2\sigma_\iota^2}\\
    -\frac{NT}{2\sigma_\iota^2}+\frac{T
    \epsilon^\top B\epsilon}{2\sigma_\iota^2}
  \end{array}
\right)
\]</span></p>
<p><span class="math display">\[ H(\theta)= \left(
\begin{array}{ll}
  -\frac{N(T-1)}{2\sigma_\nu^4}+\frac{N}{2\sigma_\iota^4}-
  \frac{\epsilon^\top W\epsilon}{\sigma_\nu^6}-\frac{\epsilon^\top B\epsilon}{\sigma_\iota^6}
  &amp; \frac{NT}{2\sigma_\iota^4}-\frac{T\epsilon^\top B\epsilon}{\sigma_\iota^6}\\
  \frac{NT}{2\sigma_\iota^4}-
  \frac{T\epsilon^\top B\epsilon}{\sigma_\iota^6}
  &amp;\frac{NT^2}{2\sigma_\iota^4} -
    \frac{T^2 \epsilon^\top B\epsilon}{\sigma_\iota ^ 6}
\end{array}
\right)
\]</span></p>
<p>To compute the expectation of this matrix, we note that <span class="math inline">\(\mbox{E}(\epsilon^\top W_\eta\epsilon)=N(T-1)\sigma_\nu^2\)</span> and <span class="math inline">\(\mbox{E}(\epsilon^\top B_\eta\epsilon)=N\sigma_\iota ^ 2\)</span>:</p>
<p><span class="math display">\[
\mbox{E}(H(\theta))= \left(
\begin{array}{cc}
  -\frac{N(T-1)}{2\sigma_\nu^4}-\frac{N}{2\sigma_\iota ^ 4}
  &amp; -\frac{NT}{2\sigma_\iota ^ 4}\\
  -\frac{NT}{2\sigma_\iota ^ 4}
  &amp; -\frac{NT^2}{2\sigma_\iota ^ 4}
\end{array}
\right)
\]</span></p>
<p>To compute the test statistic, we impose the null hypothesis: <span class="math inline">\(H_0: \sigma_\eta^2=0\)</span> (no individual effects), so that <span class="math inline">\(\sigma_\iota^2= \sigma_\nu ^ 2\)</span>. In this case, the OLS estimator is BLUE and <span class="math inline">\(\hat{\sigma}_\nu^2\)</span> is <span class="math inline">\(\hat{\epsilon}^\top\hat{\epsilon} / NT\)</span>. The estimated score and the information matrix are, with <span class="math inline">\(\hat{\theta}^\top = (\hat{\sigma}_\nu^ 2, 0)\)</span></p>
<p><span class="math display">\[
\hat{g}(\hat{\theta})=
\left(
  \begin{array}{cc}
    0 \\ -\frac{NT}{2\hat{\sigma}_\nu^2}\left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)
  \end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\hat{I}(\hat{\theta}) = \mbox{E}\left(-H(\hat{\theta})\right)=
\frac{NT}{2\hat{\sigma}_\nu^4}
\left(
\begin{array}{cc}
  1 &amp; 1 \\
  1 &amp; T
\end{array}
\right)
\]</span></p>
<p>and the inverse of the estimated information matrix is:</p>
<p><span class="math display">\[
\hat{I}(\hat{\theta}) ^ {-1}=\frac{2\hat{\sigma}_\nu^4}{NT(T-1)}
\left(
\begin{array}{cc}
  T &amp; -1 \\
  -1 &amp; 1
\end{array}
\right)
\]</span></p>
<p>Finally, the test statistic is computed as the quadratic form: <span class="math inline">\(\hat{g}(\hat{\theta}) ^ \top \hat{I}(\hat{\theta}) ^ {-1} \hat{g}(\hat{\theta})\)</span> which simplifies to:</p>
<p><span class="math display">\[ LM = \left(-\frac{NT}{2\hat{\sigma}_\nu^2}
  \left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)\right)^2
\times \frac{2\hat{\sigma}_\nu^4}{NT(T-1)} =
\frac{NT}{2(T-1)}\left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)^2
\]</span></p>
<p>Or, replacing <span class="math inline">\(\hat{\sigma}_\nu^2\)</span> by <span class="math inline">\(\hat{\epsilon}^\top\hat{\epsilon}/NT\)</span>:</p>
<p><span id="eq-bp_panel"><span class="math display">\[ LM =
\frac{NT}{2(T-1)}\left(T\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{\hat{\epsilon}^\top\hat{\epsilon}}-1\right)^2
\tag{6.8}\]</span></span></p>
<p>which is asymptotically distributed as a <span class="math inline">\(\chi^2\)</span> with 1 degree of freedom.</p>
<p><span class="citation" data-cites="BONJ:CHERK:KASK:03">Bonjour et al. (<a href="#ref-BONJ:CHERK:KASK:03" role="doc-biblioref">2003</a>)</span> estimated a Mincer equation with a sample of twins, the entity index being <code>family</code>. The response is the log of wage and the covariates are education (<code>educ</code>) and potential experience and its square (approximated by the age <code>age</code>): </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_twins</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">educ</span>, <span class="va">twins</span><span class="op">)</span></span>
<span><span class="va">lm_twins</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">##   (Intercept) poly(age, 2)1 poly(age, 2)2          educ </span></span>
<span><span class="co">##       1.03397       0.06237      -1.93282       0.07675</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then add the residuals to the data and we compute the individual mean of the residuals by grouping by entities and then using <code>mutate</code> and not <code>summarise</code> to compute the mean, <span class="math inline">\(B\epsilon\)</span> being a vector of length <span class="math inline">\(N \times T\)</span> where each value is returned <span class="math inline">\(T\)</span> times: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">twins</span> <span class="op">&lt;-</span> <span class="va">twins</span> <span class="op">%&gt;%</span> <span class="fu">add_column</span><span class="op">(</span>e <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lm_twins</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">family</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>Be <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">ungroup</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We finally compute the statistic using <a href="#eq-bp_panel">Equation&nbsp;<span>6.8</span></a>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N_tw</span> <span class="op">&lt;-</span> <span class="va">twins</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">family</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">unique</span> <span class="op">%&gt;%</span> <span class="va">length</span></span>
<span><span class="va">T_tw</span> <span class="op">&lt;-</span> <span class="fl">2</span></span>
<span><span class="va">twins</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>bp <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">T_tw</span> <span class="op">*</span> <span class="va">N_tw</span> <span class="op">/</span> <span class="op">(</span><span class="va">T_tw</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span>  <span class="op">*</span> </span>
<span>              <span class="op">(</span><span class="va">T_tw</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Be</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">e</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">pull</span></span>
<span><span class="co">## [1] 4.222</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <strong>plm</strong> package <span class="citation" data-cites="CROI:MILL:08 CROI:MILL:18">(<a href="#ref-CROI:MILL:08" role="doc-biblioref">Croissant and Millo 2008</a>, <a href="#ref-CROI:MILL:18" role="doc-biblioref">2018</a>)</span> provides different tools to deal with panel or pseudo-panel data. In particular, Breusch and Pagan’s test can easily be obtained using <code><a href="https://rdrr.io/pkg/plm/man/plmtest.html">plm::plmtest</a></code>. We set the <code>type</code> argument to <code>"bp"</code> to get the statistic of the original Breusch-Pagan test: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://cran.r-project.org/package=plm">plm</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/plmtest.html">plmtest</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">educ</span>, <span class="va">twins</span>, type <span class="op">=</span> <span class="st">"bp"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## chisq = 4.222, df: 1, pval = 0.040</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The absence of individual effects is rejected at the 5% level, but not at the 1% level.</p>
<p>The model fitted by <span class="citation" data-cites="SCHA:90">Schaller (<a href="#ref-SCHA:90" role="doc-biblioref">1990</a>)</span> is a simple linear model, the response being the rate of investment (<code>ikn</code>) and the unique covariate Tobin’s Q (<code>qn</code>): </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tobinq</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6,580 × 15
  cusip  year  isic    ikb   ikn    qb    qn kstock    ikicb  ikicn
  &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;
1  2824  1951  2835 0.230  0.205  5.61 10.9    27.3 NA       0.0120
2  2824  1952  2835 0.0403 0.200  6.01 12.2    30.5  0.193   0.0245
3  2824  1953  2835 0.0404 0.110  4.19  7.41   31.7  0.00292 0.0976
# ℹ 6,577 more rows
# ℹ 5 more variables: omphi &lt;dbl&gt;, qicb &lt;dbl&gt;, qicn &lt;dbl&gt;,
#   sb &lt;dbl&gt;, sn &lt;dbl&gt;</code></pre>
</div>
</div>
<p>The first two columns contain the firm and the time index. The Breusch-Pagan statistic is: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/plmtest.html">plmtest</a></span><span class="op">(</span><span class="va">ikn</span> <span class="op">~</span> <span class="va">qn</span>, <span class="va">tobinq</span>, type <span class="op">=</span> <span class="st">"bp"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## chisq = 8349.686, df: 1, pval = 0.000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The statistic is huge, the hypothesis of no individual effects is therefore very strongly rejected, which is a quite customary result for panel data, especially when the time dimension is high, which is the case for the <code>tobinq</code> data (35 years). </p>
<p> </p>
</section><section id="sec-bptest_system" class="level3"><h3 class="anchored" data-anchor-id="sec-bptest_system">System of equations</h3>
<p> </p>
<div class="cell" data-layout-align="center">

</div>
<p><span class="citation" data-cites="BREU:PAGA:80">Breusch and Pagan (<a href="#ref-BREU:PAGA:80" role="doc-biblioref">1980</a>)</span> also proposed a test of correlation between equations in a system of equation. Remember that in this case, the covariance matrix of the errors for the whole system is <span class="math inline">\(\Omega = \Sigma \otimes I\)</span>, where <span class="math inline">\(\Sigma\)</span> contains the variances (on the diagonal) and the covariances (off diagonal) of the errors of the <span class="math inline">\(L\)</span> equations. This symmetric matrix contains <span class="math inline">\(L \times (L + 1) / 2\)</span> distinct elements, <span class="math inline">\(L \times (L + 1) / 2 - L = L \times (L - 1) / 2\)</span> being covariances. The Breusch-Pagan test is based on the estimation of the covariance matrix using OLS residuals. Denoting <span class="math inline">\(\hat{\Xi} = (\hat{\epsilon}_1, \hat{\epsilon}_2, \ldots \hat{\epsilon}_L)\)</span> the matrix where each column contains the vector of residuals for one equation:</p>
<p><span class="math display">\[
\hat{\Omega} = \hat{\Xi} ^ \top \hat{\Xi}=
\left(
\begin{array}{cccc}
\hat{\epsilon}_1 ^ \top \hat{\epsilon_1} &amp;\hat{\epsilon}_1 ^ \top \hat{\epsilon_2} &amp; \ldots &amp;\hat{\epsilon}_1 ^ \top \hat{\epsilon_L} \\
\hat{\epsilon}_2 ^ \top \hat{\epsilon_1} &amp;\hat{\epsilon}_2 ^ \top \hat{\epsilon_2} &amp; \ldots &amp;\hat{\epsilon}_2 ^ \top \hat{\epsilon_L} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\hat{\epsilon}_L ^ \top \hat{\epsilon_1} &amp;\hat{\epsilon}_L ^ \top \hat{\epsilon_2} &amp; \ldots &amp;\hat{\epsilon}_L ^ \top \hat{\epsilon_L}
\end{array}
\right)
\]</span> The coefficients of correlations are then estimated, denoting <span class="math inline">\(\hat{\sigma} ^ 2_l = \hat{\epsilon}_l^\top \hat{\epsilon}_l/ N\)</span> the estimated variances:</p>
<p><span class="math display">\[
\hat{\rho}_{lm} = \frac{\hat{\epsilon}_l^\top \hat{\epsilon}_m / N}{\hat{\sigma}_l\hat{\sigma}_m}
\]</span> The statistic is then <span class="math inline">\(N \sum_{l=1} ^ {L-1} \sum_{m = l + 1} ^ L \hat{\rho}_{lm}^2\)</span> and is a <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(L(L-1) / 2\)</span> degrees of freedom if the hypothesis of no correlation is true.</p>
<p>We use the apple production example estimated in <a href="multiple_regression.html#sec-system_equation"><span>Section&nbsp;3.7</span></a>. The estimation for the whole system by OLS taking constraints into account was stored in an object called <code>ols_const</code>. We extract the residuals and arrange them in a <span class="math inline">\(N\times L\)</span> matrix. Taking the cross-product of this matrix and dividing by <span class="math inline">\(N\)</span>, we get <span class="math inline">\(\hat{\Sigma}\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N_ap</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">ols_const</span><span class="op">)</span> <span class="op">/</span> <span class="fl">3</span></span>
<span><span class="va">EPS</span> <span class="op">&lt;-</span> <span class="va">ols_const</span> <span class="op">%&gt;%</span> <span class="va">resid</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">EPS</span><span class="op">)</span> <span class="op">/</span> <span class="va">N_ap</span></span>
<span><span class="va">Sigma</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]
[1,]  0.088417 -0.002245  0.001650
[2,] -0.002245  0.008088 -0.006070
[3,]  0.001650 -0.006070  0.007718</code></pre>
</div>
</div>
<p>We then compute the estimated standard deviation of the errors for every equation <span class="math inline">\(\hat{\sigma}_l\)</span> and we divide <span class="math inline">\(\hat{\Sigma}\)</span> by a matrix containing the products of the standard deviations, using the <code>outer</code> function: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sig</span> <span class="op">&lt;-</span> <span class="va">Sigma</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="va">Sigma</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/outer.html">outer</a></span><span class="op">(</span><span class="va">sig</span>, <span class="va">sig</span><span class="op">)</span></span>
<span><span class="va">d</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         [,1]     [,2]     [,3]
[1,]  1.00000 -0.08394  0.06315
[2,] -0.08394  1.00000 -0.76827
[3,]  0.06315 -0.76827  1.00000</code></pre>
</div>
</div>
<p>so that we get a matrix with ones on the diagonal and coefficients of correlations off-diagonal. Then, we extract the off-diagonal elements using <code>upper.tri</code>, which returns a logical matrix with values of <code>TRUE</code> above the diagonal: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]  [,3]
[1,] FALSE  TRUE  TRUE
[2,] FALSE FALSE  TRUE
[3,] FALSE FALSE FALSE</code></pre>
</div>
</div>
<p>Then, indexing <code>d</code> by <code>upper.tri(d)</code> returns a vector containing the three elements of the matrix that are above the diagonal:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">d</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co">## [1] -0.08394  0.06315 -0.76827</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we sum the squares of the elements of this vector and multiply by the sample size to get the statistic: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bp_stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="va">d</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">d</span><span class="op">)</span><span class="op">]</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="va">N_ap</span></span>
<span><span class="va">pval_pb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">bp_stat</span>, df <span class="op">=</span> <span class="fl">3</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bp_stat</span>, <span class="va">pval_pb</span><span class="op">)</span></span>
<span><span class="co">## [1] 8.237e+01 9.497e-18</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The hypothesis of no correlation is clearly rejected. </p>
</section></section><section id="sec-sandwich" class="level2" data-number="6.3"><h2 data-number="6.3" class="anchored" data-anchor-id="sec-sandwich">
<span class="header-section-number">6.3</span> Robust inference</h2>
<p> </p>
<p>If the errors are not spherical, the simple estimator of the covariance of the OLS estimates is biased. More general estimators can then be used instead. These estimators use the residuals of the OLS estimator which, in the context of this chapter, is an inefficient but consistent estimator. We’ll present the robust estimator of the covariance matrix of the OLS estimates first in the context of the simple linear model and then for the multiple linear model.</p>
<section id="simple-linear-model" class="level3"><h3 class="anchored" data-anchor-id="simple-linear-model">Simple linear model</h3>
<p> The variance of the slope estimated by OLS is:</p>
<p><span class="math display">\[
\sigma_{\hat{\beta}} ^ 2 =
\mbox{V}(\hat{\beta} \mid x) = \frac{\mbox{E}\left(\left[\sum_n (x_n - \bar{x})\epsilon_n\right]^2\mid x\right)}{\left(\sum_n (x_n - \bar{x}) ^ 2\right) ^ 2}
\]</span></p>
<p>The numerator is the sum of the expectations of <span class="math inline">\(N ^ 2\)</span> terms. For <span class="math inline">\(N = 4\)</span>, replacing the errors <span class="math inline">\(\epsilon_n\)</span> by the OLS residuals <span class="math inline">\(\hat{\epsilon}_n\)</span> and dropping the expectation operator, these 16 terms can be presented conveniently in the following matrix:</p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[\small{
\left(
\begin{array}{cccc}
(x_1-\bar{x})^2 \hat{\epsilon}_1^2 &amp;
(x_1-\bar{x}) (x_2-\bar{x}) \hat{\epsilon}_1\hat{\epsilon}_2 &amp;
(x_1-\bar{x}) (x_3-\bar{x}) \hat{\epsilon}_1\hat{\epsilon}_3 &amp;
(x_1-\bar{x}) (x_4-\bar{x}) \hat{\epsilon}_1\hat{\epsilon}_4 \\
(x_2-\bar{x}) (x_1-\bar{x}) \hat{\epsilon}_2\hat{\epsilon}_1 &amp;
(x_2-\bar{x})^2 \hat{\epsilon}_2^2 &amp;
(x_2-\bar{x}) (x_3-\bar{x}) \hat{\epsilon}_2\hat{\epsilon}_3 &amp;
(x_2-\bar{x}) (x_4-\bar{x}) \hat{\epsilon}_2\hat{\epsilon}_4 \\
(x_3-\bar{x}) (x_1-\bar{x}) \hat{\epsilon}_3\hat{\epsilon}_1 &amp;
(x_3-\bar{x}) (x_2-\bar{x}) \hat{\epsilon}_3\hat{\epsilon}_2 &amp;
(x_3-\bar{x})^2 \hat{\epsilon}_3^2 &amp;
(x_3-\bar{x}) (x_4-\bar{x}) \hat{\epsilon}_3\hat{\epsilon}_4 \\
(x_4-\bar{x}) (x_1-\bar{x}) \hat{\epsilon}_4\hat{\epsilon}_1 &amp;
(x_4-\bar{x}) (x_2-\bar{x}) \hat{\epsilon}_4\hat{\epsilon}_2 &amp;
(x_4-\bar{x}) (x_3-\bar{x}) \hat{\epsilon}_4\hat{\epsilon}_3 &amp;
(x_4-\bar{x})^2 \hat{\epsilon}_4^2 \\
\end{array}
\right)}\]</span></p>
</div>
<p>The robust estimator is obtained by taking the sum of <em>some</em> of this terms. Note first that the sum of all these terms is <span class="math inline">\(\left(\sum_{n = 1} ^ N (x_n - \bar{x})\hat{\epsilon}_{n}\right)^2\)</span>, which is equal to zero as: <span class="math inline">\(\sum_{n = 1} ^ N (x_n - \bar{x})\hat{\epsilon}_n=0\)</span>. Therefore, it is not relevant to sum <em>all</em> the terms of this matrix to get an estimator of the variance of <span class="math inline">\(\hat{\beta}\)</span>. The first possibility is to take only the diagonal terms of this matrix, which is relevant if we maintain the hypotheses that the errors are uncorrelated. In this case, we get the so-called <strong>heteroskedastic-consistent</strong> (<strong>HC</strong>) estimator of <span class="math inline">\(\sigma_{\hat{\beta}}\)</span> proposed by <span class="citation" data-cites="WHIT:80">White (<a href="#ref-WHIT:80" role="doc-biblioref">1980</a>)</span>: </p>
<p><span id="eq-vcovHC"><span class="math display">\[
\hat{\sigma}_{\mbox{HC}\hat{\beta}} ^ 2 = \frac{1}{S_{xx} ^ 2}\sum_{n = 1}
^ N (x_n - \bar{x}) ^ 2 \hat{\epsilon}_n ^ 2
\tag{6.9}\]</span></span></p>
<p>Consider now the case where some errors are correlated. This often happens when some observations share some common unobserved characteristics which are included in their (therefore correlated) errors. For example, if observations belong to different regions, their errors may share some common unobserved features of the regions. In our four observations case, suppose that the first two observations belong to one group, and the two others to another group. Then, a consistent estimator is obtained by summing the following subset of elements of the preceding matrix: </p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[\small{\left(\begin{array}{cccc}(x_1-\bar{x})^2 \hat{\epsilon}_1^2 &amp;
(x_1-\bar{x}) (x_2-\bar{x}) \hat{\epsilon}_1\hat{\epsilon}_2 &amp;
\mbox{--} &amp;
\mbox{--} \\
(x_2-\bar{x}) (x_1-\bar{x}) \hat{\epsilon}_2\hat{\epsilon}_1 &amp;
(x_2-\bar{x})^2 \hat{\epsilon}_2^2 &amp;
\mbox{--} &amp;
\mbox{--} \\
\mbox{--} &amp;
\mbox{--} &amp;
(x_3-\bar{x})^2 \hat{\epsilon}_3^2 &amp;
(x_3-\bar{x}) (x_4-\bar{x}) \hat{\epsilon}_3\hat{\epsilon}_4 \\
\mbox{--} &amp;
\mbox{--} &amp;
(x_4-\bar{x}) (x_3-\bar{x}) \hat{\epsilon}_4\hat{\epsilon}_3 &amp;
(x_4-\bar{x})^2 \hat{\epsilon}_4^2 \\
\end{array}\right)}\]</span></p>
</div>
<p>which leads to the clustered estimated variance. More generally, for <span class="math inline">\(N\)</span> observations belonging to <span class="math inline">\(G\)</span> groups, this estimator is:</p>
<p><span id="eq-vcovCL"><span class="math display">\[
\hat{\sigma}_{\mbox{CL}\hat{\beta}} ^2= \frac{1}{S_{xx}^2}\sum_{g = 1} ^ G \left(\sum_{n \in g} (x_n -
\bar{x})\hat{\epsilon}_n \right) ^ 2
\tag{6.10}\]</span></span></p>
<p>which is consistent with the hypothesis that errors are correlated <em>within</em> a group, but uncorrelated <em>between</em> groups. To illustrate the computation of robust covariance estimators, we use the data set <code>urban_gradient</code> of <span class="citation" data-cites="DURA:PUGA:20">Duranton and Puga (<a href="#ref-DURA:PUGA:20" role="doc-biblioref">2020</a>)</span>. It contains the population, the area and the distance to the central business district for 2315 block groups<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> in Alabama.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">urban_gradient</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2,315 × 7
  msa        county  tract  blkg  area population distance
  &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;
1 Montgomery Autauga 20100     1  4.24        530     19.6
2 Montgomery Autauga 20100     2  5.57       1282     20.8
3 Montgomery Autauga 20200     1  2.06       1274     19.3
4 Montgomery Autauga 20200     2  1.28        944     18.0
5 Montgomery Autauga 20300     1  3.87       2538     18.2
# ℹ 2,310 more rows</code></pre>
</div>
</div>
<p>A classic model in urban economics states that urban density is a negative exponential function of the distance to the central business district: <span class="math inline">\(y = A e^{\beta x}\)</span> where <span class="math inline">\(y\)</span> is measured in inhabitants per square kilometers, <span class="math inline">\(x\)</span> is measured in kilometers and <span class="math inline">\(\beta&lt;0\)</span> is called the urban gradient. Taking logs, this leads to a semi-log linear regression model:</p>
<p><span class="math display">\[
\ln y_n = \alpha + \beta x_n + \epsilon_n
\]</span></p>
<p>We first compute the <code>density</code> variable and then estimate the urban gradient model. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">urban_gradient</span> <span class="op">&lt;-</span> <span class="va">urban_gradient</span> <span class="op">%&gt;%</span> <span class="fu">mutate</span><span class="op">(</span>density <span class="op">=</span> <span class="va">population</span> <span class="op">/</span> <span class="va">area</span><span class="op">)</span></span>
<span><span class="va">ols_ug</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">density</span><span class="op">)</span> <span class="op">~</span> <span class="va">distance</span>, <span class="va">urban_gradient</span><span class="op">)</span></span>
<span><span class="va">ols_ug</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##          Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## distance -0.07057    0.00182   -38.9   &lt;2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The estimated standard deviation of the slope is 0.0018, but it may be seriously biased if the errors are heteroskedastic and/or correlated. We first plot the data and the regression line in <a href="#fig-dataug">Figure&nbsp;<span>6.1</span></a>, the shape of the points depending on the metropolitan statistical area (MSA, there are 12 of them in the data set). </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">urban_gradient</span> <span class="op">%&gt;%</span> <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">distance</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>shape <span class="op">=</span> <span class="va">msa</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">.3</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_shape_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">16</span>, <span class="fl">17</span>, <span class="fl">8</span>, <span class="fl">5</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">6</span>, <span class="fl">9</span>, <span class="fl">0</span>, <span class="fl">12</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dataug" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="non_spherical_files/figure-html/fig-dataug-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.1: Data and regression line for the <code>urban_gradient</code> data set</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Heteroskedasticity seems to be present in this data set, as the size of the residuals seems to be an increasing function of the unique covariate. We first compute the HC standard deviation of the slope, computing the mean of the covariate and <span class="math inline">\(S_{xx}\)</span> and then applying <a href="#eq-vcovHC">Equation&nbsp;<span>6.9</span></a>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dist_mean</span> <span class="op">&lt;-</span> <span class="va">urban_gradient</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">distance</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">mean</span></span>
<span><span class="va">Sxx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="fu">pull</span><span class="op">(</span><span class="va">urban_gradient</span>, <span class="va">distance</span><span class="op">)</span> <span class="op">-</span> <span class="va">dist_mean</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">sd_hc</span> <span class="op">&lt;-</span> <span class="va">urban_gradient</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">distance</span> <span class="op">-</span> <span class="va">dist_mean</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">*</span> </span>
<span>                         <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">ols_ug</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">Sxx</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">pull</span></span>
<span><span class="va">sd_hc</span></span>
<span><span class="co">## [1] 0.002007</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this example, the heteroskedastic-robust standard error is just slightly higher than the one computed using the simple formula. However, we also have to investigate the potential correlation between the errors of some observations. There are 12 MSA and 22 counties in Alabama. It is possible that errors for block groups of the same county or of the same MSA are correlated (because of some unobserved common features of block groups in the same county or MSA). We compute the estimation of the clustered standard deviation of the slope (<a href="#eq-vcovCL">Equation&nbsp;<span>6.10</span></a>) at the MSA level: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">G</span> <span class="op">&lt;-</span> <span class="va">urban_gradient</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">msa</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">unique</span> <span class="op">%&gt;%</span> <span class="va">length</span></span>
<span><span class="va">sd_CL</span> <span class="op">&lt;-</span> <span class="va">urban_gradient</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_column</span><span class="op">(</span>eps <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">ols_ug</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">msa</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">distance</span> <span class="op">-</span> <span class="va">dist_mean</span><span class="op">)</span> <span class="op">*</span> <span class="va">eps</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">Sxx</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">pull</span></span>
<span><span class="va">sd_CL</span></span>
<span><span class="co">## [1] 0.006035</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This time, we get a much higher estimate of the standard deviation of the slope (about three times larger than the one obtained with the simple formula).</p>
<p></p>
</section><section id="multiple-linear-model" class="level3"><h3 class="anchored" data-anchor-id="multiple-linear-model">Multiple linear model</h3>
<p> Consider now the multiple regression model. The vector of slopes can be written as a linear combination of the response and the error vectors:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\hat{\beta}&amp;=&amp;(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top \tilde{y} \\
&amp;=&amp;(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top(\tilde{X}\beta+\epsilon)\\
&amp;=&amp;\beta+(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top\epsilon \\
\end{array}
\]</span></p>
<p><span class="math inline">\(\tilde{X} ^ \top\epsilon\)</span> is a <span class="math inline">\(K\)</span>-length vector containing the product of the covariates (the column of <span class="math inline">\(X\)</span>) in deviation from their sample mean and the vector of errors:</p>
<p><span class="math display">\[
\tilde{X} ^ \top\epsilon =
\left(
\begin{array}{c}
\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n \\
\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n \\
\vdots \\
\sum_{n=1} ^ N (x_{1n} - \bar{x}_K) \epsilon_n
\end{array}
\right)
= \sum_{n = 1} ^ N \psi_n
\]</span></p>
<p><span class="math inline">\(\psi_n\)</span> is called the vector of score, as it is proportional to the vector of the first derivatives of the sum of square residuals and is therefore equal to 0 when evaluated for <span class="math inline">\(\hat{\beta}\)</span>, the OLS estimator. The general form of the covariance of the OLS estimates was given in <a href="multiple_regression.html#eq-general_variance">Equation&nbsp;<span>3.16</span></a>:</p>
<p><span class="math display">\[
\hat{V}(\hat{\beta}) = \frac{1}{N}
\left(\frac{1}{N}
\tilde{X} ^ \top \tilde{X}\right)^{-1}\frac{1}{N}\mbox{E}(\tilde{X} ^ \top \epsilon \epsilon ^
\top \tilde{X} \mid X) \left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}
\]</span></p>
<p>This is a sandwich formula, the meat (the variance of the score) being surrounded by two slices of bread (the inverse of the covariance matrix of the covariates). Remember from <a href="multiple_regression.html#sec-variance_ols"><span>Section&nbsp;3.5.2</span></a> that the meat can be written, for <span class="math inline">\(K = 2\)</span>, as the expected value of:</p>
<p><span class="math display">\[
\scriptsize
{
\frac{1}{N}
\left(
\begin{array}{cccc}
\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) ^ 2 &amp;
\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right)
\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right)  \\
\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right)
\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right) &amp;
\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right) ^ 2
\end{array}
\right)
}
\]</span></p>
<p>If the errors are uncorrelated, but potentially heteroskedastic, we use the following estimation: </p>
<p><span class="math display">\[
\frac{1}{N}
\sum_{n = 1} ^ N
\hat{\epsilon}_n ^ 2
\left(
\begin{array}{cccc}
(x_{1n} - \bar{x}_1) ^ 2 &amp;
(x_{1n} - \bar{x}_1) (x_{2n} - \bar{x}_2) \\
(x_{1n} - \bar{x}_1) (x_{2n} - \bar{x}_2) &amp;
(x_{2n} - \bar{x}_2) ^ 2 \\
\end{array}
\right)
\]</span></p>
<p>which generalizes the scalar case of the heteroskedastic covariance matrix computed for the single regression case. This estimator of the variance of the score can easily be obtained by defining the <strong>estimating function</strong> <span class="math inline">\(F\)</span> which is obtained by multiplying (elements by elements) the columns of the matrix of covariates (in deviation from the sample means) by the vector of residuals: </p>
<p><span class="math display">\[
F =
\left(
\begin{array}{ccc}
\hat{\epsilon}_1 x_{11} &amp; \ldots &amp; \hat{\epsilon}_1 \tilde{x}_{1K} \\
\hat{\epsilon}_2 x_{21} &amp; \ldots &amp; \hat{\epsilon}_2 \tilde{x}_{2K} \\
\vdots &amp; \ddots &amp; \vdots\\
\hat{\epsilon}_N x_{N1} &amp; \ldots &amp; \hat{\epsilon}_N \tilde{x}_{NK}
\end{array}
\right)
\]</span> Then <span class="math inline">\(F ^ \top F = \sum_{n=1} ^ N \hat{\epsilon}_n ^ 2 \tilde{x}_n \tilde{x}_n ^ \top\)</span> is <span class="math inline">\(N\)</span> times the HC estimator of the meat and the HC estimator is then:</p>
<p><span id="eq-sandwich_hc_multi"><span class="math display">\[
\hat{V}(\hat{\beta}) = \frac{1}{N}\left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}
\left(\frac{1}{N}\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2 \tilde{x}_n \tilde{x}_n ^ \top\right)
\left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}
\tag{6.11}\]</span></span> To get the clustered estimator of the variance, we define for each cluster <span class="math inline">\(\psi_g = \sum_{n \in g} \psi_n\)</span>, and the clustered expression of the meat is obtained as the sum of the outer products of <span class="math inline">\(\psi_g\)</span> divided by <span class="math inline">\(N\)</span>:</p>
<p><span id="eq-clustered_meat"><span class="math display">\[
\frac{1}{N} \sum_{g=1} ^ G \hat{\psi}_g \hat{\psi}_g ^ \top
\tag{6.12}\]</span></span></p>
<p>Going back to the electricity consumption estimation, <code>lm_elec</code> is the model fitted by OLS. We first compute the estimator manually, computing the meat (the cross-products of the model matrix multiplied by the vector of residuals divided by the sample size) and the bread (the inverse of the cross-products of the model matrix divided by the sample size). The covariance matrix is then computed using <a href="#eq-sandwich_hc_multi">Equation&nbsp;<span>6.11</span></a>, and we extract the standard errors of the estimates: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">N_el</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span> <span class="op">*</span> <span class="va">Z</span><span class="op">)</span> <span class="op">/</span> <span class="va">N_el</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op">/</span> <span class="va">N_el</span><span class="op">)</span></span>
<span><span class="op">(</span><span class="va">B</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">M</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">B</span> <span class="op">/</span> <span class="va">N_el</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="co">## (Intercept)         inc    I(1/mc6)        gas6         cap </span></span>
<span><span class="co">##    458.5529      0.2012    157.5829     31.5546     91.6099</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> The <strong>sandwich</strong> package <span class="citation" data-cites="ZEIL:04 ZEIL:06 ZEIL:KOLL:NATH:20">(<a href="#ref-ZEIL:04" role="doc-biblioref">Zeileis 2004</a>, <a href="#ref-ZEIL:06" role="doc-biblioref">2006</a>; <a href="#ref-ZEIL:KOLL:NATH:20" role="doc-biblioref">Zeileis, Köll, and Graham 2020</a>.)</span>, provides specialized functions to construct the different pieces of the estimator, namely <code>estfun</code>, <code>meat</code> and <code>bread</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span></span>
<span><span class="cn">F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/estfun.html">estfun</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/meat.html">meat</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/bread.html">bread</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span></span>
<span><span class="op">(</span><span class="va">B</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">M</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">B</span> <span class="op">/</span> <span class="va">N_el</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="co">## (Intercept)         inc    I(1/mc6)        gas6         cap </span></span>
<span><span class="co">##    458.5529      0.2012    157.5829     31.5546     91.6099</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">sandwich::vcovHC</a></code> uses these functions to compute the HC estimator; its first argument is a fitted model and a <code>type</code> argument can also be supplied. Setting <code>type</code> to <code>"HC0"</code> gives the simplest version of the estimator, the one we have previously computed “by hand”. Other flavors of this heteroskedasticity-robust estimator can be obtained by setting the <code>type</code> to <code>"HC1"</code>, <span class="math inline">\(\ldots\)</span>, <code>"HC5"</code> to perform different flavors of degrees of freedom correction. For example, when <code>type = "HC1"</code> (which is the default), the covariance matrix is multiplied by <span class="math inline">\(N / (N - K)\)</span>:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_elec</span>, type <span class="op">=</span> <span class="st">"HC0"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="co">## (Intercept)         inc    I(1/mc6)        gas6         cap </span></span>
<span><span class="co">##    458.5529      0.2012    157.5829     31.5546     91.6099</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comparing the standard and the robust estimation of the standard deviations of the estimates, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">robust_sd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_elec</span>, type <span class="op">=</span> <span class="st">"HC0"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="va">standard_sd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>standard <span class="op">=</span> <span class="va">standard_sd</span>, robust <span class="op">=</span> <span class="va">robust_sd</span>, </span>
<span>      ratio <span class="op">=</span> <span class="va">robust_sd</span> <span class="op">/</span> <span class="va">standard_sd</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         (Intercept)    inc I(1/mc6)    gas6     cap
standard    498.0150 0.1819 164.8722 34.2927 98.6943
robust      458.5529 0.2012 157.5829 31.5546 91.6099
ratio         0.9208 1.1060   0.9558  0.9202  0.9282</code></pre>
</div>
</div>
<p>In this example, the robust standard errors are very close to the ones obtained using the simple formula, although the Breusch-Pagan test rejected the hypothesis of homoskedasticity. This is because the source of heteroskedasticity (the number of customers) is not a covariate of the model.</p>
<p>To illustrate the use of the clustered sandwich estimator, we use the <code>twins</code> data set. <code>lm_twins</code> is the OLS estimation of the Mincer equation. In this context, the meat can easily be computed using the <code>apply</code> and <code>tapply</code> functions:</p>
<ul>
<li>
<code>apply</code> performs an operation on one of the margins (1 for rows, 2 for columns) of a matrix,</li>
<li>
<code>tapply</code> performs an operation (the third argument) on a vector (the first argument) conditional on the values of another vector (the second argument). </li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">lm_twins</span><span class="op">)</span></span>
<span><span class="va">e</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lm_twins</span><span class="op">)</span></span>
<span><span class="va">id</span> <span class="op">&lt;-</span> <span class="va">twins</span><span class="op">$</span><span class="va">family</span></span>
<span><span class="va">Ze</span> <span class="op">&lt;-</span> <span class="va">Z</span> <span class="op">*</span> <span class="va">e</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">Ze</span>, <span class="fl">2</span>, <span class="va">tapply</span>, <span class="va">id</span>, <span class="va">sum</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">N_tw</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op">/</span> <span class="va">N_tw</span><span class="op">)</span></span>
<span><span class="va">V_1</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">B</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">M</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">B</span> <span class="op">/</span> <span class="va">N_tw</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can more simply use the <code><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">sandwich::vcovCL</a></code> function <span class="citation" data-cites="ZEIL:KOLL:NATH:20">(<a href="#ref-ZEIL:KOLL:NATH:20" role="doc-biblioref">Zeileis, Köll, and Graham 2020</a>)</span> to compute the clustered estimator. The clustering variable is defined using the <code>cluster</code> argument that can be set to a one-sided formula. The <code>type</code> argument is similar to the one of <code>vcovHC</code> and there is also a <code>cadjust</code> argument which, if <code>TRUE</code>, multiplies the covariance matrix by <span class="math inline">\(G / (G - 1)\)</span>, <span class="math inline">\(G\)</span> being the number of clusters. The default behavior of <code>vcovCL</code> is to set <code>cadjust</code> to <code>TRUE</code> and <code>type</code> to <code>"HC1"</code>, so that the adjustment is done for the number of observations and for the number of groups. For the <code>twins</code> data set, the clustering variable is <code>family</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">V_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">lm_twins</span>, <span class="op">~</span> <span class="va">family</span>, type <span class="op">=</span> <span class="st">"HC0"</span>, cadjust <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, <code>twins</code> being a pseudo-panel, the <code>plm</code> package can also be used. The OLS estimate can be fitted with <code><a href="https://rdrr.io/pkg/plm/man/plm.html">plm::plm</a></code> by setting the <code>model</code> argument to <code>"pooling"</code>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">plm_twins</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/plm/man/plm.html">plm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">educ</span>, <span class="va">twins</span>, </span>
<span>                 model <span class="op">=</span> <span class="st">"pooling"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A <code>plm</code> object is returned and the <code>vcovHC</code> method for a <code>plm</code> object returns by default the same clustered covariance matrix as previously: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">V_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">plm_twins</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>vcovHC</code> and <code>vcovCL</code> functions are particularly useful while using the testing functions of the <strong>lmtest</strong> package. For example, <code><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">lmtest::coeftest</a></code> computes the usual table of coefficients (the same as the one obtained using <code>summary</code>), but a matrix or a function that computes a covariance matrix can be passed as a supplementary argument. Therefore, <code>lmtest::coeftest(lm_twins)</code> returns exactly the same table of coefficients as <code>summary(lm_twins)</code>, but other standard errors are obtained by filling the second argument. For example, to get the clustered standard errors with our preferred specification without degrees of freedom correction (<code>type = "HC0", cadjust = FALSE</code>), we can use any of the following three equivalent syntaxes: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">lm_twins</span>, <span class="op">~</span> <span class="va">family</span>, cadjust <span class="op">=</span> <span class="cn">FALSE</span>, type <span class="op">=</span> <span class="st">"HC0"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_twins</span>, <span class="va">cl</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_twins</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> </span>
<span>  <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">x</span>, <span class="op">~</span> <span class="va">family</span>, cadjust <span class="op">=</span> <span class="cn">FALSE</span>, type <span class="op">=</span> <span class="st">"HC0"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_twins</span>, <span class="va">vcovCL</span>, cluster <span class="op">=</span> <span class="op">~</span> <span class="va">family</span>, </span>
<span>         cadjust <span class="op">=</span> <span class="cn">FALSE</span>, type <span class="op">=</span> <span class="st">"HC0"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the first expression, we provide a matrix that was previously computed. In the second expression, we use an anonymous function with our preferred options. In the last one, the two arguments of <code>vcovCL</code> are indicated as arguments of <code>coeftest</code> and are passed internally to <code>vcovCL</code>. We finally compare the ordinary and the robust estimates of the standard errors of the OLS estimate: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ord_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">lm_twins</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="va">cl_se</span> <span class="op">&lt;-</span> <span class="va">V_1</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>ordinary <span class="op">=</span> <span class="va">ord_se</span>, cluster <span class="op">=</span> <span class="va">cl_se</span>, </span>
<span>ratio <span class="op">=</span> <span class="va">cl_se</span> <span class="op">/</span> <span class="va">ord_se</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         (Intercept) poly(age, 2)1 poly(age, 2)2    educ
ordinary      0.1516        0.5451        0.5309 0.01059
cluster       0.1620        0.5744        0.6000 0.01103
ratio         1.0685        1.0539        1.1300 1.04106</code></pre>
</div>
</div>
<p>As for the electricity consumption example, we can see that the robust standard errors are almost the same as those computed using the simple formula.</p>
<!-- Finally, for our panel data example, we get: -->
<!-- ```{r} -->
<!-- ols_q <- plm(ikn ~ qn, tobinq, model = "pooling") -->
<!-- ord_sd <- vcov(ols_q) %>% stder -->
<!-- cl_sd <- vcovHC(ols_q) %>% stder -->
<!-- rbind(ordinary = ord_sd, cluster = cl_sd,  -->
<!-- ratio = cl_sd / ord_sd) -->
<!-- ``` -->
<!-- and in this case the robust standard error is much larger (about 4 times) than the simple one. -->
<p> </p>
<div style="page-break-after: always;"></div>
</section></section><section id="sec-gls" class="level2" data-number="6.4"><h2 data-number="6.4" class="anchored" data-anchor-id="sec-gls">
<span class="header-section-number">6.4</span> Generalized least squares estimator</h2>
<p></p>
<p>For a linear model <span class="math inline">\(y_n = \alpha + \beta^\top x_n + \epsilon_n = \gamma ^ \top z_n + \epsilon_n\)</span> with non-spherical disturbances (<span class="math inline">\(\Omega \neq \sigma_\epsilon ^ 2 I\)</span>), the OLS estimator is no longer BLUE. A more efficient estimator called the <strong>generalized least squares</strong> (<strong>GLS</strong>) can then be used instead.</p>
<section id="general-formulation-of-the-gls-estimator" class="level3"><h3 class="anchored" data-anchor-id="general-formulation-of-the-gls-estimator">General formulation of the GLS estimator</h3>
<p>The GLS estimator is:</p>
<p><span id="eq-glsmatrix"><span class="math display">\[
\hat{\gamma} = (Z^\top\Omega^{-1}X) ^ {-1} Z^\top \Omega ^ {-1} y
\tag{6.13}\]</span></span></p>
<p>Replacing <span class="math inline">\(y\)</span> by <span class="math inline">\(Z\gamma+\epsilon\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\gamma}=(Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} (Z\gamma + \epsilon) = \gamma + (Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \epsilon
\]</span> As for the OLS estimator, the estimator is unbiased if <span class="math inline">\(\mbox{E}\left(\epsilon\mid Z\right)=0\)</span>. The variance is:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mbox{V}(\hat{\gamma}) &amp;=&amp; \mbox{E}\left[(Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \epsilon\epsilon^\top \Omega^{-1}Z(Z^\top \Omega ^ {-1} Z)^{-1}\right]\\
&amp;=&amp; (Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \mbox{E}(\epsilon\epsilon ^ \top) \Omega Z (Z^\top\Omega^{-1}Z) ^ {-1}\\
&amp;=&amp;(Z^\top\Omega^{-1}Z)^{-1}
\end{array}
\]</span></p>
<p>Written this way, the GLS estimator is unfeasible for two reasons:</p>
<ul>
<li>the <span class="math inline">\(\Omega\)</span> matrix is a square matrix of dimension <span class="math inline">\(N\times N\)</span>, and it is computationally difficult (or impossible) to store and to invert for large samples,</li>
<li>it uses a matrix <span class="math inline">\(\Omega\)</span> which contains <span class="math inline">\(N(N+1) / 2\)</span> unknown parameters.</li>
</ul>
<p>A <strong>feasible GLS</strong> estimator is obtained by imposing some structure on <span class="math inline">\(\Omega\)</span> so that the number of unknown parameters becomes much less than <span class="math inline">\(N(N+1)/2\)</span> and by estimating these unknown parameters using residuals of a first step consistent estimation. Actually, in practice, the GLS estimator is obtained by performing OLS on transformed data. More precisely, consider the matrix <span class="math inline">\(C\)</span> such that <span class="math inline">\(C ^ \top C = \Omega ^ {-1}\)</span>. Then, <a href="#eq-glsmatrix">Equation&nbsp;<span>6.13</span></a> can be rewritten, denoting <span class="math inline">\(w^*=Cw\)</span>:</p>
<p><span id="eq-glstrans"><span class="math display">\[
\hat{\gamma} = (Z^\top C^\top CZ) ^ {-1} Z^\top C^\top C y = \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}y^*
\tag{6.14}\]</span></span></p>
<p>which is the OLS estimator of the linear model: <span class="math inline">\(y^* = Z^*\gamma + \epsilon^*\)</span>, with <span class="math inline">\(\epsilon^* = C\epsilon\)</span>. Replacing <span class="math inline">\(y^*\)</span> by <span class="math inline">\(Z^*\gamma + \epsilon^*\)</span> in <a href="#eq-glstrans">Equation&nbsp;<span>6.14</span></a>, we get:</p>
<p><span class="math display">\[
\hat{\gamma} = \gamma + \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}C\epsilon
\]</span></p>
<p>And the variance of the estimator is:</p>
<p><span id="eq-glsvar"><span class="math display">\[
\mbox{V}(\hat{\gamma}) = \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}C\Omega^{-1}C^\top Z^*\left(Z^{*\top}Z^*\right)^{-1}
\tag{6.15}\]</span></span></p>
<p>But <span class="math inline">\(C\Omega^{-1}C^\top = C (C^\top C)^{-1} C^\top = C C ^ {-1} C^{\top-1}C^\top=I\)</span><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, and therefore, <a href="#eq-glsvar">Equation&nbsp;<span>6.15</span></a> simplifies to:</p>
<p><span class="math display">\[
\mbox{V}(\hat{\gamma}) = \left(Z^{*\top}Z^*\right)^{-1}
\]</span></p>
<p>which is very similar to the formula used for the OLS estimator. Note that <span class="math inline">\(\sigma_\epsilon^2\)</span> doesn’t appear in this formula because the variance of the transformed errors is 1.</p>
</section><section id="weighted-least-squares" class="level3"><h3 class="anchored" data-anchor-id="weighted-least-squares">Weighted least squares</h3>
<p> </p>
<p>With heteroskedastic, but uncorrelated errors, <span class="math inline">\(\Omega\)</span> is diagonal and each element is the specific variance of one observation. From <a href="#eq-matheterosc">Equation&nbsp;<span>6.1</span></a>, it is obvious that the transformation matrix <span class="math inline">\(C\)</span> can be written as:</p>
<p><span class="math display">\[
C=
\left(
\begin{array}{ccccc}
1 / \sigma_{1} &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 / \sigma_{2} &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 1 / \sigma_N
\end{array}
\right)
\]</span></p>
<p>and therefore, premultiplying any vector by <span class="math inline">\(C\)</span> leads to a transformed vector where each value is divided by the standard deviation of the corresponding error: <span class="math inline">\(z^{*\top} = (z_1 / \sigma_1, z_2 / \sigma_2, \ldots, z_N / \sigma_N)\)</span>. Performing OLS on the transformed data, we get the <strong>weighted-least square</strong> (<strong>WLS</strong>) estimator. The name of this estimator comes from the fact that the estimator can be obtained by minimizing <span class="math inline">\(\sum_n \epsilon_n ^ 2 / \sigma_n ^ 2\)</span>, i.e., by minimizing not the sum of the squares of the residuals, but a linear combination of the squares of the residuals, the weight of each observation being <span class="math inline">\(1 / \sigma_n ^ 2\)</span>. Therefore, an observation <span class="math inline">\(n\)</span> for which <span class="math inline">\(\sigma_n^2\)</span> is high will receive a smaller weight in WLS compared to OLS. The weights are unknown and therefore need to be estimated. The simplest solution is to assume that the variance (or the standard deviation) of the errors is proportional to an observed variable (which may or may not be a covariate of the regression). We then get either <span class="math inline">\(\sigma_n ^ 2 = \sigma ^ 2 w_n\)</span> or <span class="math inline">\(\sigma_n ^ 2 = \sigma ^ 2 w_n ^ 2\)</span> and the weights are then respectively <span class="math inline">\(1 / w_n\)</span> or <span class="math inline">\(1 / w_n ^ 2\)</span>. The WLS estimator can then be obtained by OLS with all the variables divided either by <span class="math inline">\(\sqrt{w_n}\)</span> or <span class="math inline">\(w_n\)</span>. A more general solution is to assume a functional form for the skedastic function: <span class="math inline">\(\sigma_n ^ 2 = h(\delta^\top w_n)\)</span> where <span class="math inline">\(h\)</span> is a monotonous increasing function that returns only positive values, <span class="math inline">\(w\)</span> is a set of covariates and <span class="math inline">\(\delta\)</span> a vector of parameters. If, for example, <span class="math inline">\(h\)</span> is the exponential function (which is a very common choice), the skedastic function is: <span class="math inline">\(\ln \sigma_n ^ 2 = \delta^\top w_n\)</span> and <span class="math inline">\(\delta\)</span> can be consistently estimated by performing the following regression:</p>
<p><span id="eq-skedeq"><span class="math display">\[
\ln \hat{\epsilon}_n ^ 2 = \delta ^ \top w_n + \nu_n
\tag{6.16}\]</span></span></p>
<p>where <span class="math inline">\(\hat{\epsilon}\)</span> are the OLS residuals which are consistent estimates of the errors. The WLS estimator is then performed in three steps:</p>
<ul>
<li>estimate the model by OLS and retrieve the vector of residuals <span class="math inline">\(\hat{\epsilon}\)</span>,</li>
<li>estimate <span class="math inline">\(\hat{\gamma}\)</span> by using OLS on <a href="#eq-skedeq">Equation&nbsp;<span>6.16</span></a> and compute <span class="math inline">\(\hat{\sigma}_n ^ 2 = e^{\hat{\gamma} w_n}\)</span>,</li>
<li>divide every variable (the response and the covariates) by <span class="math inline">\(\hat{\sigma}_n\)</span> and perform OLS on the transformed variables.</li>
</ul>
<p>Note that there is no intercept in the third estimation as the “covariate” associated with the intercept (a vector of 1) becomes a vector with typical element <span class="math inline">\(1/\hat{\sigma}_n\)</span>.</p>
<p>For the electricity consumption example, we know that the unconditional variance of <span class="math inline">\(y\)</span> in city <span class="math inline">\(n\)</span> is <span class="math inline">\(\sigma_{yn} ^ 2 = \sigma_c ^ 2 / I_n\)</span>, <span class="math inline">\(I_n\)</span> being the number of consumption units in city <span class="math inline">\(n\)</span> and <span class="math inline">\(\sigma_c ^ 2\)</span> the variance of the individual consumption. Assuming that the same relation applies for the conditional variance, then <span class="math inline">\(\sigma_{\epsilon n} ^ 2 = \sigma ^ 2 / I_n\)</span> and the WLS estimator can then be obtained by computing OLS on series multiplied by <span class="math inline">\(\sqrt{I_n}\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wls_elec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">kwh</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">cust</span><span class="op">)</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">cust</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">inc</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">cust</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>                 <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">mc6</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">cust</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>  <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">gas6</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">cust</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>                 <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">cap</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">cust</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span>, <span class="va">uk_elec</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Or more simply by setting the <code>weights</code> argument of <code>lm</code> to <code>cust</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wls_elec2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">kwh</span> <span class="op">~</span> <span class="va">inc</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">mc6</span><span class="op">)</span> <span class="op">+</span>  <span class="va">gas6</span> <span class="op">+</span> <span class="va">cap</span>, <span class="va">uk_elec</span>, </span>
<span>                weights <span class="op">=</span> <span class="va">cust</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comparing the robust standard errors of the OLS estimator and those of the WLS estimator, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">std_ols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_elec</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="va">std_wls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">wls_elec2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="fu">tibble</span><span class="op">(</span>ols <span class="op">=</span> <span class="va">std_ols</span>, wls <span class="op">=</span> <span class="va">std_wls</span>, ratio <span class="op">=</span> <span class="va">wls</span> <span class="op">/</span> <span class="va">ols</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 3
      ols     wls ratio
    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
1 535.    310.    0.581
2   0.256   0.201 0.784
3 195.    125.    0.639
4  37.9    21.2   0.559
5 120.     61.9   0.515</code></pre>
</div>
</div>
<p>The efficiency gain of using WLS is substantial, as the standard errors reduce by about 25$-$50% depending on the coefficient.</p>
<p> </p>
</section><section id="sec-error_component_gls" class="level3"><h3 class="anchored" data-anchor-id="sec-error_component_gls">Error component model</h3>
<p> </p>
<p>The error component model is suitable for panel data or pseudo panel data. In the remainder of this section, we’ll mention individual means, which is the proper term for panel data, but should be replaced by entity means for pseudo-panel data. Remember that for the error component model, <a href="#eq-poweromegaec">Equation&nbsp;<span>6.3</span></a> is a general formula that can be used to compute any power of <span class="math inline">\(\Omega\)</span>, and <span class="math inline">\(C\)</span> is in this context obtained by taking <span class="math inline">\(r=-0.5\)</span>:</p>
<p><span id="eq-Cerrcomp"><span class="math display">\[
C = \Omega ^ {-0.5} = \frac{1}{\sigma_\nu} W + \frac{1}{\sigma_\iota} B = \frac{1}{\sigma_\nu}\left(W + \frac{\sigma_\nu}{\sigma_\iota} B\right)
\tag{6.17}\]</span></span></p>
<p>As <span class="math inline">\(W = I - B\)</span>, <span class="math inline">\(C\)</span> can also be rewritten as:</p>
<p><span id="eq-Cerrcomp2"><span class="math display">\[
C = \Omega ^ {-0.5} = \frac{1}{\sigma_\nu}\left(I - \left[1 -\frac{\sigma_\nu}{\sigma_\iota}\right] B\right) = \frac{1}{\sigma_\nu}(I - \theta B)
\tag{6.18}\]</span></span></p>
<p>with <span class="math inline">\(\theta = 1 - \frac{\sigma_\nu}{\sigma_\iota}\)</span>. <span class="math inline">\(\theta\)</span> can be further written as:</p>
<p><span class="math display">\[
\theta = 1 - \frac{\sigma_\nu}{\sqrt{T \sigma_\eta^2 + \sigma_\nu ^ 2}} = 1 - \frac{1}{\sqrt{T \sigma_\eta ^ 2 / \sigma_\nu^2+1}}
\]</span> Therefore <span class="math inline">\(0\leq \theta \leq 1\)</span>, so that the <span class="math inline">\(\sigma_\nu C\)</span> matrix performs, in this context, a quasi-difference from the individual mean:</p>
<p><span class="math display">\[
z_n ^ * = z_{nt} - \theta \bar{z}_{n.}
\]</span></p>
<p>The share of the individual mean that is subtracted depends on:</p>
<ul>
<li>the relative weights of the two variances: <span class="math inline">\(\theta \rightarrow 0\)</span> when <span class="math inline">\(\sigma_\eta ^ 2 / \sigma_\nu ^ 2 \rightarrow 0\)</span>, which means that there are no individual effects. As <span class="math inline">\(\sigma_\eta ^ 2 / \sigma_\nu ^ 2 \rightarrow + \infty\)</span>, <span class="math inline">\(\theta \rightarrow 1\)</span> and the transformation is the difference from the individual mean;</li>
<li>the number of observations for each individual, <span class="math inline">\(\theta \rightarrow 1\)</span> when <span class="math inline">\(T\rightarrow + \infty\)</span>; therefore the transformation is close to a difference from individual mean for large <span class="math inline">\(T\)</span>.</li>
</ul>
<p><span class="math inline">\(\sigma_\nu\)</span> and <span class="math inline">\(\sigma_\eta\)</span> are unknown parameters and have to be estimated. Consider the errors of the model <span class="math inline">\(\epsilon_{nt}\)</span>, their individual mean <span class="math inline">\(\bar{\epsilon}_{n.}\)</span> and the deviations from these individual means <span class="math inline">\(\epsilon_{nt} - \bar{\epsilon}_{n.}\)</span>. By hypothesis, we have: <span class="math inline">\(\mbox{V}\left(\epsilon_{nt}\right)=\sigma_\nu^2+\sigma_\eta^2\)</span>. For the individual means, we get:</p>
<p><span class="math display">\[
\bar{\epsilon}_{n.}=\frac{1}{T}\sum_{t=1}^T \epsilon_{nt} = \eta_n +
\frac{1}{T}\sum_{t=1}^T \nu_{nt}
\]</span></p>
<p>for which the variance is:</p>
<p><span class="math display">\[
\mbox{V}\left(\bar{\epsilon}_{n.}\right)=\sigma_{\eta}^2 + \frac{1}{T}
\sigma_{\nu}^2 = \sigma_\iota^2 / T
\]</span></p>
<p>The variance of the deviation from the individual means is easily obtained by isolating terms in <span class="math inline">\(\epsilon_{nt}\)</span>:</p>
<p><span class="math display">\[
\epsilon_{nt} - \bar{\epsilon}_{n.}=\epsilon_{nt}-\frac{1}{T}\sum_{t=1}^T
\epsilon_{nt}=\left(1-\frac{1}{T}\right)\epsilon_{nt}-
\frac{1}{T}\sum_{s \neq t} \epsilon_{ns}
\]</span></p>
<p>The variance is, noting that the sum now contains <span class="math inline">\(T-1\)</span> terms:</p>
<p><span class="math display">\[
\mbox{V}\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right) =
\left(1-\frac{1}{T}\right)^2\sigma_{\nu}^2+
\frac{1}{T^2}(T-1)\sigma_{\nu}^2 = \frac{T-1}{T}\sigma_\nu ^ 2
\]</span> If <span class="math inline">\(\epsilon\)</span> were known, natural estimators of these two variances <span class="math inline">\(\sigma_{\iota}^2\)</span> and <span class="math inline">\(\sigma_{\nu}^2\)</span> would be:</p>
<p><span id="eq-varmxt"><span class="math display">\[
\hat{\sigma}_\iota^2 = T \frac{\sum_{n=1}^N\bar{\epsilon}_{n.}^2}{N} =
T \frac{\sum_{n=1}^N\sum_{t=1}^T\bar{\epsilon}_{n.}^2}{NT}=T\frac{\epsilon^{\top}B\epsilon}{NT}=\frac{\epsilon^{\top}B\epsilon}{N}
\tag{6.19}\]</span></span></p>
<p><span id="eq-varidios"><span class="math display">\[
  \hat{\sigma}_{\nu}^2 = \frac{T}{T-1}
  \frac{\sum_{n=1}^N\sum_{t=1}^T\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right)^2}{NT}
  =\frac{\sum_{n=1}^N\sum_{t=1}^T\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right)^2}{N(T-1)}
    = \frac{\epsilon^{\top}W \epsilon}{N(T-1)}
\tag{6.20}\]</span></span></p>
<p>Several estimators of the two components of the variance have been proposed in the literature. They all consist of replacing <span class="math inline">\(\epsilon_{nt}\)</span> in the previous two equations by consistent estimates (and for some of them by applying some degrees of freedom correction). The estimator proposed by <span class="citation" data-cites="WALL:HUSS:69">Wallace and Hussain (<a href="#ref-WALL:HUSS:69" role="doc-biblioref">1969</a>)</span> is particularly simple because it uses the residuals of the OLS estimation.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>The residuals of the OLS regression were already added to the <code>twins</code> data set and <span class="math inline">\(B \hat{\epsilon}\)</span> was computed. We then compute <span class="math inline">\(W\hat{\epsilon} = \hat{\epsilon} - B \hat{\epsilon}\)</span> and we use <a href="#eq-varmxt">Equation&nbsp;<span>6.19</span></a> and <a href="#eq-varidios">Equation&nbsp;<span>6.20</span></a> to compute <span class="math inline">\(\hat{\sigma}_\iota ^ 2\)</span>, <span class="math inline">\(\hat{\sigma}_\nu ^ 2\)</span>, <span class="math inline">\(\theta = 1 - \hat{\sigma}_\nu / \hat{\sigma}_\iota\)</span> and then <span class="math inline">\(\hat{\sigma}_\eta ^ 2 = (\hat{\sigma}_\iota ^ 2 - \hat{\sigma}_\nu^2) / T\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">twins</span> <span class="op">&lt;-</span> <span class="va">twins</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>We <span class="op">=</span> <span class="va">e</span> <span class="op">-</span> <span class="va">Be</span><span class="op">)</span></span>
<span><span class="va">sigs</span> <span class="op">&lt;-</span> <span class="va">twins</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span>s2iota <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Be</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">N_tw</span>, </span>
<span>            s2nu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">We</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">N_tw</span> <span class="op">*</span> <span class="op">(</span><span class="va">T_tw</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>s2eta <span class="op">=</span> <span class="op">(</span><span class="va">s2iota</span> <span class="op">-</span> <span class="va">s2nu</span><span class="op">)</span> <span class="op">/</span> <span class="va">T_tw</span>,</span>
<span>         theta <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">s2nu</span> <span class="op">/</span> <span class="va">s2iota</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sigs</span></span>
<span><span class="co">## # A tibble: 1 × 4</span></span>
<span><span class="co">##   s2iota  s2nu  s2eta theta</span></span>
<span><span class="co">##    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">## 1  0.316 0.238 0.0389 0.132</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>plm</code> function performs the GLS estimation, i.e., an OLS regression on data transformed using quasi-differences (<span class="math inline">\(w_{nt} ^ * = w_{nt} - \theta \bar{w}_{n.}\)</span>) if the <code>model</code> argument is set to <code>"random"</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gls_twins</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/plm/man/plm.html">plm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">educ</span>, <span class="va">twins</span>, </span>
<span>                 random.method <span class="op">=</span> <span class="st">"walhus"</span>, model  <span class="op">=</span> <span class="st">"random"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gls_twins</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Oneway (individual) effect Random Effect Model 
   (Wallace-Hussain's transformation)

Call:
plm(formula = log(earning) ~ poly(age, 2) + educ, data = twins, 
    model = "random", random.method = "walhus")

Balanced Panel: n = 214, T = 2, N = 428

Effects:
                 var std.dev share
idiosyncratic 0.2380  0.4878  0.86
individual    0.0389  0.1972  0.14
theta: 0.132

Residuals:
   Min. 1st Qu.  Median 3rd Qu.    Max. 
-2.9921 -0.2492 -0.0325  0.1922  2.3954 

Coefficients:
              Estimate Std. Error z-value Pr(&gt;|z|)    
(Intercept)     1.0642     0.1573    6.76  1.3e-11 ***
poly(age, 2)1   0.0355     0.5811    0.06  0.95129    
poly(age, 2)2  -1.9428     0.5668   -3.43  0.00061 ***
educ            0.0746     0.0110    6.79  1.1e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Total Sum of Squares:    117
Residual Sum of Squares: 102
R-Squared:      0.133
Adj. R-Squared: 0.127
Chisq: 65.1443 on 3 DF, p-value: 4.67e-14</code></pre>
</div>
</div>
<p>Note that we set the <code>random.method</code> argument to <code>"walhus"</code> to select the Wallace and Hussain estimator. The output is quite similar to the one for <code>lm</code> objects, except the two parts that appear at the beginning. The dimensions of the panel are indicated (number of individuals / entities and number of time series / observations in each entity) and whether the data set is balanced or not. A panel data is balanced if all the individuals are observed for the same set of time periods. In a pseudo-panel (which is the case here), the data set is balanced if there is the same number of observations for each entity (which is obviously the case for our sample of twins). This information can be obtained using <code>pdim</code>. The first argument is a data frame, the second one, called <code>index</code>, is a vector of two characters indicating the name of the individual and of the time index. It can be omitted if the first two columns contains these indexes, which is the case for the <code>twins</code> data set: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/pdim.html">pdim</a></span><span class="op">(</span><span class="va">twins</span>, index <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"family"</span>, <span class="st">"twin"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## Balanced Panel: n = 214, T = 2, N = 428</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The second specific part of the output gives information about the variances of the two components of the error. We can see here that the individual effects (in this example, a family effect) account for only 15% of the total variance of the error. Therefore, only a small part of the individual mean is removed while performing GLS (14.3%). This information can be obtained using the <code>ercomp</code> function: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/ercomp.html">ercomp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">educ</span>, <span class="va">twins</span>, method <span class="op">=</span> <span class="st">"walhus"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The model can also be estimated by maximum likelihood, using <code><a href="https://rdrr.io/pkg/pglm/man/pglm.html">pglm::pglm</a></code>. This function adapts the behavior of the <code><a href="https://rdrr.io/r/stats/glm.html">stats::glm</a></code> function which fits generalized linear model for panel data. In particular, it has a <code>family</code> argument that is set to <code>"gaussian"</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ml_twins</span> <span class="op">&lt;-</span> <span class="fu">pglm</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/pglm/man/pglm.html">pglm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">educ</span>, </span>
<span>                       <span class="va">twins</span>, family <span class="op">=</span> <span class="va">gaussian</span><span class="op">)</span></span>
<span><span class="va">ml_twins</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Estimate Std. Error z-value Pr(&gt;|z|)
poly(age, 2)1   0.0352     0.5789    0.06  0.95149
poly(age, 2)2  -1.9429     0.5646   -3.44  0.00058
educ            0.0746     0.0110    6.79  1.1e-11
sd.id           0.1981     0.0471    4.21  2.6e-05
sd.idios        0.4875     0.0217   22.49  &lt; 2e-16</code></pre>
</div>
</div>
<p><span class="math inline">\(\sigma_\eta\)</span> and <span class="math inline">\(\sigma_\nu\)</span> are now two parameters that are directly estimated. We can see that the estimated values are very close to the ones obtained using GLS and that <span class="math inline">\(\sigma_\eta\)</span> is statistically significant. Comparing OLS and GLS standard deviations, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">lm_twins</span>, <span class="op">~</span> <span class="va">family</span>, type <span class="op">=</span> <span class="st">"HC0"</span>, cadjust <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">stder</span></span>
<span><span class="va">gls_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">gls_twins</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>ols <span class="op">=</span> <span class="va">ord_se</span>, gls <span class="op">=</span> <span class="va">gls_se</span>, ratio <span class="op">=</span> <span class="va">gls_se</span> <span class="op">/</span> <span class="va">ols_se</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      (Intercept) poly(age, 2)1 poly(age, 2)2    educ
ols        0.1516        0.5451        0.5309 0.01059
gls        0.1573        0.5811        0.5668 0.01098
ratio      0.9712        1.0115        0.9448 0.99593</code></pre>
</div>
</div>
<p>and therefore, there seems to be no gain of efficiency while using OLS instead of GLS. With Tobin’s Q example, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gls_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/plm/man/plm.html">plm</a></span><span class="op">(</span><span class="va">ikn</span> <span class="op">~</span> <span class="va">qn</span>, <span class="va">tobinq</span>, model <span class="op">=</span> <span class="st">"random"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/ercomp.html">ercomp</a></span><span class="op">(</span><span class="va">gls_q</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  var std.dev share
idiosyncratic 0.00533 0.07303  0.73
individual    0.00202 0.04493  0.27
theta: 0.735</code></pre>
</div>
</div>
<p>The share of the individual effect is now 27.5%, and the GLS is now OLS on series for which 73.5% of the individual mean has been removed, mostly because the time dimension of the panel is high (35 years). Comparing the robust standard errors of OLS and those of GLS, we get: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ikn</span> <span class="op">~</span> <span class="va">qn</span>, <span class="va">tobinq</span><span class="op">)</span></span>
<span><span class="va">sd_ols_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">ols_q</span>, <span class="op">~</span> <span class="va">cusip</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="va">sd_gls_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">gls_q</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">stder</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>ols <span class="op">=</span> <span class="va">sd_ols_q</span>, gls <span class="op">=</span> <span class="va">sd_gls_q</span>, ratio <span class="op">=</span> <span class="va">sd_gls_q</span> <span class="op">/</span> <span class="va">sd_ols_q</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      (Intercept)        qn
ols      0.003186 0.0006679
gls      0.003425 0.0001683
ratio    1.074968 0.2519458</code></pre>
</div>
</div>
<p>GLS is much more efficient than OLS as the standard error of the slope is about four times smaller. </p>
<p> </p>
</section><section id="sec-sur" class="level3"><h3 class="anchored" data-anchor-id="sec-sur">Seemingly unrelated regression</h3>
<p> </p>
<p>Because of the inter-equation correlations, the efficient estimator is the GLS estimator: <span class="math inline">\(\hat{\gamma}=(Z^\top\Omega^{-1}Z)^{-1}Z^\top\Omega^{-1}y\)</span>. This estimator, first proposed by <span class="citation" data-cites="ZELL:62">Zellner (<a href="#ref-ZELL:62" role="doc-biblioref">1962</a>)</span>, is known as a <strong>seemingly unrelated regression</strong> (<strong>SUR</strong>). It can be obtained by applying OLS on transformed data, each variable being premultiplied by <span class="math inline">\(\Omega^{-0.5}\)</span>. This matrix is simply <span class="math inline">\(\Omega^{-0.5}=\Sigma^{-0.5}\otimes I\)</span>. Denoting <span class="math inline">\(\delta_{lm}\)</span> the elements of <span class="math inline">\(\Sigma^{-0.5}\)</span>, the transformed response and covariates are:</p>
<p><span id="eq-transformation_sur"><span class="math display">\[
y ^ *=
\left(
  \begin{array}{c}
  \delta_{11} y_1 + \delta_{12} y_2 + \ldots +\delta_{1L} y_L \\
  \delta_{21} y_1 + \delta_{22} y_2 + \ldots +\delta_{2L} y_L \\
    \vdots \\
  \delta_{L1} y_1 + \delta_{L2} y_2 + \ldots +\delta_{LL} y_L
  \end{array}
\right),
Z ^ *=
\left(
  \begin{array}{cccc}
  \delta_{11} Z_1  &amp;\delta_{12} Z_2 &amp; \ldots &amp; \delta_{1L} Z_L \\
  \delta_{21} Z_1  &amp; \delta_{22} Z_2  &amp; \ldots &amp; \delta_{2L} Z_L \\
    \vdots      &amp; \vdots      &amp; \ddots &amp; \vdots \\
  \delta_{L1} Z_1  &amp; \delta_{L2} Z_2  &amp; \ldots &amp; \delta_{LL} Z_L
  \end{array}
\right)
\tag{6.21}\]</span></span></p>
<p><span class="math inline">\(\Sigma\)</span> is a matrix that contains unknown parameters, which can be estimated using residuals of a consistent but inefficient preliminary estimator, like OLS. The efficient estimator is then obtained the following way:</p>
<ul>
<li>first, estimate each equation separately by OLS and denote <span class="math inline">\(\hat{\Xi} = (\hat{\epsilon}_1, \hat{\epsilon}_2, \ldots,\hat{\epsilon}_L)\)</span> the <span class="math inline">\(N\times L\)</span> matrix for which every column is the residual vector of one of the equations in the system,</li>
<li>then, estimate the covariance matrix of the errors: <span class="math inline">\(\hat{\Sigma}=\hat{\Xi}^\top\hat{\Xi} / N\)</span>,</li>
<li>compute the matrix <span class="math inline">\(\hat{\Sigma}^{-0.5}\)</span> and use it to transform the response and the covariates of the model,</li>
<li>finally, estimate the model by applying OLS on transformed data.</li>
</ul>
<p><span class="math inline">\(\Sigma^{-0.5}\)</span> can conveniently be computed using the Cholesky decomposition, i.e., the upper-triangular matrix <span class="math inline">\(C\)</span> which is such that <span class="math inline">\(C^\top C=\Sigma^{-1}\)</span>.</p>
<!-- clarifier lower/uper triangular -->
<p>To illustrate the use of the <strong>SUR</strong> estimator, we go back to the estimation of the system of three equations (one cost function and two factor share equations) for the production of apples started in <a href="multiple_regression.html#sec-system_equation"><span>Section&nbsp;3.7</span></a>. In this section, we computed a tibble containing the three responses <code>Y</code> and the model matrices for the three equations <code>Z_c</code>, <code>Z_l</code> and <code>Z_m</code> (respectively for the cost, the labor and the materials equations). In <a href="#sec-bptest_system"><span>Section&nbsp;6.2.3</span></a>, we estimated the covariance matrix of the errors of the three equations <code>Sigma</code>. To implement the <strong>SUR</strong> estimator, we compute the Cholesky decomposition of the inverse of the estimated covariance matrix of the errors of the three equations: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">V</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]    [,2]     [,3]
[1,] 3.375  0.9645  0.03717
[2,] 0.000 17.3707 13.66170
[3,] 0.000  0.0000 11.38294</code></pre>
</div>
</div>
<p>We then transform the response and the covariates using <a href="#eq-transformation_sur">Equation&nbsp;<span>6.21</span></a>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Zs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">V</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_c</span>, <span class="va">V</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_l</span>, <span class="va">V</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_m</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">V</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_c</span>, <span class="va">V</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_l</span>, <span class="va">V</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_m</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">V</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_c</span>, <span class="va">V</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_l</span>, <span class="va">V</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">Z_m</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">as.numeric</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then the SUR estimator is computed, using <code>lm</code> on the transformed data and then using <code>clm</code> in order to impose the linear restrictions. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sur</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">Zs</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">clm</span><span class="op">(</span>R <span class="op">=</span> <span class="va">R</span><span class="op">)</span></span>
<span><span class="va">sur</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Zs(Intercept)           Zsy          Zsy2          Zspl 
      0.01478       0.43854       0.11661       0.49207 
         Zspm         Zspl2         Zsplm         Zspm2 
      0.32453       0.11775      -0.09496       0.10277 
Zs(Intercept)          Zspl          Zspm Zs(Intercept) 
      0.49207       0.11775      -0.09496       0.32453 
         Zspl          Zspm 
     -0.09496       0.10277 </code></pre>
</div>
</div>
<p>More simply, the <code><a href="https://rdrr.io/pkg/systemfit/man/systemfit.html">systemfit::systemfit</a></code> function can be used, <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> with the <code>method</code> argument set to <code>"SUR"</code>:<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/systemfit/">systemfit</a></span><span class="op">)</span></span>
<span><span class="va">sur</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/systemfit/man/systemfit.html">systemfit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>cost <span class="op">=</span> <span class="va">eq_ct</span>, labor <span class="op">=</span> <span class="va">eq_sl</span>, materials <span class="op">=</span> <span class="va">eq_sm</span><span class="op">)</span>,</span>
<span>                 data <span class="op">=</span> <span class="va">ap</span>, restrict.matrix <span class="op">=</span> <span class="va">R</span>, method <span class="op">=</span> <span class="st">"SUR"</span>, </span>
<span>                 methodResidCov <span class="op">=</span> <span class="st">"noDfCor"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The coefficients of the fitted model can be used to compute the Allen elasticities of substitution and the price elasticities. The former are defined as:</p>
<p><span class="math display">\[
\sigma_{ij} = \frac{\beta_{ij}}{s_i s_j} - 1 \; \; \forall i \neq j
\mbox{ and } \sigma_{ii} = \frac{\beta_{ij} - s_i(1 - s_i)}{s_i ^ 2}
\]</span></p>
<p>Denote as <span class="math inline">\(B\)</span> the matrix containing the coefficients <span class="math inline">\(\beta_{ij}\)</span>. Remember that, by imposing the homogeneity of degree one of the cost function, we imposed that <span class="math inline">\(\beta_{iI} = - \sum_{i=1}^{I-1}\beta_{ij}\)</span>. Therefore <span class="math inline">\(\beta_{iI}\)</span> was not estimated and we must add it to the <span class="math inline">\(B\)</span> matrix using this formula: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">sur</span><span class="op">)</span><span class="op">[</span><span class="op">-</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">8</span><span class="op">)</span><span class="op">]</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span>, <span class="op">]</span></span>
<span><span class="va">add</span> <span class="op">&lt;-</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">B</span>, <span class="fl">1</span>, <span class="va">sum</span><span class="op">)</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">B</span>, <span class="va">add</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">add</span>, <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">add</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">shares</span> <span class="op">&lt;-</span> <span class="va">ap</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span>sl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sl</span><span class="op">)</span>,</span>
<span>                           sm <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sm</span><span class="op">)</span>, sk <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">sl</span> <span class="op">-</span> <span class="va">sm</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="va">as.numeric</span></span>
<span><span class="va">elast</span> <span class="op">&lt;-</span> <span class="va">B</span> <span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/outer.html">outer</a></span><span class="op">(</span><span class="va">shares</span>, <span class="va">shares</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">elast</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">elast</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">shares</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dimnames.html">dimnames</a></span><span class="op">(</span><span class="va">elast</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"l"</span>, <span class="st">"m"</span>, <span class="st">"k"</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"l"</span>, <span class="st">"m"</span>, <span class="st">"k"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">elast</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        l       m       k
l -0.5214  0.4064  0.7469
m  0.4064 -1.1295  0.8624
k  0.7469  0.8624 -3.6371</code></pre>
</div>
</div>
<p>The three factors are substitutes, all the Allen elasticities of substitution being positive. The price elasticities are given by: <span class="math inline">\(\epsilon_{ij} = s_j \sigma_{ij}\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">elast</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">shares</span>, <span class="va">shares</span>, <span class="va">shares</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        l       m       k
l -0.2626  0.1291  0.1335
m  0.2047 -0.3588  0.1542
k  0.3761  0.2740 -0.6501</code></pre>
</div>
</div>
<p>Note that this matrix is not symmetric: for example, <span class="math inline">\(0.1675\)</span> is the elasticity of the demand of materials with the price of labor whereas <span class="math inline">\(0.1039\)</span> is the elasticity of the demand of labor with the price of materials. The price elasticities indicate that the demand for the three inputs is inelastic, and it is particularly the case for labor and materials.</p>
<p> </p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-AMEM:71" class="csl-entry" role="doc-biblioentry">
Amemiya, Takeshi. 1971. <span>“The Estimation of the Variances in a Variance–Components Model.”</span> <em>International Economic Review</em> 12: 1–13.
</div>
<div id="ref-BERN:91" class="csl-entry" role="doc-biblioentry">
Berndt, Ernst R. 1991. <em>The Practice of Econometrics, Classic and Contemporary</em>. Addison-Wesley Publishing Company.
</div>
<div id="ref-BONJ:CHERK:KASK:03" class="csl-entry" role="doc-biblioentry">
Bonjour, Dorothe, Lynn F Cherkas, Jonathan E Haskel, Denise D Hawkes, and Tim D Spector. 2003. <span>“Returns to Education: Evidence from U.K. Twins.”</span> <em>American Economic Review</em> 93 (5): 1799–1812. <a href="https://doi.org/10.1257/000282803322655554">https://doi.org/10.1257/000282803322655554</a>.
</div>
<div id="ref-BREU:PAGA:79" class="csl-entry" role="doc-biblioentry">
Breusch, T. S., and A. R. Pagan. 1979. <span>“A Simple Test for Heteroscedasticity and Random Coefficient Variation.”</span> <em>Econometrica</em> 47 (5): 1287. <a href="https://doi.org/10.2307/1911963">https://doi.org/10.2307/1911963</a>.
</div>
<div id="ref-BREU:PAGA:80" class="csl-entry" role="doc-biblioentry">
———. 1980. <span>“The Lagrange Multiplier Test and Its Applications to Model Specification in Econometrics.”</span> <em>The Review of Economic Studies</em> 47 (1): 239. <a href="https://doi.org/10.2307/2297111">https://doi.org/10.2307/2297111</a>.
</div>
<div id="ref-CROI:MILL:08" class="csl-entry" role="doc-biblioentry">
Croissant, Yves, and Giovanni Millo. 2008. <span>“Panel Data Econometrics in <span>R</span>: The <span class="nocase">plm</span> Package.”</span> <em>Journal of Statistical Software</em> 27 (2): 1–43. <a href="https://doi.org/10.18637/jss.v027.i02">https://doi.org/10.18637/jss.v027.i02</a>.
</div>
<div id="ref-CROI:MILL:18" class="csl-entry" role="doc-biblioentry">
———. 2018. <em>Panel Data Econometrics with <span>R</span></em>. Wiley.
</div>
<div id="ref-DURA:PUGA:20" class="csl-entry" role="doc-biblioentry">
Duranton, Gilles, and Diego Puga. 2020. <span>“The Economics of Urban Density.”</span> <em>Journal of Economic Perspectives</em> 34 (3): 3–26. <a href="https://doi.org/10.1257/jep.34.3.3">https://doi.org/10.1257/jep.34.3.3</a>.
</div>
<div id="ref-GREE:18" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometrics Analysis</em>. 8th ed. Pearson.
</div>
<div id="ref-HOUT:51" class="csl-entry" role="doc-biblioentry">
Houthakker, H. S. 1951. <span>“Some Calculations on Electricity Consumption in Great Britain.”</span> <em>Journal of the Royal Statistical Society. Series A (General)</em> 114 (3): 359–71. <a href="http://www.jstor.org/stable/2980781">http://www.jstor.org/stable/2980781</a>.
</div>
<div id="ref-KOEN:81" class="csl-entry" role="doc-biblioentry">
Koenker, Roger. 1981. <span>“A Note on Studentizing a Test for Heteroscedasticity.”</span> <em>Journal of Econometrics</em> 17: 107–12.
</div>
<div id="ref-NERL:71" class="csl-entry" role="doc-biblioentry">
Nerlove, M. 1971. <span>“Further Evidence on the Estimation of Dynamic Economic Relations from a Time–Series of Cross–Sections.”</span> <em>Econometrica</em> 39: 359–82.
</div>
<div id="ref-SCHA:90" class="csl-entry" role="doc-biblioentry">
Schaller, Huntley. 1990. <span>“A Re-Examination of the q Theory of Investment Using u.s. Firm Data.”</span> <em>Journal of Applied Econometrics</em> 5 (4): 309–25. <a href="http://www.jstor.org/stable/2096476">http://www.jstor.org/stable/2096476</a>.
</div>
<div id="ref-SWAM:AROR:72" class="csl-entry" role="doc-biblioentry">
Swamy, P. A. V. B., and S. S Arora. 1972. <span>“The Exact Finite Sample Properties of the Estimators of Coefficients in the Error Components Regression Models.”</span> <em>Econometrica</em> 40: 261–75.
</div>
<div id="ref-WALL:HUSS:69" class="csl-entry" role="doc-biblioentry">
Wallace, T. D., and A. Hussain. 1969. <span>“The Use of Error Components Models in Combining Cross Section with Time Series Data.”</span> <em>Econometrica</em> 37 (1): 55–72.
</div>
<div id="ref-WHIT:80" class="csl-entry" role="doc-biblioentry">
White, Halbert. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”</span> <em>Econometrica</em> 48 (4): 817. <a href="https://doi.org/10.2307/1912934">https://doi.org/10.2307/1912934</a>.
</div>
<div id="ref-ZEIL:04" class="csl-entry" role="doc-biblioentry">
Zeileis, Achim. 2004. <span>“Econometric Computing with HC and HAC Covariance Matrix Estimators.”</span> <em>Journal of Statistical Software</em> 11 (10): 1–17. <a href="https://doi.org/10.18637/jss.v011.i10">https://doi.org/10.18637/jss.v011.i10</a>.
</div>
<div id="ref-ZEIL:06" class="csl-entry" role="doc-biblioentry">
———. 2006. <span>“Object-Oriented Computation of Sandwich Estimators.”</span> <em>Journal of Statistical Software</em> 16 (9): 1–16. <a href="https://doi.org/10.18637/jss.v016.i09">https://doi.org/10.18637/jss.v016.i09</a>.
</div>
<div id="ref-ZEIL:KOLL:NATH:20" class="csl-entry" role="doc-biblioentry">
Zeileis, Achim, Susanne Köll, and Nathaniel Graham. 2020. <span>“Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in r.”</span> <em>Journal of Statistical Software</em> 95 (1): 1–36. <a href="https://doi.org/10.18637/jss.v095.i01">https://doi.org/10.18637/jss.v095.i01</a>.
</div>
<div id="ref-ZELL:62" class="csl-entry" role="doc-biblioentry">
Zellner, Arnold. 1962. <span>“An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests of Aggregation Bias.”</span> <em>Journal of the American Statistical Association</em> 57: 500–509.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>This data set is used extensively by <span class="citation" data-cites="BERN:91">Berndt (<a href="#ref-BERN:91" role="doc-biblioref">1991</a>)</span>, chapter 7.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See <a href="#sec-sandwich"><span>Section&nbsp;6.3</span></a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>As the matrix is symmetric, <code>lower.tri</code> could also have been used.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>A block group is a geographical unit which is between the census tract and the census block.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Actually, the whole data set covers the whole United States, but we use here the small subsample that concerns the state of Alabama.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>See <span class="citation" data-cites="ZEIL:06">Zeileis (<a href="#ref-ZEIL:06" role="doc-biblioref">2006</a>)</span> for more details concerning the other values of the <code>type</code> argument. <a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="math inline">\((AB) ^ {-1} = B^{-1} A^{-1}\)</span> if the inverse of the two matrices exists, see <span class="citation" data-cites="GREE:18">Greene (<a href="#ref-GREE:18" role="doc-biblioref">2018</a>)</span>, online appendix p 1074.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>See also <span class="citation" data-cites="SWAM:AROR:72">Swamy and Arora (<a href="#ref-SWAM:AROR:72" role="doc-biblioref">1972</a>)</span>, <span class="citation" data-cites="AMEM:71">Amemiya (<a href="#ref-AMEM:71" role="doc-biblioref">1971</a>)</span> and <span class="citation" data-cites="NERL:71">Nerlove (<a href="#ref-NERL:71" role="doc-biblioref">1971</a>)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The <strong>systemfit</strong> package was presented in <a href="multiple_regression.html#sec-constrained_ls"><span>Section&nbsp;3.7.2</span></a>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Note the use of the <code>methodResidCov</code> argument: setting it to <code>"noDfCor"</code>, the cross-product of the vectors of residuals is divided by the number of observations to get the estimation of the covariance matrix. Other values of this argument enables to perform different kinds of degrees of freedom correction.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../chapters/maximum_likelihood.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/endogeneity.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb67" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Non-spherical disturbances {#sec-non_spherical}</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../_commonR.R"</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>In the first part of the book, we considered a model of the form:</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>$y_n = \alpha + \beta x_n + \epsilon_n$ with the hypothesis that the errors were homoskedastic: $\mbox{V}(\epsilon_n) = \sigma_\epsilon ^ 2$ and uncorrelated: $\mbox{E}(\epsilon_n \epsilon_m) = 0 \; \forall n \neq m$. In this case, the errors (or disturbances) are</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>**spherical** and the covariance matrix of the errors $\Omega$ is, up to</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>a multiplicative constant $\sigma_\epsilon^2$, the identity matrix:</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>\Omega = \mbox{V}(\epsilon) = \mbox{E}(\epsilon\epsilon^\top) = \sigma_\epsilon ^ 2 I</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>In this chapter, we analyze cases where these hypothesis are violated.</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>This has the following consequences concerning the results established</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>in the first part of the book:</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the OLS estimator is still consistent: this means that $\hat{\beta}$</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>    estimates consistently $\beta$ and $\hat{\epsilon}$ estimates also</span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>    consistently $\epsilon$; this is an important result, as the</span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>    residuals of the OLS estimator can therefore be used to test whether</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>    the errors are spherical or not,</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the OLS estimator is no longer BLUE, i.e., it is no longer the best</span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>    linear unbiased estimator,</span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>there is another linear unbiased estimator (the **generalized least</span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a>    squares**, or **GLS**) which is more efficient (which</span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>    means that it has a smaller variance) than the OLS estimator and</span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a>    which is the BLUE estimator when the errors are non-spherical,\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares}</span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the simple formula for the variance of the OLS estimator,</span>
<span id="cb67-34"><a href="#cb67-34" aria-hidden="true" tabindex="-1"></a>    $\mbox{V}(\hat{\beta}) = \sigma_\epsilon ^ 2 \left(\tilde{X}^\top \tilde{X}\right)^{-1}$</span>
<span id="cb67-35"><a href="#cb67-35" aria-hidden="true" tabindex="-1"></a>    is no longer an unbiased estimator of the true covariance matrix of</span>
<span id="cb67-36"><a href="#cb67-36" aria-hidden="true" tabindex="-1"></a>    the OLS estimator. However, as we'll see in this chapter,</span>
<span id="cb67-37"><a href="#cb67-37" aria-hidden="true" tabindex="-1"></a>    **sandwich** estimators, which are consistent, can be used.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich}</span>
<span id="cb67-38"><a href="#cb67-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-39"><a href="#cb67-39" aria-hidden="true" tabindex="-1"></a>@sec-situations_non_spher reviews some important cases where the errors</span>
<span id="cb67-40"><a href="#cb67-40" aria-hidden="true" tabindex="-1"></a>are non-spherical. @sec-test_non_spher presents tests that</span>
<span id="cb67-41"><a href="#cb67-41" aria-hidden="true" tabindex="-1"></a>enable to detect whether the errors are spherical or not. @sec-sandwich presents robust estimators of the</span>
<span id="cb67-42"><a href="#cb67-42" aria-hidden="true" tabindex="-1"></a>variance of the OLS estimators. Finally, @sec-gls is devoted to the GLS</span>
<span id="cb67-43"><a href="#cb67-43" aria-hidden="true" tabindex="-1"></a>estimator.</span>
<span id="cb67-44"><a href="#cb67-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-45"><a href="#cb67-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## Situations where the errors are non-spherical {#sec-situations_non_spher}</span></span>
<span id="cb67-46"><a href="#cb67-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-47"><a href="#cb67-47" aria-hidden="true" tabindex="-1"></a>As stated previously, the hypothesis of spherical disturbances implies</span>
<span id="cb67-48"><a href="#cb67-48" aria-hidden="true" tabindex="-1"></a>that the errors are homoskedastic and uncorrelated. We'll describe in the next three subsections important situations where this hypothesis is violated. In each case, we'll establish the</span>
<span id="cb67-49"><a href="#cb67-49" aria-hidden="true" tabindex="-1"></a>expression of the matrix of covariance of the errors $\Omega$ and, </span>
<span id="cb67-50"><a href="#cb67-50" aria-hidden="true" tabindex="-1"></a>for a reason that will be clear in the subsequent sections, we'll also</span>
<span id="cb67-51"><a href="#cb67-51" aria-hidden="true" tabindex="-1"></a>compute the inverse of this matrix.</span>
<span id="cb67-52"><a href="#cb67-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-53"><a href="#cb67-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### Heteroskedasticity</span></span>
<span id="cb67-54"><a href="#cb67-54" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{heteroskedasticity|(}</span>
<span id="cb67-55"><a href="#cb67-55" aria-hidden="true" tabindex="-1"></a>In a linear model: $y_n = \alpha + \beta x_n + \epsilon_n$, </span>
<span id="cb67-56"><a href="#cb67-56" aria-hidden="true" tabindex="-1"></a>heteroskedasticity occurs when the conditional variance of the response</span>
<span id="cb67-57"><a href="#cb67-57" aria-hidden="true" tabindex="-1"></a>$y_n$ (the variance of $\epsilon_n$) is not a constant. </span>
<span id="cb67-58"><a href="#cb67-58" aria-hidden="true" tabindex="-1"></a>As an example, @HOUT:51\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Houthakker} analyzed electric consumption in the United Kingdom, and his data set (called <span class="in">`uk_elec`</span>)\idxdata{uk<span class="sc">\_</span>elec}{micsr.data} is a sample of 42 British cities.^<span class="co">[</span><span class="ot">This data set is used extensively by @BERN:91\index[author]{Berndt}, chapter 7.</span><span class="co">]</span> The response is the per capita consumption in kilowatt hours. Denoting $c_{ni}$ the consumption of an individual $i$ in city $n$, the response is then $y_n =\frac{1}{I_n}\sum_i^{I_n} c_{ni}$ where $I_n$ is the total number of consumers in city $n$. Then, if the standard</span>
<span id="cb67-59"><a href="#cb67-59" aria-hidden="true" tabindex="-1"></a>deviation of the individual consumption is $\sigma_c$ (the same in every</span>
<span id="cb67-60"><a href="#cb67-60" aria-hidden="true" tabindex="-1"></a>city), the variance of $y_n$ is $\sigma_c ^ 2 / I_n$ and therefore</span>
<span id="cb67-61"><a href="#cb67-61" aria-hidden="true" tabindex="-1"></a>depends on the number of consumer units of the city. Even</span>
<span id="cb67-62"><a href="#cb67-62" aria-hidden="true" tabindex="-1"></a>if covariates are taken into account, it is doubtful that the</span>
<span id="cb67-63"><a href="#cb67-63" aria-hidden="true" tabindex="-1"></a>conditional variance of $y$ will be the same for every city, and a more</span>
<span id="cb67-64"><a href="#cb67-64" aria-hidden="true" tabindex="-1"></a>reasonable hypothesis is that, as the unconditional variance, it is</span>
<span id="cb67-65"><a href="#cb67-65" aria-hidden="true" tabindex="-1"></a>inversely proportional to the number of consumer units. With heteroskedastic but uncorrelated errors, the matrix of covariance of the errors is a diagonal matrix with non-constant diagonal terms:</span>
<span id="cb67-66"><a href="#cb67-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-67"><a href="#cb67-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-68"><a href="#cb67-68" aria-hidden="true" tabindex="-1"></a>\Omega= </span>
<span id="cb67-69"><a href="#cb67-69" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-70"><a href="#cb67-70" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb67-71"><a href="#cb67-71" aria-hidden="true" tabindex="-1"></a>\sigma_{1} ^ 2 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-72"><a href="#cb67-72" aria-hidden="true" tabindex="-1"></a>0 &amp; \sigma_{2} ^ 2 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-73"><a href="#cb67-73" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-74"><a href="#cb67-74" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; \ldots &amp; \sigma_N ^ 2</span>
<span id="cb67-75"><a href="#cb67-75" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-76"><a href="#cb67-76" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-77"><a href="#cb67-77" aria-hidden="true" tabindex="-1"></a>$$ {#eq-matheterosc}</span>
<span id="cb67-78"><a href="#cb67-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-79"><a href="#cb67-79" aria-hidden="true" tabindex="-1"></a>The inverse of $\Omega$ is easily obtained:</span>
<span id="cb67-80"><a href="#cb67-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-81"><a href="#cb67-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-82"><a href="#cb67-82" aria-hidden="true" tabindex="-1"></a>\Omega ^ {-1}= </span>
<span id="cb67-83"><a href="#cb67-83" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-84"><a href="#cb67-84" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb67-85"><a href="#cb67-85" aria-hidden="true" tabindex="-1"></a>1 / \sigma_{1} ^ 2 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-86"><a href="#cb67-86" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 / \sigma_{2} ^ 2 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-87"><a href="#cb67-87" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-88"><a href="#cb67-88" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; \ldots &amp; 1/ \sigma_N ^ 2</span>
<span id="cb67-89"><a href="#cb67-89" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-90"><a href="#cb67-90" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-91"><a href="#cb67-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-92"><a href="#cb67-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-93"><a href="#cb67-93" aria-hidden="true" tabindex="-1"></a>and, more generally, $\Omega^{r}$ ($r$ being any integer or rational number) is a diagonal matrix with typical</span>
<span id="cb67-94"><a href="#cb67-94" aria-hidden="true" tabindex="-1"></a>element $\left(\sigma_n^2\right)^r$.</span>
<span id="cb67-95"><a href="#cb67-95" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{heteroskedasticity|)}</span>
<span id="cb67-96"><a href="#cb67-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-97"><a href="#cb67-97" aria-hidden="true" tabindex="-1"></a><span class="fu">### Correlation of the errors {#sec-error_component}</span></span>
<span id="cb67-98"><a href="#cb67-98" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{correlated errors|(}</span>
<span id="cb67-99"><a href="#cb67-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-100"><a href="#cb67-100" aria-hidden="true" tabindex="-1"></a>Consider now the case where we have several observations from the same</span>
<span id="cb67-101"><a href="#cb67-101" aria-hidden="true" tabindex="-1"></a>**entity**. An example is the case where the unit of</span>
<span id="cb67-102"><a href="#cb67-102" aria-hidden="true" tabindex="-1"></a>observation is the individual, but siblings are observed. In this case, each observation is doubly indexed,</span>
<span id="cb67-103"><a href="#cb67-103" aria-hidden="true" tabindex="-1"></a>the first index being the family and the second one the rank of the sibling's birth.</span>
<span id="cb67-104"><a href="#cb67-104" aria-hidden="true" tabindex="-1"></a>Another very important case is when the same individual</span>
<span id="cb67-105"><a href="#cb67-105" aria-hidden="true" tabindex="-1"></a>(in a wide sense, it can be a household, firm, country, etc.) is</span>
<span id="cb67-106"><a href="#cb67-106" aria-hidden="true" tabindex="-1"></a>observed several times, for example for different periods like years</span>
<span id="cb67-107"><a href="#cb67-107" aria-hidden="true" tabindex="-1"></a>or months. Such a data set is called a **panel data**. </span>
<span id="cb67-108"><a href="#cb67-108" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data|(}</span>
<span id="cb67-109"><a href="#cb67-109" aria-hidden="true" tabindex="-1"></a>In the subsequent sections, we'll use two data sets. The first data set, called <span class="in">`twins`</span>, </span>
<span id="cb67-110"><a href="#cb67-110" aria-hidden="true" tabindex="-1"></a>is from @BONJ:CHERK:KASK:03\idxdata{twins}{micsr.data}</span>
<span id="cb67-111"><a href="#cb67-111" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Bonjour}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Cherkas}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Haskel}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Hawkes}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Spector} who studied the return to education with a</span>
<span id="cb67-112"><a href="#cb67-112" aria-hidden="true" tabindex="-1"></a>sample of twins. The sample contains 428 observations (214 pairs</span>
<span id="cb67-113"><a href="#cb67-113" aria-hidden="true" tabindex="-1"></a>of twins), and it is reasonable to assume that, for a given pair of twins,</span>
<span id="cb67-114"><a href="#cb67-114" aria-hidden="true" tabindex="-1"></a>the two errors are correlated as they partly contain unobserved</span>
<span id="cb67-115"><a href="#cb67-115" aria-hidden="true" tabindex="-1"></a>characteristics that are common to both twins.</span>
<span id="cb67-116"><a href="#cb67-116" aria-hidden="true" tabindex="-1"></a>The second data set, called <span class="in">`tobinq`</span>, is from @SCHA:90\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Schaller} who tests the relevance of Tobins' Q theory of investment by</span>
<span id="cb67-117"><a href="#cb67-117" aria-hidden="true" tabindex="-1"></a>regressing the investment rate (the ratio of the investment and the</span>
<span id="cb67-118"><a href="#cb67-118" aria-hidden="true" tabindex="-1"></a>stock of capital) to Tobin's Q, which is the ratio of the value of the</span>
<span id="cb67-119"><a href="#cb67-119" aria-hidden="true" tabindex="-1"></a>firm and the stock of capital. The data set is a panel of 188 firms observed for 35 years (from 1951 to 1985).\idxdata{tobinq}{micsr.data}</span>
<span id="cb67-120"><a href="#cb67-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-121"><a href="#cb67-121" aria-hidden="true" tabindex="-1"></a>For such data, we'll denote $n = 1, 2, \ldots N$ the entity /</span>
<span id="cb67-122"><a href="#cb67-122" aria-hidden="true" tabindex="-1"></a>individual index and $t = 1, 2, \ldots T$ the index of the observation in the</span>
<span id="cb67-123"><a href="#cb67-123" aria-hidden="true" tabindex="-1"></a>entity / the time period. Then, the simple linear model can be written:</span>
<span id="cb67-124"><a href="#cb67-124" aria-hidden="true" tabindex="-1"></a>$y_{nt} = \alpha + \beta x_{nt} + \epsilon_{nt}$</span>
<span id="cb67-125"><a href="#cb67-125" aria-hidden="true" tabindex="-1"></a>and it is useful to write the error term as the sum of two</span>
<span id="cb67-126"><a href="#cb67-126" aria-hidden="true" tabindex="-1"></a>components:</span>
<span id="cb67-127"><a href="#cb67-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-128"><a href="#cb67-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>an entity / individual effect $\eta_n$,</span>
<span id="cb67-129"><a href="#cb67-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>an idiosyncratic effect $\nu_{nt}$.</span>
<span id="cb67-130"><a href="#cb67-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-131"><a href="#cb67-131" aria-hidden="true" tabindex="-1"></a>We therefore have $\epsilon_{nt} = \eta_n + \nu_{nt}$. This leads to the</span>
<span id="cb67-132"><a href="#cb67-132" aria-hidden="true" tabindex="-1"></a>so-called **error-component** model, which can be easily analyzed with the following</span>
<span id="cb67-133"><a href="#cb67-133" aria-hidden="true" tabindex="-1"></a>hypothesis:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{error component model}</span>
<span id="cb67-134"><a href="#cb67-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-135"><a href="#cb67-135" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the two components are homoskedastic and uncorrelated:</span>
<span id="cb67-136"><a href="#cb67-136" aria-hidden="true" tabindex="-1"></a>    $\mbox{V}(\eta_n) = \sigma_{\eta} ^ 2, \forall n$,</span>
<span id="cb67-137"><a href="#cb67-137" aria-hidden="true" tabindex="-1"></a>    $\mbox{V}(\nu_{nt}) = \sigma_{\nu} ^ 2, \forall n, t$ and</span>
<span id="cb67-138"><a href="#cb67-138" aria-hidden="true" tabindex="-1"></a>    $\mbox{cov}(\eta_n, \nu_{nt}) = 0, \forall n, t$,</span>
<span id="cb67-139"><a href="#cb67-139" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the idiosyncratic terms for the same entity are uncorrelated:</span>
<span id="cb67-140"><a href="#cb67-140" aria-hidden="true" tabindex="-1"></a>    $\mbox{E}(\nu_{nt}\nu_{ns})=0\; \forall \; n, t \neq s$,</span>
<span id="cb67-141"><a href="#cb67-141" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the two components of the errors are uncorrelated for two</span>
<span id="cb67-142"><a href="#cb67-142" aria-hidden="true" tabindex="-1"></a>    observations of different entities</span>
<span id="cb67-143"><a href="#cb67-143" aria-hidden="true" tabindex="-1"></a>    $\mbox{cov}(\nu_{nt}, \nu_{ms})=\mbox{cov}(\eta_n, \eta_m)=0\; \forall \; n \neq m, t, s$.</span>
<span id="cb67-144"><a href="#cb67-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-145"><a href="#cb67-145" aria-hidden="true" tabindex="-1"></a>With these hypotheses, we have:</span>
<span id="cb67-146"><a href="#cb67-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-147"><a href="#cb67-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-148"><a href="#cb67-148" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb67-149"><a href="#cb67-149" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb67-150"><a href="#cb67-150" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\epsilon_{nt}^2) &amp;=&amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 <span class="sc">\\</span></span>
<span id="cb67-151"><a href="#cb67-151" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\epsilon_{nt}\epsilon_{mt}) &amp;=&amp; \sigma_\eta ^ 2 <span class="sc">\\</span></span>
<span id="cb67-152"><a href="#cb67-152" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\epsilon_{nt}\epsilon_{ms}) &amp;=&amp; 0;\ \forall \; n \neq m<span class="sc">\\</span></span>
<span id="cb67-153"><a href="#cb67-153" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-154"><a href="#cb67-154" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb67-155"><a href="#cb67-155" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-156"><a href="#cb67-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-157"><a href="#cb67-157" aria-hidden="true" tabindex="-1"></a>and $\Omega$ is a block-diagonal matrix with identical blocks. For</span>
<span id="cb67-158"><a href="#cb67-158" aria-hidden="true" tabindex="-1"></a>example, with $N = 2$ and $T = 3$:</span>
<span id="cb67-159"><a href="#cb67-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-160"><a href="#cb67-160" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-161"><a href="#cb67-161" aria-hidden="true" tabindex="-1"></a>\Omega = </span>
<span id="cb67-162"><a href="#cb67-162" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-163"><a href="#cb67-163" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccccc}</span>
<span id="cb67-164"><a href="#cb67-164" aria-hidden="true" tabindex="-1"></a>\sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2 &amp; \sigma_\eta ^ 2 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-165"><a href="#cb67-165" aria-hidden="true" tabindex="-1"></a>\sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-166"><a href="#cb67-166" aria-hidden="true" tabindex="-1"></a>\sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-167"><a href="#cb67-167" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2 &amp; \sigma_\eta ^ 2 <span class="sc">\\</span></span>
<span id="cb67-168"><a href="#cb67-168" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2 &amp; \sigma_\eta ^ 2  <span class="sc">\\</span></span>
<span id="cb67-169"><a href="#cb67-169" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2  &amp; \sigma_\eta ^ 2 + \sigma_\nu ^ 2<span class="sc">\\</span></span>
<span id="cb67-170"><a href="#cb67-170" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-171"><a href="#cb67-171" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-172"><a href="#cb67-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-173"><a href="#cb67-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-174"><a href="#cb67-174" aria-hidden="true" tabindex="-1"></a>The blocks of this matrix can be written as, denoting $j$ a vector of</span>
<span id="cb67-175"><a href="#cb67-175" aria-hidden="true" tabindex="-1"></a>ones and $J=jj^\top$ a square matrix of 1:</span>
<span id="cb67-176"><a href="#cb67-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-177"><a href="#cb67-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-178"><a href="#cb67-178" aria-hidden="true" tabindex="-1"></a>\sigma^2_\nu I_T + \sigma ^ 2_\eta J_T</span>
<span id="cb67-179"><a href="#cb67-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-180"><a href="#cb67-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-181"><a href="#cb67-181" aria-hidden="true" tabindex="-1"></a>and, using Kronecker product, $\Omega$ is:\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Kronecker product}</span>
<span id="cb67-182"><a href="#cb67-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-183"><a href="#cb67-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-184"><a href="#cb67-184" aria-hidden="true" tabindex="-1"></a>\Omega = I_N \otimes \left(\sigma^2_\nu I_T + \sigma ^ 2_\eta J_T\right) = \sigma ^ 2_\nu I_{NT} + \sigma_\eta ^ 2 I_N \otimes J_T</span>
<span id="cb67-185"><a href="#cb67-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-186"><a href="#cb67-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-187"><a href="#cb67-187" aria-hidden="true" tabindex="-1"></a>Another equivalent expression which will prove to be particularly useful</span>
<span id="cb67-188"><a href="#cb67-188" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb67-189"><a href="#cb67-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-190"><a href="#cb67-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-191"><a href="#cb67-191" aria-hidden="true" tabindex="-1"></a>\Omega = \sigma ^ 2_\nu \left(I_N - I_N \otimes J_T / T\right)+ (T \sigma_\eta ^ 2 + \sigma ^ 2_\nu) \left(I_N \otimes J_T / T\right) = \sigma ^ 2_\nu W + \sigma_\iota^2 B</span>
<span id="cb67-192"><a href="#cb67-192" aria-hidden="true" tabindex="-1"></a>$$ {#eq-omega_panel}</span>
<span id="cb67-193"><a href="#cb67-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-194"><a href="#cb67-194" aria-hidden="true" tabindex="-1"></a>where $\sigma_\iota ^ 2 = T \sigma_\eta ^ 2 + \sigma ^ 2_\nu$. With our</span>
<span id="cb67-195"><a href="#cb67-195" aria-hidden="true" tabindex="-1"></a>$N=2$ and $T=3$ simple case, the two matrices are:</span>
<span id="cb67-196"><a href="#cb67-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-197"><a href="#cb67-197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-198"><a href="#cb67-198" aria-hidden="true" tabindex="-1"></a>W =</span>
<span id="cb67-199"><a href="#cb67-199" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-200"><a href="#cb67-200" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccccc}</span>
<span id="cb67-201"><a href="#cb67-201" aria-hidden="true" tabindex="-1"></a>2/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-202"><a href="#cb67-202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1/3 &amp; 2/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-203"><a href="#cb67-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1/3 &amp; - 1/3 &amp; 2/3 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-204"><a href="#cb67-204" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; 2/3 &amp; - 1/3 &amp; - 1/3 <span class="sc">\\</span></span>
<span id="cb67-205"><a href="#cb67-205" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; 2/3 &amp; - 1/3 <span class="sc">\\</span></span>
<span id="cb67-206"><a href="#cb67-206" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; 2/3 <span class="sc">\\</span></span>
<span id="cb67-207"><a href="#cb67-207" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-208"><a href="#cb67-208" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-209"><a href="#cb67-209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-210"><a href="#cb67-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-211"><a href="#cb67-211" aria-hidden="true" tabindex="-1"></a>and:</span>
<span id="cb67-212"><a href="#cb67-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-213"><a href="#cb67-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-214"><a href="#cb67-214" aria-hidden="true" tabindex="-1"></a>B = </span>
<span id="cb67-215"><a href="#cb67-215" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-216"><a href="#cb67-216" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccccc}</span>
<span id="cb67-217"><a href="#cb67-217" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-218"><a href="#cb67-218" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-219"><a href="#cb67-219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1/3 &amp; - 1/3 &amp; - 1/3 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-220"><a href="#cb67-220" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; - 1/3 <span class="sc">\\</span></span>
<span id="cb67-221"><a href="#cb67-221" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; - 1/3 <span class="sc">\\</span></span>
<span id="cb67-222"><a href="#cb67-222" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; - 1/3 &amp; - 1/3 &amp; - 1/3 <span class="sc">\\</span></span>
<span id="cb67-223"><a href="#cb67-223" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-224"><a href="#cb67-224" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-225"><a href="#cb67-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-226"><a href="#cb67-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-227"><a href="#cb67-227" aria-hidden="true" tabindex="-1"></a>The first matrix is called the **within** matrix. Premultiplying a</span>
<span id="cb67-228"><a href="#cb67-228" aria-hidden="true" tabindex="-1"></a>vector by $W$ transforms it as deviations from the individual means. The</span>
<span id="cb67-229"><a href="#cb67-229" aria-hidden="true" tabindex="-1"></a>second matrix is called the **between** matrix, and premultiplying a vector</span>
<span id="cb67-230"><a href="#cb67-230" aria-hidden="true" tabindex="-1"></a>by $B$ transforms it as a vector of individual means. These two symmetric</span>
<span id="cb67-231"><a href="#cb67-231" aria-hidden="true" tabindex="-1"></a>matrices have interesting properties:</span>
<span id="cb67-232"><a href="#cb67-232" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{within matrix}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{between matrix}</span>
<span id="cb67-233"><a href="#cb67-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-234"><a href="#cb67-234" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>they are **idempotent**, which means that $B B = B$ and $W W =W$.</span>
<span id="cb67-235"><a href="#cb67-235" aria-hidden="true" tabindex="-1"></a>    For example $W (Wz) = Wz$, as taking the deviations from the</span>
<span id="cb67-236"><a href="#cb67-236" aria-hidden="true" tabindex="-1"></a>    individual means of a vector of deviations from the individual means</span>
<span id="cb67-237"><a href="#cb67-237" aria-hidden="true" tabindex="-1"></a>    leaves this vector unchanged,\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{idempotent matrix}</span>
<span id="cb67-238"><a href="#cb67-238" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>they are **orthogonal**, which means that $W B = B W = 0$. For</span>
<span id="cb67-239"><a href="#cb67-239" aria-hidden="true" tabindex="-1"></a>    example $W (Bz) = 0$ because the deviations from the individual</span>
<span id="cb67-240"><a href="#cb67-240" aria-hidden="true" tabindex="-1"></a>    means of a vector of individual means are zero,</span>
<span id="cb67-241"><a href="#cb67-241" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>they sum to the **identity matrix**, $W + B = I$. $Wz + Bz=z$,</span>
<span id="cb67-242"><a href="#cb67-242" aria-hidden="true" tabindex="-1"></a>    because the sum of the deviations from the individual means of a</span>
<span id="cb67-243"><a href="#cb67-243" aria-hidden="true" tabindex="-1"></a>    vector and its individual means is the vector itself.</span>
<span id="cb67-244"><a href="#cb67-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-245"><a href="#cb67-245" aria-hidden="true" tabindex="-1"></a>$W$ and $B$ therefore perform an **orthogonal decomposition** of a</span>
<span id="cb67-246"><a href="#cb67-246" aria-hidden="true" tabindex="-1"></a>vector. One advantage of this decomposition is that it is very easy to</span>
<span id="cb67-247"><a href="#cb67-247" aria-hidden="true" tabindex="-1"></a>obtain powers of $\Omega$. For example, the inverse of $\Omega$ is:</span>
<span id="cb67-248"><a href="#cb67-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-249"><a href="#cb67-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-250"><a href="#cb67-250" aria-hidden="true" tabindex="-1"></a>\Omega ^ {-1} = \frac{1}{\sigma_\nu ^ 2} W + \frac{1}{\sigma_\iota ^ 2}B</span>
<span id="cb67-251"><a href="#cb67-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-252"><a href="#cb67-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-253"><a href="#cb67-253" aria-hidden="true" tabindex="-1"></a>and, more generally, for any power $r$ (either an integer or a</span>
<span id="cb67-254"><a href="#cb67-254" aria-hidden="true" tabindex="-1"></a>rational):</span>
<span id="cb67-255"><a href="#cb67-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-256"><a href="#cb67-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-257"><a href="#cb67-257" aria-hidden="true" tabindex="-1"></a>\Omega ^ {r} = {\sigma_\nu ^ 2} ^ r W + {\sigma_\iota ^ 2} ^ rB</span>
<span id="cb67-258"><a href="#cb67-258" aria-hidden="true" tabindex="-1"></a>$$ {#eq-poweromegaec}</span>
<span id="cb67-259"><a href="#cb67-259" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{correlated errors|)}</span>
<span id="cb67-260"><a href="#cb67-260" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data|)}</span>
<span id="cb67-261"><a href="#cb67-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-262"><a href="#cb67-262" aria-hidden="true" tabindex="-1"></a><span class="fu">### System of equations</span></span>
<span id="cb67-263"><a href="#cb67-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-264"><a href="#cb67-264" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation}</span>
<span id="cb67-265"><a href="#cb67-265" aria-hidden="true" tabindex="-1"></a>We have seen in @sec-system_equation that, </span>
<span id="cb67-266"><a href="#cb67-266" aria-hidden="true" tabindex="-1"></a>for example in fields such as consumption or production analysis, it is more relevant to consider the estimation of system of equations instead of the estimation of a single equation. In matrix form, the model corresponding to the whole system was presented in @eq-system_equation. We've seen in @sec-system_equation that a first advantage of considering the whole system of equations, and not an equation in isolation, is that restrictions on coefficients that concern different equations can be taken into account using the constrained least squares estimator.</span>
<span id="cb67-267"><a href="#cb67-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-268"><a href="#cb67-268" aria-hidden="true" tabindex="-1"></a>A second advantage is that, if the errors of the different equations for the same observation are correlated, these correlations can be taken into account if the whole system of equations is considered.</span>
<span id="cb67-269"><a href="#cb67-269" aria-hidden="true" tabindex="-1"></a>Denoting $\epsilon_l$ the vector of length $N$ containing the errors for the $l$^th^ equation and $\Xi = (\epsilon_1, \epsilon_2, \ldots \epsilon_L)$ the $N\times L$ matrix containing errors for the whole system, the covariance matrix of the errors for the system is:</span>
<span id="cb67-270"><a href="#cb67-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-271"><a href="#cb67-271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-272"><a href="#cb67-272" aria-hidden="true" tabindex="-1"></a>\Omega = </span>
<span id="cb67-273"><a href="#cb67-273" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\Xi \Xi^\top)</span>
<span id="cb67-274"><a href="#cb67-274" aria-hidden="true" tabindex="-1"></a>=\mbox{E}</span>
<span id="cb67-275"><a href="#cb67-275" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-276"><a href="#cb67-276" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cccc}</span>
<span id="cb67-277"><a href="#cb67-277" aria-hidden="true" tabindex="-1"></a>    \epsilon_1\epsilon_1^\top &amp; \epsilon_1 \epsilon_2^\top &amp; \ldots &amp; \epsilon_1 \epsilon_L^\top <span class="sc">\\</span></span>
<span id="cb67-278"><a href="#cb67-278" aria-hidden="true" tabindex="-1"></a>    \epsilon_2\epsilon_1^\top &amp; \epsilon_2 \epsilon_2^\top &amp; \ldots &amp; \epsilon_2 \epsilon_L^\top <span class="sc">\\</span></span>
<span id="cb67-279"><a href="#cb67-279" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-280"><a href="#cb67-280" aria-hidden="true" tabindex="-1"></a>    \epsilon_L\epsilon_1^\top &amp; \epsilon_L \epsilon_2^\top &amp; \ldots &amp; \epsilon_L \epsilon_L^\top <span class="sc">\\</span></span>
<span id="cb67-281"><a href="#cb67-281" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-282"><a href="#cb67-282" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-283"><a href="#cb67-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-284"><a href="#cb67-284" aria-hidden="true" tabindex="-1"></a>Assume that the errors of two equations $l$ and $m$ for the same</span>
<span id="cb67-285"><a href="#cb67-285" aria-hidden="true" tabindex="-1"></a>observation are correlated and that the covariance, denoted</span>
<span id="cb67-286"><a href="#cb67-286" aria-hidden="true" tabindex="-1"></a>$\sigma_{lm}$, is constant. The variance of errors for each equation $l$ is denoted $\sigma_{ll}$ and may be different from one equation to another. Moreover, we assume that errors for different individuals are uncorrelated. With these hypotheses, the covariance matrix is, denoting $I$ the identity matrix of dimension $N$:</span>
<span id="cb67-287"><a href="#cb67-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-288"><a href="#cb67-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-289"><a href="#cb67-289" aria-hidden="true" tabindex="-1"></a>\Omega=</span>
<span id="cb67-290"><a href="#cb67-290" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-291"><a href="#cb67-291" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb67-292"><a href="#cb67-292" aria-hidden="true" tabindex="-1"></a>  \sigma_1^2 I &amp; \sigma_{12} I &amp; \ldots &amp;\sigma_{1L} I <span class="sc">\\</span></span>
<span id="cb67-293"><a href="#cb67-293" aria-hidden="true" tabindex="-1"></a>  \sigma_{12} I &amp; \sigma_2^2 I &amp; \ldots &amp;\sigma_{2L} I <span class="sc">\\</span></span>
<span id="cb67-294"><a href="#cb67-294" aria-hidden="true" tabindex="-1"></a>  \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-295"><a href="#cb67-295" aria-hidden="true" tabindex="-1"></a>  \sigma_{1L} I &amp; \sigma_{2L} I &amp; \ldots &amp; \sigma_L^2 I</span>
<span id="cb67-296"><a href="#cb67-296" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-297"><a href="#cb67-297" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-298"><a href="#cb67-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-299"><a href="#cb67-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-300"><a href="#cb67-300" aria-hidden="true" tabindex="-1"></a>Denoting $\Sigma$ the $L\times L$ matrix of inter-equation variances and covariances, we have:</span>
<span id="cb67-301"><a href="#cb67-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-302"><a href="#cb67-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-303"><a href="#cb67-303" aria-hidden="true" tabindex="-1"></a>\Sigma=</span>
<span id="cb67-304"><a href="#cb67-304" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-305"><a href="#cb67-305" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cccc}</span>
<span id="cb67-306"><a href="#cb67-306" aria-hidden="true" tabindex="-1"></a>  \sigma_1^2 &amp; \sigma_{12} &amp; \ldots &amp;\sigma_{1L} <span class="sc">\\</span></span>
<span id="cb67-307"><a href="#cb67-307" aria-hidden="true" tabindex="-1"></a>  \sigma_{12} &amp; \sigma_2^2 &amp; \ldots &amp;\sigma_{2L} <span class="sc">\\</span></span>
<span id="cb67-308"><a href="#cb67-308" aria-hidden="true" tabindex="-1"></a>  \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-309"><a href="#cb67-309" aria-hidden="true" tabindex="-1"></a>  \sigma_{1L} &amp; \sigma_{2L} &amp; \ldots &amp; \sigma_L^2</span>
<span id="cb67-310"><a href="#cb67-310" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-311"><a href="#cb67-311" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-312"><a href="#cb67-312" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-313"><a href="#cb67-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-314"><a href="#cb67-314" aria-hidden="true" tabindex="-1"></a>and $\Omega=\Sigma \otimes I$. The inverse of the covariance matrix of the errors is easily obtained, as it requires only to compute the inverse of $\Sigma$: $\Omega ^ {-1} = \Sigma ^ {-1} \otimes I$.</span>
<span id="cb67-315"><a href="#cb67-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-316"><a href="#cb67-316" aria-hidden="true" tabindex="-1"></a><span class="fu">## Testing for non-spherical disturbances {#sec-test_non_spher}</span></span>
<span id="cb67-317"><a href="#cb67-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-318"><a href="#cb67-318" aria-hidden="true" tabindex="-1"></a>Numerous tests have been proposed to investigate whether, in different</span>
<span id="cb67-319"><a href="#cb67-319" aria-hidden="true" tabindex="-1"></a>contexts, the disturbances are spherical or not. Among them, we'll</span>
<span id="cb67-320"><a href="#cb67-320" aria-hidden="true" tabindex="-1"></a>present a family of tests that are based on OLS residuals. Even if the disturbances are non-spherical, OLS is</span>
<span id="cb67-321"><a href="#cb67-321" aria-hidden="true" tabindex="-1"></a>a consistent estimator and therefore OLS's residuals are a consistent</span>
<span id="cb67-322"><a href="#cb67-322" aria-hidden="true" tabindex="-1"></a>estimate of the errors of the model. Therefore, one can use these</span>
<span id="cb67-323"><a href="#cb67-323" aria-hidden="true" tabindex="-1"></a>residuals to analyze the unknown features of the errors.</span>
<span id="cb67-324"><a href="#cb67-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-325"><a href="#cb67-325" aria-hidden="true" tabindex="-1"></a><span class="fu">### Testing for heteroskedasticity</span></span>
<span id="cb67-326"><a href="#cb67-326" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!heteroskedasticity|(}</span>
<span id="cb67-327"><a href="#cb67-327" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Breusch Pagan test!heteroskedasticity|(}</span>
<span id="cb67-328"><a href="#cb67-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-329"><a href="#cb67-329" aria-hidden="true" tabindex="-1"></a>@BREU:PAGA:79\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Breusch}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Pagan} consider the following heteroskedastic model: $y_n = \gamma ^ \top z_n + \epsilon_n$ with $\epsilon_n \sim \mathcal{N}(0, \sigma_n ^ 2)$. Assume that $\sigma_n^2$ is a</span>
<span id="cb67-330"><a href="#cb67-330" aria-hidden="true" tabindex="-1"></a>function of a set of $J$ covariates denoted by $w_n$:</span>
<span id="cb67-331"><a href="#cb67-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-332"><a href="#cb67-332" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-333"><a href="#cb67-333" aria-hidden="true" tabindex="-1"></a>\sigma_n ^ 2 = h(\delta ^ \top w_n)</span>
<span id="cb67-334"><a href="#cb67-334" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-335"><a href="#cb67-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-336"><a href="#cb67-336" aria-hidden="true" tabindex="-1"></a>The first element of $w$ is 1, so that the homoskedasticity</span>
<span id="cb67-337"><a href="#cb67-337" aria-hidden="true" tabindex="-1"></a>hypothesis is that all the elements of $\delta$ except the first one are</span>
<span id="cb67-338"><a href="#cb67-338" aria-hidden="true" tabindex="-1"></a>0: $\delta_0^\top = (\alpha, 0, \ldots, 0)$, so that $\sigma ^ 2_n = h(\delta_0^\top w_n) = h_0$.</span>
<span id="cb67-339"><a href="#cb67-339" aria-hidden="true" tabindex="-1"></a>The log-likelihood function is:</span>
<span id="cb67-340"><a href="#cb67-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-341"><a href="#cb67-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-342"><a href="#cb67-342" aria-hidden="true" tabindex="-1"></a>\ln L = -\frac{N}{2}\ln 2\pi - \frac{1}{2} \sum_n \ln \sigma_n ^ 2 - \frac{1}{2}\sum_n \frac{(y_n - \gamma z_n)^2}{\sigma_n ^ 2}</span>
<span id="cb67-343"><a href="#cb67-343" aria-hidden="true" tabindex="-1"></a>$$ The derivative of $\ln L$ with $\delta$ is, denoting $h'_n = \frac{\partial h}{\partial \delta}(w_n)$:</span>
<span id="cb67-344"><a href="#cb67-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-345"><a href="#cb67-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-346"><a href="#cb67-346" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln L}{\partial \delta} = \frac{1}{2}\sum_n \left(\frac{\epsilon ^ 2}{\sigma_n ^ 4} - \frac{1}{\sigma_n ^ 2}\right) h'_n w_n</span>
<span id="cb67-347"><a href="#cb67-347" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gen_score_bp_heter}</span>
<span id="cb67-348"><a href="#cb67-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-349"><a href="#cb67-349" aria-hidden="true" tabindex="-1"></a>With the homoskedasticity hypothesis, $\sigma_n = \sigma$ and $h'_n = h_n'(\delta_0) = h_0'$ and @eq-gen_score_bp_heter simplifies to:</span>
<span id="cb67-350"><a href="#cb67-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-351"><a href="#cb67-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-352"><a href="#cb67-352" aria-hidden="true" tabindex="-1"></a>d = \frac{\partial \ln L}{\partial \delta}(\delta_0) = \frac{h'_0}{2\sigma ^ 2}\sum_n \left(\frac{\epsilon_n ^ 2}{\sigma ^ 2} - 1\right) w_n</span>
<span id="cb67-353"><a href="#cb67-353" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-354"><a href="#cb67-354" aria-hidden="true" tabindex="-1"></a>The second derivatives are:</span>
<span id="cb67-355"><a href="#cb67-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-356"><a href="#cb67-356" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-357"><a href="#cb67-357" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ln ^ 2 L}{\partial \delta \partial\delta ^ \top} =</span>
<span id="cb67-358"><a href="#cb67-358" aria-hidden="true" tabindex="-1"></a>\frac{1}{2}\sum_n\left[ h_n''\left(\frac{\epsilon_n ^ 2}{\sigma_n ^ 4}-</span>
<span id="cb67-359"><a href="#cb67-359" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma_n ^ 2} \right)- h_n^{'2} \left(\frac{2 \epsilon_n ^</span>
<span id="cb67-360"><a href="#cb67-360" aria-hidden="true" tabindex="-1"></a>2}{\sigma_n ^ 6}- \frac{1}{\sigma_n ^ 4} \right)\right]w_n w_n'</span>
<span id="cb67-361"><a href="#cb67-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-362"><a href="#cb67-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-363"><a href="#cb67-363" aria-hidden="true" tabindex="-1"></a>To get the information matrix, we take the expectation of the opposite</span>
<span id="cb67-364"><a href="#cb67-364" aria-hidden="true" tabindex="-1"></a>of this matrix. If the errors are homoskedastic, the first term disappears and $h'_n = h'_0$ so that:</span>
<span id="cb67-365"><a href="#cb67-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-366"><a href="#cb67-366" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-367"><a href="#cb67-367" aria-hidden="true" tabindex="-1"></a>I_0 = \mbox{E}\left(- \frac{\partial \ln ^ 2 L}{\partial \delta \partial\delta ^ \top}(\delta_0)\right) = \frac{h_0^{'2}}{2\sigma ^ 4}\sum_nw_n w_n'</span>
<span id="cb67-368"><a href="#cb67-368" aria-hidden="true" tabindex="-1"></a>$$ {#eq-info}</span>
<span id="cb67-369"><a href="#cb67-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-370"><a href="#cb67-370" aria-hidden="true" tabindex="-1"></a>Denoting $\hat{\epsilon}$ the vector of OLS residuals and $\hat{\sigma} ^ 2 = \hat{\epsilon} ^ \top \hat{\epsilon} / N$ the estimate of $\sigma ^ 2$, the estimated score is:</span>
<span id="cb67-371"><a href="#cb67-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-372"><a href="#cb67-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-373"><a href="#cb67-373" aria-hidden="true" tabindex="-1"></a>\hat{d} = \frac{h'_0}{2\hat{\sigma} ^ 2}\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n</span>
<span id="cb67-374"><a href="#cb67-374" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-375"><a href="#cb67-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-376"><a href="#cb67-376" aria-hidden="true" tabindex="-1"></a>and the test statistic is the quadratic form of $\hat{d}$ with the</span>
<span id="cb67-377"><a href="#cb67-377" aria-hidden="true" tabindex="-1"></a>inverse of its variance given by @eq-info:</span>
<span id="cb67-378"><a href="#cb67-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-379"><a href="#cb67-379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-380"><a href="#cb67-380" aria-hidden="true" tabindex="-1"></a>LM = \frac{1}{2} \left<span class="co">[</span><span class="ot">\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n^\top\right</span><span class="co">]</span> \left(\sum_n w_n w_n ^ \top\right)^{-1}</span>
<span id="cb67-381"><a href="#cb67-381" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot">\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n\right</span><span class="co">]</span></span>
<span id="cb67-382"><a href="#cb67-382" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-383"><a href="#cb67-383" aria-hidden="true" tabindex="-1"></a>or, in matrix form, denoting $f$ the $N$-length vector with typical</span>
<span id="cb67-384"><a href="#cb67-384" aria-hidden="true" tabindex="-1"></a>element $\left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right)$</span>
<span id="cb67-385"><a href="#cb67-385" aria-hidden="true" tabindex="-1"></a>and $W$ the matrix of covariates:</span>
<span id="cb67-386"><a href="#cb67-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-387"><a href="#cb67-387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-388"><a href="#cb67-388" aria-hidden="true" tabindex="-1"></a>LM =\frac{1}{2}f^\top W (W^\top W)^{-1} W f = \frac{1}{2}f ^ \top P_W f</span>
<span id="cb67-389"><a href="#cb67-389" aria-hidden="true" tabindex="-1"></a>$$ {#eq-bp_heteros_ESS}</span>
<span id="cb67-390"><a href="#cb67-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-391"><a href="#cb67-391" aria-hidden="true" tabindex="-1"></a>which is half the explained sum of squares of a regression of $f_n$ on</span>
<span id="cb67-392"><a href="#cb67-392" aria-hidden="true" tabindex="-1"></a>$w_n$ and is a $\chi ^ 2$ with $J$ degrees of freedom in case of homoskedasticity.</span>
<span id="cb67-393"><a href="#cb67-393" aria-hidden="true" tabindex="-1"></a>Note also that</span>
<span id="cb67-394"><a href="#cb67-394" aria-hidden="true" tabindex="-1"></a>$f^\top f / N = \sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) ^ 2 / N= \sum_n\left(\frac{\hat{\epsilon}_n ^ 4}{\hat{\sigma} ^ 4} + 1 - 2 \frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2}\right) / N$</span>
<span id="cb67-395"><a href="#cb67-395" aria-hidden="true" tabindex="-1"></a>is the total sum of squares divided by $N$. It converges to</span>
<span id="cb67-396"><a href="#cb67-396" aria-hidden="true" tabindex="-1"></a>2 as the first term is the fourth center moment of a normal variable, which is 3. Therefore, a second version of the statistic can be computed as $N$ times the R^2^ of a regression of the first-step residuals on $w$:</span>
<span id="cb67-397"><a href="#cb67-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-398"><a href="#cb67-398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-399"><a href="#cb67-399" aria-hidden="true" tabindex="-1"></a>N R^2 = \frac{f^\top P_W f}{f'f / N}</span>
<span id="cb67-400"><a href="#cb67-400" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-401"><a href="#cb67-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-402"><a href="#cb67-402" aria-hidden="true" tabindex="-1"></a>@WHIT:80\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{White} proposed a test that is directly linked to its proposition of</span>
<span id="cb67-403"><a href="#cb67-403" aria-hidden="true" tabindex="-1"></a>the heteroskedasticity-robust matrix of covariance of the OLS</span>
<span id="cb67-404"><a href="#cb67-404" aria-hidden="true" tabindex="-1"></a>estimates.^<span class="co">[</span><span class="ot">See @sec-sandwich.</span><span class="co">]</span> This matrix depends on the squares and on the</span>
<span id="cb67-405"><a href="#cb67-405" aria-hidden="true" tabindex="-1"></a>cross-products of the covariates. Therefore, he proposed to run a</span>
<span id="cb67-406"><a href="#cb67-406" aria-hidden="true" tabindex="-1"></a>regression of the squares of the first-step residuals on the covariates,</span>
<span id="cb67-407"><a href="#cb67-407" aria-hidden="true" tabindex="-1"></a>their squares and their cross-product. $N R^2$ of this regression is</span>
<span id="cb67-408"><a href="#cb67-408" aria-hidden="true" tabindex="-1"></a>asymptotically distributed as a $\chi ^ 2$ with $K(K+1)/2$ degrees of freedom. Therefore, White's test</span>
<span id="cb67-409"><a href="#cb67-409" aria-hidden="true" tabindex="-1"></a>can be viewed as a special case of Breusch and Pagan's test\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Breusch}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Pagan}.</span>
<span id="cb67-410"><a href="#cb67-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-411"><a href="#cb67-411" aria-hidden="true" tabindex="-1"></a>In his electric consumption regression, @HOUT:51\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Houthakker} used as covariates <span class="in">`inc`</span> (average yearly income in pounds), the inverse of <span class="in">`mc6`</span> (the marginal cost of electricity), <span class="in">`gas6`</span> (the marginal price of gas) and <span class="in">`cap`</span> (the average holdings of heavy electric equipment). The OLS estimation is:</span>
<span id="cb67-412"><a href="#cb67-412" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{uk<span class="sc">\_</span>elec}{micsr.data}</span>
<span id="cb67-413"><a href="#cb67-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-416"><a href="#cb67-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-417"><a href="#cb67-417" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lm_elec</span></span>
<span id="cb67-418"><a href="#cb67-418" aria-hidden="true" tabindex="-1"></a>lm_elec <span class="ot">&lt;-</span> <span class="fu">lm</span>(kwh <span class="sc">~</span> inc <span class="sc">+</span> <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> mc6) <span class="sc">+</span>  gas6 <span class="sc">+</span> cap, uk_elec)</span>
<span id="cb67-419"><a href="#cb67-419" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-420"><a href="#cb67-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-421"><a href="#cb67-421" aria-hidden="true" tabindex="-1"></a>The $f$ vector in @eq-bp_heteros_ESS is:</span>
<span id="cb67-422"><a href="#cb67-422" aria-hidden="true" tabindex="-1"></a>\idxfun{resid}{stats}</span>
<span id="cb67-423"><a href="#cb67-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-426"><a href="#cb67-426" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-427"><a href="#cb67-427" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: f_bp_heterosc</span></span>
<span id="cb67-428"><a href="#cb67-428" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">resid</span>(lm_elec) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> <span class="fu">mean</span>(<span class="fu">resid</span>(lm_elec) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb67-429"><a href="#cb67-429" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-430"><a href="#cb67-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-431"><a href="#cb67-431" aria-hidden="true" tabindex="-1"></a>We then regress $f$ on $W$ (which is here a constant and the inverse of <span class="in">`cust`</span>) and get half the explained sum of squares:</span>
<span id="cb67-432"><a href="#cb67-432" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{fitted}{stats}</span>
<span id="cb67-433"><a href="#cb67-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-436"><a href="#cb67-436" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-437"><a href="#cb67-437" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bp_heterosc_1</span></span>
<span id="cb67-438"><a href="#cb67-438" aria-hidden="true" tabindex="-1"></a>lm_elec_resid <span class="ot">&lt;-</span> <span class="fu">lm</span>(f <span class="sc">~</span> <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> cust), uk_elec)</span>
<span id="cb67-439"><a href="#cb67-439" aria-hidden="true" tabindex="-1"></a>bp1_elec <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">fitted</span>(lm_elec_resid) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb67-440"><a href="#cb67-440" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-441"><a href="#cb67-441" aria-hidden="true" tabindex="-1"></a>For the second version of the test, we compute $N$ times the $R ^ 2$:</span>
<span id="cb67-442"><a href="#cb67-442" aria-hidden="true" tabindex="-1"></a>\idxfun{nobs}{stats}\idxfun{rsq}{micsr}</span>
<span id="cb67-443"><a href="#cb67-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-446"><a href="#cb67-446" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-447"><a href="#cb67-447" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bp_heterosc_2</span></span>
<span id="cb67-448"><a href="#cb67-448" aria-hidden="true" tabindex="-1"></a>bp2_elec <span class="ot">&lt;-</span> <span class="fu">nobs</span>(lm_elec_resid) <span class="sc">*</span> <span class="fu">rsq</span>(lm_elec_resid)</span>
<span id="cb67-449"><a href="#cb67-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-450"><a href="#cb67-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-451"><a href="#cb67-451" aria-hidden="true" tabindex="-1"></a>The values and the probability values for the two versions of the Breusch-Pagan test are:</span>
<span id="cb67-452"><a href="#cb67-452" aria-hidden="true" tabindex="-1"></a>\idxfun{pchisq}{stats}</span>
<span id="cb67-453"><a href="#cb67-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-456"><a href="#cb67-456" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-457"><a href="#cb67-457" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bp_heterosc_print</span></span>
<span id="cb67-458"><a href="#cb67-458" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-459"><a href="#cb67-459" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(bp1_elec, bp2_elec)</span>
<span id="cb67-460"><a href="#cb67-460" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="fu">c</span>(bp1_elec, bp2_elec), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>, <span class="at">df =</span> <span class="dv">1</span>)</span>
<span id="cb67-461"><a href="#cb67-461" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-462"><a href="#cb67-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-463"><a href="#cb67-463" aria-hidden="true" tabindex="-1"></a>The homoskedasticity hypothesis is therefore highly rejected.</span>
<span id="cb67-464"><a href="#cb67-464" aria-hidden="true" tabindex="-1"></a>The <span class="in">`lmtest::bptest`</span> computes automatically the Breusch-Pagan test for heteroskedasticity, with two formulas: the first being the formula of the model and the second being a one-side formula for the skedasticity equation. An alternative syntax is to provide a <span class="in">`lm`</span> model as the first argument:</span>
<span id="cb67-465"><a href="#cb67-465" aria-hidden="true" tabindex="-1"></a>\idxfun{bptest}{lmtest}</span>
<span id="cb67-466"><a href="#cb67-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-469"><a href="#cb67-469" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-470"><a href="#cb67-470" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lmtest_bptest</span></span>
<span id="cb67-471"><a href="#cb67-471" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb67-472"><a href="#cb67-472" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">bptest</span>(kwh <span class="sc">~</span> inc <span class="sc">+</span> <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> mc6) <span class="sc">+</span> gas6 <span class="sc">+</span> cap, <span class="sc">~</span> <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> cust), </span>
<span id="cb67-473"><a href="#cb67-473" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> uk_elec, <span class="at">studentize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb67-474"><a href="#cb67-474" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">bptest</span>(lm_elec, <span class="sc">~</span> <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> cust),</span>
<span id="cb67-475"><a href="#cb67-475" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> uk_elec, <span class="at">studentize =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb67-476"><a href="#cb67-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-477"><a href="#cb67-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-478"><a href="#cb67-478" aria-hidden="true" tabindex="-1"></a>Note that we set the <span class="in">`studentize`</span> argument to <span class="in">`FALSE`</span>. The default value is <span class="in">`TRUE`</span> and in this case, a modified version of the test due to @KOEN:81\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Koenker} is used.</span>
<span id="cb67-479"><a href="#cb67-479" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{uk<span class="sc">\_</span>elec}{micsr.data}</span>
<span id="cb67-480"><a href="#cb67-480" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!heteroskedasticity|)}</span>
<span id="cb67-481"><a href="#cb67-481" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Breusch Pagan test!heteroskedasticity|)}</span>
<span id="cb67-482"><a href="#cb67-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-483"><a href="#cb67-483" aria-hidden="true" tabindex="-1"></a><span class="fu">### Testing for individual effects</span></span>
<span id="cb67-484"><a href="#cb67-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-485"><a href="#cb67-485" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!individual effects|(}</span>
<span id="cb67-486"><a href="#cb67-486" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Breusch Pagan test!individual effects|(}</span>
<span id="cb67-487"><a href="#cb67-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-488"><a href="#cb67-488" aria-hidden="true" tabindex="-1"></a>@BREU:PAGA:80\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Breusch}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Pagan} extend their Lagrange multiplier test  to the problem of individual (or entity) effects in a panel (or</span>
<span id="cb67-489"><a href="#cb67-489" aria-hidden="true" tabindex="-1"></a>in a pseudo-panel) setting. Assuming a normal distribution, the joint density for the whole sample is:</span>
<span id="cb67-490"><a href="#cb67-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-491"><a href="#cb67-491" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-492"><a href="#cb67-492" aria-hidden="true" tabindex="-1"></a>f(y\mid X) = \frac{1}{(2\pi) ^ {NT /2}\mid\Omega\mid}e^{-\frac{1}{2}\epsilon ^ \top \Omega ^ {-1} \epsilon}</span>
<span id="cb67-493"><a href="#cb67-493" aria-hidden="true" tabindex="-1"></a>$$ {#eq-joint_density_panel}</span>
<span id="cb67-494"><a href="#cb67-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-495"><a href="#cb67-495" aria-hidden="true" tabindex="-1"></a>We have seen (@eq-omega_panel) that $\Omega = \sigma_\nu ^ 2 W + \sigma_\iota ^ 2B$, with</span>
<span id="cb67-496"><a href="#cb67-496" aria-hidden="true" tabindex="-1"></a>$\sigma_\iota ^ 2 = \sigma_\nu + T \sigma_\eta$. Then, </span>
<span id="cb67-497"><a href="#cb67-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-498"><a href="#cb67-498" aria-hidden="true" tabindex="-1"></a>$$\epsilon^ \top \Omega ^ {-1} \epsilon = </span>
<span id="cb67-499"><a href="#cb67-499" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma_\nu ^ 2}\epsilon ^ \top W \epsilon + \frac{1}{\sigma_\iota ^ 2}\epsilon ^ \top B \epsilon$$</span>
<span id="cb67-500"><a href="#cb67-500" aria-hidden="true" tabindex="-1"></a>The determinant of $\Omega$ is the product of its eigenvalues, which are $\sigma_\nu ^ 2$ with periodicity $N(T-1)$ and $\sigma_\iota^2$ with periodicity $N$. Then, taking the logarithm of @eq-joint_density_panel and denoting $\theta^ \top = (\sigma_\nu ^ 2, \sigma_\eta ^ 2)$, we get the following log-likelihood function:</span>
<span id="cb67-501"><a href="#cb67-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-502"><a href="#cb67-502" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-503"><a href="#cb67-503" aria-hidden="true" tabindex="-1"></a>\ln L (\theta)= \frac{NT}{2} \ln 2\pi - \frac{N(T-1)}{2}\ln \sigma_\nu ^ 2 - \frac{N}{2}\ln(T\sigma_\eta ^ 2 + \sigma_\nu ^ 2) - \frac{\epsilon ^ \top W \epsilon}{2\sigma_\nu ^ 2}- \frac{\epsilon ^ \top W \epsilon}{2(\sigma_\nu ^ 2 + T \sigma_\eta ^ 2)}</span>
<span id="cb67-504"><a href="#cb67-504" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-505"><a href="#cb67-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-506"><a href="#cb67-506" aria-hidden="true" tabindex="-1"></a>The gradient and the hessian are respectively:</span>
<span id="cb67-507"><a href="#cb67-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-508"><a href="#cb67-508" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-509"><a href="#cb67-509" aria-hidden="true" tabindex="-1"></a>g(\theta)=</span>
<span id="cb67-510"><a href="#cb67-510" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-511"><a href="#cb67-511" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cc}</span>
<span id="cb67-512"><a href="#cb67-512" aria-hidden="true" tabindex="-1"></a>    \frac{\partial \ln L}{\partial \sigma_\nu^2} <span class="sc">\\</span> \frac{\partial</span>
<span id="cb67-513"><a href="#cb67-513" aria-hidden="true" tabindex="-1"></a>      \ln L}{\partial \sigma_\eta^2} <span class="sc">\\</span></span>
<span id="cb67-514"><a href="#cb67-514" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-515"><a href="#cb67-515" aria-hidden="true" tabindex="-1"></a>  \right)</span>
<span id="cb67-516"><a href="#cb67-516" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb67-517"><a href="#cb67-517" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-518"><a href="#cb67-518" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cc}</span>
<span id="cb67-519"><a href="#cb67-519" aria-hidden="true" tabindex="-1"></a>    -\frac{N(T-1)}{2\sigma_\nu^2}-\frac{N}{2\sigma_\iota^2}+</span>
<span id="cb67-520"><a href="#cb67-520" aria-hidden="true" tabindex="-1"></a>    \frac{\epsilon^\top W\epsilon}{2\sigma_\nu^4}+\frac{\epsilon^\top B_\eta\epsilon}{2\sigma_\iota^2}<span class="sc">\\</span> </span>
<span id="cb67-521"><a href="#cb67-521" aria-hidden="true" tabindex="-1"></a>    -\frac{NT}{2\sigma_\iota^2}+\frac{T</span>
<span id="cb67-522"><a href="#cb67-522" aria-hidden="true" tabindex="-1"></a>    \epsilon^\top B\epsilon}{2\sigma_\iota^2}</span>
<span id="cb67-523"><a href="#cb67-523" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-524"><a href="#cb67-524" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-525"><a href="#cb67-525" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-526"><a href="#cb67-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-527"><a href="#cb67-527" aria-hidden="true" tabindex="-1"></a>$$ H(\theta)= \left(</span>
<span id="cb67-528"><a href="#cb67-528" aria-hidden="true" tabindex="-1"></a>\begin{array}{ll}</span>
<span id="cb67-529"><a href="#cb67-529" aria-hidden="true" tabindex="-1"></a>  -\frac{N(T-1)}{2\sigma_\nu^4}+\frac{N}{2\sigma_\iota^4}-</span>
<span id="cb67-530"><a href="#cb67-530" aria-hidden="true" tabindex="-1"></a>  \frac{\epsilon^\top W\epsilon}{\sigma_\nu^6}-\frac{\epsilon^\top B\epsilon}{\sigma_\iota^6} </span>
<span id="cb67-531"><a href="#cb67-531" aria-hidden="true" tabindex="-1"></a>  &amp; \frac{NT}{2\sigma_\iota^4}-\frac{T\epsilon^\top B\epsilon}{\sigma_\iota^6}<span class="sc">\\</span></span>
<span id="cb67-532"><a href="#cb67-532" aria-hidden="true" tabindex="-1"></a>  \frac{NT}{2\sigma_\iota^4}-</span>
<span id="cb67-533"><a href="#cb67-533" aria-hidden="true" tabindex="-1"></a>  \frac{T\epsilon^\top B\epsilon}{\sigma_\iota^6} </span>
<span id="cb67-534"><a href="#cb67-534" aria-hidden="true" tabindex="-1"></a>  &amp;\frac{NT^2}{2\sigma_\iota^4} -</span>
<span id="cb67-535"><a href="#cb67-535" aria-hidden="true" tabindex="-1"></a>    \frac{T^2 \epsilon^\top B\epsilon}{\sigma_\iota ^ 6}</span>
<span id="cb67-536"><a href="#cb67-536" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-537"><a href="#cb67-537" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-538"><a href="#cb67-538" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-539"><a href="#cb67-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-540"><a href="#cb67-540" aria-hidden="true" tabindex="-1"></a>To compute the expectation of this matrix, we note that</span>
<span id="cb67-541"><a href="#cb67-541" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon^\top W_\eta\epsilon)=N(T-1)\sigma_\nu^2$ and</span>
<span id="cb67-542"><a href="#cb67-542" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon^\top B_\eta\epsilon)=N\sigma_\iota ^ 2$:</span>
<span id="cb67-543"><a href="#cb67-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-544"><a href="#cb67-544" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-545"><a href="#cb67-545" aria-hidden="true" tabindex="-1"></a>\mbox{E}(H(\theta))= \left(</span>
<span id="cb67-546"><a href="#cb67-546" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb67-547"><a href="#cb67-547" aria-hidden="true" tabindex="-1"></a>  -\frac{N(T-1)}{2\sigma_\nu^4}-\frac{N}{2\sigma_\iota ^ 4} </span>
<span id="cb67-548"><a href="#cb67-548" aria-hidden="true" tabindex="-1"></a>  &amp; -\frac{NT}{2\sigma_\iota ^ 4}<span class="sc">\\</span></span>
<span id="cb67-549"><a href="#cb67-549" aria-hidden="true" tabindex="-1"></a>  -\frac{NT}{2\sigma_\iota ^ 4} </span>
<span id="cb67-550"><a href="#cb67-550" aria-hidden="true" tabindex="-1"></a>  &amp; -\frac{NT^2}{2\sigma_\iota ^ 4}</span>
<span id="cb67-551"><a href="#cb67-551" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-552"><a href="#cb67-552" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-553"><a href="#cb67-553" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-554"><a href="#cb67-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-555"><a href="#cb67-555" aria-hidden="true" tabindex="-1"></a>To compute the test statistic, we impose the null hypothesis:</span>
<span id="cb67-556"><a href="#cb67-556" aria-hidden="true" tabindex="-1"></a>$H_0: \sigma_\eta^2=0$ (no individual effects), so that $\sigma_\iota^2= \sigma_\nu ^ 2$. In this case,</span>
<span id="cb67-557"><a href="#cb67-557" aria-hidden="true" tabindex="-1"></a>the OLS estimator is BLUE and </span>
<span id="cb67-558"><a href="#cb67-558" aria-hidden="true" tabindex="-1"></a>$\hat{\sigma}_\nu^2$ is $\hat{\epsilon}^\top\hat{\epsilon} / NT$. The</span>
<span id="cb67-559"><a href="#cb67-559" aria-hidden="true" tabindex="-1"></a>estimated score and the information matrix are, with $\hat{\theta}^\top = (\hat{\sigma}_\nu^ 2, 0)$</span>
<span id="cb67-560"><a href="#cb67-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-561"><a href="#cb67-561" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-562"><a href="#cb67-562" aria-hidden="true" tabindex="-1"></a>\hat{g}(\hat{\theta})=</span>
<span id="cb67-563"><a href="#cb67-563" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-564"><a href="#cb67-564" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cc}</span>
<span id="cb67-565"><a href="#cb67-565" aria-hidden="true" tabindex="-1"></a>    0 <span class="sc">\\</span> -\frac{NT}{2\hat{\sigma}_\nu^2}\left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)</span>
<span id="cb67-566"><a href="#cb67-566" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-567"><a href="#cb67-567" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-568"><a href="#cb67-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-569"><a href="#cb67-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-570"><a href="#cb67-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-571"><a href="#cb67-571" aria-hidden="true" tabindex="-1"></a>\hat{I}(\hat{\theta}) = \mbox{E}\left(-H(\hat{\theta})\right)=</span>
<span id="cb67-572"><a href="#cb67-572" aria-hidden="true" tabindex="-1"></a>\frac{NT}{2\hat{\sigma}_\nu^4}</span>
<span id="cb67-573"><a href="#cb67-573" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-574"><a href="#cb67-574" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb67-575"><a href="#cb67-575" aria-hidden="true" tabindex="-1"></a>  1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb67-576"><a href="#cb67-576" aria-hidden="true" tabindex="-1"></a>  1 &amp; T</span>
<span id="cb67-577"><a href="#cb67-577" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-578"><a href="#cb67-578" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-579"><a href="#cb67-579" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-580"><a href="#cb67-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-581"><a href="#cb67-581" aria-hidden="true" tabindex="-1"></a>and the inverse of the estimated information matrix is:</span>
<span id="cb67-582"><a href="#cb67-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-583"><a href="#cb67-583" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-584"><a href="#cb67-584" aria-hidden="true" tabindex="-1"></a>\hat{I}(\hat{\theta}) ^ {-1}=\frac{2\hat{\sigma}_\nu^4}{NT(T-1)}</span>
<span id="cb67-585"><a href="#cb67-585" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-586"><a href="#cb67-586" aria-hidden="true" tabindex="-1"></a>\begin{array}{cc}</span>
<span id="cb67-587"><a href="#cb67-587" aria-hidden="true" tabindex="-1"></a>  T &amp; -1 <span class="sc">\\</span></span>
<span id="cb67-588"><a href="#cb67-588" aria-hidden="true" tabindex="-1"></a>  -1 &amp; 1</span>
<span id="cb67-589"><a href="#cb67-589" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-590"><a href="#cb67-590" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-591"><a href="#cb67-591" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-592"><a href="#cb67-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-593"><a href="#cb67-593" aria-hidden="true" tabindex="-1"></a>Finally, the test statistic is computed as the quadratic form: $\hat{g}(\hat{\theta}) ^ \top \hat{I}(\hat{\theta}) ^ {-1} \hat{g}(\hat{\theta})$ which simplifies to:</span>
<span id="cb67-594"><a href="#cb67-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-595"><a href="#cb67-595" aria-hidden="true" tabindex="-1"></a>$$ LM = \left(-\frac{NT}{2\hat{\sigma}_\nu^2}</span>
<span id="cb67-596"><a href="#cb67-596" aria-hidden="true" tabindex="-1"></a>  \left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)\right)^2</span>
<span id="cb67-597"><a href="#cb67-597" aria-hidden="true" tabindex="-1"></a>\times \frac{2\hat{\sigma}_\nu^4}{NT(T-1)} =</span>
<span id="cb67-598"><a href="#cb67-598" aria-hidden="true" tabindex="-1"></a>\frac{NT}{2(T-1)}\left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)^2</span>
<span id="cb67-599"><a href="#cb67-599" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-600"><a href="#cb67-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-601"><a href="#cb67-601" aria-hidden="true" tabindex="-1"></a>Or, replacing $\hat{\sigma}_\nu^2$ by</span>
<span id="cb67-602"><a href="#cb67-602" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon}^\top\hat{\epsilon}/NT$:</span>
<span id="cb67-603"><a href="#cb67-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-604"><a href="#cb67-604" aria-hidden="true" tabindex="-1"></a>$$ LM = </span>
<span id="cb67-605"><a href="#cb67-605" aria-hidden="true" tabindex="-1"></a>\frac{NT}{2(T-1)}\left(T\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{\hat{\epsilon}^\top\hat{\epsilon}}-1\right)^2</span>
<span id="cb67-606"><a href="#cb67-606" aria-hidden="true" tabindex="-1"></a>$$ {#eq-bp_panel}</span>
<span id="cb67-607"><a href="#cb67-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-608"><a href="#cb67-608" aria-hidden="true" tabindex="-1"></a>which is asymptotically distributed as a $\chi^2$ with 1 degree of</span>
<span id="cb67-609"><a href="#cb67-609" aria-hidden="true" tabindex="-1"></a>freedom.</span>
<span id="cb67-610"><a href="#cb67-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-611"><a href="#cb67-611" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb67-612"><a href="#cb67-612" aria-hidden="true" tabindex="-1"></a>@BONJ:CHERK:KASK:03\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Bonjour}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Cherkas}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Haskel}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Hawkes}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Spector} estimated a Mincer equation with a sample of twins, the entity index being <span class="in">`family`</span>. The response is the log of wage and the covariates are education (<span class="in">`educ`</span>) and potential experience and its square (approximated by the age <span class="in">`age`</span>):</span>
<span id="cb67-613"><a href="#cb67-613" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb67-614"><a href="#cb67-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-617"><a href="#cb67-617" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-618"><a href="#cb67-618" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: twins_lm </span></span>
<span id="cb67-619"><a href="#cb67-619" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-620"><a href="#cb67-620" aria-hidden="true" tabindex="-1"></a>lm_twins <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>) <span class="sc">+</span> educ, twins)</span>
<span id="cb67-621"><a href="#cb67-621" aria-hidden="true" tabindex="-1"></a>lm_twins <span class="sc">%&gt;%</span> coef</span>
<span id="cb67-622"><a href="#cb67-622" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-623"><a href="#cb67-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-624"><a href="#cb67-624" aria-hidden="true" tabindex="-1"></a>We then add the residuals to the data and we compute the individual mean of the residuals by grouping by entities and then using <span class="in">`mutate`</span> and not <span class="in">`summarise`</span> to compute the mean,  $B\epsilon$ being a vector of length $N \times T$ where each value is returned $T$ times:</span>
<span id="cb67-625"><a href="#cb67-625" aria-hidden="true" tabindex="-1"></a>\idxfun{add<span class="sc">\_</span>column}{tibble}\idxfun{resid}{stats}\idxfun{group<span class="sc">\_</span>by}{dplyr}\idxfun{ungroup}{dplyr}</span>
<span id="cb67-626"><a href="#cb67-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-629"><a href="#cb67-629" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-630"><a href="#cb67-630" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: twins_resid_Bresid</span></span>
<span id="cb67-631"><a href="#cb67-631" aria-hidden="true" tabindex="-1"></a>twins <span class="ot">&lt;-</span> twins <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">e =</span> <span class="fu">resid</span>(lm_twins)) <span class="sc">%&gt;%</span> </span>
<span id="cb67-632"><a href="#cb67-632" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(family) <span class="sc">%&gt;%</span> </span>
<span id="cb67-633"><a href="#cb67-633" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Be =</span> <span class="fu">mean</span>(e)) <span class="sc">%&gt;%</span> ungroup</span>
<span id="cb67-634"><a href="#cb67-634" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-635"><a href="#cb67-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-636"><a href="#cb67-636" aria-hidden="true" tabindex="-1"></a>We finally compute the statistic using @eq-bp_panel:</span>
<span id="cb67-637"><a href="#cb67-637" aria-hidden="true" tabindex="-1"></a>\idxfun{pull}{dplyr}\idxfun{unique}{base}\idxfun{length}{base}\idxfun{summarise}{dplyr}</span>
<span id="cb67-638"><a href="#cb67-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-641"><a href="#cb67-641" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-642"><a href="#cb67-642" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bp_twins_manual</span></span>
<span id="cb67-643"><a href="#cb67-643" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-644"><a href="#cb67-644" aria-hidden="true" tabindex="-1"></a>N_tw <span class="ot">&lt;-</span> twins <span class="sc">%&gt;%</span> <span class="fu">pull</span>(family) <span class="sc">%&gt;%</span> unique <span class="sc">%&gt;%</span> length</span>
<span id="cb67-645"><a href="#cb67-645" aria-hidden="true" tabindex="-1"></a>T_tw <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb67-646"><a href="#cb67-646" aria-hidden="true" tabindex="-1"></a>twins <span class="sc">%&gt;%</span> </span>
<span id="cb67-647"><a href="#cb67-647" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">bp =</span> <span class="fl">0.5</span> <span class="sc">*</span> T_tw <span class="sc">*</span> N_tw <span class="sc">/</span> (T_tw <span class="sc">-</span> <span class="dv">1</span>)  <span class="sc">*</span> </span>
<span id="cb67-648"><a href="#cb67-648" aria-hidden="true" tabindex="-1"></a>              (T_tw <span class="sc">*</span> <span class="fu">sum</span>(Be <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>(e <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span> pull</span>
<span id="cb67-649"><a href="#cb67-649" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-650"><a href="#cb67-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-651"><a href="#cb67-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-652"><a href="#cb67-652" aria-hidden="true" tabindex="-1"></a>The **plm** package <span class="co">[</span><span class="ot">@CROI:MILL:08; @CROI:MILL:18</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Croissant}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Millo} provides different tools to deal with panel or pseudo-panel data. In particular, Breusch and Pagan's test\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Breusch}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Pagan} can easily be obtained using <span class="in">`plm::plmtest`</span>. We set the <span class="in">`type`</span> argument to <span class="in">`"bp"`</span> to get the statistic of the original Breusch-Pagan test:</span>
<span id="cb67-653"><a href="#cb67-653" aria-hidden="true" tabindex="-1"></a>\idxfun{plmtest}{plm}\idxfun{gaze}{micsr}</span>
<span id="cb67-654"><a href="#cb67-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-657"><a href="#cb67-657" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-658"><a href="#cb67-658" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bp_twins_plmtest</span></span>
<span id="cb67-659"><a href="#cb67-659" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-660"><a href="#cb67-660" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb67-661"><a href="#cb67-661" aria-hidden="true" tabindex="-1"></a><span class="fu">plmtest</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>) <span class="sc">+</span> educ, twins, <span class="at">type =</span> <span class="st">"bp"</span>) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb67-662"><a href="#cb67-662" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-663"><a href="#cb67-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-664"><a href="#cb67-664" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb67-665"><a href="#cb67-665" aria-hidden="true" tabindex="-1"></a>The absence of individual effects is rejected at the 5% level, but not at the 1% level.</span>
<span id="cb67-666"><a href="#cb67-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-667"><a href="#cb67-667" aria-hidden="true" tabindex="-1"></a>The model fitted by @SCHA:90\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Schaller} is a simple linear model, the response being the rate of investment (<span class="in">`ikn`</span>)  and the unique covariate Tobin's Q (<span class="in">`qn`</span>):</span>
<span id="cb67-668"><a href="#cb67-668" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb67-669"><a href="#cb67-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-672"><a href="#cb67-672" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-673"><a href="#cb67-673" aria-hidden="true" tabindex="-1"></a>tobinq <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb67-674"><a href="#cb67-674" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-675"><a href="#cb67-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-676"><a href="#cb67-676" aria-hidden="true" tabindex="-1"></a>The first two columns contain the firm and the time index. The Breusch-Pagan statistic is:</span>
<span id="cb67-677"><a href="#cb67-677" aria-hidden="true" tabindex="-1"></a>\idxfun{plmtest}{plm}\idxfun{gaze}{micsr}</span>
<span id="cb67-678"><a href="#cb67-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-681"><a href="#cb67-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-682"><a href="#cb67-682" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-683"><a href="#cb67-683" aria-hidden="true" tabindex="-1"></a><span class="fu">plmtest</span>(ikn <span class="sc">~</span> qn, tobinq, <span class="at">type =</span> <span class="st">"bp"</span>) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb67-684"><a href="#cb67-684" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-685"><a href="#cb67-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-686"><a href="#cb67-686" aria-hidden="true" tabindex="-1"></a>The statistic is huge, the hypothesis of no individual effects is therefore very strongly rejected, which is a quite customary result for panel data, especially when the time dimension is high, which is the case for the <span class="in">`tobinq`</span> data (35 years).</span>
<span id="cb67-687"><a href="#cb67-687" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb67-688"><a href="#cb67-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-689"><a href="#cb67-689" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!individual effects|)}</span>
<span id="cb67-690"><a href="#cb67-690" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Breusch Pagan test!individual effects|)}</span>
<span id="cb67-691"><a href="#cb67-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-692"><a href="#cb67-692" aria-hidden="true" tabindex="-1"></a><span class="fu">### System of equations {#sec-bptest_system}</span></span>
<span id="cb67-693"><a href="#cb67-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-694"><a href="#cb67-694" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!inter-equation correlation|(}</span>
<span id="cb67-695"><a href="#cb67-695" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Breusch Pagan test!inter-equation correlation|(}</span>
<span id="cb67-696"><a href="#cb67-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-697"><a href="#cb67-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-700"><a href="#cb67-700" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-701"><a href="#cb67-701" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: repeat_syslm</span></span>
<span id="cb67-702"><a href="#cb67-702" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb67-703"><a href="#cb67-703" aria-hidden="true" tabindex="-1"></a>ap <span class="ot">&lt;-</span> apples <span class="sc">%&gt;%</span> <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">1985</span>) <span class="sc">%&gt;%</span></span>
<span id="cb67-704"><a href="#cb67-704" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(<span class="at">y =</span> otherprod <span class="sc">+</span> apples,</span>
<span id="cb67-705"><a href="#cb67-705" aria-hidden="true" tabindex="-1"></a>              <span class="at">ct =</span> capital <span class="sc">+</span> labor <span class="sc">+</span> materials,</span>
<span id="cb67-706"><a href="#cb67-706" aria-hidden="true" tabindex="-1"></a>              <span class="at">sl =</span> labor <span class="sc">/</span> ct, <span class="at">sm =</span> materials <span class="sc">/</span> ct,</span>
<span id="cb67-707"><a href="#cb67-707" aria-hidden="true" tabindex="-1"></a>              <span class="at">pk =</span> <span class="fu">log</span>(pc <span class="sc">/</span> <span class="fu">mean</span>(pc)), <span class="at">pl =</span> <span class="fu">log</span>(pl <span class="sc">/</span> <span class="fu">mean</span>(pl)) <span class="sc">-</span> pk,</span>
<span id="cb67-708"><a href="#cb67-708" aria-hidden="true" tabindex="-1"></a>              <span class="at">pm =</span> <span class="fu">log</span>(pm <span class="sc">/</span> <span class="fu">mean</span>(pm)) <span class="sc">-</span> pk, <span class="at">ct =</span> <span class="fu">log</span>(ct <span class="sc">/</span> <span class="fu">mean</span>(ct)) <span class="sc">-</span> pk,</span>
<span id="cb67-709"><a href="#cb67-709" aria-hidden="true" tabindex="-1"></a>              <span class="at">y =</span> <span class="fu">log</span>(y <span class="sc">/</span> <span class="fu">mean</span>(y)), <span class="at">y2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> y <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb67-710"><a href="#cb67-710" aria-hidden="true" tabindex="-1"></a>              <span class="at">ct =</span> ct, <span class="at">pl2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> pl <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb67-711"><a href="#cb67-711" aria-hidden="true" tabindex="-1"></a>              <span class="at">pm2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> pm <span class="sc">^</span> <span class="dv">2</span>, <span class="at">plm =</span> pl <span class="sc">*</span> pm</span>
<span id="cb67-712"><a href="#cb67-712" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb67-713"><a href="#cb67-713" aria-hidden="true" tabindex="-1"></a>eq_ct <span class="ot">&lt;-</span> ct <span class="sc">~</span> y <span class="sc">+</span> y2 <span class="sc">+</span> pl <span class="sc">+</span> pm <span class="sc">+</span> pl2 <span class="sc">+</span> plm <span class="sc">+</span> pm2</span>
<span id="cb67-714"><a href="#cb67-714" aria-hidden="true" tabindex="-1"></a>eq_sl <span class="ot">&lt;-</span> sl <span class="sc">~</span> pl <span class="sc">+</span> pm</span>
<span id="cb67-715"><a href="#cb67-715" aria-hidden="true" tabindex="-1"></a>eq_sm <span class="ot">&lt;-</span> sm <span class="sc">~</span> pl <span class="sc">+</span> pm</span>
<span id="cb67-716"><a href="#cb67-716" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Formula)</span>
<span id="cb67-717"><a href="#cb67-717" aria-hidden="true" tabindex="-1"></a>eq_sys <span class="ot">&lt;-</span> <span class="fu">Formula</span>(ct <span class="sc">+</span> sl <span class="sc">+</span> sm <span class="sc">~</span> y <span class="sc">+</span> y2 <span class="sc">+</span> pl <span class="sc">+</span> pm <span class="sc">+</span> pl2 <span class="sc">+</span> plm <span class="sc">+</span> pm2)</span>
<span id="cb67-718"><a href="#cb67-718" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(eq_sys, ap)  ; Z_c <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_ct, mf) </span>
<span id="cb67-719"><a href="#cb67-719" aria-hidden="true" tabindex="-1"></a>Z_l <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_sl, mf) ; Z_m <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_sm, mf)</span>
<span id="cb67-720"><a href="#cb67-720" aria-hidden="true" tabindex="-1"></a>nms_cols <span class="ot">&lt;-</span> <span class="cf">function</span>(x, label)</span>
<span id="cb67-721"><a href="#cb67-721" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(label, <span class="fu">c</span>(<span class="st">"cst"</span>, <span class="fu">colnames</span>(x)[<span class="sc">-</span><span class="dv">1</span>]), <span class="at">sep =</span> <span class="st">"_"</span>)</span>
<span id="cb67-722"><a href="#cb67-722" aria-hidden="true" tabindex="-1"></a>nms_c <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_c, <span class="st">"cost"</span>) ; nms_l <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_l, <span class="st">"sl"</span>)</span>
<span id="cb67-723"><a href="#cb67-723" aria-hidden="true" tabindex="-1"></a>nms_m <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_m, <span class="st">"sm"</span>)</span>
<span id="cb67-724"><a href="#cb67-724" aria-hidden="true" tabindex="-1"></a>Zs <span class="ot">&lt;-</span> Matrix<span class="sc">::</span><span class="fu">bdiag</span>(Z_c, Z_l, Z_m) <span class="sc">%&gt;%</span> as.matrix</span>
<span id="cb67-725"><a href="#cb67-725" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Zs) <span class="ot">&lt;-</span> <span class="fu">c</span>(nms_c, nms_l, nms_m)</span>
<span id="cb67-726"><a href="#cb67-726" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">model.part</span>(eq_sys, mf, <span class="at">rhs =</span> <span class="dv">0</span>, <span class="at">lhs =</span> <span class="dv">1</span>)</span>
<span id="cb67-727"><a href="#cb67-727" aria-hidden="true" tabindex="-1"></a>ys <span class="ot">&lt;-</span> Y <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">cols_vary =</span> <span class="st">"slowest"</span>, <span class="at">names_to =</span> <span class="st">"equation"</span>,</span>
<span id="cb67-728"><a href="#cb67-728" aria-hidden="true" tabindex="-1"></a>                         <span class="at">values_to =</span> <span class="st">"response"</span>)</span>
<span id="cb67-729"><a href="#cb67-729" aria-hidden="true" tabindex="-1"></a>stack_data <span class="ot">&lt;-</span> ys <span class="sc">%&gt;%</span> <span class="fu">bind_cols</span>(Zs)</span>
<span id="cb67-730"><a href="#cb67-730" aria-hidden="true" tabindex="-1"></a>ols_unconst <span class="ot">&lt;-</span> <span class="fu">lm</span>(response <span class="sc">~</span> . <span class="sc">-</span> <span class="dv">1</span> <span class="sc">-</span> equation, stack_data)</span>
<span id="cb67-731"><a href="#cb67-731" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">14</span>)</span>
<span id="cb67-732"><a href="#cb67-732" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">1</span>, <span class="fu">c</span>(<span class="dv">4</span>,  <span class="dv">9</span>)] <span class="ot">&lt;-</span> R[<span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">12</span>)] <span class="ot">&lt;-</span> R[<span class="dv">3</span>, <span class="fu">c</span>(<span class="dv">6</span>, <span class="dv">10</span>)] <span class="ot">&lt;-</span> R[<span class="dv">4</span>, <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">11</span>)] <span class="ot">&lt;-</span></span>
<span id="cb67-733"><a href="#cb67-733" aria-hidden="true" tabindex="-1"></a>    R[<span class="dv">5</span>, <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">13</span>)] <span class="ot">&lt;-</span> R[<span class="dv">6</span>, <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">14</span>)] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb67-734"><a href="#cb67-734" aria-hidden="true" tabindex="-1"></a>ols_const <span class="ot">&lt;-</span> <span class="fu">clm</span>(ols_unconst, R)</span>
<span id="cb67-735"><a href="#cb67-735" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-736"><a href="#cb67-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-737"><a href="#cb67-737" aria-hidden="true" tabindex="-1"></a>@BREU:PAGA:80\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Breusch}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Pagan} also proposed a test of correlation between equations in a system of equation. Remember that in this case, the covariance matrix of the errors for the whole system is $\Omega = \Sigma \otimes I$, where $\Sigma$ contains the variances (on the diagonal) and the covariances (off diagonal) of the errors of the $L$ equations. This symmetric matrix contains $L \times (L + 1) / 2$ distinct elements, $L \times (L + 1) / 2 - L = L \times (L - 1) / 2$ being covariances. The Breusch-Pagan test is based on the estimation of the covariance matrix using OLS residuals. Denoting $\hat{\Xi} = (\hat{\epsilon}_1, \hat{\epsilon}_2, \ldots \hat{\epsilon}_L)$ the matrix where each column contains the vector of residuals for one equation:</span>
<span id="cb67-738"><a href="#cb67-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-739"><a href="#cb67-739" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-740"><a href="#cb67-740" aria-hidden="true" tabindex="-1"></a>\hat{\Omega} = \hat{\Xi} ^ \top \hat{\Xi}=</span>
<span id="cb67-741"><a href="#cb67-741" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-742"><a href="#cb67-742" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb67-743"><a href="#cb67-743" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_1 ^ \top \hat{\epsilon_1} &amp;\hat{\epsilon}_1 ^ \top \hat{\epsilon_2} &amp; \ldots &amp;\hat{\epsilon}_1 ^ \top \hat{\epsilon_L} <span class="sc">\\</span></span>
<span id="cb67-744"><a href="#cb67-744" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_2 ^ \top \hat{\epsilon_1} &amp;\hat{\epsilon}_2 ^ \top \hat{\epsilon_2} &amp; \ldots &amp;\hat{\epsilon}_2 ^ \top \hat{\epsilon_L} <span class="sc">\\</span></span>
<span id="cb67-745"><a href="#cb67-745" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-746"><a href="#cb67-746" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_L ^ \top \hat{\epsilon_1} &amp;\hat{\epsilon}_L ^ \top \hat{\epsilon_2} &amp; \ldots &amp;\hat{\epsilon}_L ^ \top \hat{\epsilon_L}</span>
<span id="cb67-747"><a href="#cb67-747" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-748"><a href="#cb67-748" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-749"><a href="#cb67-749" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-750"><a href="#cb67-750" aria-hidden="true" tabindex="-1"></a>The coefficients of correlations are then estimated, denoting $\hat{\sigma} ^ 2_l = \hat{\epsilon}_l^\top \hat{\epsilon}_l/ N$ the estimated variances:</span>
<span id="cb67-751"><a href="#cb67-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-752"><a href="#cb67-752" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-753"><a href="#cb67-753" aria-hidden="true" tabindex="-1"></a>\hat{\rho}_{lm} = \frac{\hat{\epsilon}_l^\top \hat{\epsilon}_m / N}{\hat{\sigma}_l\hat{\sigma}_m}</span>
<span id="cb67-754"><a href="#cb67-754" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-755"><a href="#cb67-755" aria-hidden="true" tabindex="-1"></a>The statistic is then $N \sum_{l=1} ^ {L-1} \sum_{m = l + 1} ^ L \hat{\rho}_{lm}^2$ and is a $\chi^2$ with $L(L-1) / 2$ degrees of freedom if the hypothesis of no correlation is true.</span>
<span id="cb67-756"><a href="#cb67-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-757"><a href="#cb67-757" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb67-758"><a href="#cb67-758" aria-hidden="true" tabindex="-1"></a>We use the apple production example estimated in @sec-system_equation. The estimation for the whole system by OLS taking constraints into account was stored in an object called <span class="in">`ols_const`</span>. We extract the residuals and arrange them in a $N\times L$ matrix. Taking the cross-product of this matrix and dividing by $N$, we get $\hat{\Sigma}$:</span>
<span id="cb67-759"><a href="#cb67-759" aria-hidden="true" tabindex="-1"></a>\idxfun{nobs}{stats}\idxfun{resid}{stats}\idxfun{matrix}{base}\idxfun{crossprod}{base}</span>
<span id="cb67-760"><a href="#cb67-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-761"><a href="#cb67-761" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-762"><a href="#cb67-762" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: est_Sigma</span></span>
<span id="cb67-763"><a href="#cb67-763" aria-hidden="true" tabindex="-1"></a>N_ap <span class="ot">&lt;-</span> <span class="fu">nobs</span>(ols_const) <span class="sc">/</span> <span class="dv">3</span></span>
<span id="cb67-764"><a href="#cb67-764" aria-hidden="true" tabindex="-1"></a>EPS <span class="ot">&lt;-</span> ols_const <span class="sc">%&gt;%</span> resid <span class="sc">%&gt;%</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb67-765"><a href="#cb67-765" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(EPS) <span class="sc">/</span> N_ap</span>
<span id="cb67-766"><a href="#cb67-766" aria-hidden="true" tabindex="-1"></a>Sigma</span>
<span id="cb67-767"><a href="#cb67-767" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-768"><a href="#cb67-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-769"><a href="#cb67-769" aria-hidden="true" tabindex="-1"></a>We then compute the estimated standard deviation of the errors for every equation $\hat{\sigma}_l$ and we divide $\hat{\Sigma}$ by a matrix containing the products of the standard deviations, using the <span class="in">`outer`</span> function:</span>
<span id="cb67-770"><a href="#cb67-770" aria-hidden="true" tabindex="-1"></a>\idxfun{stder}{micsr}\idxfun{outer}{base}</span>
<span id="cb67-771"><a href="#cb67-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-772"><a href="#cb67-772" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-773"><a href="#cb67-773" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: est_corr_matrix</span></span>
<span id="cb67-774"><a href="#cb67-774" aria-hidden="true" tabindex="-1"></a>sig <span class="ot">&lt;-</span> Sigma <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-775"><a href="#cb67-775" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> Sigma <span class="sc">/</span> <span class="fu">outer</span>(sig, sig)</span>
<span id="cb67-776"><a href="#cb67-776" aria-hidden="true" tabindex="-1"></a>d</span>
<span id="cb67-777"><a href="#cb67-777" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-778"><a href="#cb67-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-779"><a href="#cb67-779" aria-hidden="true" tabindex="-1"></a>so that we get a matrix with ones on the diagonal and coefficients of</span>
<span id="cb67-780"><a href="#cb67-780" aria-hidden="true" tabindex="-1"></a>correlations off-diagonal. Then, we extract the off-diagonal elements</span>
<span id="cb67-781"><a href="#cb67-781" aria-hidden="true" tabindex="-1"></a>using <span class="in">`upper.tri`</span>, which returns a logical matrix with values of <span class="in">`TRUE`</span> above the diagonal:</span>
<span id="cb67-782"><a href="#cb67-782" aria-hidden="true" tabindex="-1"></a>\idxfun{upper.tri}{base}</span>
<span id="cb67-783"><a href="#cb67-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-786"><a href="#cb67-786" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-787"><a href="#cb67-787" aria-hidden="true" tabindex="-1"></a><span class="fu">upper.tri</span>(d)</span>
<span id="cb67-788"><a href="#cb67-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-789"><a href="#cb67-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-790"><a href="#cb67-790" aria-hidden="true" tabindex="-1"></a>Then, indexing <span class="in">`d`</span> by <span class="in">`upper.tri(d)`</span> returns a vector containing the three elements of the matrix that are above the diagonal:^<span class="co">[</span><span class="ot">As the matrix is symmetric, `lower.tri` could also have been used.</span><span class="co">]</span></span>
<span id="cb67-791"><a href="#cb67-791" aria-hidden="true" tabindex="-1"></a>\idxfun{upper.tri}{base}</span>
<span id="cb67-792"><a href="#cb67-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-795"><a href="#cb67-795" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-796"><a href="#cb67-796" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-797"><a href="#cb67-797" aria-hidden="true" tabindex="-1"></a>d[<span class="fu">upper.tri</span>(d)]</span>
<span id="cb67-798"><a href="#cb67-798" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-799"><a href="#cb67-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-800"><a href="#cb67-800" aria-hidden="true" tabindex="-1"></a>Finally, we sum the squares of the elements of this vector and multiply by the sample size to get the statistic:</span>
<span id="cb67-801"><a href="#cb67-801" aria-hidden="true" tabindex="-1"></a>\idxfun{upper.tri}{base}\idxfun{pchisq}{stats}</span>
<span id="cb67-802"><a href="#cb67-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-805"><a href="#cb67-805" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-806"><a href="#cb67-806" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: bptest_panel</span></span>
<span id="cb67-807"><a href="#cb67-807" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-808"><a href="#cb67-808" aria-hidden="true" tabindex="-1"></a>bp_stat <span class="ot">&lt;-</span> <span class="fu">sum</span>( d[<span class="fu">upper.tri</span>(d)] <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">*</span> N_ap</span>
<span id="cb67-809"><a href="#cb67-809" aria-hidden="true" tabindex="-1"></a>pval_pb <span class="ot">&lt;-</span> <span class="fu">pchisq</span>(bp_stat, <span class="at">df =</span> <span class="dv">3</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb67-810"><a href="#cb67-810" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(bp_stat, pval_pb)</span>
<span id="cb67-811"><a href="#cb67-811" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-812"><a href="#cb67-812" aria-hidden="true" tabindex="-1"></a>The hypothesis of no correlation is clearly rejected.</span>
<span id="cb67-813"><a href="#cb67-813" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{test!inter-equation correlation|)}</span>
<span id="cb67-814"><a href="#cb67-814" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Breusch Pagan test!inter-equation correlation|)}</span>
<span id="cb67-815"><a href="#cb67-815" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb67-816"><a href="#cb67-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-817"><a href="#cb67-817" aria-hidden="true" tabindex="-1"></a><span class="fu">## Robust inference {#sec-sandwich}</span></span>
<span id="cb67-818"><a href="#cb67-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-819"><a href="#cb67-819" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich|(}</span>
<span id="cb67-820"><a href="#cb67-820" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!sandwich|(}</span>
<span id="cb67-821"><a href="#cb67-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-822"><a href="#cb67-822" aria-hidden="true" tabindex="-1"></a>If the errors are not spherical, the simple estimator of the covariance</span>
<span id="cb67-823"><a href="#cb67-823" aria-hidden="true" tabindex="-1"></a>of the OLS estimates is biased. More general estimators can then be used</span>
<span id="cb67-824"><a href="#cb67-824" aria-hidden="true" tabindex="-1"></a>instead. These estimators use the residuals of the OLS estimator which,</span>
<span id="cb67-825"><a href="#cb67-825" aria-hidden="true" tabindex="-1"></a>in the context of this chapter, is an inefficient but consistent</span>
<span id="cb67-826"><a href="#cb67-826" aria-hidden="true" tabindex="-1"></a>estimator. We'll present the robust estimator of the covariance matrix of the OLS estimates first in the context of</span>
<span id="cb67-827"><a href="#cb67-827" aria-hidden="true" tabindex="-1"></a>the simple linear model and then for the multiple linear model.</span>
<span id="cb67-828"><a href="#cb67-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-829"><a href="#cb67-829" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple linear model</span></span>
<span id="cb67-830"><a href="#cb67-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-831"><a href="#cb67-831" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!simple linear regression!sandwich|(}</span>
<span id="cb67-832"><a href="#cb67-832" aria-hidden="true" tabindex="-1"></a>The variance of the slope estimated by OLS is:</span>
<span id="cb67-833"><a href="#cb67-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-834"><a href="#cb67-834" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-835"><a href="#cb67-835" aria-hidden="true" tabindex="-1"></a>\sigma_{\hat{\beta}} ^ 2 = </span>
<span id="cb67-836"><a href="#cb67-836" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta} \mid x) = \frac{\mbox{E}\left(\left<span class="co">[</span><span class="ot">\sum_n (x_n - \bar{x})\epsilon_n\right</span><span class="co">]</span>^2\mid x\right)}{\left(\sum_n (x_n - \bar{x}) ^ 2\right) ^ 2}</span>
<span id="cb67-837"><a href="#cb67-837" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-838"><a href="#cb67-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-839"><a href="#cb67-839" aria-hidden="true" tabindex="-1"></a>The numerator is the sum of the expectations of $N ^ 2$ terms. For</span>
<span id="cb67-840"><a href="#cb67-840" aria-hidden="true" tabindex="-1"></a>$N = 4$, replacing the errors $\epsilon_n$ by the OLS residuals</span>
<span id="cb67-841"><a href="#cb67-841" aria-hidden="true" tabindex="-1"></a>$\hat{\epsilon}_n$ and dropping the expectation operator, these 16 terms</span>
<span id="cb67-842"><a href="#cb67-842" aria-hidden="true" tabindex="-1"></a>can be presented conveniently in the following matrix:</span>
<span id="cb67-843"><a href="#cb67-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-844"><a href="#cb67-844" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-845"><a href="#cb67-845" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: robust_simple_linear_general</span></span>
<span id="cb67-846"><a href="#cb67-846" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb67-847"><a href="#cb67-847" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: "asis"</span></span>
<span id="cb67-848"><a href="#cb67-848" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"$$</span><span class="sc">\\</span><span class="st">small{</span><span class="sc">\n\\</span><span class="st">left(</span><span class="sc">\n\\</span><span class="st">begin{array}{cccc}</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb67-849"><a href="#cb67-849" aria-hidden="true" tabindex="-1"></a>strgr <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb67-850"><a href="#cb67-850" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb67-851"><a href="#cb67-851" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb67-852"><a href="#cb67-852" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="sc">==</span> j){</span>
<span id="cb67-853"><a href="#cb67-853" aria-hidden="true" tabindex="-1"></a>            strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">paste</span>(<span class="st">"(x_"</span>, i, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x})^2 </span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">epsilon}_"</span>, i, <span class="st">"^2"</span>, <span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb67-854"><a href="#cb67-854" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb67-855"><a href="#cb67-855" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb67-856"><a href="#cb67-856" aria-hidden="true" tabindex="-1"></a>            strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">paste</span>(<span class="st">"(x_"</span>, i, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x}) (x_"</span>, j, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x}) </span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">epsilon}_"</span>, i, <span class="st">"</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">epsilon}_"</span>, j,<span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb67-857"><a href="#cb67-857" aria-hidden="true" tabindex="-1"></a>        strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">ifelse</span>(j <span class="sc">==</span> <span class="dv">4</span>, <span class="st">" </span><span class="sc">\\\\</span><span class="st"> </span><span class="sc">\n</span><span class="st">"</span>, <span class="st">" &amp; </span><span class="sc">\n</span><span class="st">"</span>))</span>
<span id="cb67-858"><a href="#cb67-858" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb67-859"><a href="#cb67-859" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb67-860"><a href="#cb67-860" aria-hidden="true" tabindex="-1"></a>strgr <span class="ot">&lt;-</span> <span class="fu">paste</span>(strgr, <span class="at">collapse =</span> <span class="st">""</span>)</span>
<span id="cb67-861"><a href="#cb67-861" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(strgr)</span>
<span id="cb67-862"><a href="#cb67-862" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">end{array}</span><span class="sc">\n\\</span><span class="st">right)}$$"</span>)</span>
<span id="cb67-863"><a href="#cb67-863" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-864"><a href="#cb67-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-865"><a href="#cb67-865" aria-hidden="true" tabindex="-1"></a>The robust estimator is obtained by taking the sum of *some* of this</span>
<span id="cb67-866"><a href="#cb67-866" aria-hidden="true" tabindex="-1"></a>terms. Note first that the sum of all these terms is</span>
<span id="cb67-867"><a href="#cb67-867" aria-hidden="true" tabindex="-1"></a>$\left(\sum_{n = 1} ^ N (x_n - \bar{x})\hat{\epsilon}_{n}\right)^2$,</span>
<span id="cb67-868"><a href="#cb67-868" aria-hidden="true" tabindex="-1"></a>which is equal to zero as:</span>
<span id="cb67-869"><a href="#cb67-869" aria-hidden="true" tabindex="-1"></a>$\sum_{n = 1} ^ N (x_n - \bar{x})\hat{\epsilon}_n=0$. Therefore, it is</span>
<span id="cb67-870"><a href="#cb67-870" aria-hidden="true" tabindex="-1"></a>not relevant to sum *all* the terms of this matrix to get an estimator</span>
<span id="cb67-871"><a href="#cb67-871" aria-hidden="true" tabindex="-1"></a>of the variance of $\hat{\beta}$.</span>
<span id="cb67-872"><a href="#cb67-872" aria-hidden="true" tabindex="-1"></a>The first possibility is to take only the diagonal terms of this matrix,</span>
<span id="cb67-873"><a href="#cb67-873" aria-hidden="true" tabindex="-1"></a>which is relevant if we maintain the hypotheses that the errors are</span>
<span id="cb67-874"><a href="#cb67-874" aria-hidden="true" tabindex="-1"></a>uncorrelated. In this case, we get the so-called</span>
<span id="cb67-875"><a href="#cb67-875" aria-hidden="true" tabindex="-1"></a>**heteroskedastic-consistent** (**HC**) estimator of</span>
<span id="cb67-876"><a href="#cb67-876" aria-hidden="true" tabindex="-1"></a>$\sigma_{\hat{\beta}}$ proposed by @WHIT:80:\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{White}</span>
<span id="cb67-877"><a href="#cb67-877" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!heteroskedastic-consistent}</span>
<span id="cb67-878"><a href="#cb67-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-879"><a href="#cb67-879" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-880"><a href="#cb67-880" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\mbox{HC}\hat{\beta}} ^ 2 = \frac{1}{S_{xx} ^ 2}\sum_{n = 1}</span>
<span id="cb67-881"><a href="#cb67-881" aria-hidden="true" tabindex="-1"></a>^ N (x_n - \bar{x}) ^ 2 \hat{\epsilon}_n ^ 2 </span>
<span id="cb67-882"><a href="#cb67-882" aria-hidden="true" tabindex="-1"></a>$$ {#eq-vcovHC}</span>
<span id="cb67-883"><a href="#cb67-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-884"><a href="#cb67-884" aria-hidden="true" tabindex="-1"></a>Consider now the case where some errors are correlated. This often</span>
<span id="cb67-885"><a href="#cb67-885" aria-hidden="true" tabindex="-1"></a>happens when some observations share some common unobserved</span>
<span id="cb67-886"><a href="#cb67-886" aria-hidden="true" tabindex="-1"></a>characteristics which are included in their (therefore correlated)</span>
<span id="cb67-887"><a href="#cb67-887" aria-hidden="true" tabindex="-1"></a>errors. For example, if observations belong to different regions, their</span>
<span id="cb67-888"><a href="#cb67-888" aria-hidden="true" tabindex="-1"></a>errors may share some common unobserved features of the regions.</span>
<span id="cb67-889"><a href="#cb67-889" aria-hidden="true" tabindex="-1"></a>In our four observations case, suppose that the first two</span>
<span id="cb67-890"><a href="#cb67-890" aria-hidden="true" tabindex="-1"></a>observations belong to one group, and the two others to another group.</span>
<span id="cb67-891"><a href="#cb67-891" aria-hidden="true" tabindex="-1"></a>Then, a consistent estimator is obtained by summing the</span>
<span id="cb67-892"><a href="#cb67-892" aria-hidden="true" tabindex="-1"></a>following subset of elements of the preceding matrix:</span>
<span id="cb67-893"><a href="#cb67-893" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!cluster|(}</span>
<span id="cb67-894"><a href="#cb67-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-895"><a href="#cb67-895" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-896"><a href="#cb67-896" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb67-897"><a href="#cb67-897" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: asis</span></span>
<span id="cb67-898"><a href="#cb67-898" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: robust_simple_linear_cluster</span></span>
<span id="cb67-899"><a href="#cb67-899" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"$$</span><span class="sc">\\</span><span class="st">small{</span><span class="sc">\\</span><span class="st">left(</span><span class="sc">\\</span><span class="st">begin{array}{cccc}"</span>)</span>
<span id="cb67-900"><a href="#cb67-900" aria-hidden="true" tabindex="-1"></a>strgr <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb67-901"><a href="#cb67-901" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb67-902"><a href="#cb67-902" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb67-903"><a href="#cb67-903" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="sc">==</span> j){</span>
<span id="cb67-904"><a href="#cb67-904" aria-hidden="true" tabindex="-1"></a>            strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">paste</span>(<span class="st">"(x_"</span>, i, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x})^2 </span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">epsilon}_"</span>, i, <span class="st">"^2"</span>, <span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb67-905"><a href="#cb67-905" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb67-906"><a href="#cb67-906" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>{</span>
<span id="cb67-907"><a href="#cb67-907" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ( ( (i <span class="sc">%in%</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>) <span class="sc">&amp;</span> (j <span class="sc">%in%</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>) ) <span class="sc">|</span> ( (i <span class="sc">%in%</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>) <span class="sc">&amp;</span> (j <span class="sc">%in%</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>) ))</span>
<span id="cb67-908"><a href="#cb67-908" aria-hidden="true" tabindex="-1"></a>                strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">paste</span>(<span class="st">"(x_"</span>, i, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x}) (x_"</span>, j, <span class="st">"-</span><span class="sc">\\</span><span class="st">bar{x}) </span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">epsilon}_"</span>,</span>
<span id="cb67-909"><a href="#cb67-909" aria-hidden="true" tabindex="-1"></a>                                        i, <span class="st">"</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">epsilon}_"</span>, j,<span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb67-910"><a href="#cb67-910" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="st">"</span><span class="sc">\\</span><span class="st">mbox{--}"</span>)</span>
<span id="cb67-911"><a href="#cb67-911" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb67-912"><a href="#cb67-912" aria-hidden="true" tabindex="-1"></a>        strgr <span class="ot">&lt;-</span> <span class="fu">c</span>(strgr, <span class="fu">ifelse</span>(j <span class="sc">==</span> <span class="dv">4</span>, <span class="st">" </span><span class="sc">\\\\</span><span class="st"> </span><span class="sc">\n</span><span class="st">"</span>, <span class="st">" &amp; </span><span class="sc">\n</span><span class="st">"</span>))</span>
<span id="cb67-913"><a href="#cb67-913" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb67-914"><a href="#cb67-914" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb67-915"><a href="#cb67-915" aria-hidden="true" tabindex="-1"></a>strgr <span class="ot">&lt;-</span> <span class="fu">paste</span>(strgr, <span class="at">collapse =</span> <span class="st">""</span>)</span>
<span id="cb67-916"><a href="#cb67-916" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(strgr)</span>
<span id="cb67-917"><a href="#cb67-917" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">end{array}</span><span class="sc">\\</span><span class="st">right)}$$</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb67-918"><a href="#cb67-918" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-919"><a href="#cb67-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-920"><a href="#cb67-920" aria-hidden="true" tabindex="-1"></a>which leads to the clustered estimated variance. More generally, for $N$</span>
<span id="cb67-921"><a href="#cb67-921" aria-hidden="true" tabindex="-1"></a>observations belonging to $G$ groups, this estimator is:</span>
<span id="cb67-922"><a href="#cb67-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-923"><a href="#cb67-923" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-924"><a href="#cb67-924" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\mbox{CL}\hat{\beta}} ^2= \frac{1}{S_{xx}^2}\sum_{g = 1} ^ G \left(\sum_{n \in g} (x_n -</span>
<span id="cb67-925"><a href="#cb67-925" aria-hidden="true" tabindex="-1"></a>\bar{x})\hat{\epsilon}_n \right) ^ 2</span>
<span id="cb67-926"><a href="#cb67-926" aria-hidden="true" tabindex="-1"></a>$$ {#eq-vcovCL}</span>
<span id="cb67-927"><a href="#cb67-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-928"><a href="#cb67-928" aria-hidden="true" tabindex="-1"></a>which is consistent with the hypothesis that errors are correlated</span>
<span id="cb67-929"><a href="#cb67-929" aria-hidden="true" tabindex="-1"></a>*within* a group, but uncorrelated *between* groups.</span>
<span id="cb67-930"><a href="#cb67-930" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!cluster|)}</span>
<span id="cb67-931"><a href="#cb67-931" aria-hidden="true" tabindex="-1"></a>To illustrate the computation of robust covariance estimators, we use</span>
<span id="cb67-932"><a href="#cb67-932" aria-hidden="true" tabindex="-1"></a>the data set <span class="in">`urban_gradient`</span> of @DURA:PUGA:20\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Duranton}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Puga}. It contains the</span>
<span id="cb67-933"><a href="#cb67-933" aria-hidden="true" tabindex="-1"></a>population, the area and the distance to the central business district</span>
<span id="cb67-934"><a href="#cb67-934" aria-hidden="true" tabindex="-1"></a>for 2315 block groups^<span class="co">[</span><span class="ot">A block group is a geographical unit which is between the census tract and the census block.</span><span class="co">]</span> in Alabama.^<span class="co">[</span><span class="ot">Actually, the whole data set covers the whole United States, but we use here the small subsample that concerns the state of Alabama.</span><span class="co">]</span></span>
<span id="cb67-935"><a href="#cb67-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-938"><a href="#cb67-938" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-939"><a href="#cb67-939" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: urban_gradient</span></span>
<span id="cb67-940"><a href="#cb67-940" aria-hidden="true" tabindex="-1"></a>urban_gradient <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">5</span>)</span>
<span id="cb67-941"><a href="#cb67-941" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-942"><a href="#cb67-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-943"><a href="#cb67-943" aria-hidden="true" tabindex="-1"></a>A classic model in urban economics states that urban density is a</span>
<span id="cb67-944"><a href="#cb67-944" aria-hidden="true" tabindex="-1"></a>negative exponential function of the distance to the central business</span>
<span id="cb67-945"><a href="#cb67-945" aria-hidden="true" tabindex="-1"></a>district: $y = A e^{\beta x}$ where $y$ is measured in inhabitants per</span>
<span id="cb67-946"><a href="#cb67-946" aria-hidden="true" tabindex="-1"></a>square kilometers, $x$ is measured in kilometers and $\beta&lt;0$ is</span>
<span id="cb67-947"><a href="#cb67-947" aria-hidden="true" tabindex="-1"></a>called the urban gradient. Taking logs, this leads to a semi-log linear</span>
<span id="cb67-948"><a href="#cb67-948" aria-hidden="true" tabindex="-1"></a>regression model:</span>
<span id="cb67-949"><a href="#cb67-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-950"><a href="#cb67-950" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-951"><a href="#cb67-951" aria-hidden="true" tabindex="-1"></a>\ln y_n = \alpha + \beta x_n + \epsilon_n</span>
<span id="cb67-952"><a href="#cb67-952" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-953"><a href="#cb67-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-954"><a href="#cb67-954" aria-hidden="true" tabindex="-1"></a>We first compute the <span class="in">`density`</span> variable and then estimate the urban</span>
<span id="cb67-955"><a href="#cb67-955" aria-hidden="true" tabindex="-1"></a>gradient model.</span>
<span id="cb67-956"><a href="#cb67-956" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{urban<span class="sc">\_</span>gradient}{micsr.data}</span>
<span id="cb67-957"><a href="#cb67-957" aria-hidden="true" tabindex="-1"></a>\idxfun{mutate}{dplyr}\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb67-958"><a href="#cb67-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-961"><a href="#cb67-961" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-962"><a href="#cb67-962" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: urban_gradient_lm</span></span>
<span id="cb67-963"><a href="#cb67-963" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-964"><a href="#cb67-964" aria-hidden="true" tabindex="-1"></a>urban_gradient <span class="ot">&lt;-</span> urban_gradient <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">density =</span> population <span class="sc">/</span> area)</span>
<span id="cb67-965"><a href="#cb67-965" aria-hidden="true" tabindex="-1"></a>ols_ug <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(density) <span class="sc">~</span> distance, urban_gradient)</span>
<span id="cb67-966"><a href="#cb67-966" aria-hidden="true" tabindex="-1"></a>ols_ug <span class="sc">%&gt;%</span> gaze</span>
<span id="cb67-967"><a href="#cb67-967" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-968"><a href="#cb67-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-969"><a href="#cb67-969" aria-hidden="true" tabindex="-1"></a>The estimated standard deviation of the slope is</span>
<span id="cb67-970"><a href="#cb67-970" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(sqrt(vcov(ols_ug)[2, 2]), 4)`</span>, but it may be seriously biased if the</span>
<span id="cb67-971"><a href="#cb67-971" aria-hidden="true" tabindex="-1"></a>errors are heteroskedastic and/or correlated. We first plot the data</span>
<span id="cb67-972"><a href="#cb67-972" aria-hidden="true" tabindex="-1"></a>and the regression line in @fig-dataug, the shape of the points</span>
<span id="cb67-973"><a href="#cb67-973" aria-hidden="true" tabindex="-1"></a>depending on the metropolitan statistical area (MSA, there are 12 of them in</span>
<span id="cb67-974"><a href="#cb67-974" aria-hidden="true" tabindex="-1"></a>the data set).</span>
<span id="cb67-975"><a href="#cb67-975" aria-hidden="true" tabindex="-1"></a>\idxfun{ggplot}{ggplot2}\idxfun{aes}{ggplot2}\idxfun{geom<span class="sc">\_</span>smooth}{ggplot2}\idxfun{scale<span class="sc">\_</span>shape<span class="sc">\_</span>manual}{ggplot2}</span>
<span id="cb67-976"><a href="#cb67-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-979"><a href="#cb67-979" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-980"><a href="#cb67-980" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dataug</span></span>
<span id="cb67-981"><a href="#cb67-981" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Data and regression line for the `urban_gradient` data set"</span></span>
<span id="cb67-982"><a href="#cb67-982" aria-hidden="true" tabindex="-1"></a>urban_gradient <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(distance, <span class="fu">log</span>(density))) <span class="sc">+</span> </span>
<span id="cb67-983"><a href="#cb67-983" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">shape =</span> msa), <span class="at">size =</span> .<span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb67-984"><a href="#cb67-984" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb67-985"><a href="#cb67-985" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_shape_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">17</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">0</span>, <span class="dv">12</span>))</span>
<span id="cb67-986"><a href="#cb67-986" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-987"><a href="#cb67-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-988"><a href="#cb67-988" aria-hidden="true" tabindex="-1"></a>Heteroskedasticity seems to be present in this data set, as the size of</span>
<span id="cb67-989"><a href="#cb67-989" aria-hidden="true" tabindex="-1"></a>the residuals seems to be an increasing function of the unique</span>
<span id="cb67-990"><a href="#cb67-990" aria-hidden="true" tabindex="-1"></a>covariate. We first compute the HC standard deviation of the slope, computing the mean of the covariate and $S_{xx}$ and then applying @eq-vcovHC:</span>
<span id="cb67-991"><a href="#cb67-991" aria-hidden="true" tabindex="-1"></a>\idxfun{pull}{dplyr}\idxfun{resid}{stats}\idxfun{summarise}{dplyr}</span>
<span id="cb67-992"><a href="#cb67-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-995"><a href="#cb67-995" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-996"><a href="#cb67-996" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: hc_urban_gradient</span></span>
<span id="cb67-997"><a href="#cb67-997" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-998"><a href="#cb67-998" aria-hidden="true" tabindex="-1"></a>dist_mean <span class="ot">&lt;-</span> urban_gradient <span class="sc">%&gt;%</span> <span class="fu">pull</span>(distance) <span class="sc">%&gt;%</span> mean</span>
<span id="cb67-999"><a href="#cb67-999" aria-hidden="true" tabindex="-1"></a>Sxx <span class="ot">&lt;-</span> <span class="fu">sum</span>( (<span class="fu">pull</span>(urban_gradient, distance) <span class="sc">-</span> dist_mean) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb67-1000"><a href="#cb67-1000" aria-hidden="true" tabindex="-1"></a>sd_hc <span class="ot">&lt;-</span> urban_gradient <span class="sc">%&gt;%</span> </span>
<span id="cb67-1001"><a href="#cb67-1001" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="fu">sqrt</span>(<span class="fu">sum</span>( (distance <span class="sc">-</span> dist_mean) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">*</span> </span>
<span id="cb67-1002"><a href="#cb67-1002" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">resid</span>(ols_ug) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> Sxx <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1003"><a href="#cb67-1003" aria-hidden="true" tabindex="-1"></a>  pull</span>
<span id="cb67-1004"><a href="#cb67-1004" aria-hidden="true" tabindex="-1"></a>sd_hc</span>
<span id="cb67-1005"><a href="#cb67-1005" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1006"><a href="#cb67-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1007"><a href="#cb67-1007" aria-hidden="true" tabindex="-1"></a>In this example, the heteroskedastic-robust standard error is just slightly higher than the one computed using the simple formula. However, we also have to investigate the potential correlation</span>
<span id="cb67-1008"><a href="#cb67-1008" aria-hidden="true" tabindex="-1"></a>between the errors of some observations.</span>
<span id="cb67-1009"><a href="#cb67-1009" aria-hidden="true" tabindex="-1"></a>There are 12 MSA and 22 counties in Alabama. It is possible that </span>
<span id="cb67-1010"><a href="#cb67-1010" aria-hidden="true" tabindex="-1"></a>errors for block groups of the same county or of the same MSA</span>
<span id="cb67-1011"><a href="#cb67-1011" aria-hidden="true" tabindex="-1"></a>are correlated (because of some unobserved common features of block</span>
<span id="cb67-1012"><a href="#cb67-1012" aria-hidden="true" tabindex="-1"></a>groups in the same county or MSA). We compute the estimation of the</span>
<span id="cb67-1013"><a href="#cb67-1013" aria-hidden="true" tabindex="-1"></a>clustered standard deviation of the slope (@eq-vcovCL) at the MSA level:</span>
<span id="cb67-1014"><a href="#cb67-1014" aria-hidden="true" tabindex="-1"></a>\idxfun{pull}{dplyr}\idxfun{unique}{base}\idxfun{length}{base}\idxfun{add<span class="sc">\_</span>column}{tibble}\idxfun{group<span class="sc">\_</span>by}{dplyr}\idxfun{resid}{stats}\idxfun{summarise}{dplyr}</span>
<span id="cb67-1015"><a href="#cb67-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1016"><a href="#cb67-1016" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1017"><a href="#cb67-1017" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: cl_urban_gradient</span></span>
<span id="cb67-1018"><a href="#cb67-1018" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-1019"><a href="#cb67-1019" aria-hidden="true" tabindex="-1"></a>G <span class="ot">&lt;-</span> urban_gradient <span class="sc">%&gt;%</span> <span class="fu">pull</span>(msa) <span class="sc">%&gt;%</span> unique <span class="sc">%&gt;%</span> length</span>
<span id="cb67-1020"><a href="#cb67-1020" aria-hidden="true" tabindex="-1"></a>sd_CL <span class="ot">&lt;-</span> urban_gradient <span class="sc">%&gt;%</span> </span>
<span id="cb67-1021"><a href="#cb67-1021" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">eps =</span> <span class="fu">resid</span>(ols_ug)) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1022"><a href="#cb67-1022" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(msa) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1023"><a href="#cb67-1023" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">z =</span> <span class="fu">sum</span>( (distance <span class="sc">-</span> dist_mean) <span class="sc">*</span> eps) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1024"><a href="#cb67-1024" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(z)) <span class="sc">/</span> Sxx) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1025"><a href="#cb67-1025" aria-hidden="true" tabindex="-1"></a>  pull</span>
<span id="cb67-1026"><a href="#cb67-1026" aria-hidden="true" tabindex="-1"></a>sd_CL</span>
<span id="cb67-1027"><a href="#cb67-1027" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1028"><a href="#cb67-1028" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{urban<span class="sc">\_</span>gradient}{micsr.data}</span>
<span id="cb67-1029"><a href="#cb67-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1030"><a href="#cb67-1030" aria-hidden="true" tabindex="-1"></a>This time, we get a much higher estimate of the standard deviation of the slope</span>
<span id="cb67-1031"><a href="#cb67-1031" aria-hidden="true" tabindex="-1"></a>(about three times larger than the one obtained with the simple formula).</span>
<span id="cb67-1032"><a href="#cb67-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1033"><a href="#cb67-1033" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!simple linear regression!sandwich|)}</span>
<span id="cb67-1034"><a href="#cb67-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1035"><a href="#cb67-1035" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiple linear model</span></span>
<span id="cb67-1036"><a href="#cb67-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1037"><a href="#cb67-1037" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!multiple linear regression!sandwich|)}</span>
<span id="cb67-1038"><a href="#cb67-1038" aria-hidden="true" tabindex="-1"></a>Consider now the multiple regression model. The vector of slopes can be</span>
<span id="cb67-1039"><a href="#cb67-1039" aria-hidden="true" tabindex="-1"></a>written as a linear combination of the response and the error vectors:</span>
<span id="cb67-1040"><a href="#cb67-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1041"><a href="#cb67-1041" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1042"><a href="#cb67-1042" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb67-1043"><a href="#cb67-1043" aria-hidden="true" tabindex="-1"></a>\hat{\beta}&amp;=&amp;(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top \tilde{y} <span class="sc">\\</span></span>
<span id="cb67-1044"><a href="#cb67-1044" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top(\tilde{X}\beta+\epsilon)<span class="sc">\\</span></span>
<span id="cb67-1045"><a href="#cb67-1045" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;\beta+(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top\epsilon <span class="sc">\\</span></span>
<span id="cb67-1046"><a href="#cb67-1046" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1047"><a href="#cb67-1047" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1048"><a href="#cb67-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1049"><a href="#cb67-1049" aria-hidden="true" tabindex="-1"></a>$\tilde{X} ^ \top\epsilon$ is a $K$-length vector containing the product</span>
<span id="cb67-1050"><a href="#cb67-1050" aria-hidden="true" tabindex="-1"></a>of the covariates (the column of $X$) in deviation from their sample</span>
<span id="cb67-1051"><a href="#cb67-1051" aria-hidden="true" tabindex="-1"></a>mean and the vector of errors:</span>
<span id="cb67-1052"><a href="#cb67-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1053"><a href="#cb67-1053" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1054"><a href="#cb67-1054" aria-hidden="true" tabindex="-1"></a>\tilde{X} ^ \top\epsilon =</span>
<span id="cb67-1055"><a href="#cb67-1055" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1056"><a href="#cb67-1056" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb67-1057"><a href="#cb67-1057" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n <span class="sc">\\</span></span>
<span id="cb67-1058"><a href="#cb67-1058" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n <span class="sc">\\</span></span>
<span id="cb67-1059"><a href="#cb67-1059" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb67-1060"><a href="#cb67-1060" aria-hidden="true" tabindex="-1"></a>\sum_{n=1} ^ N (x_{1n} - \bar{x}_K) \epsilon_n</span>
<span id="cb67-1061"><a href="#cb67-1061" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1062"><a href="#cb67-1062" aria-hidden="true" tabindex="-1"></a>\right) </span>
<span id="cb67-1063"><a href="#cb67-1063" aria-hidden="true" tabindex="-1"></a>= \sum_{n = 1} ^ N \psi_n</span>
<span id="cb67-1064"><a href="#cb67-1064" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-1065"><a href="#cb67-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1066"><a href="#cb67-1066" aria-hidden="true" tabindex="-1"></a>$\psi_n$ is called the vector of score, as it is proportional to the</span>
<span id="cb67-1067"><a href="#cb67-1067" aria-hidden="true" tabindex="-1"></a>vector of the first derivatives of the sum of square residuals and is</span>
<span id="cb67-1068"><a href="#cb67-1068" aria-hidden="true" tabindex="-1"></a>therefore equal to 0 when evaluated for $\hat{\beta}$, the OLS</span>
<span id="cb67-1069"><a href="#cb67-1069" aria-hidden="true" tabindex="-1"></a>estimator. The general form of the covariance of the OLS estimates was</span>
<span id="cb67-1070"><a href="#cb67-1070" aria-hidden="true" tabindex="-1"></a>given in @eq-general_variance:</span>
<span id="cb67-1071"><a href="#cb67-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1072"><a href="#cb67-1072" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1073"><a href="#cb67-1073" aria-hidden="true" tabindex="-1"></a>\hat{V}(\hat{\beta}) = \frac{1}{N}</span>
<span id="cb67-1074"><a href="#cb67-1074" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N}</span>
<span id="cb67-1075"><a href="#cb67-1075" aria-hidden="true" tabindex="-1"></a>\tilde{X} ^ \top \tilde{X}\right)^{-1}\frac{1}{N}\mbox{E}(\tilde{X} ^ \top \epsilon \epsilon ^</span>
<span id="cb67-1076"><a href="#cb67-1076" aria-hidden="true" tabindex="-1"></a>\top \tilde{X} \mid X) \left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}</span>
<span id="cb67-1077"><a href="#cb67-1077" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1078"><a href="#cb67-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1079"><a href="#cb67-1079" aria-hidden="true" tabindex="-1"></a>This is a sandwich formula, the meat (the variance of the score) being surrounded by two slices of bread (the inverse of the covariance matrix of the covariates).</span>
<span id="cb67-1080"><a href="#cb67-1080" aria-hidden="true" tabindex="-1"></a>Remember from @sec-variance_ols that the meat can be written, for $K = 2$, as the expected value of:</span>
<span id="cb67-1081"><a href="#cb67-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1082"><a href="#cb67-1082" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1083"><a href="#cb67-1083" aria-hidden="true" tabindex="-1"></a>\scriptsize</span>
<span id="cb67-1084"><a href="#cb67-1084" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb67-1085"><a href="#cb67-1085" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}</span>
<span id="cb67-1086"><a href="#cb67-1086" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1087"><a href="#cb67-1087" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb67-1088"><a href="#cb67-1088" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) ^ 2 &amp;</span>
<span id="cb67-1089"><a href="#cb67-1089" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) </span>
<span id="cb67-1090"><a href="#cb67-1090" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right)  <span class="sc">\\</span></span>
<span id="cb67-1091"><a href="#cb67-1091" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) </span>
<span id="cb67-1092"><a href="#cb67-1092" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right) &amp;</span>
<span id="cb67-1093"><a href="#cb67-1093" aria-hidden="true" tabindex="-1"></a>\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right) ^ 2</span>
<span id="cb67-1094"><a href="#cb67-1094" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1095"><a href="#cb67-1095" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-1096"><a href="#cb67-1096" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb67-1097"><a href="#cb67-1097" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1098"><a href="#cb67-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1099"><a href="#cb67-1099" aria-hidden="true" tabindex="-1"></a>If the errors are uncorrelated, but potentially heteroskedastic, we use</span>
<span id="cb67-1100"><a href="#cb67-1100" aria-hidden="true" tabindex="-1"></a>the following estimation:</span>
<span id="cb67-1101"><a href="#cb67-1101" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!heteroskedastic-consistent|(}</span>
<span id="cb67-1102"><a href="#cb67-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1103"><a href="#cb67-1103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1104"><a href="#cb67-1104" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}</span>
<span id="cb67-1105"><a href="#cb67-1105" aria-hidden="true" tabindex="-1"></a>\sum_{n = 1} ^ N</span>
<span id="cb67-1106"><a href="#cb67-1106" aria-hidden="true" tabindex="-1"></a> \hat{\epsilon}_n ^ 2</span>
<span id="cb67-1107"><a href="#cb67-1107" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1108"><a href="#cb67-1108" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb67-1109"><a href="#cb67-1109" aria-hidden="true" tabindex="-1"></a> (x_{1n} - \bar{x}_1) ^ 2 &amp;</span>
<span id="cb67-1110"><a href="#cb67-1110" aria-hidden="true" tabindex="-1"></a> (x_{1n} - \bar{x}_1) (x_{2n} - \bar{x}_2) <span class="sc">\\</span></span>
<span id="cb67-1111"><a href="#cb67-1111" aria-hidden="true" tabindex="-1"></a> (x_{1n} - \bar{x}_1) (x_{2n} - \bar{x}_2) &amp; </span>
<span id="cb67-1112"><a href="#cb67-1112" aria-hidden="true" tabindex="-1"></a> (x_{2n} - \bar{x}_2) ^ 2 <span class="sc">\\</span></span>
<span id="cb67-1113"><a href="#cb67-1113" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1114"><a href="#cb67-1114" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-1115"><a href="#cb67-1115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1116"><a href="#cb67-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1117"><a href="#cb67-1117" aria-hidden="true" tabindex="-1"></a>which generalizes the scalar case of the heteroskedastic covariance</span>
<span id="cb67-1118"><a href="#cb67-1118" aria-hidden="true" tabindex="-1"></a>matrix computed for the single regression case. This estimator of the variance of the score can easily be obtained by defining the **estimating function** $F$ which is obtained by multiplying (elements by elements) the columns of the matrix of covariates (in deviation from the sample means) by the vector of residuals:</span>
<span id="cb67-1119"><a href="#cb67-1119" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{estimating function}</span>
<span id="cb67-1120"><a href="#cb67-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1121"><a href="#cb67-1121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1122"><a href="#cb67-1122" aria-hidden="true" tabindex="-1"></a>F =</span>
<span id="cb67-1123"><a href="#cb67-1123" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1124"><a href="#cb67-1124" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccc}</span>
<span id="cb67-1125"><a href="#cb67-1125" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_1 x_{11} &amp; \ldots &amp; \hat{\epsilon}_1 \tilde{x}_{1K} <span class="sc">\\</span></span>
<span id="cb67-1126"><a href="#cb67-1126" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_2 x_{21} &amp; \ldots &amp; \hat{\epsilon}_2 \tilde{x}_{2K} <span class="sc">\\</span></span>
<span id="cb67-1127"><a href="#cb67-1127" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \ddots &amp; \vdots<span class="sc">\\</span></span>
<span id="cb67-1128"><a href="#cb67-1128" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_N x_{N1} &amp; \ldots &amp; \hat{\epsilon}_N \tilde{x}_{NK}</span>
<span id="cb67-1129"><a href="#cb67-1129" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1130"><a href="#cb67-1130" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-1131"><a href="#cb67-1131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1132"><a href="#cb67-1132" aria-hidden="true" tabindex="-1"></a>Then $F ^ \top F = \sum_{n=1} ^ N \hat{\epsilon}_n ^ 2 \tilde{x}_n \tilde{x}_n ^ \top$ is $N$ times the HC estimator of the meat and the HC estimator is then:</span>
<span id="cb67-1133"><a href="#cb67-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1134"><a href="#cb67-1134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1135"><a href="#cb67-1135" aria-hidden="true" tabindex="-1"></a>\hat{V}(\hat{\beta}) = \frac{1}{N}\left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}</span>
<span id="cb67-1136"><a href="#cb67-1136" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N}\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2 \tilde{x}_n \tilde{x}_n ^ \top\right)</span>
<span id="cb67-1137"><a href="#cb67-1137" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}</span>
<span id="cb67-1138"><a href="#cb67-1138" aria-hidden="true" tabindex="-1"></a>$$ {#eq-sandwich_hc_multi}</span>
<span id="cb67-1139"><a href="#cb67-1139" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!heteroskedastic-consistent|)}</span>
<span id="cb67-1140"><a href="#cb67-1140" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!cluster|(}</span>
<span id="cb67-1141"><a href="#cb67-1141" aria-hidden="true" tabindex="-1"></a>To get the clustered estimator of the variance, we define for each</span>
<span id="cb67-1142"><a href="#cb67-1142" aria-hidden="true" tabindex="-1"></a>cluster $\psi_g = \sum_{n \in g} \psi_n$, and the clustered expression of the meat is obtained</span>
<span id="cb67-1143"><a href="#cb67-1143" aria-hidden="true" tabindex="-1"></a>as the sum of the outer products of $\psi_g$ divided by $N$:</span>
<span id="cb67-1144"><a href="#cb67-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1145"><a href="#cb67-1145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1146"><a href="#cb67-1146" aria-hidden="true" tabindex="-1"></a>\frac{1}{N} \sum_{g=1} ^ G \hat{\psi}_g \hat{\psi}_g ^ \top</span>
<span id="cb67-1147"><a href="#cb67-1147" aria-hidden="true" tabindex="-1"></a>$$ {#eq-clustered_meat}</span>
<span id="cb67-1148"><a href="#cb67-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1149"><a href="#cb67-1149" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{uk<span class="sc">\_</span>elec}{micsr.data}</span>
<span id="cb67-1150"><a href="#cb67-1150" aria-hidden="true" tabindex="-1"></a>Going back to the electricity consumption estimation, <span class="in">`lm_elec`</span> is the</span>
<span id="cb67-1151"><a href="#cb67-1151" aria-hidden="true" tabindex="-1"></a>model fitted by OLS. We first compute the estimator manually,</span>
<span id="cb67-1152"><a href="#cb67-1152" aria-hidden="true" tabindex="-1"></a>computing the meat (the cross-products of the model matrix multiplied</span>
<span id="cb67-1153"><a href="#cb67-1153" aria-hidden="true" tabindex="-1"></a>by the vector of residuals divided by the sample size) and the bread</span>
<span id="cb67-1154"><a href="#cb67-1154" aria-hidden="true" tabindex="-1"></a>(the inverse of the cross-products of the model matrix divided by the sample size). The covariance matrix is then computed using @eq-sandwich_hc_multi, and we extract the standard errors of the estimates:</span>
<span id="cb67-1155"><a href="#cb67-1155" aria-hidden="true" tabindex="-1"></a>\idxfun{nobs}{stats}\idxfun{model.matrix}{stats}\idxfun{resid}{stats}\idxfun{crossprod}{base}\idxfun{solve}{base}</span>
<span id="cb67-1156"><a href="#cb67-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1159"><a href="#cb67-1159" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1160"><a href="#cb67-1160" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vcov_hc_multi_manual</span></span>
<span id="cb67-1161"><a href="#cb67-1161" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-1162"><a href="#cb67-1162" aria-hidden="true" tabindex="-1"></a>N_el <span class="ot">&lt;-</span> <span class="fu">nobs</span>(lm_elec)</span>
<span id="cb67-1163"><a href="#cb67-1163" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(lm_elec)</span>
<span id="cb67-1164"><a href="#cb67-1164" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">resid</span>(lm_elec) <span class="sc">*</span> Z) <span class="sc">/</span> N_el</span>
<span id="cb67-1165"><a href="#cb67-1165" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">crossprod</span>(Z) <span class="sc">/</span> N_el)</span>
<span id="cb67-1166"><a href="#cb67-1166" aria-hidden="true" tabindex="-1"></a>(B <span class="sc">%*%</span> M <span class="sc">%*%</span> B <span class="sc">/</span> N_el) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1167"><a href="#cb67-1167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1168"><a href="#cb67-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1169"><a href="#cb67-1169" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich!cluster|(}</span>
<span id="cb67-1170"><a href="#cb67-1170" aria-hidden="true" tabindex="-1"></a>The **sandwich** package <span class="co">[</span><span class="ot">@ZEIL:04; @ZEIL:06; @ZEIL:KOLL:NATH:20.</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Zeileis}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Köll}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Graham}, provides specialized functions to construct the different pieces of the estimator, namely <span class="in">`estfun`</span>, <span class="in">`meat`</span> and <span class="in">`bread`</span>:</span>
<span id="cb67-1171"><a href="#cb67-1171" aria-hidden="true" tabindex="-1"></a>\idxfun{estfun}{sandwich}\idxfun{meat}{sandwich}\idxfun{bread}{sandwich}\idxfun{stder}{micsr}</span>
<span id="cb67-1172"><a href="#cb67-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1175"><a href="#cb67-1175" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1176"><a href="#cb67-1176" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vcov_hc_multi_pieces</span></span>
<span id="cb67-1177"><a href="#cb67-1177" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-1178"><a href="#cb67-1178" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sandwich)</span>
<span id="cb67-1179"><a href="#cb67-1179" aria-hidden="true" tabindex="-1"></a>F <span class="ot">&lt;-</span> <span class="fu">estfun</span>(lm_elec)</span>
<span id="cb67-1180"><a href="#cb67-1180" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fu">meat</span>(lm_elec)</span>
<span id="cb67-1181"><a href="#cb67-1181" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">bread</span>(lm_elec)</span>
<span id="cb67-1182"><a href="#cb67-1182" aria-hidden="true" tabindex="-1"></a>(B <span class="sc">%*%</span> M <span class="sc">%*%</span> B <span class="sc">/</span> N_el) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1183"><a href="#cb67-1183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1184"><a href="#cb67-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1185"><a href="#cb67-1185" aria-hidden="true" tabindex="-1"></a><span class="in">`sandwich::vcovHC`</span> uses these functions to compute the HC estimator; its first</span>
<span id="cb67-1186"><a href="#cb67-1186" aria-hidden="true" tabindex="-1"></a>argument is a fitted model and a <span class="in">`type`</span> argument can also be supplied. Setting <span class="in">`type`</span> to <span class="in">`"HC0"`</span> gives the simplest version of the estimator, the one we have</span>
<span id="cb67-1187"><a href="#cb67-1187" aria-hidden="true" tabindex="-1"></a>previously computed "by hand". Other flavors of this</span>
<span id="cb67-1188"><a href="#cb67-1188" aria-hidden="true" tabindex="-1"></a>heteroskedasticity-robust estimator can be obtained by setting the</span>
<span id="cb67-1189"><a href="#cb67-1189" aria-hidden="true" tabindex="-1"></a><span class="in">`type`</span> to <span class="in">`"HC1"`</span>, $\ldots$, <span class="in">`"HC5"`</span> to perform different flavors of degrees of</span>
<span id="cb67-1190"><a href="#cb67-1190" aria-hidden="true" tabindex="-1"></a>freedom correction. For example, when <span class="in">`type = "HC1"`</span> (which is the default), the covariance</span>
<span id="cb67-1191"><a href="#cb67-1191" aria-hidden="true" tabindex="-1"></a>matrix is multiplied by $N / (N - K)$:<span class="ot">[^non_spherical-2]</span></span>
<span id="cb67-1192"><a href="#cb67-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1193"><a href="#cb67-1193" aria-hidden="true" tabindex="-1"></a><span class="ot">[^non_spherical-2]: </span>See @ZEIL:06\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Zeileis} for more details concerning the other</span>
<span id="cb67-1194"><a href="#cb67-1194" aria-hidden="true" tabindex="-1"></a>    values of the <span class="in">`type`</span> argument.</span>
<span id="cb67-1195"><a href="#cb67-1195" aria-hidden="true" tabindex="-1"></a>\idxfun{vcovHC}{sandwich}</span>
<span id="cb67-1196"><a href="#cb67-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1199"><a href="#cb67-1199" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1200"><a href="#cb67-1200" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: vcov_hc_multi_auto</span></span>
<span id="cb67-1201"><a href="#cb67-1201" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-1202"><a href="#cb67-1202" aria-hidden="true" tabindex="-1"></a><span class="fu">vcovHC</span>(lm_elec, <span class="at">type =</span> <span class="st">"HC0"</span>) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1203"><a href="#cb67-1203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1204"><a href="#cb67-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1205"><a href="#cb67-1205" aria-hidden="true" tabindex="-1"></a>Comparing the standard and the robust estimation of the standard deviations of the estimates, we get:</span>
<span id="cb67-1206"><a href="#cb67-1206" aria-hidden="true" tabindex="-1"></a>\idxfun{vcovHC}{sandwich}\idxfun{vcov}{stats}\idxfun{stder}{micsr}\idxfun{rbind}{base}</span>
<span id="cb67-1207"><a href="#cb67-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1210"><a href="#cb67-1210" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1211"><a href="#cb67-1211" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: robust_standard_sd</span></span>
<span id="cb67-1212"><a href="#cb67-1212" aria-hidden="true" tabindex="-1"></a>robust_sd <span class="ot">&lt;-</span> <span class="fu">vcovHC</span>(lm_elec, <span class="at">type =</span> <span class="st">"HC0"</span>) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1213"><a href="#cb67-1213" aria-hidden="true" tabindex="-1"></a>standard_sd <span class="ot">&lt;-</span> <span class="fu">vcov</span>(lm_elec) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1214"><a href="#cb67-1214" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="at">standard =</span> standard_sd, <span class="at">robust =</span> robust_sd, </span>
<span id="cb67-1215"><a href="#cb67-1215" aria-hidden="true" tabindex="-1"></a>      <span class="at">ratio =</span> robust_sd <span class="sc">/</span> standard_sd)</span>
<span id="cb67-1216"><a href="#cb67-1216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1217"><a href="#cb67-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1218"><a href="#cb67-1218" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{uk<span class="sc">\_</span>elec}{micsr.data}</span>
<span id="cb67-1219"><a href="#cb67-1219" aria-hidden="true" tabindex="-1"></a>In this example, the robust standard errors are very close to the ones obtained using the simple formula, although the Breusch-Pagan test rejected the hypothesis of homoskedasticity. This is because the source of heteroskedasticity (the number of customers) is not a covariate of the model.</span>
<span id="cb67-1220"><a href="#cb67-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1221"><a href="#cb67-1221" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb67-1222"><a href="#cb67-1222" aria-hidden="true" tabindex="-1"></a>To illustrate the use of the clustered sandwich estimator, we use the <span class="in">`twins`</span> data set. <span class="in">`lm_twins`</span> is the OLS estimation of the Mincer equation. In this context, the meat can easily be computed using the <span class="in">`apply`</span> and <span class="in">`tapply`</span> functions:</span>
<span id="cb67-1223"><a href="#cb67-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1224"><a href="#cb67-1224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`apply`</span> performs an operation on one of the margins (1 for rows, 2 for columns) of a matrix,</span>
<span id="cb67-1225"><a href="#cb67-1225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`tapply`</span> performs an operation (the third argument) on a vector (the first argument) conditional on the values of another vector (the second argument).</span>
<span id="cb67-1226"><a href="#cb67-1226" aria-hidden="true" tabindex="-1"></a>\idxfun{model.matrix}{stats}\idxfun{resid}{stats}\idxfun{crossprod}{base}\idxfun{tapply}{base}\idxfun{apply}{base}\idxfun{solve}{base}</span>
<span id="cb67-1227"><a href="#cb67-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1230"><a href="#cb67-1230" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1231"><a href="#cb67-1231" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(lm_twins)</span>
<span id="cb67-1232"><a href="#cb67-1232" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">resid</span>(lm_twins)</span>
<span id="cb67-1233"><a href="#cb67-1233" aria-hidden="true" tabindex="-1"></a>id <span class="ot">&lt;-</span> twins<span class="sc">$</span>family</span>
<span id="cb67-1234"><a href="#cb67-1234" aria-hidden="true" tabindex="-1"></a>Ze <span class="ot">&lt;-</span> Z <span class="sc">*</span> e</span>
<span id="cb67-1235"><a href="#cb67-1235" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">apply</span>(Ze, <span class="dv">2</span>, tapply, id, sum)) <span class="sc">/</span> N_tw</span>
<span id="cb67-1236"><a href="#cb67-1236" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">crossprod</span>(Z) <span class="sc">/</span> N_tw)</span>
<span id="cb67-1237"><a href="#cb67-1237" aria-hidden="true" tabindex="-1"></a>V_1 <span class="ot">&lt;-</span> (B <span class="sc">%*%</span> M <span class="sc">%*%</span> B <span class="sc">/</span> N_tw)</span>
<span id="cb67-1238"><a href="#cb67-1238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1239"><a href="#cb67-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1240"><a href="#cb67-1240" aria-hidden="true" tabindex="-1"></a>We can more simply use the <span class="in">`sandwich::vcovCL`</span> function <span class="co">[</span><span class="ot">@ZEIL:KOLL:NATH:20</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Zeileis}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Köll}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Graham} to compute the clustered estimator. The clustering variable is defined using the <span class="in">`cluster`</span> argument</span>
<span id="cb67-1241"><a href="#cb67-1241" aria-hidden="true" tabindex="-1"></a>that can be set to a one-sided formula. The <span class="in">`type`</span> argument is similar</span>
<span id="cb67-1242"><a href="#cb67-1242" aria-hidden="true" tabindex="-1"></a>to the one of <span class="in">`vcovHC`</span> and there is also a <span class="in">`cadjust`</span> argument which, if</span>
<span id="cb67-1243"><a href="#cb67-1243" aria-hidden="true" tabindex="-1"></a><span class="in">`TRUE`</span>, multiplies the covariance matrix by $G / (G - 1)$, $G$ being the</span>
<span id="cb67-1244"><a href="#cb67-1244" aria-hidden="true" tabindex="-1"></a>number of clusters. The default behavior of <span class="in">`vcovCL`</span> is to set <span class="in">`cadjust`</span> to <span class="in">`TRUE`</span> and</span>
<span id="cb67-1245"><a href="#cb67-1245" aria-hidden="true" tabindex="-1"></a><span class="in">`type`</span> to <span class="in">`"HC1"`</span>, so that the adjustment is done for the number of</span>
<span id="cb67-1246"><a href="#cb67-1246" aria-hidden="true" tabindex="-1"></a>observations and for the number of groups.</span>
<span id="cb67-1247"><a href="#cb67-1247" aria-hidden="true" tabindex="-1"></a>For the <span class="in">`twins`</span> data set, the clustering variable is <span class="in">`family`</span>:</span>
<span id="cb67-1248"><a href="#cb67-1248" aria-hidden="true" tabindex="-1"></a>\idxfun{vcovCL}{sandwich}</span>
<span id="cb67-1249"><a href="#cb67-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1252"><a href="#cb67-1252" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1253"><a href="#cb67-1253" aria-hidden="true" tabindex="-1"></a>V_2 <span class="ot">&lt;-</span> <span class="fu">vcovCL</span>(lm_twins, <span class="sc">~</span> family, <span class="at">type =</span> <span class="st">"HC0"</span>, <span class="at">cadjust =</span> <span class="cn">FALSE</span>)</span>
<span id="cb67-1254"><a href="#cb67-1254" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1255"><a href="#cb67-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1256"><a href="#cb67-1256" aria-hidden="true" tabindex="-1"></a>Finally, <span class="in">`twins`</span> being a pseudo-panel, the <span class="in">`plm`</span> package can also be used. The OLS estimate can be fitted with <span class="in">`plm::plm`</span> by setting the <span class="in">`model`</span> argument to <span class="in">`"pooling"`</span>.</span>
<span id="cb67-1257"><a href="#cb67-1257" aria-hidden="true" tabindex="-1"></a>\idxfun{plm}{plm}</span>
<span id="cb67-1258"><a href="#cb67-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1261"><a href="#cb67-1261" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1262"><a href="#cb67-1262" aria-hidden="true" tabindex="-1"></a>plm_twins <span class="ot">&lt;-</span> <span class="fu">plm</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>) <span class="sc">+</span> educ, twins, </span>
<span id="cb67-1263"><a href="#cb67-1263" aria-hidden="true" tabindex="-1"></a>                 <span class="at">model =</span> <span class="st">"pooling"</span>)</span>
<span id="cb67-1264"><a href="#cb67-1264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1265"><a href="#cb67-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1266"><a href="#cb67-1266" aria-hidden="true" tabindex="-1"></a>A <span class="in">`plm`</span> object is returned and the <span class="in">`vcovHC`</span> method for a <span class="in">`plm`</span> object returns by default the same clustered covariance matrix as previously:</span>
<span id="cb67-1267"><a href="#cb67-1267" aria-hidden="true" tabindex="-1"></a>\idxfun{vcovHC}{sandwich}</span>
<span id="cb67-1268"><a href="#cb67-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1271"><a href="#cb67-1271" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1272"><a href="#cb67-1272" aria-hidden="true" tabindex="-1"></a>V_3 <span class="ot">&lt;-</span> <span class="fu">vcovHC</span>(plm_twins)</span>
<span id="cb67-1273"><a href="#cb67-1273" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1274"><a href="#cb67-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1275"><a href="#cb67-1275" aria-hidden="true" tabindex="-1"></a>The <span class="in">`vcovHC`</span> and <span class="in">`vcovCL`</span> functions are particularly useful while using the testing functions of the **lmtest** package. For example, <span class="in">`lmtest::coeftest`</span> computes the usual table of coefficients (the same as the one obtained using <span class="in">`summary`</span>), but a matrix or a function that computes a covariance matrix can be passed as a supplementary argument. Therefore, <span class="in">`lmtest::coeftest(lm_twins)`</span> returns exactly the same table of coefficients as <span class="in">`summary(lm_twins)`</span>, but other standard errors are obtained by filling the second argument. For example, to get the clustered standard errors with our preferred specification without degrees of freedom correction (<span class="in">`type = "HC0", cadjust = FALSE`</span>), we can use any of the following three equivalent syntaxes:</span>
<span id="cb67-1276"><a href="#cb67-1276" aria-hidden="true" tabindex="-1"></a>\idxfun{vcovCL}{sandwich}\idxfun{coeftest}{lmtest}\idxfun{function}{base}</span>
<span id="cb67-1277"><a href="#cb67-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1280"><a href="#cb67-1280" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1281"><a href="#cb67-1281" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb67-1282"><a href="#cb67-1282" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">vcovCL</span>(lm_twins, <span class="sc">~</span> family, <span class="at">cadjust =</span> <span class="cn">FALSE</span>, <span class="at">type =</span> <span class="st">"HC0"</span>)</span>
<span id="cb67-1283"><a href="#cb67-1283" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb67-1284"><a href="#cb67-1284" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(lm_twins, cl)</span>
<span id="cb67-1285"><a href="#cb67-1285" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(lm_twins, <span class="cf">function</span>(x) </span>
<span id="cb67-1286"><a href="#cb67-1286" aria-hidden="true" tabindex="-1"></a>  <span class="fu">vcovCL</span>(x, <span class="sc">~</span> family, <span class="at">cadjust =</span> <span class="cn">FALSE</span>, <span class="at">type =</span> <span class="st">"HC0"</span>))</span>
<span id="cb67-1287"><a href="#cb67-1287" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(lm_twins, vcovCL, <span class="at">cluster =</span> <span class="sc">~</span> family, </span>
<span id="cb67-1288"><a href="#cb67-1288" aria-hidden="true" tabindex="-1"></a>         <span class="at">cadjust =</span> <span class="cn">FALSE</span>, <span class="at">type =</span> <span class="st">"HC0"</span>)</span>
<span id="cb67-1289"><a href="#cb67-1289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1290"><a href="#cb67-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1291"><a href="#cb67-1291" aria-hidden="true" tabindex="-1"></a>In the first expression, we provide a matrix that was previously computed. In the second expression, we use an anonymous function with our preferred options. In the last one, the two arguments of <span class="in">`vcovCL`</span> are indicated as arguments of <span class="in">`coeftest`</span> and are passed internally to <span class="in">`vcovCL`</span>.</span>
<span id="cb67-1292"><a href="#cb67-1292" aria-hidden="true" tabindex="-1"></a>We finally compare the ordinary and the robust estimates of the standard errors of the OLS estimate:</span>
<span id="cb67-1293"><a href="#cb67-1293" aria-hidden="true" tabindex="-1"></a>\idxfun{vcov}{stats}\idxfun{stder}{micsr}\idxfun{rbind}{base}</span>
<span id="cb67-1294"><a href="#cb67-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1297"><a href="#cb67-1297" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1298"><a href="#cb67-1298" aria-hidden="true" tabindex="-1"></a>ord_se <span class="ot">&lt;-</span> <span class="fu">vcov</span>(lm_twins) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1299"><a href="#cb67-1299" aria-hidden="true" tabindex="-1"></a>cl_se <span class="ot">&lt;-</span> V_1 <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1300"><a href="#cb67-1300" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="at">ordinary =</span> ord_se, <span class="at">cluster =</span> cl_se, </span>
<span id="cb67-1301"><a href="#cb67-1301" aria-hidden="true" tabindex="-1"></a><span class="at">ratio =</span> cl_se <span class="sc">/</span> ord_se)</span>
<span id="cb67-1302"><a href="#cb67-1302" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1303"><a href="#cb67-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1304"><a href="#cb67-1304" aria-hidden="true" tabindex="-1"></a>As for the electricity consumption example, we can see that the robust standard errors are almost the same as those computed using the simple formula.\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb67-1305"><a href="#cb67-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1306"><a href="#cb67-1306" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Finally, for our panel data example, we get: --&gt;</span></span>
<span id="cb67-1307"><a href="#cb67-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1308"><a href="#cb67-1308" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r} --&gt;</span></span>
<span id="cb67-1309"><a href="#cb67-1309" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ols_q &lt;- plm(ikn ~ qn, tobinq, model = "pooling") --&gt;</span></span>
<span id="cb67-1310"><a href="#cb67-1310" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ord_sd &lt;- vcov(ols_q) %&gt;% stder --&gt;</span></span>
<span id="cb67-1311"><a href="#cb67-1311" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- cl_sd &lt;- vcovHC(ols_q) %&gt;% stder --&gt;</span></span>
<span id="cb67-1312"><a href="#cb67-1312" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- rbind(ordinary = ord_sd, cluster = cl_sd,  --&gt;</span></span>
<span id="cb67-1313"><a href="#cb67-1313" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ratio = cl_sd / ord_sd) --&gt;</span></span>
<span id="cb67-1314"><a href="#cb67-1314" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb67-1315"><a href="#cb67-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1316"><a href="#cb67-1316" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- and in this case the robust standard error is much larger (about 4 times) than the simple one. --&gt;</span></span>
<span id="cb67-1317"><a href="#cb67-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1318"><a href="#cb67-1318" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!multiple linear regression!sandwich|)}</span>
<span id="cb67-1319"><a href="#cb67-1319" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{sandwich|)}</span>
<span id="cb67-1320"><a href="#cb67-1320" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{covariance matrix estimation!sandwich|)}</span>
<span id="cb67-1321"><a href="#cb67-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1322"><a href="#cb67-1322" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb67-1323"><a href="#cb67-1323" aria-hidden="true" tabindex="-1"></a><span class="fu">## Generalized least squares estimator {#sec-gls}</span></span>
<span id="cb67-1324"><a href="#cb67-1324" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares|(}</span>
<span id="cb67-1325"><a href="#cb67-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1326"><a href="#cb67-1326" aria-hidden="true" tabindex="-1"></a>For a linear model</span>
<span id="cb67-1327"><a href="#cb67-1327" aria-hidden="true" tabindex="-1"></a>$y_n = \alpha + \beta^\top x_n + \epsilon_n = \gamma ^ \top z_n + \epsilon_n$</span>
<span id="cb67-1328"><a href="#cb67-1328" aria-hidden="true" tabindex="-1"></a>with non-spherical disturbances ($\Omega \neq \sigma_\epsilon ^ 2 I$),</span>
<span id="cb67-1329"><a href="#cb67-1329" aria-hidden="true" tabindex="-1"></a>the OLS estimator is no longer BLUE. A more efficient estimator called</span>
<span id="cb67-1330"><a href="#cb67-1330" aria-hidden="true" tabindex="-1"></a>the **generalized least squares** (**GLS**) can then be used instead.</span>
<span id="cb67-1331"><a href="#cb67-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1332"><a href="#cb67-1332" aria-hidden="true" tabindex="-1"></a><span class="fu">### General formulation of the GLS estimator</span></span>
<span id="cb67-1333"><a href="#cb67-1333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1334"><a href="#cb67-1334" aria-hidden="true" tabindex="-1"></a>The GLS estimator is:</span>
<span id="cb67-1335"><a href="#cb67-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1336"><a href="#cb67-1336" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1337"><a href="#cb67-1337" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = (Z^\top\Omega^{-1}X) ^ {-1} Z^\top \Omega ^ {-1} y</span>
<span id="cb67-1338"><a href="#cb67-1338" aria-hidden="true" tabindex="-1"></a>$$ {#eq-glsmatrix}</span>
<span id="cb67-1339"><a href="#cb67-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1340"><a href="#cb67-1340" aria-hidden="true" tabindex="-1"></a>Replacing $y$ by</span>
<span id="cb67-1341"><a href="#cb67-1341" aria-hidden="true" tabindex="-1"></a>$Z\gamma+\epsilon$, we get:</span>
<span id="cb67-1342"><a href="#cb67-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1343"><a href="#cb67-1343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1344"><a href="#cb67-1344" aria-hidden="true" tabindex="-1"></a>\hat{\gamma}=(Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} (Z\gamma + \epsilon) = \gamma + (Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \epsilon</span>
<span id="cb67-1345"><a href="#cb67-1345" aria-hidden="true" tabindex="-1"></a>$$ As for the OLS estimator, the estimator is unbiased if</span>
<span id="cb67-1346"><a href="#cb67-1346" aria-hidden="true" tabindex="-1"></a>$\mbox{E}\left(\epsilon\mid Z\right)=0$. The variance is:</span>
<span id="cb67-1347"><a href="#cb67-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1348"><a href="#cb67-1348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1349"><a href="#cb67-1349" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb67-1350"><a href="#cb67-1350" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\gamma}) &amp;=&amp; \mbox{E}\left<span class="co">[</span><span class="ot">(Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \epsilon\epsilon^\top \Omega^{-1}Z(Z^\top \Omega ^ {-1} Z)^{-1}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb67-1351"><a href="#cb67-1351" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; (Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \mbox{E}(\epsilon\epsilon ^ \top) \Omega Z (Z^\top\Omega^{-1}Z) ^ {-1}<span class="sc">\\</span></span>
<span id="cb67-1352"><a href="#cb67-1352" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;(Z^\top\Omega^{-1}Z)^{-1}</span>
<span id="cb67-1353"><a href="#cb67-1353" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1354"><a href="#cb67-1354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1355"><a href="#cb67-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1356"><a href="#cb67-1356" aria-hidden="true" tabindex="-1"></a>Written this way, the GLS estimator is unfeasible for two reasons:</span>
<span id="cb67-1357"><a href="#cb67-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1358"><a href="#cb67-1358" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the $\Omega$ matrix is a square matrix of dimension $N\times N$, and</span>
<span id="cb67-1359"><a href="#cb67-1359" aria-hidden="true" tabindex="-1"></a>    it is computationally difficult (or impossible) to store and to</span>
<span id="cb67-1360"><a href="#cb67-1360" aria-hidden="true" tabindex="-1"></a>    invert for large samples,</span>
<span id="cb67-1361"><a href="#cb67-1361" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>it uses a matrix $\Omega$ which contains $N(N+1) / 2$ unknown</span>
<span id="cb67-1362"><a href="#cb67-1362" aria-hidden="true" tabindex="-1"></a>    parameters.</span>
<span id="cb67-1363"><a href="#cb67-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1364"><a href="#cb67-1364" aria-hidden="true" tabindex="-1"></a>A **feasible GLS** estimator is obtained by imposing some structure on</span>
<span id="cb67-1365"><a href="#cb67-1365" aria-hidden="true" tabindex="-1"></a>$\Omega$ so that the number of unknown parameters becomes much less than</span>
<span id="cb67-1366"><a href="#cb67-1366" aria-hidden="true" tabindex="-1"></a>$N(N+1)/2$ and by estimating these unknown parameters using residuals of</span>
<span id="cb67-1367"><a href="#cb67-1367" aria-hidden="true" tabindex="-1"></a>a first step consistent estimation. Actually, in practice, the GLS</span>
<span id="cb67-1368"><a href="#cb67-1368" aria-hidden="true" tabindex="-1"></a>estimator is obtained by performing OLS on transformed data. More</span>
<span id="cb67-1369"><a href="#cb67-1369" aria-hidden="true" tabindex="-1"></a>precisely, consider the matrix $C$ such that</span>
<span id="cb67-1370"><a href="#cb67-1370" aria-hidden="true" tabindex="-1"></a>$C ^ \top C = \Omega ^ {-1}$. Then, @eq-glsmatrix can be rewritten,</span>
<span id="cb67-1371"><a href="#cb67-1371" aria-hidden="true" tabindex="-1"></a>denoting $w^*=Cw$:</span>
<span id="cb67-1372"><a href="#cb67-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1373"><a href="#cb67-1373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1374"><a href="#cb67-1374" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = (Z^\top C^\top CZ) ^ {-1} Z^\top C^\top C y = \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}y^*</span>
<span id="cb67-1375"><a href="#cb67-1375" aria-hidden="true" tabindex="-1"></a>$$ {#eq-glstrans}</span>
<span id="cb67-1376"><a href="#cb67-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1377"><a href="#cb67-1377" aria-hidden="true" tabindex="-1"></a>which is the OLS estimator of the linear model:</span>
<span id="cb67-1378"><a href="#cb67-1378" aria-hidden="true" tabindex="-1"></a>$y^* = Z^*\gamma + \epsilon^*$, with $\epsilon^* = C\epsilon$. Replacing</span>
<span id="cb67-1379"><a href="#cb67-1379" aria-hidden="true" tabindex="-1"></a>$y^*$ by $Z^*\gamma + \epsilon^*$ in @eq-glstrans, we get:</span>
<span id="cb67-1380"><a href="#cb67-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1381"><a href="#cb67-1381" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1382"><a href="#cb67-1382" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = \gamma + \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}C\epsilon</span>
<span id="cb67-1383"><a href="#cb67-1383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1384"><a href="#cb67-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1385"><a href="#cb67-1385" aria-hidden="true" tabindex="-1"></a>And the variance of the estimator is:</span>
<span id="cb67-1386"><a href="#cb67-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1387"><a href="#cb67-1387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1388"><a href="#cb67-1388" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\gamma}) = \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}C\Omega^{-1}C^\top Z^*\left(Z^{*\top}Z^*\right)^{-1}</span>
<span id="cb67-1389"><a href="#cb67-1389" aria-hidden="true" tabindex="-1"></a>$$ {#eq-glsvar}</span>
<span id="cb67-1390"><a href="#cb67-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1391"><a href="#cb67-1391" aria-hidden="true" tabindex="-1"></a>But</span>
<span id="cb67-1392"><a href="#cb67-1392" aria-hidden="true" tabindex="-1"></a>$C\Omega^{-1}C^\top = C (C^\top C)^{-1} C^\top = C C ^ {-1} C^{\top-1}C^\top=I$<span class="ot">[^non_spherical-3]</span>,</span>
<span id="cb67-1393"><a href="#cb67-1393" aria-hidden="true" tabindex="-1"></a>and therefore, @eq-glsvar simplifies to:</span>
<span id="cb67-1394"><a href="#cb67-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1395"><a href="#cb67-1395" aria-hidden="true" tabindex="-1"></a><span class="ot">[^non_spherical-3]: </span>$(AB) ^ {-1} = B^{-1} A^{-1}$ if the inverse of the</span>
<span id="cb67-1396"><a href="#cb67-1396" aria-hidden="true" tabindex="-1"></a>    two matrices exists, see @GREE:18\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Greene}, online appendix p 1074.</span>
<span id="cb67-1397"><a href="#cb67-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1398"><a href="#cb67-1398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1399"><a href="#cb67-1399" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\gamma}) = \left(Z^{*\top}Z^*\right)^{-1}</span>
<span id="cb67-1400"><a href="#cb67-1400" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-1401"><a href="#cb67-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1402"><a href="#cb67-1402" aria-hidden="true" tabindex="-1"></a>which is very similar to the formula used for the OLS estimator. Note that</span>
<span id="cb67-1403"><a href="#cb67-1403" aria-hidden="true" tabindex="-1"></a>$\sigma_\epsilon^2$ doesn't appear in this formula because the variance</span>
<span id="cb67-1404"><a href="#cb67-1404" aria-hidden="true" tabindex="-1"></a>of the transformed errors is 1.</span>
<span id="cb67-1405"><a href="#cb67-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1406"><a href="#cb67-1406" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weighted least squares</span></span>
<span id="cb67-1407"><a href="#cb67-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1408"><a href="#cb67-1408" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{weighted least squares|(}</span>
<span id="cb67-1409"><a href="#cb67-1409" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares!weighted least squares|(}</span>
<span id="cb67-1410"><a href="#cb67-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1411"><a href="#cb67-1411" aria-hidden="true" tabindex="-1"></a>With heteroskedastic, but uncorrelated errors,</span>
<span id="cb67-1412"><a href="#cb67-1412" aria-hidden="true" tabindex="-1"></a>$\Omega$ is diagonal and each element is the specific variance of one</span>
<span id="cb67-1413"><a href="#cb67-1413" aria-hidden="true" tabindex="-1"></a>observation. From @eq-matheterosc, it is obvious that the transformation</span>
<span id="cb67-1414"><a href="#cb67-1414" aria-hidden="true" tabindex="-1"></a>matrix $C$ can be written as:</span>
<span id="cb67-1415"><a href="#cb67-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1416"><a href="#cb67-1416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1417"><a href="#cb67-1417" aria-hidden="true" tabindex="-1"></a>C= </span>
<span id="cb67-1418"><a href="#cb67-1418" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1419"><a href="#cb67-1419" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccccc}</span>
<span id="cb67-1420"><a href="#cb67-1420" aria-hidden="true" tabindex="-1"></a>1 / \sigma_{1} &amp; 0 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-1421"><a href="#cb67-1421" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 / \sigma_{2} &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb67-1422"><a href="#cb67-1422" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-1423"><a href="#cb67-1423" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \ldots &amp; 1 / \sigma_N</span>
<span id="cb67-1424"><a href="#cb67-1424" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb67-1425"><a href="#cb67-1425" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-1426"><a href="#cb67-1426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1427"><a href="#cb67-1427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1428"><a href="#cb67-1428" aria-hidden="true" tabindex="-1"></a>and therefore, premultiplying any vector by $C$ leads to a transformed</span>
<span id="cb67-1429"><a href="#cb67-1429" aria-hidden="true" tabindex="-1"></a>vector where each value is divided by the standard deviation of the</span>
<span id="cb67-1430"><a href="#cb67-1430" aria-hidden="true" tabindex="-1"></a>corresponding error:</span>
<span id="cb67-1431"><a href="#cb67-1431" aria-hidden="true" tabindex="-1"></a>$z^{*\top} = (z_1 / \sigma_1, z_2 / \sigma_2, \ldots, z_N / \sigma_N)$.</span>
<span id="cb67-1432"><a href="#cb67-1432" aria-hidden="true" tabindex="-1"></a>Performing OLS on the transformed data, we get the **weighted-least square** (**WLS**) estimator. The name of this estimator comes from the fact that the estimator can be obtained by minimizing $\sum_n \epsilon_n ^ 2 / \sigma_n ^ 2$, i.e., by</span>
<span id="cb67-1433"><a href="#cb67-1433" aria-hidden="true" tabindex="-1"></a>minimizing not the sum of the squares of the residuals, but a linear combination of</span>
<span id="cb67-1434"><a href="#cb67-1434" aria-hidden="true" tabindex="-1"></a>the squares of the residuals, the weight of each observation being $1 / \sigma_n ^ 2$. Therefore, an observation $n$ for which $\sigma_n^2$ is high will</span>
<span id="cb67-1435"><a href="#cb67-1435" aria-hidden="true" tabindex="-1"></a>receive a smaller weight in WLS compared to OLS. The weights</span>
<span id="cb67-1436"><a href="#cb67-1436" aria-hidden="true" tabindex="-1"></a>are unknown and therefore need to be estimated. The simplest solution is to assume that the variance (or the standard deviation) of the errors is</span>
<span id="cb67-1437"><a href="#cb67-1437" aria-hidden="true" tabindex="-1"></a>proportional to an observed variable (which may or may not be a covariate of</span>
<span id="cb67-1438"><a href="#cb67-1438" aria-hidden="true" tabindex="-1"></a>the regression). We then get either $\sigma_n ^ 2 = \sigma ^ 2 w_n$ or $\sigma_n ^ 2 = \sigma ^ 2 w_n ^ 2$ and the weights are then respectively $1 / w_n$ or $1 / w_n ^ 2$. The WLS estimator can then be obtained by OLS with all the variables divided either by $\sqrt{w_n}$ or $w_n$. </span>
<span id="cb67-1439"><a href="#cb67-1439" aria-hidden="true" tabindex="-1"></a>A more general solution is to assume a functional form</span>
<span id="cb67-1440"><a href="#cb67-1440" aria-hidden="true" tabindex="-1"></a>for the skedastic function: $\sigma_n ^ 2 = h(\delta^\top w_n)$ where</span>
<span id="cb67-1441"><a href="#cb67-1441" aria-hidden="true" tabindex="-1"></a>$h$ is a monotonous increasing function</span>
<span id="cb67-1442"><a href="#cb67-1442" aria-hidden="true" tabindex="-1"></a>that returns only positive values, $w$ is a set of covariates and</span>
<span id="cb67-1443"><a href="#cb67-1443" aria-hidden="true" tabindex="-1"></a>$\delta$ a vector of parameters. If, for example, $h$ is the</span>
<span id="cb67-1444"><a href="#cb67-1444" aria-hidden="true" tabindex="-1"></a>exponential function (which is a very common choice), the skedastic</span>
<span id="cb67-1445"><a href="#cb67-1445" aria-hidden="true" tabindex="-1"></a>function is: $\ln \sigma_n ^ 2 = \delta^\top w_n$ and $\delta$</span>
<span id="cb67-1446"><a href="#cb67-1446" aria-hidden="true" tabindex="-1"></a>can be consistently estimated by performing the following regression:</span>
<span id="cb67-1447"><a href="#cb67-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1448"><a href="#cb67-1448" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1449"><a href="#cb67-1449" aria-hidden="true" tabindex="-1"></a>\ln \hat{\epsilon}_n ^ 2 = \delta ^ \top w_n + \nu_n</span>
<span id="cb67-1450"><a href="#cb67-1450" aria-hidden="true" tabindex="-1"></a>$$ {#eq-skedeq} </span>
<span id="cb67-1451"><a href="#cb67-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1452"><a href="#cb67-1452" aria-hidden="true" tabindex="-1"></a>where $\hat{\epsilon}$ are the OLS residuals which are</span>
<span id="cb67-1453"><a href="#cb67-1453" aria-hidden="true" tabindex="-1"></a>consistent estimates of the errors. The WLS estimator is then performed</span>
<span id="cb67-1454"><a href="#cb67-1454" aria-hidden="true" tabindex="-1"></a>in three steps:</span>
<span id="cb67-1455"><a href="#cb67-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1456"><a href="#cb67-1456" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>estimate the model by OLS and retrieve the vector of residuals</span>
<span id="cb67-1457"><a href="#cb67-1457" aria-hidden="true" tabindex="-1"></a>    $\hat{\epsilon}$,</span>
<span id="cb67-1458"><a href="#cb67-1458" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>estimate $\hat{\gamma}$ by using OLS on @eq-skedeq and compute</span>
<span id="cb67-1459"><a href="#cb67-1459" aria-hidden="true" tabindex="-1"></a>    $\hat{\sigma}_n ^ 2 = e^{\hat{\gamma} w_n}$,</span>
<span id="cb67-1460"><a href="#cb67-1460" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>divide every variable (the response and the covariates) by</span>
<span id="cb67-1461"><a href="#cb67-1461" aria-hidden="true" tabindex="-1"></a>    $\hat{\sigma}_n$ and perform OLS on the transformed variables.</span>
<span id="cb67-1462"><a href="#cb67-1462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1463"><a href="#cb67-1463" aria-hidden="true" tabindex="-1"></a>Note that there is no intercept in the third estimation as the</span>
<span id="cb67-1464"><a href="#cb67-1464" aria-hidden="true" tabindex="-1"></a>"covariate" associated with the intercept (a vector of 1) becomes a vector</span>
<span id="cb67-1465"><a href="#cb67-1465" aria-hidden="true" tabindex="-1"></a>with typical element $1/\hat{\sigma}_n$.</span>
<span id="cb67-1466"><a href="#cb67-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1467"><a href="#cb67-1467" aria-hidden="true" tabindex="-1"></a>For the electricity consumption example, we know that the unconditional variance of $y$ in city $n$ is $\sigma_{yn} ^ 2 = \sigma_c ^ 2 / I_n$, $I_n$ being the number of consumption units in city $n$ and $\sigma_c ^ 2$ the variance of the individual consumption. Assuming that the same relation applies for the conditional variance, then $\sigma_{\epsilon n} ^ 2 = \sigma ^ 2 / I_n$ and the WLS estimator can then be obtained by computing OLS on series multiplied by $\sqrt{I_n}$:</span>
<span id="cb67-1468"><a href="#cb67-1468" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{uk<span class="sc">\_</span>elec}{micsr.data}\idxfun{lm}{stats}</span>
<span id="cb67-1469"><a href="#cb67-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1472"><a href="#cb67-1472" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1473"><a href="#cb67-1473" aria-hidden="true" tabindex="-1"></a>wls_elec <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">I</span>(kwh <span class="sc">*</span> <span class="fu">sqrt</span>(cust)) <span class="sc">~</span> <span class="fu">sqrt</span>(cust) <span class="sc">+</span> <span class="fu">I</span>(inc <span class="sc">*</span> <span class="fu">sqrt</span>(cust)) <span class="sc">+</span> </span>
<span id="cb67-1474"><a href="#cb67-1474" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> mc6 <span class="sc">*</span> <span class="fu">sqrt</span>(cust)) <span class="sc">+</span>  <span class="fu">I</span>(gas6 <span class="sc">*</span> <span class="fu">sqrt</span>(cust)) <span class="sc">+</span> </span>
<span id="cb67-1475"><a href="#cb67-1475" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">I</span>(cap <span class="sc">*</span> <span class="fu">sqrt</span>(cust)) <span class="sc">-</span> <span class="dv">1</span>, uk_elec)</span>
<span id="cb67-1476"><a href="#cb67-1476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1477"><a href="#cb67-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1478"><a href="#cb67-1478" aria-hidden="true" tabindex="-1"></a>Or more simply by setting the <span class="in">`weights`</span> argument of <span class="in">`lm`</span> to <span class="in">`cust`</span>:</span>
<span id="cb67-1479"><a href="#cb67-1479" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}</span>
<span id="cb67-1480"><a href="#cb67-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1483"><a href="#cb67-1483" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1484"><a href="#cb67-1484" aria-hidden="true" tabindex="-1"></a>wls_elec2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(kwh <span class="sc">~</span> inc <span class="sc">+</span> <span class="fu">I</span>(<span class="dv">1</span> <span class="sc">/</span> mc6) <span class="sc">+</span>  gas6 <span class="sc">+</span> cap, uk_elec, </span>
<span id="cb67-1485"><a href="#cb67-1485" aria-hidden="true" tabindex="-1"></a>                <span class="at">weights =</span> cust)</span>
<span id="cb67-1486"><a href="#cb67-1486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1487"><a href="#cb67-1487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1488"><a href="#cb67-1488" aria-hidden="true" tabindex="-1"></a>Comparing the robust standard errors of the OLS estimator and those of the WLS estimator, we get:</span>
<span id="cb67-1489"><a href="#cb67-1489" aria-hidden="true" tabindex="-1"></a>\idxfun{vcov}{stats}\idxfun{vcovHC}{sandwich}\idxfun{stder}{micsr}\idxfun{tibble}{tibble}</span>
<span id="cb67-1490"><a href="#cb67-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1493"><a href="#cb67-1493" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1494"><a href="#cb67-1494" aria-hidden="true" tabindex="-1"></a>std_ols <span class="ot">&lt;-</span> <span class="fu">vcovHC</span>(lm_elec) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1495"><a href="#cb67-1495" aria-hidden="true" tabindex="-1"></a>std_wls <span class="ot">&lt;-</span> <span class="fu">vcov</span>(wls_elec2) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1496"><a href="#cb67-1496" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">ols =</span> std_ols, <span class="at">wls =</span> std_wls, <span class="at">ratio =</span> wls <span class="sc">/</span> ols)</span>
<span id="cb67-1497"><a href="#cb67-1497" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1498"><a href="#cb67-1498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1499"><a href="#cb67-1499" aria-hidden="true" tabindex="-1"></a>The efficiency gain of using WLS is substantial, as the standard errors reduce by about 25$-$50% depending on the coefficient.\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{uk<span class="sc">\_</span>elec}{micsr.data}</span>
<span id="cb67-1500"><a href="#cb67-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1501"><a href="#cb67-1501" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{weighted least squares|)}</span>
<span id="cb67-1502"><a href="#cb67-1502" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares!weighted least squares|)}</span>
<span id="cb67-1503"><a href="#cb67-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1504"><a href="#cb67-1504" aria-hidden="true" tabindex="-1"></a><span class="fu">### Error component model {#sec-error_component_gls}</span></span>
<span id="cb67-1505"><a href="#cb67-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1506"><a href="#cb67-1506" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{error component model|(}</span>
<span id="cb67-1507"><a href="#cb67-1507" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!error component model|(}</span>
<span id="cb67-1508"><a href="#cb67-1508" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares!error component model|(}</span>
<span id="cb67-1509"><a href="#cb67-1509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1510"><a href="#cb67-1510" aria-hidden="true" tabindex="-1"></a>The error component model is suitable for panel data or pseudo panel data. In the remainder of this section, we'll mention individual means, which is the proper term for panel data, but should be replaced by entity means for pseudo-panel data.</span>
<span id="cb67-1511"><a href="#cb67-1511" aria-hidden="true" tabindex="-1"></a>Remember that for the error component model, @eq-poweromegaec is a general</span>
<span id="cb67-1512"><a href="#cb67-1512" aria-hidden="true" tabindex="-1"></a>formula that can be used to compute any power of $\Omega$, and $C$ is in</span>
<span id="cb67-1513"><a href="#cb67-1513" aria-hidden="true" tabindex="-1"></a>this context obtained by taking $r=-0.5$:</span>
<span id="cb67-1514"><a href="#cb67-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1515"><a href="#cb67-1515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1516"><a href="#cb67-1516" aria-hidden="true" tabindex="-1"></a>C = \Omega ^ {-0.5} = \frac{1}{\sigma_\nu} W + \frac{1}{\sigma_\iota} B = \frac{1}{\sigma_\nu}\left(W + \frac{\sigma_\nu}{\sigma_\iota} B\right)</span>
<span id="cb67-1517"><a href="#cb67-1517" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Cerrcomp}</span>
<span id="cb67-1518"><a href="#cb67-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1519"><a href="#cb67-1519" aria-hidden="true" tabindex="-1"></a>As $W = I - B$, $C$ can also be rewritten as:</span>
<span id="cb67-1520"><a href="#cb67-1520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1521"><a href="#cb67-1521" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1522"><a href="#cb67-1522" aria-hidden="true" tabindex="-1"></a>C = \Omega ^ {-0.5} = \frac{1}{\sigma_\nu}\left(I - \left<span class="co">[</span><span class="ot">1 -\frac{\sigma_\nu}{\sigma_\iota}\right</span><span class="co">]</span> B\right) = \frac{1}{\sigma_\nu}(I - \theta B)</span>
<span id="cb67-1523"><a href="#cb67-1523" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Cerrcomp2}</span>
<span id="cb67-1524"><a href="#cb67-1524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1525"><a href="#cb67-1525" aria-hidden="true" tabindex="-1"></a>with $\theta = 1 - \frac{\sigma_\nu}{\sigma_\iota}$. $\theta$ can be</span>
<span id="cb67-1526"><a href="#cb67-1526" aria-hidden="true" tabindex="-1"></a>further written as:</span>
<span id="cb67-1527"><a href="#cb67-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1528"><a href="#cb67-1528" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1529"><a href="#cb67-1529" aria-hidden="true" tabindex="-1"></a>\theta = 1 - \frac{\sigma_\nu}{\sqrt{T \sigma_\eta^2 + \sigma_\nu ^ 2}} = 1 - \frac{1}{\sqrt{T \sigma_\eta ^ 2 / \sigma_\nu^2+1}}</span>
<span id="cb67-1530"><a href="#cb67-1530" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb67-1531"><a href="#cb67-1531" aria-hidden="true" tabindex="-1"></a>Therefore $0\leq \theta \leq 1$, so that the $\sigma_\nu C$ matrix performs, in</span>
<span id="cb67-1532"><a href="#cb67-1532" aria-hidden="true" tabindex="-1"></a>this context, a quasi-difference from the individual mean:</span>
<span id="cb67-1533"><a href="#cb67-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1534"><a href="#cb67-1534" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1535"><a href="#cb67-1535" aria-hidden="true" tabindex="-1"></a>z_n ^ * = z_{nt} - \theta \bar{z}_{n.}</span>
<span id="cb67-1536"><a href="#cb67-1536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1537"><a href="#cb67-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1538"><a href="#cb67-1538" aria-hidden="true" tabindex="-1"></a>The share of the individual mean that is subtracted depends on:</span>
<span id="cb67-1539"><a href="#cb67-1539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1540"><a href="#cb67-1540" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the relative weights of the two variances: $\theta \rightarrow 0$</span>
<span id="cb67-1541"><a href="#cb67-1541" aria-hidden="true" tabindex="-1"></a>    when $\sigma_\eta ^ 2 / \sigma_\nu ^ 2 \rightarrow 0$, which means</span>
<span id="cb67-1542"><a href="#cb67-1542" aria-hidden="true" tabindex="-1"></a>    that there are no individual effects. As</span>
<span id="cb67-1543"><a href="#cb67-1543" aria-hidden="true" tabindex="-1"></a>    $\sigma_\eta ^ 2 / \sigma_\nu ^ 2 \rightarrow + \infty$,</span>
<span id="cb67-1544"><a href="#cb67-1544" aria-hidden="true" tabindex="-1"></a>    $\theta \rightarrow 1$ and the transformation is the difference from</span>
<span id="cb67-1545"><a href="#cb67-1545" aria-hidden="true" tabindex="-1"></a>    the individual mean;</span>
<span id="cb67-1546"><a href="#cb67-1546" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the number of observations for each individual,</span>
<span id="cb67-1547"><a href="#cb67-1547" aria-hidden="true" tabindex="-1"></a>    $\theta \rightarrow 1$ when $T\rightarrow + \infty$; therefore the</span>
<span id="cb67-1548"><a href="#cb67-1548" aria-hidden="true" tabindex="-1"></a>    transformation is close to a difference from individual mean for</span>
<span id="cb67-1549"><a href="#cb67-1549" aria-hidden="true" tabindex="-1"></a>    large $T$.</span>
<span id="cb67-1550"><a href="#cb67-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1551"><a href="#cb67-1551" aria-hidden="true" tabindex="-1"></a>$\sigma_\nu$ and $\sigma_\eta$ are unknown parameters and have to be</span>
<span id="cb67-1552"><a href="#cb67-1552" aria-hidden="true" tabindex="-1"></a>estimated.</span>
<span id="cb67-1553"><a href="#cb67-1553" aria-hidden="true" tabindex="-1"></a>Consider the errors of the model $\epsilon_{nt}$, their individual mean</span>
<span id="cb67-1554"><a href="#cb67-1554" aria-hidden="true" tabindex="-1"></a>$\bar{\epsilon}_{n.}$ and the deviations from these individual means</span>
<span id="cb67-1555"><a href="#cb67-1555" aria-hidden="true" tabindex="-1"></a>$\epsilon_{nt} - \bar{\epsilon}_{n.}$. By hypothesis, we have:</span>
<span id="cb67-1556"><a href="#cb67-1556" aria-hidden="true" tabindex="-1"></a>$\mbox{V}\left(\epsilon_{nt}\right)=\sigma_\nu^2+\sigma_\eta^2$. For the</span>
<span id="cb67-1557"><a href="#cb67-1557" aria-hidden="true" tabindex="-1"></a>individual means, we get:</span>
<span id="cb67-1558"><a href="#cb67-1558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1559"><a href="#cb67-1559" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1560"><a href="#cb67-1560" aria-hidden="true" tabindex="-1"></a>\bar{\epsilon}_{n.}=\frac{1}{T}\sum_{t=1}^T \epsilon_{nt} = \eta_n +</span>
<span id="cb67-1561"><a href="#cb67-1561" aria-hidden="true" tabindex="-1"></a>\frac{1}{T}\sum_{t=1}^T \nu_{nt}</span>
<span id="cb67-1562"><a href="#cb67-1562" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1563"><a href="#cb67-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1564"><a href="#cb67-1564" aria-hidden="true" tabindex="-1"></a>for which the variance is:</span>
<span id="cb67-1565"><a href="#cb67-1565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1566"><a href="#cb67-1566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1567"><a href="#cb67-1567" aria-hidden="true" tabindex="-1"></a>\mbox{V}\left(\bar{\epsilon}_{n.}\right)=\sigma_{\eta}^2 + \frac{1}{T}</span>
<span id="cb67-1568"><a href="#cb67-1568" aria-hidden="true" tabindex="-1"></a>\sigma_{\nu}^2 = \sigma_\iota^2 / T</span>
<span id="cb67-1569"><a href="#cb67-1569" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1570"><a href="#cb67-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1571"><a href="#cb67-1571" aria-hidden="true" tabindex="-1"></a>The variance of the deviation from the individual means is easily</span>
<span id="cb67-1572"><a href="#cb67-1572" aria-hidden="true" tabindex="-1"></a>obtained by isolating terms in $\epsilon_{nt}$:</span>
<span id="cb67-1573"><a href="#cb67-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1574"><a href="#cb67-1574" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1575"><a href="#cb67-1575" aria-hidden="true" tabindex="-1"></a>\epsilon_{nt} - \bar{\epsilon}_{n.}=\epsilon_{nt}-\frac{1}{T}\sum_{t=1}^T</span>
<span id="cb67-1576"><a href="#cb67-1576" aria-hidden="true" tabindex="-1"></a>\epsilon_{nt}=\left(1-\frac{1}{T}\right)\epsilon_{nt}-</span>
<span id="cb67-1577"><a href="#cb67-1577" aria-hidden="true" tabindex="-1"></a>\frac{1}{T}\sum_{s \neq t} \epsilon_{ns}</span>
<span id="cb67-1578"><a href="#cb67-1578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1579"><a href="#cb67-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1580"><a href="#cb67-1580" aria-hidden="true" tabindex="-1"></a>The variance is, noting that the sum now contains $T-1$ terms:</span>
<span id="cb67-1581"><a href="#cb67-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1582"><a href="#cb67-1582" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1583"><a href="#cb67-1583" aria-hidden="true" tabindex="-1"></a>\mbox{V}\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right) =</span>
<span id="cb67-1584"><a href="#cb67-1584" aria-hidden="true" tabindex="-1"></a>\left(1-\frac{1}{T}\right)^2\sigma_{\nu}^2+</span>
<span id="cb67-1585"><a href="#cb67-1585" aria-hidden="true" tabindex="-1"></a>\frac{1}{T^2}(T-1)\sigma_{\nu}^2 = \frac{T-1}{T}\sigma_\nu ^ 2</span>
<span id="cb67-1586"><a href="#cb67-1586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1587"><a href="#cb67-1587" aria-hidden="true" tabindex="-1"></a>If $\epsilon$ were known, natural estimators of these two variances</span>
<span id="cb67-1588"><a href="#cb67-1588" aria-hidden="true" tabindex="-1"></a>$\sigma_{\iota}^2$ and $\sigma_{\nu}^2$ would be:</span>
<span id="cb67-1589"><a href="#cb67-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1590"><a href="#cb67-1590" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1591"><a href="#cb67-1591" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_\iota^2 = T \frac{\sum_{n=1}^N\bar{\epsilon}_{n.}^2}{N} =</span>
<span id="cb67-1592"><a href="#cb67-1592" aria-hidden="true" tabindex="-1"></a>T \frac{\sum_{n=1}^N\sum_{t=1}^T\bar{\epsilon}_{n.}^2}{NT}=T\frac{\epsilon^{\top}B\epsilon}{NT}=\frac{\epsilon^{\top}B\epsilon}{N}</span>
<span id="cb67-1593"><a href="#cb67-1593" aria-hidden="true" tabindex="-1"></a>$$ {#eq-varmxt}</span>
<span id="cb67-1594"><a href="#cb67-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1595"><a href="#cb67-1595" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1596"><a href="#cb67-1596" aria-hidden="true" tabindex="-1"></a>  \hat{\sigma}_{\nu}^2 = \frac{T}{T-1}</span>
<span id="cb67-1597"><a href="#cb67-1597" aria-hidden="true" tabindex="-1"></a>  \frac{\sum_{n=1}^N\sum_{t=1}^T\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right)^2}{NT}</span>
<span id="cb67-1598"><a href="#cb67-1598" aria-hidden="true" tabindex="-1"></a>  =\frac{\sum_{n=1}^N\sum_{t=1}^T\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right)^2}{N(T-1)}</span>
<span id="cb67-1599"><a href="#cb67-1599" aria-hidden="true" tabindex="-1"></a>    = \frac{\epsilon^{\top}W \epsilon}{N(T-1)}</span>
<span id="cb67-1600"><a href="#cb67-1600" aria-hidden="true" tabindex="-1"></a>$$ {#eq-varidios}</span>
<span id="cb67-1601"><a href="#cb67-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1602"><a href="#cb67-1602" aria-hidden="true" tabindex="-1"></a>Several estimators of the two components of the variance have been</span>
<span id="cb67-1603"><a href="#cb67-1603" aria-hidden="true" tabindex="-1"></a>proposed in the literature. They all consist of replacing</span>
<span id="cb67-1604"><a href="#cb67-1604" aria-hidden="true" tabindex="-1"></a>$\epsilon_{nt}$ in the previous two equations by consistent estimates</span>
<span id="cb67-1605"><a href="#cb67-1605" aria-hidden="true" tabindex="-1"></a>(and for some of them by applying some degrees of freedom correction).</span>
<span id="cb67-1606"><a href="#cb67-1606" aria-hidden="true" tabindex="-1"></a>The estimator proposed by @WALL:HUSS:69\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Wallace}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Hussain} is particularly simple because</span>
<span id="cb67-1607"><a href="#cb67-1607" aria-hidden="true" tabindex="-1"></a>it uses the residuals of the OLS estimation.<span class="ot">[^non_spherical-4]</span></span>
<span id="cb67-1608"><a href="#cb67-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1609"><a href="#cb67-1609" aria-hidden="true" tabindex="-1"></a><span class="ot">[^non_spherical-4]: </span>See also @SWAM:AROR:72\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Swamy}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Arora}, @AMEM:71\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Amemiya} and @NERL:71\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Nerlove}.</span>
<span id="cb67-1610"><a href="#cb67-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1611"><a href="#cb67-1611" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb67-1612"><a href="#cb67-1612" aria-hidden="true" tabindex="-1"></a>The residuals of the OLS regression were already added to the <span class="in">`twins`</span> data set and </span>
<span id="cb67-1613"><a href="#cb67-1613" aria-hidden="true" tabindex="-1"></a>$B \hat{\epsilon}$ was computed. We then compute $W\hat{\epsilon} = \hat{\epsilon} - B \hat{\epsilon}$ and we use @eq-varmxt and @eq-varidios to compute $\hat{\sigma}_\iota ^ 2$, $\hat{\sigma}_\nu ^ 2$, $\theta = 1 - \hat{\sigma}_\nu / \hat{\sigma}_\iota$ and then $\hat{\sigma}_\eta ^ 2 = (\hat{\sigma}_\iota ^ 2 - \hat{\sigma}_\nu^2) / T$.</span>
<span id="cb67-1614"><a href="#cb67-1614" aria-hidden="true" tabindex="-1"></a>\idxfun{mutate}{dplyr}\idxfun{summarise}{dplyr}</span>
<span id="cb67-1615"><a href="#cb67-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1616"><a href="#cb67-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1619"><a href="#cb67-1619" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1620"><a href="#cb67-1620" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-1621"><a href="#cb67-1621" aria-hidden="true" tabindex="-1"></a>twins <span class="ot">&lt;-</span> twins <span class="sc">%&gt;%</span> </span>
<span id="cb67-1622"><a href="#cb67-1622" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">We =</span> e <span class="sc">-</span> Be)</span>
<span id="cb67-1623"><a href="#cb67-1623" aria-hidden="true" tabindex="-1"></a>sigs <span class="ot">&lt;-</span> twins <span class="sc">%&gt;%</span> </span>
<span id="cb67-1624"><a href="#cb67-1624" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">s2iota =</span> <span class="fu">sum</span>(Be <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> N_tw, </span>
<span id="cb67-1625"><a href="#cb67-1625" aria-hidden="true" tabindex="-1"></a>            <span class="at">s2nu =</span> <span class="fu">sum</span>(We <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> (N_tw <span class="sc">*</span> (T_tw <span class="sc">-</span> <span class="dv">1</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1626"><a href="#cb67-1626" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">s2eta =</span> (s2iota <span class="sc">-</span> s2nu) <span class="sc">/</span> T_tw,</span>
<span id="cb67-1627"><a href="#cb67-1627" aria-hidden="true" tabindex="-1"></a>         <span class="at">theta =</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sqrt</span>(s2nu <span class="sc">/</span> s2iota))</span>
<span id="cb67-1628"><a href="#cb67-1628" aria-hidden="true" tabindex="-1"></a>sigs</span>
<span id="cb67-1629"><a href="#cb67-1629" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1630"><a href="#cb67-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1631"><a href="#cb67-1631" aria-hidden="true" tabindex="-1"></a>The <span class="in">`plm`</span> function performs the GLS estimation, i.e., an OLS regression on data transformed using quasi-differences ($w_{nt} ^ * = w_{nt} - \theta \bar{w}_{n.}$) if the <span class="in">`model`</span> argument is set to <span class="in">`"random"`</span>:</span>
<span id="cb67-1632"><a href="#cb67-1632" aria-hidden="true" tabindex="-1"></a>\idxfun{plm}{plm}</span>
<span id="cb67-1633"><a href="#cb67-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1636"><a href="#cb67-1636" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1637"><a href="#cb67-1637" aria-hidden="true" tabindex="-1"></a>gls_twins <span class="ot">&lt;-</span> <span class="fu">plm</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>) <span class="sc">+</span> educ, twins, </span>
<span id="cb67-1638"><a href="#cb67-1638" aria-hidden="true" tabindex="-1"></a>                 <span class="at">random.method =</span> <span class="st">"walhus"</span>, <span class="at">model  =</span> <span class="st">"random"</span>)</span>
<span id="cb67-1639"><a href="#cb67-1639" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gls_twins)</span>
<span id="cb67-1640"><a href="#cb67-1640" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1641"><a href="#cb67-1641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1642"><a href="#cb67-1642" aria-hidden="true" tabindex="-1"></a>Note that we set the <span class="in">`random.method`</span> argument to <span class="in">`"walhus"`</span> to select the Wallace and Hussain estimator.</span>
<span id="cb67-1643"><a href="#cb67-1643" aria-hidden="true" tabindex="-1"></a>The output is quite similar to the one for <span class="in">`lm`</span> objects, except the two parts that appear at the beginning. The dimensions of the panel are indicated (number of individuals / entities and number of time series / observations in each entity) and whether the data set is balanced or not. A panel data is balanced if all the individuals are observed for the same set of time periods. In a pseudo-panel (which is the case here), the data set is balanced if there is the same number of observations for each entity (which is obviously the case for our sample of twins). This information can be obtained using <span class="in">`pdim`</span>. The first argument is a data frame, the second one, called <span class="in">`index`</span>, is a vector of two characters indicating the name of the individual and of the time index. It can be omitted if the first two columns contains these indexes, which is the case for the <span class="in">`twins`</span> data set:</span>
<span id="cb67-1644"><a href="#cb67-1644" aria-hidden="true" tabindex="-1"></a>\idxfun{pdim}{plm}</span>
<span id="cb67-1645"><a href="#cb67-1645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1648"><a href="#cb67-1648" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1649"><a href="#cb67-1649" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb67-1650"><a href="#cb67-1650" aria-hidden="true" tabindex="-1"></a><span class="fu">pdim</span>(twins, <span class="at">index =</span> <span class="fu">c</span>(<span class="st">"family"</span>, <span class="st">"twin"</span>))</span>
<span id="cb67-1651"><a href="#cb67-1651" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1652"><a href="#cb67-1652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1653"><a href="#cb67-1653" aria-hidden="true" tabindex="-1"></a>The second specific part of the output gives information about the variances of the two components of the error. We can see here that the individual effects (in this example, a family effect) account for only 15% of the total variance of the error. Therefore, only a small part of the individual mean is removed while performing GLS (14.3%). This information can be obtained using the <span class="in">`ercomp`</span> function:</span>
<span id="cb67-1654"><a href="#cb67-1654" aria-hidden="true" tabindex="-1"></a>\idxfun{ercomp}{plm}</span>
<span id="cb67-1655"><a href="#cb67-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1658"><a href="#cb67-1658" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1659"><a href="#cb67-1659" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ercomp</span></span>
<span id="cb67-1660"><a href="#cb67-1660" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb67-1661"><a href="#cb67-1661" aria-hidden="true" tabindex="-1"></a><span class="fu">ercomp</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>) <span class="sc">+</span> educ, twins, <span class="at">method =</span> <span class="st">"walhus"</span>)</span>
<span id="cb67-1662"><a href="#cb67-1662" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1663"><a href="#cb67-1663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1664"><a href="#cb67-1664" aria-hidden="true" tabindex="-1"></a>The model can also be estimated by maximum likelihood, using <span class="in">`pglm::pglm`</span>. This function adapts the behavior of the <span class="in">`stats::glm`</span> function which fits generalized linear model for panel data. In particular, it has a <span class="in">`family`</span> argument that is set to <span class="in">`"gaussian"`</span>:</span>
<span id="cb67-1665"><a href="#cb67-1665" aria-hidden="true" tabindex="-1"></a>\idxfun{pglm}{pglm}\idxfun{gaze}{micsr}</span>
<span id="cb67-1666"><a href="#cb67-1666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1669"><a href="#cb67-1669" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1670"><a href="#cb67-1670" aria-hidden="true" tabindex="-1"></a>ml_twins <span class="ot">&lt;-</span> pglm<span class="sc">::</span><span class="fu">pglm</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>) <span class="sc">+</span> educ, </span>
<span id="cb67-1671"><a href="#cb67-1671" aria-hidden="true" tabindex="-1"></a>                       twins, <span class="at">family =</span> gaussian)</span>
<span id="cb67-1672"><a href="#cb67-1672" aria-hidden="true" tabindex="-1"></a>ml_twins <span class="sc">%&gt;%</span> gaze</span>
<span id="cb67-1673"><a href="#cb67-1673" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1674"><a href="#cb67-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1675"><a href="#cb67-1675" aria-hidden="true" tabindex="-1"></a>$\sigma_\eta$ and $\sigma_\nu$ are now two parameters that are directly estimated. We can see that the estimated values are very close to the ones obtained using GLS and that $\sigma_\eta$ is statistically significant.</span>
<span id="cb67-1676"><a href="#cb67-1676" aria-hidden="true" tabindex="-1"></a>Comparing OLS and GLS standard deviations, we get:</span>
<span id="cb67-1677"><a href="#cb67-1677" aria-hidden="true" tabindex="-1"></a>\idxfun{vcov}{stats}\idxfun{vcovCL}{sandwich}\idxfun{stder}{micsr}\idxfun{rbind}{base}</span>
<span id="cb67-1678"><a href="#cb67-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1681"><a href="#cb67-1681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1682"><a href="#cb67-1682" aria-hidden="true" tabindex="-1"></a>ols_se <span class="ot">&lt;-</span> <span class="fu">vcovCL</span>(lm_twins, <span class="sc">~</span> family, <span class="at">type =</span> <span class="st">"HC0"</span>, <span class="at">cadjust =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb67-1683"><a href="#cb67-1683" aria-hidden="true" tabindex="-1"></a>  stder</span>
<span id="cb67-1684"><a href="#cb67-1684" aria-hidden="true" tabindex="-1"></a>gls_se <span class="ot">&lt;-</span> <span class="fu">vcov</span>(gls_twins) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1685"><a href="#cb67-1685" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="at">ols =</span> ord_se, <span class="at">gls =</span> gls_se, <span class="at">ratio =</span> gls_se <span class="sc">/</span> ols_se)</span>
<span id="cb67-1686"><a href="#cb67-1686" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1687"><a href="#cb67-1687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1688"><a href="#cb67-1688" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb67-1689"><a href="#cb67-1689" aria-hidden="true" tabindex="-1"></a>and therefore, there seems to be no gain of efficiency while using OLS instead of GLS. With Tobin's Q example, we get:</span>
<span id="cb67-1690"><a href="#cb67-1690" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{tobinq}{micsr.data} \idxfun{plm}{plm}\idxfun{ercomp}{plm}</span>
<span id="cb67-1691"><a href="#cb67-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1694"><a href="#cb67-1694" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1695"><a href="#cb67-1695" aria-hidden="true" tabindex="-1"></a>gls_q <span class="ot">&lt;-</span> <span class="fu">plm</span>(ikn <span class="sc">~</span> qn, tobinq, <span class="at">model =</span> <span class="st">"random"</span>)</span>
<span id="cb67-1696"><a href="#cb67-1696" aria-hidden="true" tabindex="-1"></a><span class="fu">ercomp</span>(gls_q)</span>
<span id="cb67-1697"><a href="#cb67-1697" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1698"><a href="#cb67-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1699"><a href="#cb67-1699" aria-hidden="true" tabindex="-1"></a>The share of the individual effect is now 27.5%, and the GLS is now OLS on series for which 73.5% of the individual mean has been removed, mostly because the time dimension of the panel is high (35 years). Comparing the robust standard errors of OLS and those of GLS, we get:</span>
<span id="cb67-1700"><a href="#cb67-1700" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{vcovCL}{sandwich}\idxfun{stder}{micsr}\idxfun{vcov}{stats}\idxfun{rbind}{base}</span>
<span id="cb67-1701"><a href="#cb67-1701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1704"><a href="#cb67-1704" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1705"><a href="#cb67-1705" aria-hidden="true" tabindex="-1"></a>ols_q <span class="ot">&lt;-</span> <span class="fu">lm</span>(ikn <span class="sc">~</span> qn, tobinq)</span>
<span id="cb67-1706"><a href="#cb67-1706" aria-hidden="true" tabindex="-1"></a>sd_ols_q <span class="ot">&lt;-</span> <span class="fu">vcovCL</span>(ols_q, <span class="sc">~</span> cusip) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1707"><a href="#cb67-1707" aria-hidden="true" tabindex="-1"></a>sd_gls_q <span class="ot">&lt;-</span> <span class="fu">vcov</span>(gls_q) <span class="sc">%&gt;%</span> stder</span>
<span id="cb67-1708"><a href="#cb67-1708" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="at">ols =</span> sd_ols_q, <span class="at">gls =</span> sd_gls_q, <span class="at">ratio =</span> sd_gls_q <span class="sc">/</span> sd_ols_q)</span>
<span id="cb67-1709"><a href="#cb67-1709" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1710"><a href="#cb67-1710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1711"><a href="#cb67-1711" aria-hidden="true" tabindex="-1"></a>GLS is much more efficient than OLS as the standard error of the slope is about four times smaller.</span>
<span id="cb67-1712"><a href="#cb67-1712" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb67-1713"><a href="#cb67-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1714"><a href="#cb67-1714" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{error component model|)}</span>
<span id="cb67-1715"><a href="#cb67-1715" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!error component model|)}</span>
<span id="cb67-1716"><a href="#cb67-1716" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares!error component model|)}</span>
<span id="cb67-1717"><a href="#cb67-1717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1718"><a href="#cb67-1718" aria-hidden="true" tabindex="-1"></a><span class="fu">### Seemingly unrelated regression {#sec-sur}</span></span>
<span id="cb67-1719"><a href="#cb67-1719" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation!seemingly unrelated regression|(}</span>
<span id="cb67-1720"><a href="#cb67-1720" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{seemingly unrelated regression|(}</span>
<span id="cb67-1721"><a href="#cb67-1721" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares!seemingly unrelated regression|(}</span>
<span id="cb67-1722"><a href="#cb67-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1723"><a href="#cb67-1723" aria-hidden="true" tabindex="-1"></a>Because of the inter-equation correlations, the efficient estimator is</span>
<span id="cb67-1724"><a href="#cb67-1724" aria-hidden="true" tabindex="-1"></a>the GLS estimator:</span>
<span id="cb67-1725"><a href="#cb67-1725" aria-hidden="true" tabindex="-1"></a>$\hat{\gamma}=(Z^\top\Omega^{-1}Z)^{-1}Z^\top\Omega^{-1}y$. This</span>
<span id="cb67-1726"><a href="#cb67-1726" aria-hidden="true" tabindex="-1"></a>estimator, first proposed by @ZELL:62\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Zellner}, is known as a **seemingly unrelated regression** (**SUR**). It can be obtained by applying</span>
<span id="cb67-1727"><a href="#cb67-1727" aria-hidden="true" tabindex="-1"></a>OLS on transformed data, each variable being premultiplied by</span>
<span id="cb67-1728"><a href="#cb67-1728" aria-hidden="true" tabindex="-1"></a>$\Omega^{-0.5}$. This matrix is simply</span>
<span id="cb67-1729"><a href="#cb67-1729" aria-hidden="true" tabindex="-1"></a>$\Omega^{-0.5}=\Sigma^{-0.5}\otimes I$. Denoting $\delta_{lm}$ the</span>
<span id="cb67-1730"><a href="#cb67-1730" aria-hidden="true" tabindex="-1"></a>elements of $\Sigma^{-0.5}$, the transformed response and covariates</span>
<span id="cb67-1731"><a href="#cb67-1731" aria-hidden="true" tabindex="-1"></a>are:</span>
<span id="cb67-1732"><a href="#cb67-1732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1733"><a href="#cb67-1733" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1734"><a href="#cb67-1734" aria-hidden="true" tabindex="-1"></a>y ^ *=</span>
<span id="cb67-1735"><a href="#cb67-1735" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1736"><a href="#cb67-1736" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb67-1737"><a href="#cb67-1737" aria-hidden="true" tabindex="-1"></a>  \delta_{11} y_1 + \delta_{12} y_2 + \ldots +\delta_{1L} y_L <span class="sc">\\</span></span>
<span id="cb67-1738"><a href="#cb67-1738" aria-hidden="true" tabindex="-1"></a>  \delta_{21} y_1 + \delta_{22} y_2 + \ldots +\delta_{2L} y_L <span class="sc">\\</span></span>
<span id="cb67-1739"><a href="#cb67-1739" aria-hidden="true" tabindex="-1"></a>    \vdots <span class="sc">\\</span></span>
<span id="cb67-1740"><a href="#cb67-1740" aria-hidden="true" tabindex="-1"></a>  \delta_{L1} y_1 + \delta_{L2} y_2 + \ldots +\delta_{LL} y_L</span>
<span id="cb67-1741"><a href="#cb67-1741" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-1742"><a href="#cb67-1742" aria-hidden="true" tabindex="-1"></a>\right),</span>
<span id="cb67-1743"><a href="#cb67-1743" aria-hidden="true" tabindex="-1"></a>Z ^ *=</span>
<span id="cb67-1744"><a href="#cb67-1744" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb67-1745"><a href="#cb67-1745" aria-hidden="true" tabindex="-1"></a>  \begin{array}{cccc}</span>
<span id="cb67-1746"><a href="#cb67-1746" aria-hidden="true" tabindex="-1"></a>  \delta_{11} Z_1  &amp;\delta_{12} Z_2 &amp; \ldots &amp; \delta_{1L} Z_L <span class="sc">\\</span></span>
<span id="cb67-1747"><a href="#cb67-1747" aria-hidden="true" tabindex="-1"></a>  \delta_{21} Z_1  &amp; \delta_{22} Z_2  &amp; \ldots &amp; \delta_{2L} Z_L <span class="sc">\\</span></span>
<span id="cb67-1748"><a href="#cb67-1748" aria-hidden="true" tabindex="-1"></a>    \vdots      &amp; \vdots      &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb67-1749"><a href="#cb67-1749" aria-hidden="true" tabindex="-1"></a>  \delta_{L1} Z_1  &amp; \delta_{L2} Z_2  &amp; \ldots &amp; \delta_{LL} Z_L</span>
<span id="cb67-1750"><a href="#cb67-1750" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb67-1751"><a href="#cb67-1751" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb67-1752"><a href="#cb67-1752" aria-hidden="true" tabindex="-1"></a>$$ {#eq-transformation_sur}</span>
<span id="cb67-1753"><a href="#cb67-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1754"><a href="#cb67-1754" aria-hidden="true" tabindex="-1"></a>$\Sigma$ is a matrix that contains unknown parameters, which can be</span>
<span id="cb67-1755"><a href="#cb67-1755" aria-hidden="true" tabindex="-1"></a>estimated using residuals of a consistent but inefficient preliminary</span>
<span id="cb67-1756"><a href="#cb67-1756" aria-hidden="true" tabindex="-1"></a>estimator, like OLS. The efficient estimator is then obtained the</span>
<span id="cb67-1757"><a href="#cb67-1757" aria-hidden="true" tabindex="-1"></a>following way:</span>
<span id="cb67-1758"><a href="#cb67-1758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1759"><a href="#cb67-1759" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>first, estimate each equation separately by OLS and denote</span>
<span id="cb67-1760"><a href="#cb67-1760" aria-hidden="true" tabindex="-1"></a>    $\hat{\Xi} = (\hat{\epsilon}_1, \hat{\epsilon}_2, \ldots,\hat{\epsilon}_L)$</span>
<span id="cb67-1761"><a href="#cb67-1761" aria-hidden="true" tabindex="-1"></a>    the $N\times L$ matrix for which every column is the residual vector</span>
<span id="cb67-1762"><a href="#cb67-1762" aria-hidden="true" tabindex="-1"></a>    of one of the equations in the system,</span>
<span id="cb67-1763"><a href="#cb67-1763" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>then, estimate the covariance matrix of the errors:</span>
<span id="cb67-1764"><a href="#cb67-1764" aria-hidden="true" tabindex="-1"></a>    $\hat{\Sigma}=\hat{\Xi}^\top\hat{\Xi} / N$,</span>
<span id="cb67-1765"><a href="#cb67-1765" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>compute the matrix $\hat{\Sigma}^{-0.5}$ and use it to transform the</span>
<span id="cb67-1766"><a href="#cb67-1766" aria-hidden="true" tabindex="-1"></a>    response and the covariates of the model,</span>
<span id="cb67-1767"><a href="#cb67-1767" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>finally, estimate the model by applying OLS on transformed data.</span>
<span id="cb67-1768"><a href="#cb67-1768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1769"><a href="#cb67-1769" aria-hidden="true" tabindex="-1"></a>$\Sigma^{-0.5}$ can conveniently be computed using the Cholesky</span>
<span id="cb67-1770"><a href="#cb67-1770" aria-hidden="true" tabindex="-1"></a>decomposition, i.e., the upper-triangular matrix $C$ which is</span>
<span id="cb67-1771"><a href="#cb67-1771" aria-hidden="true" tabindex="-1"></a>such that $C^\top C=\Sigma^{-1}$.</span>
<span id="cb67-1772"><a href="#cb67-1772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1773"><a href="#cb67-1773" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- clarifier lower/uper triangular --&gt;</span></span>
<span id="cb67-1774"><a href="#cb67-1774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1775"><a href="#cb67-1775" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{apples}{micsr}</span>
<span id="cb67-1776"><a href="#cb67-1776" aria-hidden="true" tabindex="-1"></a>To illustrate the use of the **SUR** estimator, we go back to the</span>
<span id="cb67-1777"><a href="#cb67-1777" aria-hidden="true" tabindex="-1"></a>estimation of the system of three equations (one cost</span>
<span id="cb67-1778"><a href="#cb67-1778" aria-hidden="true" tabindex="-1"></a>function and two factor share equations) for the production of apples</span>
<span id="cb67-1779"><a href="#cb67-1779" aria-hidden="true" tabindex="-1"></a>started in @sec-system_equation. In this section, we computed a tibble containing the three responses <span class="in">`Y`</span> and the model matrices for the three equations <span class="in">`Z_c`</span>, <span class="in">`Z_l`</span> and <span class="in">`Z_m`</span> (respectively for the cost, the labor and the materials equations). In @sec-bptest_system, we estimated the covariance matrix of the errors of the three equations <span class="in">`Sigma`</span>.</span>
<span id="cb67-1780"><a href="#cb67-1780" aria-hidden="true" tabindex="-1"></a>To implement the **SUR** estimator, we compute the Cholesky decomposition of the inverse of the estimated covariance matrix of the errors of the three equations:</span>
<span id="cb67-1781"><a href="#cb67-1781" aria-hidden="true" tabindex="-1"></a>\idxfun{transmute}{dplyr}\idxfun{filter}{dplyr}\idxfun{model.frame}{stats}\idxfun{model.matrix}{stats}\idxfun{model.response}{stats}\idxfun{Formula}{Formula}\idxfun{paste}{base}\idxfun{bdiag}{Matrix}\idxfun{as.matrix}{base}\idxfun{model.part}{Formula}\idxfun{pivot<span class="sc">\_</span>longer}{tidyr}\idxfun{bind<span class="sc">\_</span>cols}{dplyr}\idxfun{nobs}{stats}\idxfun{clm}{micsr}\idxfun{lm}{stats}\idxfun{resid}{stats}\idxfun{crossprod}{base}\idxfun{matrix}{base}\idxfun{colnames}{base}</span>
<span id="cb67-1782"><a href="#cb67-1782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1783"><a href="#cb67-1783" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1784"><a href="#cb67-1784" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb67-1785"><a href="#cb67-1785" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb67-1786"><a href="#cb67-1786" aria-hidden="true" tabindex="-1"></a>ap <span class="ot">&lt;-</span> apples <span class="sc">%&gt;%</span> <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">1985</span>) <span class="sc">%&gt;%</span></span>
<span id="cb67-1787"><a href="#cb67-1787" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(<span class="at">y =</span> otherprod <span class="sc">+</span> apples,</span>
<span id="cb67-1788"><a href="#cb67-1788" aria-hidden="true" tabindex="-1"></a>              <span class="at">ct =</span> capital <span class="sc">+</span> labor <span class="sc">+</span> materials,</span>
<span id="cb67-1789"><a href="#cb67-1789" aria-hidden="true" tabindex="-1"></a>              <span class="at">sl =</span> labor <span class="sc">/</span> ct, <span class="at">sm =</span> materials <span class="sc">/</span> ct,</span>
<span id="cb67-1790"><a href="#cb67-1790" aria-hidden="true" tabindex="-1"></a>              <span class="at">pk =</span> <span class="fu">log</span>(pc <span class="sc">/</span> <span class="fu">mean</span>(pc)), <span class="at">pl =</span> <span class="fu">log</span>(pl <span class="sc">/</span> <span class="fu">mean</span>(pl)) <span class="sc">-</span> pk,</span>
<span id="cb67-1791"><a href="#cb67-1791" aria-hidden="true" tabindex="-1"></a>              <span class="at">pm =</span> <span class="fu">log</span>(pm <span class="sc">/</span> <span class="fu">mean</span>(pm)) <span class="sc">-</span> pk, <span class="at">ct =</span> <span class="fu">log</span>(ct <span class="sc">/</span> <span class="fu">mean</span>(ct)) <span class="sc">-</span> pk,</span>
<span id="cb67-1792"><a href="#cb67-1792" aria-hidden="true" tabindex="-1"></a>              <span class="at">y =</span> <span class="fu">log</span>(y <span class="sc">/</span> <span class="fu">mean</span>(y)), <span class="at">y2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> y <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb67-1793"><a href="#cb67-1793" aria-hidden="true" tabindex="-1"></a>              <span class="at">ct =</span> ct, <span class="at">pl2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> pl <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb67-1794"><a href="#cb67-1794" aria-hidden="true" tabindex="-1"></a>              <span class="at">pm2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> pm <span class="sc">^</span> <span class="dv">2</span>, <span class="at">plm =</span> pl <span class="sc">*</span> pm</span>
<span id="cb67-1795"><a href="#cb67-1795" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb67-1796"><a href="#cb67-1796" aria-hidden="true" tabindex="-1"></a>eq_ct <span class="ot">&lt;-</span> ct <span class="sc">~</span> y <span class="sc">+</span> y2 <span class="sc">+</span> pl <span class="sc">+</span> pm <span class="sc">+</span> pl2 <span class="sc">+</span> plm <span class="sc">+</span> pm2</span>
<span id="cb67-1797"><a href="#cb67-1797" aria-hidden="true" tabindex="-1"></a>eq_sl <span class="ot">&lt;-</span> sl <span class="sc">~</span> pl <span class="sc">+</span> pm</span>
<span id="cb67-1798"><a href="#cb67-1798" aria-hidden="true" tabindex="-1"></a>eq_sm <span class="ot">&lt;-</span> sm <span class="sc">~</span> pl <span class="sc">+</span> pm</span>
<span id="cb67-1799"><a href="#cb67-1799" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Formula)</span>
<span id="cb67-1800"><a href="#cb67-1800" aria-hidden="true" tabindex="-1"></a>eq_sys <span class="ot">&lt;-</span> <span class="fu">Formula</span>(ct <span class="sc">+</span> sl <span class="sc">+</span> sm <span class="sc">~</span> y <span class="sc">+</span> y2 <span class="sc">+</span> pl <span class="sc">+</span> pm <span class="sc">+</span> pl2 <span class="sc">+</span> plm <span class="sc">+</span> pm2)</span>
<span id="cb67-1801"><a href="#cb67-1801" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(eq_sys, ap)  ; Z_c <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_ct, mf) </span>
<span id="cb67-1802"><a href="#cb67-1802" aria-hidden="true" tabindex="-1"></a>Z_l <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_sl, mf) ; Z_m <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_sm, mf)</span>
<span id="cb67-1803"><a href="#cb67-1803" aria-hidden="true" tabindex="-1"></a>nms_cols <span class="ot">&lt;-</span> <span class="cf">function</span>(x, label)</span>
<span id="cb67-1804"><a href="#cb67-1804" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(label, <span class="fu">c</span>(<span class="st">"cst"</span>, <span class="fu">colnames</span>(x)[<span class="sc">-</span><span class="dv">1</span>]), <span class="at">sep =</span> <span class="st">"_"</span>)</span>
<span id="cb67-1805"><a href="#cb67-1805" aria-hidden="true" tabindex="-1"></a>nms_c <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_c, <span class="st">"cost"</span>) ; nms_l <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_l, <span class="st">"sl"</span>)</span>
<span id="cb67-1806"><a href="#cb67-1806" aria-hidden="true" tabindex="-1"></a>nms_m <span class="ot">&lt;-</span> <span class="fu">nms_cols</span>(Z_m, <span class="st">"sm"</span>)</span>
<span id="cb67-1807"><a href="#cb67-1807" aria-hidden="true" tabindex="-1"></a>Zs <span class="ot">&lt;-</span> Matrix<span class="sc">::</span><span class="fu">bdiag</span>(Z_c, Z_l, Z_m) <span class="sc">%&gt;%</span> as.matrix</span>
<span id="cb67-1808"><a href="#cb67-1808" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Zs) <span class="ot">&lt;-</span> <span class="fu">c</span>(nms_c, nms_l, nms_m)</span>
<span id="cb67-1809"><a href="#cb67-1809" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">model.part</span>(eq_sys, mf, <span class="at">rhs =</span> <span class="dv">0</span>, <span class="at">lhs =</span> <span class="dv">1</span>)</span>
<span id="cb67-1810"><a href="#cb67-1810" aria-hidden="true" tabindex="-1"></a>ys <span class="ot">&lt;-</span> Y <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">cols_vary =</span> <span class="st">"slowest"</span>, <span class="at">names_to =</span> <span class="st">"equation"</span>,</span>
<span id="cb67-1811"><a href="#cb67-1811" aria-hidden="true" tabindex="-1"></a>                         <span class="at">values_to =</span> <span class="st">"response"</span>)</span>
<span id="cb67-1812"><a href="#cb67-1812" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">14</span>)</span>
<span id="cb67-1813"><a href="#cb67-1813" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">1</span>, <span class="fu">c</span>(<span class="dv">4</span>,  <span class="dv">9</span>)] <span class="ot">&lt;-</span> R[<span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">12</span>)] <span class="ot">&lt;-</span> R[<span class="dv">3</span>, <span class="fu">c</span>(<span class="dv">6</span>, <span class="dv">10</span>)] <span class="ot">&lt;-</span> R[<span class="dv">4</span>, <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">11</span>)] <span class="ot">&lt;-</span></span>
<span id="cb67-1814"><a href="#cb67-1814" aria-hidden="true" tabindex="-1"></a>    R[<span class="dv">5</span>, <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">13</span>)] <span class="ot">&lt;-</span> R[<span class="dv">6</span>, <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">14</span>)] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb67-1815"><a href="#cb67-1815" aria-hidden="true" tabindex="-1"></a>stack_data <span class="ot">&lt;-</span> ys <span class="sc">%&gt;%</span> <span class="fu">bind_cols</span>(Zs)</span>
<span id="cb67-1816"><a href="#cb67-1816" aria-hidden="true" tabindex="-1"></a>ols_unconst <span class="ot">&lt;-</span> <span class="fu">lm</span>(response <span class="sc">~</span> . <span class="sc">-</span> <span class="dv">1</span> <span class="sc">-</span> equation, stack_data)</span>
<span id="cb67-1817"><a href="#cb67-1817" aria-hidden="true" tabindex="-1"></a>ols_const <span class="ot">&lt;-</span> <span class="fu">clm</span>(ols_unconst, R)</span>
<span id="cb67-1818"><a href="#cb67-1818" aria-hidden="true" tabindex="-1"></a>N_ap <span class="ot">&lt;-</span> <span class="fu">nobs</span>(ols_const) <span class="sc">/</span> <span class="dv">3</span></span>
<span id="cb67-1819"><a href="#cb67-1819" aria-hidden="true" tabindex="-1"></a>EPS <span class="ot">&lt;-</span> ols_const <span class="sc">%&gt;%</span> resid <span class="sc">%&gt;%</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb67-1820"><a href="#cb67-1820" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(EPS) <span class="sc">/</span> N_ap</span>
<span id="cb67-1821"><a href="#cb67-1821" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1822"><a href="#cb67-1822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1823"><a href="#cb67-1823" aria-hidden="true" tabindex="-1"></a>\idxfun{chol}{base}\idxfun{solve}{base}</span>
<span id="cb67-1824"><a href="#cb67-1824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1825"><a href="#cb67-1825" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1826"><a href="#cb67-1826" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="fu">chol</span>(<span class="fu">solve</span>(Sigma))</span>
<span id="cb67-1827"><a href="#cb67-1827" aria-hidden="true" tabindex="-1"></a>V</span>
<span id="cb67-1828"><a href="#cb67-1828" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1829"><a href="#cb67-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1830"><a href="#cb67-1830" aria-hidden="true" tabindex="-1"></a>We then transform the response and the covariates using @eq-transformation_sur:</span>
<span id="cb67-1831"><a href="#cb67-1831" aria-hidden="true" tabindex="-1"></a>\idxfun{rbind}{base}\idxfun{as.matrix}{base}\idxfun{as.numeric}{base}</span>
<span id="cb67-1832"><a href="#cb67-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1835"><a href="#cb67-1835" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb67-1836"><a href="#cb67-1836" aria-hidden="true" tabindex="-1"></a>Zs <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">cbind</span>(V[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> Z_c, V[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">*</span> Z_l, V[<span class="dv">1</span>, <span class="dv">3</span>] <span class="sc">*</span> Z_m),</span>
<span id="cb67-1837"><a href="#cb67-1837" aria-hidden="true" tabindex="-1"></a>            <span class="fu">cbind</span>(V[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span> Z_c, V[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">*</span> Z_l, V[<span class="dv">2</span>, <span class="dv">3</span>] <span class="sc">*</span> Z_m),</span>
<span id="cb67-1838"><a href="#cb67-1838" aria-hidden="true" tabindex="-1"></a>            <span class="fu">cbind</span>(V[<span class="dv">3</span>, <span class="dv">1</span>] <span class="sc">*</span> Z_c, V[<span class="dv">3</span>, <span class="dv">2</span>] <span class="sc">*</span> Z_l, V[<span class="dv">3</span>, <span class="dv">3</span>] <span class="sc">*</span> Z_m))</span>
<span id="cb67-1839"><a href="#cb67-1839" aria-hidden="true" tabindex="-1"></a>ys <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(Y) <span class="sc">%*%</span> <span class="fu">t</span>(V) <span class="sc">%&gt;%</span> as.numeric</span>
<span id="cb67-1840"><a href="#cb67-1840" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1841"><a href="#cb67-1841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1842"><a href="#cb67-1842" aria-hidden="true" tabindex="-1"></a>Then the SUR estimator is computed, using <span class="in">`lm`</span> on the transformed</span>
<span id="cb67-1843"><a href="#cb67-1843" aria-hidden="true" tabindex="-1"></a>data and then using <span class="in">`clm`</span> in order to impose the linear restrictions.</span>
<span id="cb67-1844"><a href="#cb67-1844" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{clm}{micsr}\idxfun{coef}{stats}</span>
<span id="cb67-1845"><a href="#cb67-1845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1846"><a href="#cb67-1846" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1847"><a href="#cb67-1847" aria-hidden="true" tabindex="-1"></a>sur <span class="ot">&lt;-</span> <span class="fu">lm</span>(ys <span class="sc">~</span> Zs <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> <span class="fu">clm</span>(<span class="at">R =</span> R)</span>
<span id="cb67-1848"><a href="#cb67-1848" aria-hidden="true" tabindex="-1"></a>sur <span class="sc">%&gt;%</span> coef</span>
<span id="cb67-1849"><a href="#cb67-1849" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1850"><a href="#cb67-1850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1851"><a href="#cb67-1851" aria-hidden="true" tabindex="-1"></a>More simply, the <span class="in">`systemfit::systemfit`</span> function can be used, ^<span class="co">[</span><span class="ot">The **systemfit** package was presented in @sec-constrained_ls.</span><span class="co">]</span> with the <span class="in">`method`</span> argument set to <span class="in">`"SUR"`</span>:^<span class="co">[</span><span class="ot">Note the use of the `methodResidCov` argument: setting it to `"noDfCor"`, the cross-product of the vectors of residuals is divided by the number of observations to get the estimation of the covariance matrix. Other values of this argument enables to perform different kinds of degrees of freedom correction.</span><span class="co">]</span></span>
<span id="cb67-1852"><a href="#cb67-1852" aria-hidden="true" tabindex="-1"></a>\idxfun{systemfit}{systemfit}</span>
<span id="cb67-1853"><a href="#cb67-1853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1854"><a href="#cb67-1854" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1855"><a href="#cb67-1855" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(systemfit)</span>
<span id="cb67-1856"><a href="#cb67-1856" aria-hidden="true" tabindex="-1"></a>sur <span class="ot">&lt;-</span> <span class="fu">systemfit</span>(<span class="fu">list</span>(<span class="at">cost =</span> eq_ct, <span class="at">labor =</span> eq_sl, <span class="at">materials =</span> eq_sm),</span>
<span id="cb67-1857"><a href="#cb67-1857" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> ap, <span class="at">restrict.matrix =</span> R, <span class="at">method =</span> <span class="st">"SUR"</span>, </span>
<span id="cb67-1858"><a href="#cb67-1858" aria-hidden="true" tabindex="-1"></a>                 <span class="at">methodResidCov =</span> <span class="st">"noDfCor"</span>)</span>
<span id="cb67-1859"><a href="#cb67-1859" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1860"><a href="#cb67-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1861"><a href="#cb67-1861" aria-hidden="true" tabindex="-1"></a>The coefficients of the fitted model can be used to compute the Allen elasticities of</span>
<span id="cb67-1862"><a href="#cb67-1862" aria-hidden="true" tabindex="-1"></a>substitution and the price elasticities. The former are defined as:</span>
<span id="cb67-1863"><a href="#cb67-1863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1864"><a href="#cb67-1864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1865"><a href="#cb67-1865" aria-hidden="true" tabindex="-1"></a>\sigma_{ij} = \frac{\beta_{ij}}{s_i s_j} - 1 \; \; \forall i \neq j</span>
<span id="cb67-1866"><a href="#cb67-1866" aria-hidden="true" tabindex="-1"></a>\mbox{ and } \sigma_{ii} = \frac{\beta_{ij} - s_i(1 - s_i)}{s_i ^ 2}</span>
<span id="cb67-1867"><a href="#cb67-1867" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb67-1868"><a href="#cb67-1868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1869"><a href="#cb67-1869" aria-hidden="true" tabindex="-1"></a>Denote as $B$ the matrix containing the coefficients $\beta_{ij}$. Remember that, by imposing the homogeneity of degree one of the cost function, we imposed that $\beta_{iI} = - \sum_{i=1}^{I-1}\beta_{ij}$. Therefore $\beta_{iI}$ was not estimated and we must add it to the $B$ matrix using this formula:</span>
<span id="cb67-1870"><a href="#cb67-1870" aria-hidden="true" tabindex="-1"></a>\idxfun{matrix}{base}\idxfun{coef}{stats}\idxfun{apply}{base}\idxfun{cbind}{base}\idxfun{rbind}{base}\idxfun{diag}{base}\idxfun{outer}{base}\idxfun{list}{base}\idxfun{dimnames}{base}</span>
<span id="cb67-1871"><a href="#cb67-1871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1872"><a href="#cb67-1872" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1873"><a href="#cb67-1873" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">coef</span>(sur)[<span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)], <span class="at">ncol =</span> <span class="dv">2</span>)[<span class="sc">-</span><span class="dv">1</span>, ]</span>
<span id="cb67-1874"><a href="#cb67-1874" aria-hidden="true" tabindex="-1"></a>add <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fu">apply</span>(B, <span class="dv">1</span>, sum)</span>
<span id="cb67-1875"><a href="#cb67-1875" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rbind</span>(B, add), <span class="fu">c</span>(add, <span class="sc">-</span> <span class="fu">sum</span>(add)))</span>
<span id="cb67-1876"><a href="#cb67-1876" aria-hidden="true" tabindex="-1"></a>shares <span class="ot">&lt;-</span> ap <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">sl =</span> <span class="fu">mean</span>(sl),</span>
<span id="cb67-1877"><a href="#cb67-1877" aria-hidden="true" tabindex="-1"></a>                           <span class="at">sm =</span> <span class="fu">mean</span>(sm), <span class="at">sk =</span> <span class="dv">1</span> <span class="sc">-</span> sl <span class="sc">-</span> sm) <span class="sc">%&gt;%</span></span>
<span id="cb67-1878"><a href="#cb67-1878" aria-hidden="true" tabindex="-1"></a>    as.numeric</span>
<span id="cb67-1879"><a href="#cb67-1879" aria-hidden="true" tabindex="-1"></a>elast <span class="ot">&lt;-</span> B <span class="sc">/</span><span class="fu">outer</span>(shares, shares) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb67-1880"><a href="#cb67-1880" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(elast) <span class="ot">&lt;-</span> <span class="fu">diag</span>(elast) <span class="sc">-</span> <span class="dv">1</span> <span class="sc">/</span> shares</span>
<span id="cb67-1881"><a href="#cb67-1881" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(elast) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="st">"l"</span>, <span class="st">"m"</span>, <span class="st">"k"</span>), <span class="fu">c</span>(<span class="st">"l"</span>, <span class="st">"m"</span>, <span class="st">"k"</span>))</span>
<span id="cb67-1882"><a href="#cb67-1882" aria-hidden="true" tabindex="-1"></a>elast</span>
<span id="cb67-1883"><a href="#cb67-1883" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1884"><a href="#cb67-1884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1885"><a href="#cb67-1885" aria-hidden="true" tabindex="-1"></a>The three factors are substitutes, all the Allen elasticities of</span>
<span id="cb67-1886"><a href="#cb67-1886" aria-hidden="true" tabindex="-1"></a>substitution being positive. </span>
<span id="cb67-1887"><a href="#cb67-1887" aria-hidden="true" tabindex="-1"></a>The price elasticities are given by: $\epsilon_{ij} = s_j \sigma_{ij}$.</span>
<span id="cb67-1888"><a href="#cb67-1888" aria-hidden="true" tabindex="-1"></a>\idxfun{rbind}{base}</span>
<span id="cb67-1889"><a href="#cb67-1889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1890"><a href="#cb67-1890" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb67-1891"><a href="#cb67-1891" aria-hidden="true" tabindex="-1"></a>elast <span class="sc">*</span> <span class="fu">rbind</span>(shares, shares, shares)</span>
<span id="cb67-1892"><a href="#cb67-1892" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb67-1893"><a href="#cb67-1893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1894"><a href="#cb67-1894" aria-hidden="true" tabindex="-1"></a>Note that this matrix is not symmetric: for example, $0.1675$ is the</span>
<span id="cb67-1895"><a href="#cb67-1895" aria-hidden="true" tabindex="-1"></a>elasticity of the demand of materials with the price of labor whereas $0.1039$</span>
<span id="cb67-1896"><a href="#cb67-1896" aria-hidden="true" tabindex="-1"></a>is the elasticity of the demand of labor with the price of materials.</span>
<span id="cb67-1897"><a href="#cb67-1897" aria-hidden="true" tabindex="-1"></a>The price elasticities indicate that the demand for the three inputs is</span>
<span id="cb67-1898"><a href="#cb67-1898" aria-hidden="true" tabindex="-1"></a>inelastic, and it is particularly the case for labor and materials.</span>
<span id="cb67-1899"><a href="#cb67-1899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-1900"><a href="#cb67-1900" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation!seemingly unrelated regression|)}</span>
<span id="cb67-1901"><a href="#cb67-1901" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{seemingly unrelated regression|)}</span>
<span id="cb67-1902"><a href="#cb67-1902" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares!seemingly unrelated regression|)}</span>
<span id="cb67-1903"><a href="#cb67-1903" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{generalized least squares|)}</span>
<span id="cb67-1904"><a href="#cb67-1904" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{apples}{micsr}</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>