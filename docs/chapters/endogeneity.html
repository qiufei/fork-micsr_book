<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Microeconometrics with R - 7&nbsp; Endogeneity</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/treateff.html" rel="next">
<link href="../chapters/non_spherical.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Microeconometrics with R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../OLS.html" class="sidebar-item-text sidebar-link">Ordinary least squares estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/simple_regression_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical properties of the simple linear estimator</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/multiple_regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple regression model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coefficients.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpretation of the Coefficients</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../beyond_OLS.html" class="sidebar-item-text sidebar-link">Beyond the OLS estimator</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/maximum_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum likelihood estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/non_spherical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/endogeneity.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/treateff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/spatial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Spatial econometrics</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../special_responses.html" class="sidebar-item-text sidebar-link">Special responses</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/binomial.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Binomial models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/tobit.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Censored and truncated models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/count.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Count data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/duration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Duration models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discrete choice models</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-source_endog" id="toc-sec-source_endog" class="nav-link active" data-scroll-target="#sec-source_endog"><span class="toc-section-number">7.1</span>  Sources of endogeneity</a>
  <ul class="collapse">
<li><a href="#errors-in-variables" id="toc-errors-in-variables" class="nav-link" data-scroll-target="#errors-in-variables">Errors in variables</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">Omitted variable bias</a></li>
  <li><a href="#simultaneity-bias" id="toc-simultaneity-bias" class="nav-link" data-scroll-target="#simultaneity-bias">Simultaneity bias</a></li>
  </ul>
</li>
  <li>
<a href="#sec-simple_iv" id="toc-sec-simple_iv" class="nav-link" data-scroll-target="#sec-simple_iv"><span class="toc-section-number">7.2</span>  Simple instrumental variable estimator</a>
  <ul class="collapse">
<li><a href="#computation-of-the-simple-instrumental-variable-estimator" id="toc-computation-of-the-simple-instrumental-variable-estimator" class="nav-link" data-scroll-target="#computation-of-the-simple-instrumental-variable-estimator">Computation of the simple instrumental variable estimator</a></li>
  <li><a href="#sec-ssprop_iv" id="toc-sec-ssprop_iv" class="nav-link" data-scroll-target="#sec-ssprop_iv">Small sample properties of the <strong>IV</strong> estimator</a></li>
  <li><a href="#an-example-segregation-effects-on-urban-poverty-and-inequality" id="toc-an-example-segregation-effects-on-urban-poverty-and-inequality" class="nav-link" data-scroll-target="#an-example-segregation-effects-on-urban-poverty-and-inequality">An example: Segregation effects on urban poverty and inequality</a></li>
  <li><a href="#sec-wald_estimator" id="toc-sec-wald_estimator" class="nav-link" data-scroll-target="#sec-wald_estimator">Wald estimator</a></li>
  </ul>
</li>
  <li>
<a href="#sec-general_iv" id="toc-sec-general_iv" class="nav-link" data-scroll-target="#sec-general_iv"><span class="toc-section-number">7.3</span>  General IV estimator</a>
  <ul class="collapse">
<li><a href="#computation-of-the-estimator" id="toc-computation-of-the-estimator" class="nav-link" data-scroll-target="#computation-of-the-estimator">Computation of the estimator</a></li>
  <li><a href="#an-example-long-term-effects-of-slave-trade" id="toc-an-example-long-term-effects-of-slave-trade" class="nav-link" data-scroll-target="#an-example-long-term-effects-of-slave-trade">An example: long-term effects of slave trade</a></li>
  </ul>
</li>
  <li>
<a href="#sec-three_sls" id="toc-sec-three_sls" class="nav-link" data-scroll-target="#sec-three_sls"><span class="toc-section-number">7.4</span>  Three-stage least squares</a>
  <ul class="collapse">
<li><a href="#computation-of-the-three-stage-least-square-estimator" id="toc-computation-of-the-three-stage-least-square-estimator" class="nav-link" data-scroll-target="#computation-of-the-three-stage-least-square-estimator">Computation of the three-stage least square estimator</a></li>
  <li><a href="#an-example-the-watermelon-market" id="toc-an-example-the-watermelon-market" class="nav-link" data-scroll-target="#an-example-the-watermelon-market">An example: the watermelon market</a></li>
  </ul>
</li>
  <li>
<a href="#sec-fixed_effects" id="toc-sec-fixed_effects" class="nav-link" data-scroll-target="#sec-fixed_effects"><span class="toc-section-number">7.5</span>  Fixed effects model</a>
  <ul class="collapse">
<li><a href="#computation-of-the-fixed-effects-estimator" id="toc-computation-of-the-fixed-effects-estimator" class="nav-link" data-scroll-target="#computation-of-the-fixed-effects-estimator">Computation of the fixed effects estimator</a></li>
  <li><a href="#application-mincer-earning-function-using-a-sample-of-twins" id="toc-application-mincer-earning-function-using-a-sample-of-twins" class="nav-link" data-scroll-target="#application-mincer-earning-function-using-a-sample-of-twins">Application: Mincer earning function using a sample of twins</a></li>
  <li><a href="#application-testing-tobins-q-theory-of-investment-using-panel-data" id="toc-application-testing-tobins-q-theory-of-investment-using-panel-data" class="nav-link" data-scroll-target="#application-testing-tobins-q-theory-of-investment-using-panel-data">Application: Testing Tobin’s Q theory of investment using panel data</a></li>
  </ul>
</li>
  <li>
<a href="#sec-tests_endog" id="toc-sec-tests_endog" class="nav-link" data-scroll-target="#sec-tests_endog"><span class="toc-section-number">7.6</span>  Specification tests</a>
  <ul class="collapse">
<li><a href="#hausman-test" id="toc-hausman-test" class="nav-link" data-scroll-target="#hausman-test">Hausman test</a></li>
  <li><a href="#weak-instruments" id="toc-weak-instruments" class="nav-link" data-scroll-target="#weak-instruments">Weak instruments</a></li>
  <li><a href="#sargan-test" id="toc-sargan-test" class="nav-link" data-scroll-target="#sargan-test">Sargan test</a></li>
  <li><a href="#individual-effects" id="toc-individual-effects" class="nav-link" data-scroll-target="#individual-effects">Individual effects</a></li>
  <li><a href="#panel-application-testing-tobins-q-theory-of-investment" id="toc-panel-application-testing-tobins-q-theory-of-investment" class="nav-link" data-scroll-target="#panel-application-testing-tobins-q-theory-of-investment">Panel application: Testing Tobin’s Q theory of investment</a></li>
  <li><a href="#sec-test_sltr" id="toc-sec-test_sltr" class="nav-link" data-scroll-target="#sec-test_sltr">Instrumental variable application: slave trade</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-endogeneity" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Endogeneity</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>The unbiasedness and the consistency of the OLS estimator rest on the hypothesis that the conditional expectation of the error is constant (and can safely be set to zero if the model contains an intercept). Namely, starting with the simple linear model: <span class="math inline">\(y_n = \alpha + \beta x_n + \epsilon_n\)</span>, <span class="math inline">\(\mbox{E}(\epsilon \mid x) = 0\)</span>, or equivalently <span class="math inline">\(\mbox{E}(y \mid x) = \alpha + \beta x_n\)</span>. The same property can also be described using the covariance between the covariate and the error that can be written, using the rule of repeated expectation:</p>
<p><span class="math display">\[
\mbox{cov}(x, \epsilon) = \mbox{E}\left((x - \mu_x)\epsilon\right) =
\mbox{E}_x\left[\mbox{E}_\epsilon\left((x - \mu_x)\epsilon\mid
x\right)\right]
= \mbox{E}_x\left[(x - \mu_x)\mbox{E}_\epsilon\left(\epsilon\mid
x\right)\right]
\]</span></p>
<p>If the conditional expectation of <span class="math inline">\(\epsilon\)</span> is a constant <span class="math inline">\(\mbox{E}_\epsilon(\epsilon\mid x) = \mu_\epsilon\)</span> (not necessarily 0), the covariance is <span class="math inline">\(\mbox{cov}(x, \epsilon) = \mu_\epsilon \mbox{E}_x(x - \mu_x) = 0\)</span>. Stated differently, <span class="math inline">\(x\)</span> is supposed to be exogenous, or <span class="math inline">\(x\)</span> is assumed to be uncorrelated with <span class="math inline">\(\epsilon\)</span>. This is a reasonable assumption in an experimental setting, when the values of <span class="math inline">\(x\)</span> in the sample are set by the researcher. Unfortunately, most of the data used in microeconometrics are not experimental, so the problem of endogeneity is severe.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><a href="#sec-source_endog"><span>Section&nbsp;7.1</span></a> presents the circumstances where some covariates are endogenous. <a href="#sec-simple_iv"><span>Section&nbsp;7.2</span></a> presents the simple instrumental variable estimator (one endogenous covariate and one instrument), <a href="#sec-general_iv"><span>Section&nbsp;7.3</span></a> extends this model to the case where there may be more than one endogenous covariate and several instruments and <a href="#sec-three_sls"><span>Section&nbsp;7.4</span></a> to the estimation of systems of equations. <a href="#sec-fixed_effects"><span>Section&nbsp;7.5</span></a> presents the fixed effects model. Finally, several testing procedures are presented in <a href="#sec-tests_endog"><span>Section&nbsp;7.6</span></a>.</p>
<section id="sec-source_endog" class="level2" data-number="7.1"><h2 data-number="7.1" class="anchored" data-anchor-id="sec-source_endog">
<span class="header-section-number">7.1</span> Sources of endogeneity</h2>
<p>In economics, the problem of endogenous covariates happens mainly in three circumstances: errors in variables, omitted variables and simultaneity.</p>
<section id="errors-in-variables" class="level3"><h3 class="anchored" data-anchor-id="errors-in-variables">Errors in variables</h3>
<p></p>
<p>Data used in economics, especially micro-data, are prone to errors of measurement. This problem can affect either the response or some of the covariates. Suppose that the model that we seek to estimate is: <span class="math inline">\(y^*_n = \alpha + \beta x^*_n + \epsilon^*_n\)</span>, where the covariate is exogenous, which implies that <span class="math inline">\(\mbox{cov}(x^*, \epsilon^*)=0\)</span>. Suppose that the response is observed with error, namely that the observed value of the response is <span class="math inline">\(y_n = y ^*_n + \nu_n\)</span>, where <span class="math inline">\(\nu_n\)</span> is the measurement error of the response. In terms of the observed response, the model is now:</p>
<p><span class="math display">\[
y_n = \alpha + \beta x_n ^ * + (\epsilon^*_n + \nu_n)
\]</span></p>
<p>The error of the estimable model is then <span class="math inline">\(\epsilon_n = \epsilon^*_n + \nu_n\)</span> which is still uncorrelated with <span class="math inline">\(x\)</span> if <span class="math inline">\(\nu\)</span> is uncorrelated with <span class="math inline">\(x\)</span>, which means that the error of measurement of the response is uncorrelated with the covariate. In this case, the measurement error only increases the size of the error, which implies that the coefficients are estimated less precisely and that the R<sup>2</sup> is lower compared to a model with a correctly measured response.</p>
<p>Now consider that the covariate is measured with error and that the observable values of the covariate is <span class="math inline">\(x_n = x_n ^ * + \nu_n\)</span>. If the measurement error is uncorrelated with the value of the covariate, the variance of the observed covariate is therefore <span class="math inline">\(\sigma_x ^ 2 = \sigma_x ^ {*2} + \sigma_\nu ^ 2\)</span>. Moreover, the covariance between the observed covariate and the measurement error is equal to the variance of the measurement error: <span class="math inline">\(\sigma_{x\nu} = \mbox{E} \left((x ^ * + \nu - \mu_x) \nu\right) = \sigma_\nu ^ 2\)</span>, because the measurement error is uncorrelated with the covariate. Rewriting the model in terms of <span class="math inline">\(x\)</span>, we get: <span class="math inline">\(y_n = \alpha + \beta x_n + \epsilon_n\)</span>, with <span class="math inline">\(\epsilon_n = \epsilon_n ^ *- \beta \nu_n\)</span>. The error of this model is now correlated with <span class="math inline">\(x\)</span>, as <span class="math inline">\(\mbox{cov}(x, \epsilon_n) = \mbox{cov}(x^* + \nu,\epsilon ^ * - \beta \nu) = - \beta \sigma_\nu ^ 2\)</span>. The OLS estimator can be written as usual as:</p>
<p><span class="math display">\[
\hat{\beta} =
\frac{\sum_n (x_n - \bar{x})(y_n - \bar{y})}{\sum_n (x_n - \bar{x}) ^
2} =
\beta + \frac{\sum_n (x_n - \bar{x})\epsilon_n}{\sum_n (x_n -
\bar{x}) ^ 2}
\]</span></p>
<p>Taking the expectations, we have <span class="math inline">\(\mbox{E}\left[(x - \bar{x}) \epsilon\right] = -\beta \sigma_\nu ^ 2\)</span> and the expected value of the estimator is then:</p>
<p><span class="math display">\[
\mbox{E}(\hat{\beta}) = \beta\left(1 - \frac{\sigma_\nu ^ 2}{\sum (x_n
-\bar{x}) ^ 2 / N}\right)
= \beta\left(1 - \frac{\sigma_\nu ^ 2}{\hat{\sigma}_x ^ 2}\right)
\]</span></p>
<p>Therefore, the OLS estimator is biased and the term in brackets is the minus the share of the variance of <span class="math inline">\(x\)</span> that is due to measurement errors.Therefore <span class="math inline">\(\mid\hat{\beta}\mid &lt; \beta\)</span>. This kind of bias is called an <strong>attenuation bias</strong> (the absolute value of the estimator is lower than the true value), which can be either a lower or an upper bias depending on the sign of <span class="math inline">\(\beta\)</span>. This bias clearly doesn’t attenuate in large samples. As <span class="math inline">\(N\)</span> grows, the empirical variances/covariances converge to the population ones, and the estimator therefore converges to: <span class="math inline">\(\mbox{plim} \;\hat{\beta} = \beta \left(1 - \sigma_\nu ^ 2 / \sigma_x ^ 2\right)\)</span>. For example, if the measurement error accounts for 20% of the total variance of <span class="math inline">\(x\)</span>, <span class="math inline">\(\hat{\beta}\)</span> converges to 80% of the true parameter. </p>
</section><section id="omitted-variable-bias" class="level3"><h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted variable bias</h3>
<p> </p>
<p>Suppose that the true model is: <span class="math inline">\(y_n = \alpha + \beta_x x_n + \beta_z z_n + \epsilon_n\)</span>, where the conditional expectation of <span class="math inline">\(\epsilon\)</span> with respect to <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> is 0. Therefore, this model can be consistently estimated by least squares. Consider now that <span class="math inline">\(z\)</span> is unobserved. Therefore, the model to be estimated is <span class="math inline">\(y_n = \alpha + \beta_x x_n + \eta_n\)</span>, with <span class="math inline">\(\eta_n = \beta_z z_n + \epsilon_n\)</span>. The omission of a relevant (<span class="math inline">\(\beta_z \neq 0\)</span>) covariate has two consequences:</p>
<ul>
<li>the variance of the error is now <span class="math inline">\(\sigma_\eta ^ 2 = \beta_z ^ 2 \sigma_z^2 + \sigma_\epsilon^2\)</span>, and is therefore greater than the one of the initial model for which <span class="math inline">\(z\)</span> is observed and used as a covariate,</li>
<li>the covariance between the error and <span class="math inline">\(x\)</span> is <span class="math inline">\(\mbox{cov}(x, \eta) = \beta_z \mbox{cov}(x, z)\)</span>; therefore, if the covariate is correlated with the omitted variable, the covariate and the error of the model are correlated.</li>
</ul>
<p>As the variance of the OLS estimator is proportional to the variance of the errors, omission of a relevant covariate will always induce a less precise estimation of the slopes and a lower R<sup>2</sup>. Moreover, if the omitted covariate is correlated with the covariate used in the regression, the estimation will be biased and inconsistent. This <strong>omitted variable bias</strong> can be computed as follows:</p>
<p><span class="math display">\[
\hat{\beta}_x = \beta_x + \frac{\sum_n (x_n - \bar{x})(\beta_z z_n +
\epsilon_n)}{\sum_n (x_n - \bar{x}) ^ 2}=
\beta_x + \beta_z \frac{\sum_n (x_n - \bar{x}) (z_n - \bar{z})}{\sum_n
(x_n - \bar{x}) ^ 2} +
\frac{\sum_n (x_n - \bar{x}) \epsilon_n}{\sum_n (x_n - \bar{x}) ^ 2}
\]</span></p>
<p>Taking the conditional expectation, the last term disappears, so that:</p>
<p><span class="math display">\[
\mbox{E}(\hat{\beta}_x\mid x, z) = \beta_x + \beta_z
\frac{\hat{\sigma}_{xz}}{\hat{\sigma}_x ^ 2}
\]</span> There is an upper bias if the signs of the covariance between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> and <span class="math inline">\(\beta_z\)</span> are the same, and a lower bias if they have opposite signs. As <span class="math inline">\(N\)</span> tends to infinity, the OLS estimator converges to:</p>
<p><span class="math display">\[
\mbox{plim} \;\hat{\beta}_x = \beta_x + \beta_z \frac{\sigma_{xz}}{\sigma_x ^ 2}=
\beta_x + \beta_z \beta_{z / x}
\]</span></p>
<p>where <span class="math inline">\(\beta_{z/x}\)</span> is the true value of the slope of the regression of <span class="math inline">\(z\)</span> on <span class="math inline">\(x\)</span>. This formula makes clear what <span class="math inline">\(\hat{\beta}_x\)</span> really estimates in a linear regression:</p>
<ul>
<li>the direct effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> which is <span class="math inline">\(\beta_x\)</span>,</li>
<li>the indirect effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> which is the product of the effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(z\)</span> (<span class="math inline">\(\beta_{z/x}\)</span>) times the effect of <span class="math inline">\(z\)</span> on <span class="math inline">\(\eta\)</span> and therefore on <span class="math inline">\(y\)</span> (<span class="math inline">\(\beta_z\)</span>).</li>
</ul>
<p>A classic example of omitted variable bias occurs in the estimation of a Mincer earning function which relates wage (<span class="math inline">\(w\)</span>), education (<span class="math inline">\(e\)</span> in years) and experience (<span class="math inline">\(s\)</span> in weeks):</p>
<p><span class="math display">\[
\ln w = \beta_o + \beta_e e + \beta_s s + \beta_{ss} s ^ 2 + \epsilon
\]</span></p>
<p><span class="math inline">\(\beta_e = \frac{d\ln w}{de} = \frac{d w / w}{de}\)</span> is the percentage increase of the wage for one more year of education. To illustrate the estimation of a Mincer function, we use the data of <span class="citation" data-cites="KOOP:POIR:TOBI:05">Koop, Poirier, and Tobias (<a href="#ref-KOOP:POIR:TOBI:05" role="doc-biblioref">2005</a>)</span>, which is a sample of 303 white males taken from the National Longitudinal Survey of Youth and is available as <code>sibling_educ</code>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sibling_educ</span> <span class="op">&lt;-</span> <span class="va">sibling_educ</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>experience <span class="op">=</span> <span class="va">experience</span> <span class="op">/</span> <span class="fl">52</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">experience</span>, <span class="fl">2</span><span class="op">)</span>, <span class="va">sibling_educ</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                     Estimate Std. Error t value Pr(&gt;|t|)
educ                   0.1000     0.0118    8.48  1.0e-15
poly(experience, 2)1   2.3913     0.4554    5.25  2.9e-07
poly(experience, 2)2   0.4822     0.4542    1.06     0.29</code></pre>
</div>
</div>
<p>Results indicate that one more year of education increases on average the wage by 10%. One concern about this kind of estimation is that individuals have different abilities (<span class="math inline">\(a\)</span>), and that more abilities have a positive effect on wage, but may also have a positive effect on education. If this is the case, <span class="math inline">\(\beta_{a} &gt; 0\)</span>, <span class="math inline">\(\beta_{a/e} &gt; 0\)</span> and therefore <span class="math inline">\(\mbox{plim} \;\hat{\beta}_e = \beta_e + \beta_{a} \beta_{a/e} &gt; \beta_e\)</span> and the OLS estimator is upward biased. This is the case because more education:</p>
<ul>
<li>increases, for a given level of ability, the expected wage by <span class="math inline">\(\beta_e\)</span>,</li>
<li>means that, on average, the level of ability is higher, this effect being <span class="math inline">\(\beta_{a/e}\)</span> (which in this case doesn’t imply a causality of <span class="math inline">\(e\)</span> on <span class="math inline">\(a\)</span>, but simply a correlation), so the wage will also be higher (<span class="math inline">\(\beta_a &gt; 0\)</span>).</li>
</ul>
<p>Numerous studies of the Mincer function deal with this problem of endogeneity of the education level. But in the data set we used, there is a measure of the ability, which is the standardized AFQT test score. If we introduce ability in the regression, education is no more endogenous and least squares will give a consistent estimation of the effect of education on wage.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> We first check that education and ability are positively correlated: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sibling_educ</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">educ</span>, <span class="va">ability</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">pull</span></span>
<span><span class="co">## [1] 0.6056</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Therefore, adding ability as a covariate in the previous regression should decrease the coefficient on education: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">experience</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">ability</span>, <span class="va">sibling_educ</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gaze</span><span class="op">(</span>coef <span class="op">=</span> <span class="st">"educ"</span><span class="op">)</span></span>
<span><span class="co">##      Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## educ   0.0887     0.0150    5.92  8.7e-09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The effect of one more year of education is now an increase of 8.7% of the wage (compared to 10% previously). </p>
</section><section id="simultaneity-bias" class="level3"><h3 class="anchored" data-anchor-id="simultaneity-bias">Simultaneity bias</h3>
<p> </p>
<p>Often in economics, the phenomenon of interest is not described by a single equation, but by a system of equations. Consider for example a market equilibrium. The two equations relate the quantity demanded / supplied (<span class="math inline">\(q ^ d\)</span> and <span class="math inline">\(q ^ o\)</span>) to the unit price and to some specific covariates to the demand and to the supply side of the market. <span class="citation" data-cites="MADD:LAHI:09">Maddala and Lahiri (<a href="#ref-MADD:LAHI:09" role="doc-biblioref">2009</a>)</span>, pp. 376-377 studied the market for commercial loans using monthly US data for 1979-1984. The data set is available as <code>loan_market</code>. Total commercial loans (<code>loans</code>) are in billions of dollars; the price is the average prime rate charged by banks (<code>prime_rate</code>). <code>aaa_rate</code> is the AAA corporate bond rate, which is the price of an alternative financing to firms. Therefore, it enters only the demand equation, with an expected positive sign. <code>treas_rate</code> is the treasure bill rate. As it is a substitute to commercial loans from a bank, it should enter only the supply equation, with an expected negative sign. For the sake of simplicity, we denote <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span> as the quantity (in logarithm) and the price for this loan market model and, <span class="math inline">\(d\)</span> and <span class="math inline">\(s\)</span> as the two rates that enter only, respectively, the demand and the supply equation: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">loan</span> <span class="op">&lt;-</span> <span class="va">loan_market</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">transmute</span><span class="op">(</span>q <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">loans</span><span class="op">)</span>,  p <span class="op">=</span> <span class="va">prime_rate</span>,</span>
<span>              d <span class="op">=</span> <span class="va">aaa_rate</span>, s <span class="op">=</span> <span class="va">treas_rate</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The equilibrium on the loan market is then defined by a system of three equations:</p>
<p><span class="math display">\[
\left\{
  \begin{array}{rcl}
    q_d &amp;=&amp; \alpha_d + \beta_d p + \gamma_d d + \epsilon_d \\
    q_s &amp;=&amp; \alpha_s + \beta_s p + \gamma_s s + \epsilon_s \\
    q_d &amp;=&amp; q_s
  \end{array}
\right.
  \]</span></p>
<p>The last equation is non-stochastic and states that the market should be in equilibrium. The demand curve should be decreasing (<span class="math inline">\(\beta_d &lt; 0\)</span>) and the supply curve increasing (<span class="math inline">\(\beta_s &gt; 0\)</span>). The equilibrium is depicted in <a href="#fig-market_equilibrium">Figure&nbsp;<span>7.1</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-market_equilibrium" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="./tikz/fig/equilibre.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.1: Market equilibrium</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The OLS estimation of the demand and the supply equations is given below: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols_d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">q</span> <span class="op">~</span> <span class="va">p</span> <span class="op">+</span> <span class="va">d</span>, data <span class="op">=</span> <span class="va">loan</span><span class="op">)</span></span>
<span><span class="va">ols_s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">q</span> <span class="op">~</span> <span class="va">p</span> <span class="op">+</span> <span class="va">s</span>, data <span class="op">=</span> <span class="va">loan</span><span class="op">)</span></span>
<span><span class="va">ols_d</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##   Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## p -0.04337    0.00429   -10.1  3.1e-15</span></span>
<span><span class="co">## d  0.10346    0.00817    12.7  &lt; 2e-16</span></span>
<span><span class="va">ols_s</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##   Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## p  0.00136    0.01478    0.09     0.93</span></span>
<span><span class="co">## s -0.01838    0.01911   -0.96     0.34</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The two coefficients of the demand equation have the predicted sign and are highly significant: a 1 point of percentage increase of the prime rate decreases loans by 4.3%, and a one point of percentage increase of the corporate bond rate increases loans by 10.3%. The fit of the supply equation is very bad and, even if the two coefficients have the predicted sign, the values are very low and insignificant.</p>
<p>What is actually observed for each observation in the sample is a price-quantity combination at an equilibrium. A positive shock on the demand equation will move upward the demand curve and will lead to a new equilibrium with a higher equilibrium quantity <span class="math inline">\(q'\)</span> and also a higher equilibrium price <span class="math inline">\(p'\)</span> (except in the special case where the supply curve is vertical, which means that the price elasticity of supply is infinite). This means that <span class="math inline">\(p\)</span> is correlated with <span class="math inline">\(\epsilon_ d\)</span>, which leads to a bias in the estimation of <span class="math inline">\(\beta_d\)</span> by OLS. The same reasoning applies of course to the supply curve.</p>
<p>One solution would be to use the equilibrium condition and then to solve for <span class="math inline">\(p\)</span>, and then for <span class="math inline">\(q\)</span>. This <strong>reduced-form</strong> system of two equations:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcccccccccc}
p &amp;=&amp; \displaystyle \frac{\alpha_d - \alpha_s}{\beta_s - \beta_d} &amp;+&amp;
\displaystyle \frac{\gamma_d}{\beta_s - \beta_d} d &amp;-&amp;
\displaystyle \frac{\gamma_s}{\beta_s - \beta_d} s &amp;+&amp;
\displaystyle \frac{\epsilon_d - \epsilon_s}{\beta_s - \beta_d} \\
&amp;=&amp; \pi_o &amp;+&amp; \pi_d d &amp;+&amp; \pi_s s &amp;+&amp; \nu_p \\
q &amp;=&amp; \displaystyle \frac{\alpha_d \beta_s - \beta_d \alpha_s}{\beta_s - \beta_d} &amp;+&amp;
\displaystyle \frac{\beta_s \gamma_d}{\beta_s - \beta_d}d &amp;-&amp;
\displaystyle \frac{\beta_d \gamma_s}{\beta_s - \beta_d} s &amp;+&amp;
\displaystyle \frac{- \beta_d \epsilon_s + \beta_s
\epsilon_d}{\beta_s - \beta_d} \\
&amp;=&amp;  \delta_o &amp;+&amp; \delta_d d &amp;+&amp; \delta_s s &amp;+&amp; \nu_q \\
\end{array}
\right.
\]</span> makes clear that both <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> depend on <span class="math inline">\(\epsilon_s\)</span> and <span class="math inline">\(\epsilon_d\)</span>. More precisely, as <span class="math inline">\(\beta_s - \beta_d &gt; 0\)</span>, <span class="math inline">\(\epsilon_d\)</span> and <span class="math inline">\(\epsilon_s\)</span> are respectively positively and negatively correlated with the price. <span class="math inline">\(\Delta \epsilon_d &gt; 0\)</span> will shift the demand curve upward and therefore will increase the equilibrium price. Therefore, the OLS estimator of the slope of the demand curve is biased upward which means, as it is negative, that it is biased downward in absolute value. A positive shock on the supply side (<span class="math inline">\(\Delta \epsilon_s &gt; 0\)</span>) will move the supply curve upward and therefore will decrease the equilibrium price. The OLS estimator of the slope of the supply curve is then downward biased. The parameters of these two equations can be consistently estimated by least squares, as <span class="math inline">\(d\)</span> and <span class="math inline">\(s\)</span> are uncorrelated with the two error terms. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ols_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">q</span> <span class="op">~</span> <span class="va">d</span> <span class="op">+</span> <span class="va">s</span>, data <span class="op">=</span> <span class="va">loan</span><span class="op">)</span></span>
<span><span class="va">ols_p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">p</span> <span class="op">~</span> <span class="va">d</span> <span class="op">+</span> <span class="va">s</span>, data <span class="op">=</span> <span class="va">loan</span><span class="op">)</span></span>
<span><span class="va">ols_q</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##   Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## d  0.09437    0.00837    11.3  &lt; 2e-16</span></span>
<span><span class="co">## s -0.05061    0.00569    -8.9  4.6e-13</span></span>
<span><span class="va">ols_p</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##   Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## d   0.2876     0.1096    2.62    0.011</span></span>
<span><span class="co">## s   1.0667     0.0745   14.32   &lt;2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The reduced form coefficients are not meaningful by themselves, but only if they enable to retrieve the structural parameters. This is actually the case here as <span class="math inline">\(\frac{\delta_s}{\pi_s}=\beta_d\)</span> and <span class="math inline">\(\frac{\delta_d}{\pi_d}=\beta_s\)</span>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">price_coefs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unname.html">unname</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ols_q</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ols_p</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">price_coefs</span></span>
<span><span class="co">## [1]  0.32810 -0.04744</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The price coefficients are as expected higher in absolute values than those obtained using OLS on the demand and on the supply equation. It is particularly the case for <span class="math inline">\(\beta_s\)</span> (0.328 vs.&nbsp;0.001). On the contrary, the absolute value of <span class="math inline">\(\beta_d\)</span> increases very slightly (<span class="math inline">\(-0.047\)</span> vs.&nbsp;<span class="math inline">\(-0.043\)</span>). </p>
<p>Actually, the case we have considered is special because there is one and only one extra covariate in both equations. Consider as an example the following system of equations:</p>
<p><span class="math display">\[
\left\{
  \begin{array}{rcl}
    y_d &amp;=&amp; \alpha_d + \beta_d p + \gamma_1 d_1 + \gamma_2 d_2 + \epsilon_d \\
    y_s &amp;=&amp; \alpha_s + \beta_s p + \epsilon_s \\
    y_d &amp;=&amp; y_s
  \end{array}
\right.
\]</span></p>
<p>where there are two extra covariates in the demand equation and no covariates (except the price) in the supply equation. Solving for the two endogenous variables <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, we get in this case:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcccccccccc}
p &amp;=&amp; \displaystyle \frac{\alpha_d - \alpha_s}{\beta_s - \beta_d} &amp;+&amp;
\displaystyle\frac{\gamma_1}{\beta_s - \beta_d} d_1 &amp;+&amp;
\displaystyle \frac{\gamma_2}{\beta_s - \beta_d} d_2 &amp;+&amp;
\displaystyle \frac{\epsilon_d - \epsilon_s}{\beta_s - \beta_d} \\
&amp;=&amp; \pi_s &amp;+&amp; \pi_1 d_1 &amp;+&amp; \pi_2 d_2 &amp;+&amp; \nu_p \\
q &amp;=&amp; \displaystyle \frac{\beta_s \alpha_d - \beta_d \alpha_s}{\beta_s - \beta_d} &amp;+&amp;
\displaystyle \frac{\beta_s \gamma_1}{\beta_s - \beta d}d_1 &amp;+&amp;
\displaystyle \frac{\beta_s \gamma_2}{\beta_s - \beta_d} d_2 &amp;+&amp;
\displaystyle \frac{\beta_s \epsilon_d - \beta_d \epsilon_s}{\beta_s - \beta_d} \\
&amp;=&amp; \delta_s &amp;+&amp; \delta_1 d_1 &amp;+&amp; \delta_2 d_2 &amp;+&amp; \nu_q \\
\end{array}
\right.
\]</span></p>
<p>we now have <span class="math inline">\(\frac{\delta_1}{\pi_1} = \frac{\delta_2}{\pi_2} = \beta_s\)</span>; there are two ratios of the reduced parameters that give the value of the slope of the supply curve, but it is very implausible that these two values will be equal. On the contrary, there is no way to retrieve the slope of the demand curve from the reduced form parameters. Therefore, this <strong>indirect least squares</strong> approach is of limited interest. In the general case, as we have seen, some coefficients like <span class="math inline">\(\beta_s\)</span> in our example may be <strong>over-identified</strong> and some other like <span class="math inline">\(\beta_d\)</span> are <strong>under-identified</strong>.</p>
<p> </p>
<p></p>
</section></section><section id="sec-simple_iv" class="level2" data-number="7.2"><h2 data-number="7.2" class="anchored" data-anchor-id="sec-simple_iv">
<span class="header-section-number">7.2</span> Simple instrumental variable estimator</h2>
<p>The general idea of the <strong>instrumental variable</strong> (<strong>IV</strong>) estimator is to find variables which are correlated with the endogenous covariates and uncorrelated with the error of the structural equation. This means that the instruments don’t have a direct effect on the response, but only an indirect effect because of their correlation with the endogenous covariates. These instruments allow to get an exogenous source of variation of the covariate, i.e., a source of variation that has nothing to do with the process of interest. We’ll start with the simple case where the number of instruments equals the number of endogenous covariates, i.e., the <strong>just-identified</strong> case.</p>
<section id="computation-of-the-simple-instrumental-variable-estimator" class="level3"><h3 class="anchored" data-anchor-id="computation-of-the-simple-instrumental-variable-estimator">Computation of the simple instrumental variable estimator</h3>
<p> Consider a simple linear regression: <span class="math inline">\(y_n = \alpha + \beta x_n + \epsilon_n\)</span>, with <span class="math inline">\(\mbox{E}(\epsilon \mid x) \neq 0\)</span>. From <a href="simple_regression.html#eq-normal_equation">Equation&nbsp;<span>1.9</span></a>, the first-order conditions for the minimization of the sum of square residuals can be written as:</p>
<p><span id="eq-normal_eq2"><span class="math display">\[
\frac{1}{N} \sum_n \left[ (y_n - \bar{y}) - \hat{\beta} (x_n - \bar{x})\right]
(x_n - \bar{x}) = \frac{1}{N} \sum_n \hat{\epsilon}_n (x_n - \bar{x}) =
\hat{\sigma}_{\hat{\epsilon} x}= 0
\tag{7.1}\]</span></span></p>
<p><a href="#eq-normal_eq2">Equation&nbsp;<span>7.1</span></a> states that the OLS estimator is obtained by imposing the sample equivalent to the moment condition <span class="math inline">\(\mbox{E}(\epsilon \mid x) = 0\)</span>, i.e., that the covariance between the residuals and the covariate is exactly 0 in the sample. This of course leads to a biased and inconsistent estimator if <span class="math inline">\(\mbox{E}(\epsilon \mid x)\neq 0\)</span>. Now suppose that a variable <span class="math inline">\(w\)</span> exists, which is correlated with the covariate, but not with the error of the model (this latter hypothesis is called the <strong>exclusion restriction</strong>). Such a variable has no direct effect on the response, but only an indirect effect due to its correlation with the covariate. As by hypothesis, <span class="math inline">\(\mbox{E}(\epsilon \mid w) = 0\)</span>, a consistent estimator can be obtained by imposing the sample equivalent to this moment condition, i.e., by setting the estimator to a value such that the covariance between the residuals and the instrument is 0:</p>
<p><span id="eq-normal_inst_eq"><span class="math display">\[
\frac{1}{N} \sum_n \left[ (y_n - \bar{y}) - \hat{\beta} (x_n - \bar{x})\right]
(w_n - \bar{w}) = \frac{1}{N} \sum_n \hat{\epsilon}_n (w_n - \bar{w}) =
\hat{\sigma}_{\hat{\epsilon} w}= 0
\tag{7.2}\]</span></span></p>
<p>Solving <a href="#eq-normal_inst_eq">Equation&nbsp;<span>7.2</span></a> for <span class="math inline">\(\hat{\beta}\)</span>, we get the instrumental variable estimator:</p>
<p><span id="eq-iv_simple"><span class="math display">\[
\hat{\beta} = \frac{\sum_n (w_n - \bar{w})(y_n - \bar{y})}{\sum_n (w_n - \bar{w})(x_n - \bar{x})} = \frac{\hat{\sigma}_{wy}}{\hat{\sigma}_{wx}}
\tag{7.3}\]</span></span></p>
<p>which is the ratio of the empirical covariances of <span class="math inline">\(w\)</span> with <span class="math inline">\(y\)</span> and with <span class="math inline">\(x\)</span>. Dividing both sides of the ratio by the empirical variance of <span class="math inline">\(w\)</span> (<span class="math inline">\(\hat{\sigma}_w ^ 2\)</span>), <a href="#eq-iv_simple">Equation&nbsp;<span>7.3</span></a> can also be written as:</p>
<p><span id="eq-iv_ratio_me"><span class="math display">\[
\hat{\beta}  = \frac{\hat{\sigma}_{wy} / \hat{\sigma}_w ^
2}{\hat{\sigma}_{wx} / \hat{\sigma}_w ^2}
= \frac{\hat{\beta}_{y/w}}{\hat{\beta}_{x/w}} = \frac{dy /
dw}{dx / dw}
\tag{7.4}\]</span></span></p>
<p>where <span class="math inline">\(\hat{\beta}_{y/w}\)</span> and <span class="math inline">\(\hat{\beta}_{x/w}\)</span> are the slopes of the regressions of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span>. Therefore, the IV estimator is also the ratio of the marginal effects of <span class="math inline">\(w\)</span> on <span class="math inline">\(y\)</span> and on <span class="math inline">\(x\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Replacing <span class="math inline">\(y_n - \bar{y}\)</span> by <span class="math inline">\(\beta(x_n - \bar{x}) + \epsilon_n\)</span> in <a href="#eq-iv_simple">Equation&nbsp;<span>7.3</span></a>, we get:</p>
<p><span class="math display">\[
\hat{\beta} = \beta + \frac{\sum_n (w_n - \bar{w}) \epsilon_n}{\sum_n
(w_n - \bar{w}) (x_n - \bar{x})} = \beta +
\frac{\hat{\sigma}_{w\epsilon}}{\hat{\sigma}_{wx}}
\]</span></p>
<p>As <span class="math inline">\(N\)</span> grows, the two empirical covariances converge to the population covariances and, as by hypothesis, the instrument is uncorrelated with the error (<span class="math inline">\(\sigma_{w\epsilon} = 0\)</span>) and is correlated with the covariate (<span class="math inline">\(\sigma_{wx} \neq0\)</span>), the instrumental variable estimator is consistent (<span class="math inline">\(\mbox{plim} \;\hat{\beta} = \beta + \frac{\sigma_{w\epsilon}}{\sigma_{wx}} = \beta\)</span>). Assuming spherical disturbances, the variance of the IV estimator is:</p>
<p><span class="math display">\[
\mbox{V}(\hat{\beta}) = \mbox{E}\left[(\hat{\beta}-\beta)^2\right] =
\frac{\sigma_\epsilon ^ 2 \sum_n (w_n - \bar{w}) ^ 2}{\left[\sum_n
(w_n - \bar{w})(x_n - \bar{x})\right] ^ 2} = \frac{\sigma_\epsilon ^ 2
\hat{\sigma}_w ^ 2}{N \hat{\sigma}_{wx} ^ 2}
= \frac{\sigma_\epsilon ^ 2}{N \hat{\sigma}_x ^ 2 \hat{\rho}_{wx} ^ 2}
=\left(\frac{\sigma_\epsilon}{\sqrt{N} \hat{\sigma}_x \mid \hat{\rho}_{wx}\mid}\right) ^ 2
\]</span></p>
<p>The last expression, which introduces the coefficient of correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>, is particularly appealing as it shows that, compared to the standard error of the OLS estimator, the standard error of the IV estimator is inflated by <span class="math inline">\(1 / \mid \hat{\rho}_{wx}\mid\)</span>. This term is close to 1 if the correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span> is high and, in this case, the loss of precision implied by the use of the IV estimator is low. On the contrary, if the correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span> is low, the IV estimator will be very imprecisely estimated.</p>
<p> </p>
<p>The IV estimator can also be obtained by first regressing <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span> and then by regressing <span class="math inline">\(y\)</span> on the fitted values of the previous regression. Consider the linear model that relates the instrument to the covariate: <span class="math inline">\(x_n = \gamma + \delta w_n + \nu_n\)</span>. We estimate this model by OLS and then we express the fitted values (<span class="math inline">\(\hat{x}\)</span>) as a function of <span class="math inline">\(\hat{\delta}\)</span>: <span class="math inline">\(\hat{x}_n - \bar{x} = \hat{\delta}(w_n - \bar{w})\)</span>. We can therefore rewrite the numerator and the denominator of the IV estimator given by <a href="#eq-iv_simple">Equation&nbsp;<span>7.3</span></a> as:</p>
<ul>
<li>
<span class="math inline">\(\sum_n (w_n - \bar{w})(y_n - \bar{y}) = \sum_n (\hat{x}_n - \bar{x}) (y_n - \bar{y})/ \hat{\delta}\)</span>,</li>
<li>
<span class="math inline">\(\sum_n (w_n - \bar{w})(x_n - \bar{x}) = \sum_n (w_n - \bar{w})(\hat{x}_n + \hat{\epsilon}_n- \bar{x}) = \sum_n (\hat{x}_n -\bar{x}) ^ 2/ \hat{\delta}\)</span> (as <span class="math inline">\(\sum_n (w_n - \bar{w}) \hat{\epsilon}_n=0\)</span>),</li>
</ul>
<p>so that:</p>
<p><span class="math display">\[
\hat{\beta} = \frac{\sum_n(\hat{x}_n - \bar{x})(y_n -
\bar{y})}{\sum_n(\hat{x}_n - \bar{x}) ^ 2}
\]</span></p>
<p>which is the OLS estimator of <span class="math inline">\(y\)</span> on <span class="math inline">\(\hat{x}\)</span>, <span class="math inline">\(\hat{x}\)</span> being the fitted values of the regression of <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span>. Therefore, the IV estimator can be obtained by running two OLS regressions and is for this reason also called the <strong>two-stage least square</strong> (<strong>2SLS</strong>) estimator. As <span class="math inline">\(x\)</span> is correlated with <span class="math inline">\(\epsilon\)</span>, but <span class="math inline">\(w\)</span> is not, the idea of the 2SLS estimator is to replace <span class="math inline">\(x\)</span> by a linear transformation of <span class="math inline">\(w\)</span> which is as close as possible to <span class="math inline">\(x\)</span>, and this is simply the fitted values of the regression of <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span>.</p>
<p> </p>
<p>The extension of the case when there are <span class="math inline">\(K &gt; 1\)</span> covariates and <span class="math inline">\(K\)</span> instruments is straightforward. The two sets of variables <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span> can overlap because, if some of the covariates are exogenous, they should also be used as instruments. Denoting <span class="math inline">\(w\)</span> as the vector of instruments, the moment conditions are <span class="math inline">\(\mbox{E}(\epsilon \mid w) = 0\)</span> and the sample equivalent is:</p>
<p><span class="math display">\[
\frac{1}{N} W ^ \top (y - Z \gamma) = 0
\]</span></p>
<p>where <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> are <span class="math inline">\(N\times (K+1)\)</span> matrix containing respectively the covariates and the instruments (with a vector of 1). Solving for <span class="math inline">\(\gamma\)</span>, we get the instrumental variable estimator:</p>
<p><span class="math display">\[
\hat{\gamma} = (W^\top X) ^ {-1} W^\top y
\]</span></p>
</section><section id="sec-ssprop_iv" class="level3"><h3 class="anchored" data-anchor-id="sec-ssprop_iv">Small sample properties of the <strong>IV</strong> estimator</h3>
<p> Although it is consistent, the instrumental variable estimator isn’t unbiased. Actually, it doesn’t even have an expected value in the just-identified case. This result can be easily shown starting with the following system of equations:<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
y &amp;=&amp; \alpha + \beta x + \sigma_\epsilon \epsilon\\
x &amp;=&amp; \gamma + \delta w + \sigma_\nu \nu\\
\end{array}
\right.
\]</span></p>
<p>where, for convenience, the two error terms are written as standard normal deviates. Moreover, we can write <span class="math inline">\(\epsilon = \rho \nu + \iota\)</span>, so <span class="math inline">\(\rho\)</span> is the coefficient of correlation between the two error terms and <span class="math inline">\(\iota\)</span> is by construction uncorrelated with <span class="math inline">\(\nu\)</span>. The IV estimator is then:</p>
<p><span id="eq-simpleiv_bias1"><span class="math display">\[
\hat{\beta} = \frac{\sum_n (w_n - \bar{w})(y_n - \bar{y})}
{\sum_n (w_n - \bar{w})(x_n - \bar{x})} = \beta + \frac{\sigma_{\epsilon}\sum_n (w_n -
\bar{w})(\rho \nu_n + \iota_n)}{\sum_n (w_n - \bar{w})(x_n - \bar{x})}
\tag{7.5}\]</span></span></p>
<p>The denominator is closely linked to the OLS estimator of <span class="math inline">\(\gamma\)</span>, which is:</p>
<p><span id="eq-simpleiv_bias2"><span class="math display">\[
\hat{\delta} = \frac{\sum_n (w_n - \bar{w})(x_n - \bar{x})}{\sum_n
(w_n - \bar{w}) ^ 2} = \delta +
\frac{\sigma_\nu\sum_n (w_n - \bar{w})\nu_n}{\sum_n (w_n - \bar{w}) ^ 2}
\tag{7.6}\]</span></span></p>
<p>From <a href="#eq-simpleiv_bias1">Equation&nbsp;<span>7.5</span></a> and <a href="#eq-simpleiv_bias2">Equation&nbsp;<span>7.6</span></a>, we get:</p>
<p><span class="math display">\[
\hat{\beta} = \beta + \frac{\sigma_\epsilon \rho \sum_n (w_n - \bar{w})\nu_n +
\sigma_\epsilon \sum_n (w_n - \bar{w}) \iota_n}
{\delta \sum_n(w_n - \bar{w}) ^ 2 + \sigma_\nu\sum_n(w_n - \bar{w}) \nu_n}
\]</span></p>
<p>Denoting <span class="math inline">\(c_n = \frac{w_n - \bar{w}}{\sqrt{\sum_n (w_n - \bar{w}) ^ 2}}\)</span>, with <span class="math inline">\(\sum_n c_n ^ 2 = 1\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\beta} = \beta + \frac{\sigma_\epsilon \rho \sum_n c_n\nu_n +
\sigma_\epsilon \sum_n c_n\iota_n}
{\gamma \sqrt{\sum_n(w_n - \bar{w}) ^ 2} + \sigma_\nu\sum_n c_n \nu_n}
\]</span></p>
<p>As, by construction, <span class="math inline">\(\mbox{E}(\iota \mid \nu) = 0\)</span>, denoting <span class="math inline">\(\omega = \sum_n c_n \nu_n\)</span>, which is a standard normal deviate and <span class="math inline">\(a = \delta \sqrt{\sum_n (w_n - \bar{w}) ^ 2} / \sigma_\nu\)</span> we finally get:</p>
<p><span class="math display">\[
\hat{\beta} = \beta + \frac{\sigma_\epsilon
\rho}{\sigma_\nu}\frac{\omega}{\omega + a}
\]</span> Then the expected value of <span class="math inline">\(\hat{\beta}\)</span> is obtained by integrating out this expression with respect to <span class="math inline">\(\omega\)</span>:</p>
<p><span class="math display">\[
\mbox{E}(\hat{\beta}) = \beta  + \frac{\sigma_\epsilon
\rho}{\sigma_\nu}\int_{-\infty}^{\infty} \frac{\omega}{\omega + a}\phi(\omega)d\omega
\]</span></p>
<p>but this integral is divergent as <span class="math inline">\(\omega / (a + \omega)\)</span> tends to infinity as <span class="math inline">\(\omega\)</span> approach <span class="math inline">\(-a\)</span>. Therefore, <span class="math inline">\(\hat{\beta}\)</span> has no expected value. The 2SLS derivation of the IV estimator also gives an intuition of the reason why the IV estimator is not unbiased. It would be if, for the second OLS estimation, <span class="math inline">\(\mbox{E}(x_n\mid w_n) = \gamma + \delta w_n\)</span> were used as the regressor. But actually, <span class="math inline">\(\hat{x}_n = \hat{\gamma} + \hat{\delta} w_n\)</span> is used and, as the OLS estimator over-fits, the fitted values of <span class="math inline">\(x\)</span> will be partly correlated with <span class="math inline">\(\epsilon\)</span>. Of course, when the sample size grows, as the OLS estimator is consistent, <span class="math inline">\(\hat{x}_n\)</span> converges to <span class="math inline">\(\gamma + \delta z_n\)</span> and the asymptotic bias vanishes. This can be usefully illustrated by simulation. The <code>iv_data</code> function draws a sample of <span class="math inline">\(y\)</span>, <span class="math inline">\(x\)</span> and one instrument <span class="math inline">\(w\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">iv_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">N</span> <span class="op">=</span> <span class="fl">5E01</span>, <span class="va">R</span> <span class="op">=</span> <span class="fl">1E03</span>, </span>
<span>                      <span class="va">r_xe</span> <span class="op">=</span> <span class="fl">0.5</span>, <span class="va">r_xw</span> <span class="op">=</span> <span class="fl">0.2</span>, <span class="va">r_we</span> <span class="op">=</span> <span class="fl">0</span>,</span>
<span>                      <span class="va">alpha</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">beta</span> <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                      <span class="va">sds</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">1</span>, e <span class="op">=</span> <span class="fl">1</span>, w <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>                      <span class="va">mns</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, w <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">nms</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x"</span>, <span class="st">"e"</span>, <span class="st">"w"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">sds</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">nms</span> ;  <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">mns</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x"</span>, <span class="st">"w"</span><span class="op">)</span></span>
<span>    <span class="va">b_wx</span> <span class="op">&lt;-</span> <span class="va">r_xw</span> <span class="op">*</span> <span class="va">sds</span><span class="op">[</span><span class="st">"x"</span><span class="op">]</span> <span class="op">/</span> <span class="va">sds</span><span class="op">[</span><span class="st">"w"</span><span class="op">]</span></span>
<span>    <span class="va">a_wx</span> <span class="op">&lt;-</span> <span class="va">mns</span><span class="op">[</span><span class="st">"x"</span><span class="op">]</span> <span class="op">-</span> <span class="va">b_wx</span> <span class="op">*</span> <span class="va">mns</span><span class="op">[</span><span class="st">"w"</span><span class="op">]</span></span>
<span>    <span class="va">cors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">r_xe</span>, <span class="va">r_xw</span>, <span class="va">r_xe</span>, <span class="fl">1</span>, <span class="va">r_we</span>, <span class="va">r_xw</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span>    <span class="va">XEW</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="va">R</span> <span class="op">*</span> <span class="fl">3</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">N</span> <span class="op">*</span> <span class="va">R</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span><span class="op">(</span><span class="va">cors</span><span class="op">)</span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">XEW</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">nms</span></span>
<span>    <span class="va">XEW</span> <span class="op">%&gt;%</span></span>
<span>        <span class="va">as_tibble</span> <span class="op">%&gt;%</span></span>
<span>        <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span> <span class="op">*</span> <span class="va">sds</span><span class="op">[</span><span class="st">"x"</span><span class="op">]</span> <span class="op">+</span> <span class="va">mns</span><span class="op">[</span><span class="st">"x"</span><span class="op">]</span>,</span>
<span>               w <span class="op">=</span> <span class="va">w</span> <span class="op">*</span> <span class="va">sds</span><span class="op">[</span><span class="st">"w"</span><span class="op">]</span> <span class="op">+</span> <span class="va">mns</span><span class="op">[</span><span class="st">"w"</span><span class="op">]</span>,</span>
<span>               e <span class="op">=</span> <span class="va">e</span> <span class="op">*</span> <span class="va">sds</span><span class="op">[</span><span class="st">"e"</span><span class="op">]</span>,</span>
<span>               Exw <span class="op">=</span> <span class="va">a_wx</span> <span class="op">+</span> <span class="va">b_wx</span> <span class="op">*</span> <span class="va">w</span>,</span>
<span>               y <span class="op">=</span> <span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">e</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>        <span class="fu">add_column</span><span class="op">(</span>id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">R</span>, each <span class="op">=</span> <span class="va">N</span><span class="op">)</span><span class="op">)</span>, .before <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The arguments of the function are the sample size (<code>N</code>), the number of samples (<code>R</code>), the correlations between <span class="math inline">\(x\)</span> and <span class="math inline">\(\epsilon\)</span> (the default is 0.5), between <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span> (0.2 by default) and between <span class="math inline">\(w\)</span> and <span class="math inline">\(\epsilon\)</span>. The default value for this last correlation is 0, which is a necessary condition for the IV estimator to be consistent. <span class="math inline">\(x\)</span>, <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(w\)</span> are assumed by default to be standard normal deviates, but the means of <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span> and the standard deviations of <span class="math inline">\(x\)</span>, <span class="math inline">\(w\)</span> and <span class="math inline">\(\epsilon\)</span> can be customized using the <code>mns</code> and <code>sds</code> argument. Finally, the coefficients of the linear relation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are <code>alpha</code> and <code>beta</code> and these two values are set by default to 1. First, a matrix of normal standard deviates <code>XEW</code> is constructed. This matrix is post-multiplied by the Cholesky decomposition of the matrix of correlation, which introduces the desired correlation between <span class="math inline">\(x\)</span>, <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(w\)</span>, which are then adjusted for non-zero means and non-unity standard deviations if necessary. Finally the vector of response is computed, along with the conditional expectation of <span class="math inline">\(x\)</span>: <span class="math inline">\(\mbox{E}(x\mid w) = \gamma + \delta w\)</span>. The <code>iv_coefs</code> function computes the IV estimator using the 2SLS approach. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">iv_coefs</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">xh</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">i</span><span class="op">$</span><span class="va">w</span><span class="op">)</span>, <span class="va">i</span><span class="op">$</span><span class="va">x</span><span class="op">)</span><span class="op">$</span><span class="va">fitted.values</span></span>
<span>    <span class="va">ols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">i</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>, <span class="va">i</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span>    <span class="va">ivo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">i</span><span class="op">$</span><span class="va">Exw</span><span class="op">)</span>, <span class="va">i</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span>    <span class="va">iv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">xh</span><span class="op">)</span>, <span class="va">i</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="va">unname</span></span>
<span>    <span class="fu">tibble</span><span class="op">(</span>ols <span class="op">=</span> <span class="va">ols</span>, ivo <span class="op">=</span> <span class="va">ivo</span>, iv <span class="op">=</span> <span class="va">iv</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It uses <code>lm.fit</code> to regress <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> We compute <code>xh</code> which is <span class="math inline">\(\hat{x} = \hat{\gamma} + \hat{\delta} w_n\)</span>, the fitted values of the regression of <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span>. <code>iv_coefs</code> computes three estimators:</p>
<ul>
<li>
<code>ols</code>, which is the OLS estimator of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>,</li>
<li>
<code>iv</code>, which is the 2SLS estimator of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> using <span class="math inline">\(w\)</span> as instruments,</li>
<li>
<code>ivo</code>, which is an estimator that can only be computed in the context of simulations and uses <span class="math inline">\(\mbox{E}(x_n\mid w_n)\)</span> instead of <span class="math inline">\(\hat{x}_n\)</span> as the regressor in the second OLS estimation.</li>
</ul>
<p>We first generate a unique sample of 100 observations and we compute the three estimators. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu">iv_data</span><span class="op">(</span>R <span class="op">=</span> <span class="fl">1</span>, N <span class="op">=</span> <span class="fl">1E02</span><span class="op">)</span></span>
<span><span class="fu">iv_coefs</span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span>
<span><span class="co">## # A tibble: 1 × 3</span></span>
<span><span class="co">##     ols   ivo    iv</span></span>
<span><span class="co">##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">## 1  1.50 0.623 0.752</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To empirically analyze the distribution of these three estimators, we then generate several samples by setting the <code>R</code> argument, compute the estimators for every sample and analyze the empirical distribution of the estimators. This can be easily done using <code>tidyr::nests</code> and <code>tidyr::unnests</code> functions, as in <a href="simple_regression.html#sec-random_numbers_simulations"><span>Section&nbsp;1.6.2</span></a>. </p>
<div class="cell" data-layout-align="center" data-hash="endogeneity_cache/html/simul_nest_unnest_3b873089cb7b96729cfcf8ea9b69b451">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu">iv_data</span><span class="op">(</span>R <span class="op">=</span> <span class="fl">1E04</span>, N <span class="op">=</span> <span class="fl">50</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">nest</span><span class="op">(</span>.by <span class="op">=</span> <span class="va">id</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">transmute</span><span class="op">(</span>model <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">data</span>, <span class="va">iv_coefs</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">unnest</span><span class="op">(</span>cols <span class="op">=</span> <span class="va">model</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10,000 × 3
    ols    ivo     iv
  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
1  1.56 0.437  0.769 
2  1.42 0.605  0.415 
3  1.64 0.0185 0.0786
# ℹ 9,997 more rows</code></pre>
</div>
</div>
<p>We then compute the mean, the median and the standard deviation for the three estimators: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>,</span>
<span>                       <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="va">mean</span>, median <span class="op">=</span> <span class="va">median</span>, </span>
<span>                            sd <span class="op">=</span> <span class="va">sd</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">pivot_longer</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">separate</span><span class="op">(</span><span class="va">name</span>, into <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"model"</span>, <span class="st">"stat"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">pivot_wider</span><span class="op">(</span>names_from <span class="op">=</span> <span class="va">stat</span>, values_from <span class="op">=</span> <span class="va">value</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 7
  iv_mean iv_median iv_sd model  mean median    sd
    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
1   -6.39      1.10  747. ols    1.50  1.50  0.126
2   -6.39      1.10  747. ivo    1.00  0.996 1.26 </code></pre>
</div>
</div>
<p>The distribution of the three estimators is presented in <a href="#fig-dist_iv">Figure&nbsp;<span>7.2</span></a>. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dsties</span> <span class="op">&lt;-</span> <span class="va">d</span> <span class="op">%&gt;%</span> <span class="va">as_tibble</span> <span class="op">%&gt;%</span> <span class="fu">pivot_longer</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">value</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_density</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>linetype <span class="op">=</span> <span class="va">name</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">scale_x_continuous</span><span class="op">(</span>limits <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dsties</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dist_iv" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="endogeneity_files/figure-html/fig-dist_iv-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.2: Distribution of the IV estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The OLS estimator is severely biased, as the central value of its distribution is about 1.5 and it has a small variance. The “pseudo-IV” estimator seems unbiased, as its mean (and median) is very close to one. The standard deviation is about 10 times larger than that of the OLS estimator, so the density curve is extremely flat. The mode of the density curve of the IV estimator is slightly larger than 1. Moreover, it has extremely fat tails (much more than the ones of the pseudo-IV estimator), which explains why the expected value and the variance don’t exist. This feature becomes obvious if we zoom in extreme values of the estimator, for example on the <span class="math inline">\((-4,-3)\)</span> range (see <a href="#fig-dist_iv_zoom">Figure&nbsp;<span>7.3</span></a>). </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dsties</span> <span class="op">+</span> <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4</span>, <span class="op">-</span><span class="fl">3</span><span class="op">)</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.015</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dist_iv_zoom" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="endogeneity_files/figure-html/fig-dist_iv_zoom-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.3: Distribution of the IV estimator (zoom)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p> </p>
</section><section id="an-example-segregation-effects-on-urban-poverty-and-inequality" class="level3"><h3 class="anchored" data-anchor-id="an-example-segregation-effects-on-urban-poverty-and-inequality">An example: Segregation effects on urban poverty and inequality</h3>
<p><span class="citation" data-cites="ANAN:11">Ananat (<a href="#ref-ANAN:11" role="doc-biblioref">2011</a>)</span> investigates the causal effect of segregation on urban poverty and inequality. Several responses are used, especially the Gini index and the poverty rate for the black populations of 121 American cities. The data set is called <code>tracks_side</code>. The level of segregation is measured by the following dissimilarity index:</p>
<p><span class="math display">\[
\frac{1}{2}\sum_{n=1} ^ N \left| b_n - w_n \right|
\]</span></p>
<p>where <span class="math inline">\(b_n\)</span> and <span class="math inline">\(w_n\)</span> are respectively the share of the black and white population of the whole city that lives in census track <span class="math inline">\(n\)</span> of the city. This index ranges from 0 (no segregation) to 1 (perfect segregation). In the sample of 121 cities used, the segregation index ranges from 0.33 to 0.87, with a median value of 0.57. We’ll focus on the effect of segregation on the poverty rate of black people: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_yx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">povb</span> <span class="op">~</span> <span class="va">segregation</span>, <span class="va">tracks_side</span><span class="op">)</span></span>
<span><span class="va">lm_yx</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## segregation   0.1818     0.0514    3.54  0.00058</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The coefficient of segregation is positive and highly significant. It indicates that a 1 point increase of the segregation index raises the poverty rate of black people by about 0.18 point. The correlation between segregation and bad economic outcome for black people is well established but, according to the author, the OLS estimator cannot easily be considered as a measure of the causal relationship of segregation on income, as there are some other variables that both influence segregation and outcome for black people. As an example, the situation of Detroit is described, which is a highly segregated city with poor economic outcomes, but other characteristics of the city (political corruption, legacy of a manufacturing economy) can be the cause of these two phenomena <span class="citation" data-cites="ANAN:11">(<a href="#ref-ANAN:11" role="doc-biblioref">Ananat 2011, 35</a>)</span>. Therefore, the OLS estimator is suspected to be biased and inconsistent because of the omitted variable bias. The instrumental variable estimator can be used in this context, but it requires the use of a good instrumental variable, i.e., a variable which is correlated with the endogenous covariate (segregation), but not directly with the response (the poverty rate). The author suggests that the way cities were subdivided by railroads into a large number of neighborhoods can be used as an instrument. Moreover, the tracks were mostly built during the nineteenth century, prior to the great migration (between 1915 to 1950) of African Americans from the south. More precisely, the index is defined as follow:</p>
<p><span class="math display">\[
1 - \sum_n \left(\frac{a_n}{A}\right) ^ 2
\]</span></p>
<p>where <span class="math inline">\(a_n\)</span> is the area of neighborhood <span class="math inline">\(n\)</span> defined by the rail tracks and <span class="math inline">\(A\)</span> is the total area of the city. The index is 0 if the city is completely undivided and tends to 1 if the number of neighborhoods tends to infinity. This index ranges from 0.24 to 0.99 in the sample, with a median value of 0.74. The regression of <span class="math inline">\(x\)</span> (<code>segregation</code>) on <span class="math inline">\(w\)</span> (<code>raildiv</code>) gives: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_xw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">segregation</span> <span class="op">~</span> <span class="va">raildiv</span>, <span class="va">tracks_side</span><span class="op">)</span></span>
<span><span class="va">lm_xw</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##         Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## raildiv   0.3995     0.0796    5.02  1.8e-06</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the 2SLS interpretation of the IV estimator, this is the <strong>first stage regression</strong>. The coefficient of <code>raildiv</code> is, as expected, positive and highly significant. It is important to check that the correlation between the covariate and the instrument is strong enough to get a precise IV estimator. This can be performed by computing their coefficient of correlation, or using the R<sup>2</sup> or the <span class="math inline">\(F\)</span> statistic of the first stage regression: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tracks_side</span> <span class="op">%&gt;%</span> <span class="fu">summarise</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">segregation</span>, <span class="va">raildiv</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">pull</span></span>
<span><span class="co">## [1] 0.418</span></span>
<span><span class="va">lm_xw</span> <span class="op">%&gt;%</span> <span class="va">ftest</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## F = 25.190, df: 1-119, pval = 0.000</span></span>
<span><span class="va">lm_xw</span> <span class="op">%&gt;%</span> <span class="va">rsq</span></span>
<span><span class="co">## [1] 0.1747</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The IV estimator can be obtained by regressing the response on the fitted values of the first stage regression: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_yhx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">povb</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">lm_xw</span><span class="op">)</span>, <span class="va">tracks_side</span><span class="op">)</span></span>
<span><span class="va">lm_yhx</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##               Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## fitted(lm_xw)    0.231      0.128    1.81    0.072</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As expected, the IV estimator is larger than the OLS estimator (0.23 vs.&nbsp;0.18). It can also be obtained by dividing the OLS coefficients of the regressions of <span class="math inline">\(y\)</span> on <span class="math inline">\(w\)</span> and of <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span>. The latter has already been computed, the former is: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_yw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">povb</span> <span class="op">~</span> <span class="va">raildiv</span>, <span class="va">tracks_side</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_yw</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="co">## raildiv </span></span>
<span><span class="co">## 0.09233</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_yw</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_xw</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="co">## raildiv </span></span>
<span><span class="co">##  0.2311</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A 1 point increase of <code>raildiv</code> is associated with a 0.4 point increase of the discrimination index and with a 0.09 point of the poverty rate. Therefore, the 0.4 point increase of <code>segregation</code> increases <code>povb</code> by 0.09, which means that an increase of 1 point of <code>segregation</code> would increase <code>povb</code> by <span class="math inline">\(0.09 / 0.4 = 0.23\)</span>, which is the value of the IV estimator. </p>
<p></p>
</section><section id="sec-wald_estimator" class="level3"><h3 class="anchored" data-anchor-id="sec-wald_estimator">Wald estimator</h3>
<p> </p>
<p>The Wald estimator is the special case of the instrumental variable estimator where the instrument <span class="math inline">\(w\)</span> is a binary variable and therefore defines two groups (<span class="math inline">\(w=0\)</span> and <span class="math inline">\(w=1\)</span>). In this case, the slope of the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(z\)</span> is <span class="math inline">\(\hat{\beta}_{y/w} = \bar{y}_1 - \bar{y}_0\)</span>, where <span class="math inline">\(\bar{y}_i\)</span> is the sample mean of <span class="math inline">\(y\)</span> in group <span class="math inline">\(i=0,1\)</span> and, similarly, the regression of <span class="math inline">\(x\)</span> on <span class="math inline">\(w\)</span> is <span class="math inline">\(\hat{\beta}_{x/w} = \bar{x}_1 - \bar{x}_0\)</span>. The Wald estimator is then:</p>
<p><span class="math display">\[
\hat{\beta} = \frac{\bar{y}_1 - \bar{y}_0}{\bar{x}_1 - \bar{x}_0} = \frac{\hat{\beta}_{yw}}{\hat{\beta}_{xw}}
\]</span></p>
<p>As <span class="math inline">\((\bar{x}_1 - \bar{x}_0)\)</span> converges to a constant, the asymptotic standard deviation of <span class="math inline">\(\hat{\beta}\)</span> is:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><span id="eq-std_wald"><span class="math display">\[
\hat{\sigma}_{\hat{\beta}} = \frac{\hat{\sigma}_{\bar{y}_1 - \bar{y}_0}}{\bar{x}_1 - \bar{x}_0}
\tag{7.7}\]</span></span></p>
<p><span class="citation" data-cites="ANGR:90">Angrist (<a href="#ref-ANGR:90" role="doc-biblioref">1990</a>)</span> studied whether veterans (in his article the Vietnam war) experience a long-term loss of income, and therefore should legitimately receive a benefit to compensate this loss. An obvious way to detect a loss would be to consider a sample with two groups (veterans and non-veterans) and to compare the mean income in these two groups. However, enrollment in the army is not random, and therefore it is probable that “certain types of men are more likely to serve in the armed forces than others”.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> In this situation, income difference is likely to be a biased estimator of the effect of military enrollment. To overcome this difficulty, <span class="citation" data-cites="ANGR:90">Angrist (<a href="#ref-ANGR:90" role="doc-biblioref">1990</a>)</span> used the fact that a draft lottery was used during the Vietnam war to select the young men who were enrolled in the army. More precisely, the 365 possible days of birth were randomly drawn and ordered and, later on, the army announced the number of days of birth that would lead to an enrollment (depending on the year, this number was between 95 and 195). All the lottery-eligible men didn’t go to the war and some that were not eligible fought, but the lottery produces an exogenous variation of the probability to be enrolled. <!-- - the 1970 lottery concerns men borned from 1944 to 1950 and the --> <!--   maximum eligible rank is 195, --> <!-- - the 1971 lottery concerns men borned in 1951 and the maximum --> <!--   eligible rank is 125, --> <!-- - the 1972 lottery concerns men borned in 1952-53 and the maximum --> <!--   eligible rank is 95. --> Two data sources are used and contained in the <code>vietnam</code> data set. The first one is a 1% sample of all the social security numbers and indicates the yearly income. A subset of the <code>vietnam</code> data set (defined by <code>variable == income</code>) contains the average and the standard deviation of income (the FICA definition) for every combination of birth year (<code>birth</code>, from 1950 to 1953), draft eligibility (<code>eligible</code> a factor with levels <code>yes</code> or <code>no</code>), race (<code>white</code> or <code>nonwhite</code>) and year (from 1966 to 1984 for men born in 1950 and starting in 1967, 1968 and 1969 for men born respectively in 1951, 1952 and 1953).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">vietnam</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 234 × 8
  variable birth eligible race   year  mean    sd   cpi
  &lt;chr&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 income    1950 no       white  1966  479.  8.35  97.2
2 income    1950 no       white  1967  825. 10.8  100  
# ℹ 232 more rows</code></pre>
</div>
</div>
<p>We divide the mean income and its standard deviation by the consumer price index (<code>cpi</code>), we then create two columns for eligible and non-eligible men and we compute the mean difference (<code>dmean</code>) and its standard deviation (<code>sd_dmean</code>): </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dinc_elig</span> <span class="op">&lt;-</span> <span class="va">vietnam</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">variable</span> <span class="op">==</span> <span class="st">"income"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>mean <span class="op">=</span> <span class="va">mean</span> <span class="op">/</span> <span class="va">cpi</span> <span class="op">*</span> <span class="fl">100</span>, sd <span class="op">=</span> <span class="va">sd</span> <span class="op">/</span> <span class="va">cpi</span> <span class="op">*</span> <span class="fl">100</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">select</span><span class="op">(</span><span class="op">-</span> <span class="va">variable</span>, <span class="op">-</span> <span class="va">cpi</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">pivot_wider</span><span class="op">(</span>names_from <span class="op">=</span> <span class="va">eligible</span>, values_from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">mean</span>, <span class="va">sd</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>dmean <span class="op">=</span> <span class="va">mean_yes</span> <span class="op">-</span> <span class="va">mean_no</span>, </span>
<span>           sd_dmean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sd_no</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">sd_yes</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">mean_no</span><span class="op">:</span><span class="va">sd_yes</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dinc_elig</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 108 × 5
  birth race   year  dmean sd_dmean
  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
1  1950 white  1966 -22.4      12.1
2  1950 white  1967  -8.02     15.1
# ℹ 106 more rows</code></pre>
</div>
</div>
<!-- This income difference between eligible and ineligible young men to -->
<!-- the draft lottery can be plotted for every year of birth and for income -->
<!-- from 1966 to 1984. We can see that eligible men experience a loss of -->
<!-- income by the time they can be enrolled and that this loss is still -->
<!-- present more than ten years after the potential enrollement. Note the -->
<!-- use of `ggplot2::geom_ribbon` to draw a confidence interval for the -->
<!-- mean income difference. -->
<p>The results match table 1 of <span class="citation" data-cites="ANGR:90">Angrist (<a href="#ref-ANGR:90" role="doc-biblioref">1990</a>)</span> page 318 (except that, in the table, the income is not divided by the cpi). The mean income difference is the numerator of the Wald estimator. The denominator is the difference of enrollment probability between draft eligible and ineligible young men. The author estimates this difference using the data of the 1984 Survey of Income and Program Participation (SIPP), which are available in the subset of the <code>vietnam</code> data set obtained with <code>variable == "veteran"</code> and contains the share of veterans and the standard deviation of this share for eligible and non-eligible young men by year of birth and race (<code>birth</code> and <code>race</code>): </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">veterans</span> <span class="op">&lt;-</span> <span class="va">vietnam</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">variable</span> <span class="op">==</span> <span class="st">"veteran"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">select</span><span class="op">(</span><span class="op">-</span> <span class="va">year</span>, <span class="op">-</span> <span class="va">cpi</span>, <span class="op">-</span> <span class="va">variable</span><span class="op">)</span></span>
<span><span class="va">veterans</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 18 × 5
  birth eligible race   mean     sd
  &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;
1  1950 no       white 0.193 0.0169
2  1950 yes      white 0.353 0.0223
3  1950 total    white 0.267 0.0140
# ℹ 15 more rows</code></pre>
</div>
</div>
<p>We then create one column for eligible and non-eligible young men, and we compute the difference of shares of enrollment and its standard deviation: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dshare_elig</span> <span class="op">&lt;-</span> <span class="va">veterans</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">pivot_wider</span><span class="op">(</span>names_from <span class="op">=</span> <span class="va">eligible</span>, values_from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">mean</span>, <span class="va">sd</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>dshare <span class="op">=</span> <span class="va">mean_yes</span> <span class="op">-</span> <span class="va">mean_no</span>, </span>
<span>           sd_dshare <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sd_yes</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">sd_no</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">select</span><span class="op">(</span><span class="op">-</span> <span class="op">(</span><span class="va">mean_no</span><span class="op">:</span><span class="va">sd_total</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dshare_elig</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 4
  birth race  dshare sd_dshare
  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;
1  1950 white  0.159    0.0280
2  1951 white  0.136    0.0277
# ℹ 4 more rows</code></pre>
</div>
</div>
<p>Finally, we join the two tables by race and year of birth; we compute the Wald estimator of the income difference, its standard deviation (using <a href="#eq-std_wald">Equation&nbsp;<span>7.7</span></a>) and the Student statistic, which is their ratio: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wald</span> <span class="op">&lt;-</span> <span class="va">dinc_elig</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">left_join</span><span class="op">(</span><span class="va">dshare_elig</span>, by <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"birth"</span>, <span class="st">"race"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>wald <span class="op">=</span> <span class="va">dmean</span> <span class="op">/</span> <span class="va">dshare</span>,</span>
<span>           sd <span class="op">=</span> <span class="va">sd_dmean</span> <span class="op">/</span> <span class="va">dshare</span>,</span>
<span>           z <span class="op">=</span> <span class="va">wald</span> <span class="op">/</span> <span class="va">sd</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">select</span><span class="op">(</span><span class="va">birth</span>, <span class="va">race</span>, <span class="va">year</span>, <span class="va">wald</span>, <span class="va">sd</span>, <span class="va">z</span><span class="op">)</span></span>
<span><span class="va">wald</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 108 × 6
  birth race   year   wald    sd      z
  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
1  1950 white  1966 -141.   76.0 -1.85 
2  1950 white  1967  -50.3  94.5 -0.532
# ℹ 106 more rows</code></pre>
</div>
</div>
<p>The income differentials are depicted in <a href="#fig-veterans">Figure&nbsp;<span>7.4</span></a>; note the use of <code>geom_ribbon</code> to draw a confidence interval for the mean income difference. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wald</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">race</span> <span class="op">==</span> <span class="st">"white"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">year</span>, <span class="va">wald</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_ribbon</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">wald</span> <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">sd</span>, ymax <span class="op">=</span> <span class="va">wald</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">sd</span><span class="op">)</span>, </span>
<span>                fill <span class="op">=</span> <span class="st">"lightgrey"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_hline</span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_smooth</span><span class="op">(</span>se <span class="op">=</span> <span class="cn">FALSE</span>, span <span class="op">=</span> <span class="fl">0.2</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">facet_grid</span><span class="op">(</span><span class="op">~</span> <span class="va">birth</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-veterans" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="endogeneity_files/figure-html/fig-veterans-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.4: Income differentials between veterans and non-veterans</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The income differential between veterans and non-veterans is substantial and persistent (about $1000 of 1984) and is most of the time significant at the 5% level (except for men born in 1951). </p>
</section></section><section id="sec-general_iv" class="level2" data-number="7.3"><h2 data-number="7.3" class="anchored" data-anchor-id="sec-general_iv">
<span class="header-section-number">7.3</span> General IV estimator</h2>
<p> Consider now the general case. Among the covariates, some of them are endogenous and other are not and should be included in the instrument list. Moreover, there may be more instruments than endogenous covariates.</p>
<section id="computation-of-the-estimator" class="level3"><h3 class="anchored" data-anchor-id="computation-of-the-estimator">Computation of the estimator</h3>
<p>There are <span class="math inline">\(K\)</span> covariates, <span class="math inline">\(J\)</span> endogenous covariates, <span class="math inline">\(K-J\)</span> exogenous covariates and <span class="math inline">\(G\)</span> external instruments. We denote <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> the matrix of covariates and instruments. The number of columns of these two matrices are respectively <span class="math inline">\(K + 1\)</span> and <span class="math inline">\(G + K - J + 1\)</span>. For the model to be identified, we must have <span class="math inline">\(G + K - J + 1 \geq K + 1\)</span> or <span class="math inline">\(G \geq J\)</span>. Therefore, there must be at least as many external instruments as there are endogenous covariates. We’ll now denote <span class="math inline">\(L + 1= G + K - J + 1\)</span> the number of columns of <span class="math inline">\(W\)</span>. The <span class="math inline">\(L+1\)</span> moment conditions are <span class="math inline">\(\mbox{E}(\epsilon | w) = 0\)</span> which implies <span class="math inline">\(\mbox{E}(\epsilon w) = 0\)</span>. The sample equivalent is the vector of <span class="math inline">\(L + 1\)</span> empirical moments: <span class="math inline">\(m = \frac{1}{N}W^\top \hat{\epsilon}\)</span>. As <span class="math inline">\(\hat{\epsilon} = y - Z \hat{\gamma}\)</span>, this is a system of <span class="math inline">\(L + 1\)</span> equation with <span class="math inline">\(K + 1\)</span> unknown parameter. The system is over-identified if <span class="math inline">\(L &gt; K\)</span> and in this case it is not possible to find a vector of estimates <span class="math inline">\(\hat{\beta}\)</span> for which all the empirical moments are 0. The instrumental variable estimator is, in this setting, the vector of parameters that makes the vector of empirical moments as close as possible to a vector of 0. If the errors are spherical, the variance of the vector of empirical moments is:</p>
<p><span id="eq-var_homosc"><span class="math display">\[
\mbox{V}(m) = \mbox{V}\left(\frac{1}{N}W^\top \epsilon\right)=
\frac{1}{N ^ 2} \mbox{E}\left(W^\top \epsilon \epsilon ^ \top
W\right)=
\frac{\sigma_\epsilon ^ 2}{N ^ 2}W^\top W
\tag{7.8}\]</span></span></p>
<p>and the IV estimator minimizes the quadratic form of the vector of moments with the inverse of its covariance matrix:</p>
<p><span class="math display">\[
\frac{N ^ 2}{\sigma_\epsilon^2}m^\top (W ^\top W) ^ {-1} m
=\frac{1}{\sigma_\epsilon ^ 2} \epsilon^\top W (W ^\top W) ^{-1} W ^
\top \epsilon
=\frac{1}{\sigma_\epsilon ^ 2} \epsilon^\top P_W \epsilon
\]</span></p>
<p>where <span class="math inline">\(P_W\)</span> is the projection matrix of <span class="math inline">\(W\)</span>, i.e., <span class="math inline">\(P_Wz\)</span> is the vector of fitted values of <span class="math inline">\(z\)</span> obtained by regressing <span class="math inline">\(z\)</span> on <span class="math inline">\(W\)</span>. Therefore, the IV estimator minimizes:</p>
<p><span class="math display">\[
\frac{1}{\sigma_\epsilon ^ 2} (y - Z\gamma)^\top P_W (y - Z\gamma) =
\frac{1}{\sigma_\epsilon ^ 2} (P_W y - P_W Z\gamma)^\top(P_Wy - P_WZ\gamma)
\]</span></p>
<p>and therefore (because <span class="math inline">\(P_W\)</span> is idempotent):</p>
<p><span id="eq-overidentified_iv"><span class="math display">\[
\hat{\gamma} = (Z^\top P_W Z) ^ {-1} (Z^\top P_W y)
\tag{7.9}\]</span></span></p>
<p>The 2SLS interpretation of the IV estimator is clear, as it is the OLS estimator of a model with <span class="math inline">\(y\)</span> or (<span class="math inline">\(P_W y\)</span>) as the response and <span class="math inline">\(P_W Z\)</span> the covariate. Therefore, it can be obtained in a first step by regressing all the covariates on the instruments and in a second steps by regressing the response on the fitted values of all the covariates obtained in the first step. Replacing <span class="math inline">\(y\)</span> by <span class="math inline">\(Z\gamma + \epsilon\)</span> in <a href="#eq-overidentified_iv">Equation&nbsp;<span>7.9</span></a>, we get:</p>
<p><span class="math display">\[
\hat{\gamma} = \gamma + \left(\frac{1}{N}Z^\top P_W Z\right) ^ {-1} \left(\frac{1}{N}Z^\top P_W \epsilon\right)
\]</span></p>
<p>and the IV estimator is consistent if <span class="math inline">\(\mbox{plim} \frac{1}{N} Z^\top P_W\epsilon = 0\)</span>, or <span class="math inline">\(\mbox{plim} \frac{1}N W ^ \top \epsilon = 0\)</span>. With spherical disturbances, the variance of the IV estimator is:</p>
<p><span class="math display">\[
\mbox{V}(\hat{\gamma}) = \left(\frac{1}{N}Z^\top P_WZ\right) ^ {-1}
\left(\frac{1}{N ^ 2}Z^\top P_W \epsilon \epsilon ^ \top P_W Z\right)  
\left(\frac{1}{N}Z^\top P_W Z\right) ^ {-1} =
\sigma_\epsilon ^ 2\left(Z^\top P_W Z\right) ^ {-1}
\]</span> </p>
<p>If the errors are heteroskedastic (or correlated), <a href="#eq-var_homosc">Equation&nbsp;<span>7.8</span></a> is a biased estimator of the variance of the moments, as <span class="math inline">\(\mbox{E}(\epsilon\epsilon^\top) \neq \sigma_\epsilon ^ 2 I\)</span>. In case of heteroskedasticity, <span class="math inline">\(\mbox{E}(W^\top\epsilon\epsilon^\top W)\)</span> can be consistently estimated by <span class="math inline">\(\hat{S} = \sum_n \hat{\epsilon}_n ^ 2 w_n w_n^\top\)</span> where <span class="math inline">\(\hat{\epsilon}\)</span> are the residuals of a consistent estimation, for example the residuals of the IV estimator previously described. Then, the objective function is:</p>
<p><span class="math display">\[
(y - Z\gamma)^\top W \hat{S}^{-1} W^\top (y - Z\gamma)
\]</span></p>
<p>Minimizing this quadratic form leads to the <strong>general method of moments</strong> (<strong>GMM</strong>) estimator, also called the <strong>two-stage IV</strong> estimator.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><span id="eq-two_steps_iv"><span class="math display">\[
\hat{\gamma} = \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}Z^\top W \hat{S}^{-1} W ^ \top y
\tag{7.10}\]</span></span></p>
<p>Replacing <span class="math inline">\(y\)</span> in <a href="#eq-two_steps_iv">Equation&nbsp;<span>7.10</span></a> by <span class="math inline">\(Z\gamma + \epsilon\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\gamma} = \gamma + \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}Z^\top W \hat{S}^{-1} W ^ \top \epsilon
\]</span></p>
<p>which leads to the following covariance matrix:</p>
<p><span class="math display">\[
\hat{\mbox{V}}(\hat{\gamma}) = \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}
\]</span></p>
<p>To estimate this covariance matrix, one can use an estimation of <span class="math inline">\(S\)</span> based on the residuals of the regression of the second step.</p>
<p> </p>
</section><section id="an-example-long-term-effects-of-slave-trade" class="level3"><h3 class="anchored" data-anchor-id="an-example-long-term-effects-of-slave-trade">An example: long-term effects of slave trade</h3>
<p>Africa experienced poor economic performances during the second half of the twentieth century, which can be explained by its experience of slave trade and colonialism. In particular, slave trade may induce long-term negative effects on the economic development of African countries because of induced corruption, ethnic fragmentation and weakening of established states. Africa experienced, between 1400 and 1900 four slave trades: the trans-Atlantic slave trade (the most important), but also the trans-Saharan, the Red Sea and the Indian Ocean slave trades. Not including those who died during the slave trade process, about 18 millions slaves were exported from Africa. <span class="citation" data-cites="NUNN:08">Nunn (<a href="#ref-NUNN:08" role="doc-biblioref">2008</a>)</span> conducted a quantitative analysis of the effects of slave trade on economic performances, by regressing the 2000 GDP per capita of 52 African countries on a measure of the level of slave extraction. The <code>slave_trade</code> data set is provided by the <strong>necountries</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sltd</span> <span class="op">&lt;-</span> <span class="fu">necountries</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/necountries/man/slave_trade.html">slave_trade</a></span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The response is <code>gdp</code> and the main covariate is a measure of the level of slave extraction, which is the number of slaves normalized by the area of the country. In <a href="#fig-gdp_slaves">Figure&nbsp;<span>7.5</span></a>, we first use a scatterplot, using log scales for both variables, which clearly indicates a negative relationship between slaves extraction and per capita GDP in 2000. Note the use of <code><a href="https://ggrepel.slowkow.com/reference/geom_text_repel.html">ggrepel::geom_label_repel</a></code>: <strong>ggplot2</strong> provides two geoms to draw labels (<code>geom_text</code> and <code>geom_label</code>), but the labels may overlap. <code>geom_label_repel</code> computes a position for the labels that prevent this overlapping. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sltd</span> <span class="op">%&gt;%</span> <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">slavesarea</span>, <span class="va">gdp</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_x_continuous</span><span class="op">(</span>trans <span class="op">=</span> <span class="st">"log10"</span>, expand <span class="op">=</span> <span class="fu">expansion</span><span class="op">(</span>mult <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">scale_y_log10</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">ggrepel</span><span class="fu">::</span><span class="fu"><a href="https://ggrepel.slowkow.com/reference/geom_text_repel.html">geom_label_repel</a></span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>label <span class="op">=</span> <span class="va">country</span><span class="op">)</span>,</span>
<span>                            size <span class="op">=</span> <span class="fl">2</span>, max.overlaps <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gdp_slaves" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="endogeneity_files/figure-html/fig-gdp_slaves-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.5: Per capita GDP and slave extraction</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="citation" data-cites="NUNN:08">Nunn (<a href="#ref-NUNN:08" role="doc-biblioref">2008</a>)</span> in table 2 presents a series of linear regressions, with different sets of controls. We just consider Nunn’s first specification, which includes only dummies for the colonizer as supplementary covariates. <code>colony</code> is a factor with eight levels: we use <code><a href="https://forcats.tidyverse.org/reference/fct_lump.html">forcats::fct_lump_min</a></code> to merge the most infrequent levels: <code>"none"</code> (2 countries), <code>"spain"</code>, <code>"germany"</code> and <code>"italy"</code> (1 country). </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sltd</span> <span class="op">&lt;-</span> <span class="va">sltd</span> <span class="op">%&gt;%</span> <span class="fu">mutate</span><span class="op">(</span>colony <span class="op">=</span> <span class="fu">fct_lump_min</span><span class="op">(</span><span class="va">colony</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">slaves_ols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">slavesarea</span><span class="op">)</span> <span class="op">+</span> <span class="va">colony</span>, <span class="va">sltd</span><span class="op">)</span></span>
<span><span class="va">slaves_ols</span> <span class="op">%&gt;%</span> <span class="fu">gaze</span><span class="op">(</span>coef <span class="op">=</span> <span class="st">"log(slavesarea)"</span><span class="op">)</span></span>
<span><span class="co">##                 Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## log(slavesarea)  -0.1231     0.0234   -5.26  3.7e-06</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The coefficient is negative and highly significant; it implies that a 10% increase of slave extraction induces a reduction of 1% of GDP per capita. As noticed by <span class="citation" data-cites="NUNN:08">Nunn (<a href="#ref-NUNN:08" role="doc-biblioref">2008</a>)</span>, the estimation of the effect of slave trade on GDP can be inconsistent for two reasons:</p>
<ul>
<li>the level of slave extraction, which is based on information of the ethnicity of individual slaves and then aggregated at the current countries’ level can be prone to error of measurement; moreover, for countries inside the continent (compared to coastal countries), a lot of slaves died during the journey to the coastal port of export, so the level of extraction may be underestimated for these countries,</li>
<li>the average economic conditions may be different for countries that suffered a large extraction, compared to the others; in particular, if countries where the trade was particularly important were poor, their current poor economic conditions can be explained by their poor economic conditions 600 years ago and not by slave trades.</li>
</ul>
<p>Measurement error induces an attenuation bias, which means that without measurement error, the negative effect of slave trades on GDP per capita would be stronger. The second effect would induce an upward-bias (in absolute value) of the coefficient on slave trades. But, actually, <span class="citation" data-cites="NUNN:08">Nunn (<a href="#ref-NUNN:08" role="doc-biblioref">2008</a>)</span> showed that areas of Africa that suffered the most slave trade were in general not the poorest areas, but the most developed ones. In this case, the OLS estimator would underestimate the effect of slave trades on GDP per capita. <span class="citation" data-cites="NUNN:08">Nunn (<a href="#ref-NUNN:08" role="doc-biblioref">2008</a>)</span> then performs instrumental variable regressions, using as instruments the distance between the centroid of the countries and the closest major market for the four slave trades (for example Mauritius and Oman for the Indian Ocean slave trade and Massawa, Suakin and Djibouti for the Red Sea slave trade). The IV regression can be performed by first regressing the endogenous covariate on the external instruments (<code>atlantic</code>, <code>indian</code>, <code>redsea</code> and <code>sahara</code>) and on the exogenous covariates (here <code>colony</code>, the factor indicating the previous colonizer). This is the so-called <strong>first-stage regression</strong>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">slaves_first</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">slavesarea</span><span class="op">)</span> <span class="op">~</span> <span class="va">colony</span> <span class="op">+</span> <span class="va">atlantic</span> <span class="op">+</span> <span class="va">indian</span> <span class="op">+</span></span>
<span>                       <span class="va">redsea</span> <span class="op">+</span> <span class="va">sahara</span>, <span class="va">sltd</span><span class="op">)</span></span>
<span><span class="va">slaves_first</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##                Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## colonyfrance     -1.873      1.323   -1.42  0.16391</span></span>
<span><span class="co">## colonyportugal   -0.294      1.846   -0.16  0.87423</span></span>
<span><span class="co">## colonybelgium    -3.260      2.236   -1.46  0.15213</span></span>
<span><span class="co">## colonyOther      -1.360      1.946   -0.70  0.48840</span></span>
<span><span class="co">## atlantic         -1.516      0.382   -3.97  0.00027</span></span>
<span><span class="co">## indian           -1.108      0.410   -2.70  0.00982</span></span>
<span><span class="co">## redsea           -0.384      0.765   -0.50  0.61845</span></span>
<span><span class="co">## sahara           -2.569      0.881   -2.92  0.00561</span></span>
<span><span class="va">slaves_first</span> <span class="op">%&gt;%</span> <span class="va">rsq</span></span>
<span><span class="co">## [1] 0.3315</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Except for the <code>redsea</code> variable, the coefficients are highly significant, and the four instruments and the exogenous covariate explain more than one-fourth of the variance of slave extraction. The second stage is obtained by regressing the response on the fitted values of the first-step estimation: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sltd</span> <span class="op">&lt;-</span> <span class="va">sltd</span> <span class="op">%&gt;%</span> <span class="fu">add_column</span><span class="op">(</span>hlslarea <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">slaves_first</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">slaves_second</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp</span><span class="op">)</span> <span class="op">~</span> <span class="va">hlslarea</span>, <span class="va">sltd</span><span class="op">)</span></span>
<span><span class="va">slaves_second</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">##          Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">## hlslarea  -0.1736     0.0459   -3.78  0.00041</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The coefficient has almost doubled, compared to the OLS estimator, which confirms that this latter estimator is biased, with an attenuation bias due to measurement error and a downward-bias caused by the fact that the most developed African regions were more affected by slave trade. The two-stage IV estimator is then computed. We use the <strong>Formula</strong> package which enables to write complex formulas with multiple set of variables, separated by the <code>|</code> operator:<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">Formula</span><span class="op">)</span></span>
<span><span class="va">form</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Formula/man/Formula.html">Formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">slavesarea</span><span class="op">)</span> <span class="op">+</span> <span class="va">colony</span><span class="op">|</span> <span class="va">colony</span> <span class="op">+</span> </span>
<span>                   <span class="va">redsea</span> <span class="op">+</span> <span class="va">atlantic</span> <span class="op">+</span> <span class="va">sahara</span> <span class="op">+</span> <span class="va">indian</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first part contains the covariates and the second part the instruments. We then compute the model frame and we extract the covariates, the instruments and the response: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.frame.html">model.frame</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">sltd</span><span class="op">)</span></span>
<span><span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">sltd</span>, rhs <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">sltd</span>, rhs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Formula/man/model.frame.Formula.html">model.part</a></span><span class="op">(</span><span class="va">form</span>, <span class="va">mf</span>, lhs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">pull</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then compute the cross-products of the instruments and the covariates (<span class="math inline">\(Z^\top W\)</span>) and <span class="math inline">\(\hat{S}\)</span> </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ZPW</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">Z</span>, <span class="va">W</span><span class="op">)</span></span>
<span><span class="va">S</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">slaves_second</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">W</span><span class="op">)</span></span>
<span><span class="va">vcov_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">ZPW</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">ZPW</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">iv2s</span> <span class="op">&lt;-</span> <span class="va">vcov_1</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">ZPW</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="va">W</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">drop</span></span>
<span><span class="va">resid2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">Z</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">iv2s</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">drop</span></span>
<span><span class="va">S2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">resid2</span><span class="op">)</span> <span class="op">*</span> <span class="va">W</span><span class="op">)</span></span>
<span><span class="va">vcov_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">ZPW</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">S2</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">ZPW</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span>coef <span class="op">=</span> <span class="va">iv2s</span>, sd1 <span class="op">=</span> <span class="fu">stder</span><span class="op">(</span><span class="va">vcov_1</span><span class="op">)</span>,</span>
<span>      sd2 <span class="op">=</span> <span class="fu">stder</span><span class="op">(</span><span class="va">vcov_2</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span>, <span class="op">]</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">##   coef    sd1    sd2 </span></span>
<span><span class="co">## -0.213  0.044  0.044</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The results are very similar to those of the one-step IV estimator. The IV estimator can also be computed using the <code><a href="https://zeileis.github.io/ivreg/reference/ivreg.html">ivreg::ivreg</a></code> function. The main argument is a two-part formula, the first part containing the covariates and the second part the instruments: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ivreg</span><span class="fu">::</span><span class="fu"><a href="https://zeileis.github.io/ivreg/reference/ivreg.html">ivreg</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">slavesarea</span><span class="op">)</span> <span class="op">+</span> <span class="va">colony</span><span class="op">|</span> <span class="va">colony</span> <span class="op">+</span> </span>
<span>                 <span class="va">redsea</span> <span class="op">+</span> <span class="va">atlantic</span> <span class="op">+</span> <span class="va">sahara</span> <span class="op">+</span> <span class="va">indian</span>, data <span class="op">=</span> <span class="va">sltd</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output of the <code>ivreg</code> function will be presented in <a href="#sec-test_sltr"><span>Section&nbsp;7.6.6</span></a>. </p>
<p></p>
</section></section><section id="sec-three_sls" class="level2" data-number="7.4"><h2 data-number="7.4" class="anchored" data-anchor-id="sec-three_sls">
<span class="header-section-number">7.4</span> Three-stage least squares</h2>
<p> </p>
<p>Consider now the case where the model is defined by a system of equations, some of the covariates entering these equations being endogenous. We consider therefore a system of <span class="math inline">\(L\)</span> equations denoted by <span class="math inline">\(y_l=Z_l\gamma_l+\epsilon_l\)</span>, with <span class="math inline">\(l=1\ldots L\)</span>. This situation has already encountered in <a href="multiple_regression.html#sec-sys_eq_ols"><span>Section&nbsp;3.7.1</span></a>, <a href="non_spherical.html#sec-bptest_system"><span>Section&nbsp;6.2.3</span></a> and <a href="non_spherical.html#sec-sur"><span>Section&nbsp;6.4.4</span></a>. In this latter section, we considered that all the covariates were exogenous and we presented the seemingly unrelated regression estimator, which is a GLS estimator that takes into account the correlation between the errors of the different equations. Remember that, in matrix form, the system can be written as follows:</p>
<p><span class="math display">\[
\left(
  \begin{array}{c}
    y_1 \\ y_2 \\ \vdots \\ y_L
  \end{array}
\right)
=
\left(
  \begin{array}{ccccc}
    Z_1 &amp; 0 &amp; \ldots &amp; 0 \\
    0 &amp; Z_2 &amp; \ldots &amp; 0 \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; \ldots &amp; Z_L
  \end{array}
\right)
\left(
  \begin{array}{c}
    \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_L
  \end{array}
\right)
+
\left(
  \begin{array}{c}
    \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_L
  \end{array}
\right)
\]</span></p>
<p>And the covariance of the error vector for the whole system is assumed to be:</p>
<p><span class="math display">\[
\Omega=
\left(
\begin{array}{cccc}
  \sigma_{11} I &amp; \sigma_{12} I &amp; \ldots &amp;\sigma_{1L} I \\
  \sigma_{12} I &amp; \sigma_{22} I &amp; \ldots &amp;\sigma_{2L} I \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \sigma_{1L} I &amp; \sigma_{2L} I &amp; \ldots &amp; \sigma_{LL} I
  \end{array}
\right)
= \Sigma \otimes I
\]</span></p>
<p>where <span class="math inline">\(\otimes\)</span> is the Kronecker product and <span class="math inline">\(\Sigma\)</span> is a symmetric matrix of dimensions <span class="math inline">\(L\)</span> for which the diagonal elements are the variance of the errors for a given equation and the off-diagonal elements the covariances between the pairs of errors of two different equations.</p>
<section id="computation-of-the-three-stage-least-square-estimator" class="level3"><h3 class="anchored" data-anchor-id="computation-of-the-three-stage-least-square-estimator">Computation of the three-stage least square estimator</h3>
<p>If some covariates are endogenous, we should consider, for each equation, a matrix of instruments:</p>
<p><span class="math display">\[
W =
\left(
  \begin{array}{ccccc}
    W_1 &amp; 0 &amp; \ldots &amp; 0 \\
    0 &amp; W_2 &amp; \ldots &amp; 0 \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; \ldots &amp; W_L
  \end{array}
\right)
\]</span></p>
<p>The moment conditions for the whole system are then:</p>
<p><span class="math display">\[
m =
\frac{1}{N}W^\top \epsilon=
\frac{1}{N}\left(
\begin{array}{c}
W_1 ^ \top \epsilon_1 \\
W_2 ^ \top \epsilon_2 \\
\vdots \\
W_L^\top \epsilon_L
\end{array}
\right)
\]</span></p>
<p>and the variance of the vector of moments is:</p>
<p><span class="math display">\[
\mbox{V}(m) = \frac{1}{N ^ 2}\mbox{E}\left(m m ^ \top\right) =
\frac{1}{N ^ 2}W ^ \top \mbox{E}\left(\epsilon \epsilon ^ \top\right) W =
\frac{1}{N ^ 2}W ^ \top \Omega W = \frac{1}{N ^ 2}W ^ \top (\Sigma \otimes I) W
\]</span></p>
<p>The <strong>three-stage least squares</strong> (<strong>3SLS</strong>) estimator<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> minimizes the quadratic form of the moments with the inverse of this variance matrix:</p>
<p><span class="math display">\[
m ^ \top (W ^ \top \Omega W) ^ {-1} m = (y - Z \gamma) ^ \top (W ^ \top
\Omega W) ^ {-1} (y - Z \gamma)
\]</span></p>
<p>which leads to the following estimator:</p>
<p><span id="eq-threesls"><span class="math display">\[
\hat{\gamma} = \left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top
Z\right) ^ {-1}
\left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top y\right)
\tag{7.11}\]</span></span></p>
<p>This estimator can actually be computed using least squares on transformed data. Denote <span class="math inline">\(\Psi = \Sigma ^ {- 0.5} \otimes I\)</span> the matrix such that <span class="math inline">\(\Psi ^ \top \Psi = \Sigma ^ {-1} \otimes I = \Omega ^ {-1}\)</span>. Then, premultiply the covariates and the response by <span class="math inline">\(\Psi\)</span> and the instruments by <span class="math inline">\((\Psi^{-1}) ^ \top\)</span>. Then the projection matrix of <span class="math inline">\(\tilde{W} = {\Psi^{-1}} ^ \top W\)</span> is:</p>
<p><span class="math display">\[
P_{\tilde{W}} = (\Psi^{-1}) ^ \top W\left(W ^ \top \Psi ^ {-1} (\Psi ^ {-1})
^ \top W\right) ^ {-1} W ^ \top \Psi^{-1}
\]</span></p>
<p>but <span class="math inline">\(\Psi ^ {-1} (\Psi ^ {-1}) ^ \top = \Psi ^ {-1} (\Psi ^ \top ) ^ {-1} = (\Psi ^ \top \Psi) ^ {-1} = \Omega\)</span>. Therefore:</p>
<p><span class="math display">\[
P_{\tilde{W}} = (\Psi^{-1}) ^ \top W \left(W ^ \top \Omega W\right) ^ {-1} W ^ \top \Psi^{-1}
\]</span></p>
<p>The transformed covariates and response are <span class="math inline">\(\tilde{Z} = \Psi Z\)</span> and <span class="math inline">\(\tilde{y} = \Psi y\)</span>, so that performing the instrumental variable estimator on the transformed data, we get:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\hat{\gamma} &amp;=&amp; \left(\tilde{Z}^\top P_{\tilde{W}} \tilde{Z}\right) ^
{-1} \left(\tilde{Z}^\top P_{\tilde{W}} \tilde{y}\right) \\
&amp;=&amp; \left(Z ^ \top \Psi ^ \top (\Psi^{-1}) ^ \top W \left(W ^ \top \Omega
W\right) ^ {-1} W ^ \top \Psi^{-1} \Psi Z\right) ^ {-1} \\
&amp;\times &amp; \left(Z ^ \top \Psi ^ \top (\Psi^{-1}) ^ \top W \left(W ^ \top \Omega
W\right) ^ {-1} W ^ \top \Psi^{-1} \Psi y\right) \\
&amp;=&amp; \left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top Z\right)^ {-1}
\left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top y\right)
\end{array}
\]</span></p>
<p>which is <a href="#eq-threesls">Equation&nbsp;<span>7.11</span></a>. Therefore, the 3SLS estimator can be computed in the following way:</p>
<ol type="1">
<li>First compute the 2SLS estimator and retrieve the vectors of residuals (<span class="math inline">\(\hat{\Xi} = (\hat{\epsilon}_1, \ldots, \hat{\epsilon}_L)\)</span>).</li>
<li>Estimate <span class="math inline">\(\Sigma\)</span> using the cross-products of the vectors of residuals: <span class="math inline">\(\hat{\Sigma} = \hat{\Xi} ^ \top \hat{\Xi} / N\)</span>,</li>
<li>Use the Cholesky decomposition of <span class="math inline">\(\hat{\Sigma} ^ {-1}\)</span> to get <span class="math inline">\(V\)</span> such that <span class="math inline">\(V ^ \top V = \Sigma ^ {-1}\)</span>,</li>
<li>Premultiply the covariates and the response by <span class="math inline">\(\hat{\Psi} = V \otimes I\)</span> and the instruments by <span class="math inline">\(\left(\hat{\Psi} ^ {-1}\right) ^ {\top} = (V ^ {-1}) ^ \top \otimes I\)</span>,</li>
<li>Regress the transformed covariates on the transformed instruments and retrieve the fitted values,</li>
<li>Regress the transformed response on the fitted values of the previous regression.</li>
</ol>
<p> </p>
</section><section id="an-example-the-watermelon-market" class="level3"><h3 class="anchored" data-anchor-id="an-example-the-watermelon-market">An example: the watermelon market</h3>
<p><span class="citation" data-cites="SUIT:55">Suits (<a href="#ref-SUIT:55" role="doc-biblioref">1955</a>)</span> built an econometric model of the watermelon market using a time series for the United States, and his study was complemented by <span class="citation" data-cites="WOLD:58">Wold (<a href="#ref-WOLD:58" role="doc-biblioref">1958</a>)</span> who rebuilt the data set. This data set (called <code>watermelon</code>) is a good example of the use of system estimation with endogeneity, and its use for teaching purposes is advocated by <span class="citation" data-cites="STEW:19">Stewart (<a href="#ref-STEW:19" role="doc-biblioref">2019</a>)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">watermelon</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 22 × 12
   year     q     h     y     n     p    pc    pv     w    pf    d1
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1  1930  1.93  1.90  4.87  2.09  2.07 0.976 0.367  1.46  1.10     0
2  1931  1.89  1.88  4.80  2.09  2.00 0.753 1.18   1.36  1.11     0
# ℹ 20 more rows
# ℹ 1 more variable: d2 &lt;dbl&gt;</code></pre>
</div>
</div>
<p>On the supply side, two quantities of watermelons are distinguished: <code>q</code> are crop of watermelons available for harvest (millions) and <code>h</code> are watermelons actually harvested (millions). <code>q</code> depends on planting decisions made on information of the previous season; more specifically, <code>q</code> depends on lag values of the average farm price of watermelon <code>p</code> (in dollars per thousand), the average annual net farm price per pound of cottons <code>pc</code> (in dollars), the average farm price of vegetables <code>pv</code> (an index) and on two dummy variables for government cotton acreage allotment program <code>d1</code> (one for the 1934-1951 period) and for World War 2 <code>d2</code> (one for 1943-1946 period).</p>
<p>The amount of watermelons actually harvested <code>h</code> depends on current price farm of watermelons <code>p</code>, wages <code>w</code> (the major cost of harvesting) and is of course bounded by <code>q</code>, the amount of watermelon available for harvest. More specifically, the relative price of watermelon and wage is considered. On the demand side, farm price depends on per capita harvest, per capita income (<code>yn</code>) and transportation cost (<code>pf</code>). An inverse demand function is estimated, of the form:</p>
<p><span class="math display">\[
p = \alpha + \beta_q q + \beta_r r + \ldots
\]</span></p>
<p>and, all the variables being in logarithm, the price and income elasticities are therefore respectively <span class="math inline">\(1 / \beta_q\)</span> and <span class="math inline">\(- \beta_r / \beta_q\)</span>. We compute the relative price of watermelons in terms of wage (<code>pw</code>), the income per capita (<code>yn</code>) and harvest (<code>hn</code>) and the first lags for <code>p</code>, <code>pc</code> and <code>pv</code>. We also remove the first and the last observation (because of the lag for the first one and because of the missing value for <code>pv</code> and <code>w</code> for the last one). </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wm</span> <span class="op">&lt;-</span> <span class="va">watermelon</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>yn <span class="op">=</span> <span class="va">y</span> <span class="op">-</span> <span class="va">n</span>, hn <span class="op">=</span> <span class="va">h</span> <span class="op">-</span> <span class="va">n</span>,</span>
<span>           pw <span class="op">=</span> <span class="va">p</span> <span class="op">-</span> <span class="va">w</span>, lp <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lag.html">lag</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>,</span>
<span>           lpc <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lag.html">lag</a></span><span class="op">(</span><span class="va">pc</span><span class="op">)</span>, lpv <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lag.html">lag</a></span><span class="op">(</span><span class="va">pv</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="op">!</span> <span class="va">year</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1930</span>, <span class="fl">1951</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now define the set of three equations:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">eq_c</span> <span class="op">&lt;-</span> <span class="va">q</span> <span class="op">~</span> <span class="va">lp</span> <span class="op">+</span> <span class="va">lpc</span> <span class="op">+</span> <span class="va">lpv</span> <span class="op">+</span> <span class="va">d1</span> <span class="op">+</span> <span class="va">d2</span></span>
<span><span class="va">eq_s</span> <span class="op">&lt;-</span> <span class="va">h</span> <span class="op">~</span> <span class="va">pw</span> <span class="op">+</span> <span class="va">q</span></span>
<span><span class="va">eq_d</span> <span class="op">&lt;-</span> <span class="va">p</span> <span class="op">~</span> <span class="va">hn</span> <span class="op">+</span> <span class="va">yn</span> <span class="op">+</span> <span class="va">pf</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The exogenous variables are <code>w</code>, <code>n</code>, <code>yn</code>, <code>pf</code>, <code>d1</code>, <code>d2</code> and the lagged values of the price of watermelons (<code>lp</code>), cotton (<code>lpc</code>) and vegetables (<code>lpv</code>). We form a one-sided formula for this set of instruments:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">eq_inst</span> <span class="op">&lt;-</span> <span class="op">~</span> <span class="va">w</span> <span class="op">+</span> <span class="va">n</span> <span class="op">+</span> <span class="va">yn</span> <span class="op">+</span> <span class="va">lp</span> <span class="op">+</span> <span class="va">pf</span> <span class="op">+</span> <span class="va">d1</span> <span class="op">+</span> <span class="va">d2</span> <span class="op">+</span> <span class="va">lpc</span> <span class="op">+</span> <span class="va">lpv</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then extract the three matrices of covariates, the matrix of instruments and the matrix of responses: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_inst</span>, <span class="va">wm</span><span class="op">)</span></span>
<span><span class="va">X1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_c</span>, <span class="va">wm</span><span class="op">)</span></span>
<span><span class="va">X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_s</span>, <span class="va">wm</span><span class="op">)</span></span>
<span><span class="va">X3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">eq_d</span>, <span class="va">wm</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">W</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu">select</span><span class="op">(</span><span class="va">wm</span>, <span class="va">q</span>, <span class="va">h</span>, <span class="va">p</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">as.matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first compute the 2SLS estimator, using the <strong>systemfit</strong> package: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/systemfit/">systemfit</a></span><span class="op">)</span></span>
<span><span class="va">twosls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/systemfit/man/systemfit.html">systemfit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>crop <span class="op">=</span> <span class="va">eq_c</span>, supply <span class="op">=</span> <span class="va">eq_s</span>, demand <span class="op">=</span> <span class="va">eq_d</span><span class="op">)</span>,</span>
<span>                    inst <span class="op">=</span> <span class="va">eq_inst</span>, method <span class="op">=</span> <span class="st">"2SLS"</span>, data <span class="op">=</span> <span class="va">wm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>From this consistent, but inefficient estimator, we extract the data frame of residuals (one column per equation, one line per observation), we coerce it to a matrix and we estimate the matrix of covariance for the system of equation (<span class="math inline">\(\Sigma\)</span>): </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/matrix-products.html">crossprod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">twosls</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">N</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then compute <code>V</code> using the Cholesky decomposition of the inverse of <span class="math inline">\(\Sigma\)</span>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">V</span> <span class="op">&lt;-</span> <span class="va">Sigma</span> <span class="op">%&gt;%</span> <span class="va">solve</span> <span class="op">%&gt;%</span> <span class="va">chol</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using <span class="math inline">\(V\)</span>, we apply the relevant transformation for the response and for the covariate: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Xt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">V</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">X1</span>, <span class="va">V</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">X2</span>, <span class="va">V</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">X3</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">V</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">X1</span>, <span class="va">V</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">X2</span>, <span class="va">V</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">X3</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">V</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">X1</span>, <span class="va">V</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">X2</span>, <span class="va">V</span><span class="op">[</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="va">X3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">yt</span> <span class="op">&lt;-</span> <span class="va">Y</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">as.numeric</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then apply the transformation for the instruments, using <span class="math inline">\(\left(V ^ {-1}\right) ^ \top\)</span>. The matrix of instruments being the same for all the equations, the transformation can be obtained more simply using a Kronecker product: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Wt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/kronecker.html">%x%</a></span> <span class="va">W</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, 2SLS is performed by first regressing <code>Zt</code> on <code>Xt</code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">first</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Xt</span> <span class="op">~</span> <span class="va">Wt</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and then by regressing the fitted values of this first step regression on the transformed response: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">second</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">yt</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">first</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Identical results are obtained using <code>systemfit</code> and setting <code>method</code> to <code>"3SLS"</code>:<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">threesls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/systemfit/man/systemfit.html">systemfit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>crop <span class="op">=</span> <span class="va">eq_c</span>, supply <span class="op">=</span> <span class="va">eq_s</span>,</span>
<span>                           demand <span class="op">=</span> <span class="va">eq_d</span><span class="op">)</span>, inst <span class="op">=</span> <span class="va">eq_inst</span>,</span>
<span>                      method <span class="op">=</span> <span class="st">"3SLS"</span>, data <span class="op">=</span> <span class="va">wm</span>,</span>
<span>                      methodResidCov<span class="op">=</span>  <span class="st">"noDfCor"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">threesls</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  crop_(Intercept)            crop_lp           crop_lpc 
           1.05294            0.57926           -0.32113 
          crop_lpv            crop_d1            crop_d2 
          -0.12373            0.03173           -0.15660 
supply_(Intercept)          supply_pw           supply_q 
          -0.21490            0.12145            1.06224 
demand_(Intercept)          demand_hn          demand_yn 
          -1.43808           -0.89257            1.55942 
         demand_pf 
          -0.85591 </code></pre>
</div>
</div>
<p>The relative price of watermelon is significantly positive in the supply equation, with a value of 0.12 which is the price elasticity of supply. In the inverse demand function, the coefficients of per capita quantity of watermelons and of per capita income have the expected sign (respectively negative and positive) and are highly significant. The estimated price and income elasticities are <span class="math inline">\(1 / -0.89 = -1.12\)</span> and <span class="math inline">\(- 1.56 / -0.89 = 1.75\)</span>. </p>
</section></section><section id="sec-fixed_effects" class="level2" data-number="7.5"><h2 data-number="7.5" class="anchored" data-anchor-id="sec-fixed_effects">
<span class="header-section-number">7.5</span> Fixed effects model</h2>
<p> </p>
<p>In <a href="non_spherical.html#sec-error_component"><span>Section&nbsp;6.1.2</span></a>, we developed the error component model for panel or pseudo-panel data sets. Remember that when we have several observations (<span class="math inline">\(t\)</span>) for the same entities (<span class="math inline">\(n\)</span>), the simple linear model can be written as:</p>
<p><span class="math display">\[
y_{nt} = \alpha + \beta x_{nt} + \epsilon_{nt} = \alpha + \beta x_{nt} + \eta_n + \nu_{nt}
\]</span> the error term being the sum of two components: an entity / individual effect <span class="math inline">\(\eta_n\)</span> and an idiosyncratic effect <span class="math inline">\(\nu_{nt}\)</span>. In <a href="non_spherical.html#sec-error_component"><span>Section&nbsp;6.1.2</span></a>, we assumed that <span class="math inline">\(x\)</span> was uncorrelated with <span class="math inline">\(\epsilon_{nt}\)</span>. OLS was still a consistent estimator, but the BLUE estimator was GLS which takes into account the correlation between errors of the observations of the same entity / individual caused by the presence of a common individual effect <span class="math inline">\(\eta_n\)</span>. Consider now that <span class="math inline">\(x\)</span> is endogenous; the OLS and GLS estimators are then biased and inconsistent. If <span class="math inline">\(x\)</span> is correlated with <span class="math inline">\(\eta\)</span>: <span class="math inline">\(\mbox{E}(\eta \mid x) \neq 0\)</span> but not with <span class="math inline">\(\nu\)</span>: <span class="math inline">\(\mbox{E}(\nu \mid x) = 0\)</span>, unbiased and consistent OLS estimators can be obtained when the individual effect <span class="math inline">\(\eta\)</span> is either estimated or if the estimation is performed on a transformation of the covariate and the response that removes the individual effect. This is called the <strong>fixed effects</strong> estimator.</p>
<section id="computation-of-the-fixed-effects-estimator" class="level3"><h3 class="anchored" data-anchor-id="computation-of-the-fixed-effects-estimator">Computation of the fixed effects estimator</h3>
<p>The linear model: <span class="math inline">\(y_{nt} = \beta^\top x_{nt} + \eta_n + \nu_{nt}\)</span> can be written in matrix form as:</p>
<p><span class="math display">\[
\left(
\begin{array}{c}
y_{11}\\
y_{12}\\
\vdots\\
y_{1T} \\
y_{21}\\
y_{22}\\
\vdots\\
y_{2T}\\
\vdots\\
y_{N1}\\
y_{N2}\\
\vdots\\
y_{NT}
\end{array}
\right) =
\left(
\begin{array}{c}
x_{11}^\top\\
x_{12}^\top\\
\vdots\\
x_{1T}^\top\\
x_{21}^\top\\
x_{22}^\top\\
\vdots\\
x_{2T}^\top\\
\vdots\\
x_{N1}^\top\\
x_{N2}^\top\\
\vdots\\
x_{NT}^\top
\end{array}
\right)
\beta
+
\left(
\begin{array}{cccc}
1 &amp; 0 &amp; \ldots &amp; 0 \\
1 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; 1 \\
0 &amp; 0 &amp; \ldots &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; 1 \\
\end{array}
\right)
\left(
\begin{array}{c}
\eta_1 \\
\eta_2 \\
\vdots \\
\eta_N
\end{array}
\right)+
\left(
\begin{array}{c}
\nu_{11}\\
\nu_{12}\\
\vdots\\
\nu_{1T} \\
\nu_{21}\\
\nu_{22}\\
\vdots\\
\nu_{2T}\\
\vdots\\
\nu_{N1}\\
\nu_{N2}\\
\vdots\\
\nu_{NT}
\end{array}
\right)
\]</span> or:</p>
<p><span class="math display">\[
y = X \beta + D \eta + \nu
\]</span> Note that this model doesn’t contain an intercept: as <span class="math inline">\(\eta\)</span> are considered as parameters to be estimated, with an intercept <span class="math inline">\(\alpha\)</span>, only <span class="math inline">\(N\)</span> parameters like <span class="math inline">\(\alpha + \eta_n\)</span> can be estimated and therefore, <span class="math inline">\(\alpha\)</span> can safely be set to 0. <span class="math inline">\(D = j_T \otimes I_N\)</span> is a <span class="math inline">\(NT\times N\)</span> matrix where each column contains a dummy variable for one individual. The <strong>least squares dummy variable</strong> (<strong>LSDV</strong>) estimator consists of estimating by least squares <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\eta\)</span>. Instead of estimating the <span class="math inline">\(N\)</span> <span class="math inline">\(\eta\)</span> parameters, it is simpler and more efficient to use the Frisch-Waugh theorem: first <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span> are regressed on <span class="math inline">\(D\)</span>, then the residuals of the first regression of <span class="math inline">\(y\)</span> are regressed on those of <span class="math inline">\(X\)</span>.</p>
<p>The OLS estimate of a variable <span class="math inline">\(z\)</span> on <span class="math inline">\(D\)</span> is: <span class="math inline">\(\hat{\delta} = (D ^ \top D) ^ {-1} D^\top z\)</span>. But <span class="math inline">\(D^\top D = T I\)</span>, so that <span class="math inline">\((D ^ \top D) ^ {-1} = I / T\)</span>. Moreover:</p>
<p><span class="math display">\[
D^\top z = \left(
\begin{array}{c}
\sum_t z_{1t} \\
\sum_t z_{2t} \\
\vdots \\
\sum_t z_{Nt} \\
\end{array}
\right)
\]</span> and therefore <span class="math inline">\(\hat{\delta}\)</span> is a vector of individual mean of <span class="math inline">\(z\)</span>, with typical element <span class="math inline">\(\hat{\delta}_n = \bar{z}_{n.} = \sum_t z_{nt} / T\)</span>. Therefore, applying the Frisch-Waugh theorem implies that regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span> is equivalent to regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(X\)</span> with both the response and the covariates measured in deviations from their individual means. For one observation, we have: <span class="math inline">\(y_{nt} = \beta^\top x_{nt} + \eta_n + \nu_{nt}\)</span>. Taking the individual mean of this equation, we get: <span class="math inline">\(\bar{y}_{n.} = \beta^\top \bar{x}_{n.} + \eta_n + \bar{\nu}_{n.}\)</span>. Taking deviations from the individual means, the model to estimate is then:</p>
<p><span class="math display">\[
y_{nt} - \bar{y}_{n.} = \beta ^ \top (x_{nt} - \bar{x}_{n.}) + (\nu_{nt} - \bar{\nu}_{n.})
\]</span></p>
<p>Therefore, if <span class="math inline">\(x\)</span> is correlated with <span class="math inline">\(\eta\)</span>, but not with <span class="math inline">\(\nu\)</span>, the OLS estimator of this model is consistent because the error doesn’t contain <span class="math inline">\(\eta\)</span> anymore and is therefore uncorrelated with <span class="math inline">\(x\)</span>. This model is called the fixed effects estimator and also the <strong>within</strong> estimator in the panel data literature. With a single regressor, the estimator of the unique slope is: <span id="eq-within_est"><span class="math display">\[
\hat{\beta} = \frac{\sum_{n=1} ^ N \sum_{t = 1} ^ T (y_{nt} - \bar{y}_{n.})(x_{nt} - \bar{x}_{n.})}
{\sum_{n=1} ^ N \sum_{t = 1} ^ T (x_{nt} - \bar{x}_{n.}) ^ 2}
\tag{7.12}\]</span></span></p>
<p>The deviation from individual means is obviously not the only transformation that enables to get rid of the entity effects. Consider the special case where <span class="math inline">\(T = 2\)</span> for all individuals. An interesting example of this particular case is samples of twins. In this case:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
y_{n1} = \beta x_{n1} + \eta_n + \nu_{n1} \\
y_{n2} = \beta x_{n2} + \eta_n + \nu_{n2} \\
\end{array}
\right.
\]</span> And the difference between the two equations enables to get rid of the entity effect:</p>
<p><span class="math display">\[
y_{n1} - y_{n2} = \beta(x_{n1} -x_{n2}) + \nu_{n1} - \nu_{n2}
\]</span></p>
<p>The least squares estimator of the slope is the following <strong>first-difference</strong> estimator: </p>
<p><span class="math display">\[
\hat{\beta} = \frac{\sum_n(y_{n1} - y_{n2})(x_{n1} - x_{n2})}{\sum_n(x_{n1} - x_{n2})}
\]</span> which is in this case identical to the within estimator. This result can be established by remarking that the numerator of <a href="#eq-within_est">Equation&nbsp;<span>7.12</span></a> is a sum of <span class="math inline">\(N\)</span> terms of the form:</p>
<p><span class="math display">\[
(y_{n1} - \bar{y}_{n.})(x_{n1} - \bar{x}_{n.})+(y_{n2} - \bar{y}_{n.})(x_{n2} - \bar{x}_{n.})
\]</span> But <span class="math inline">\(z_{n1} - \bar{z}_{n.} = z_{n1} - \frac{1}{2}(z_{n1} + z_{n2}) = \frac{1}{2}(z_{n1} - z_{n2})\)</span>. Similarly, <span class="math inline">\(z_{n2} - \bar{z}_{n.} = -\frac{1}{2}(z_{n1} - z_{n2})\)</span>. Therefore, each term of the numerator of <a href="#eq-within_est">Equation&nbsp;<span>7.12</span></a> reduces to <span class="math inline">\(\frac{1}{2} (y_{n1} - y_{n2})(x_{n1} - x_{n2})\)</span> and similarly, each term of the denominator reduces to <span class="math inline">\(\frac{1}{2}(x_{n1} - x_{n2}) ^ 2\)</span> which proves the equivalence between the first-difference and the within estimators. When <span class="math inline">\(T &gt; 2\)</span> the within and the first-difference estimators differ:</p>
<ul>
<li>the within estimator is more efficient, as it uses all the observations; on the contrary, while performing the first difference, the first observation for every individual is lost,</li>
<li>the error of the within estimator is <span class="math inline">\(\nu_{nt} - \bar{\nu}_{n.}\)</span> and therefore contains the whole series of <span class="math inline">\(\nu_n\)</span>; on the contrary, the error of the first difference is <span class="math inline">\(\nu_{nt} - \nu_{n(t-1)}\)</span> and therefore contains the values of <span class="math inline">\(\nu_n\)</span> only for the current and the previous period.</li>
</ul>
<p> </p>
</section><section id="application-mincer-earning-function-using-a-sample-of-twins" class="level3"><h3 class="anchored" data-anchor-id="application-mincer-earning-function-using-a-sample-of-twins">Application: Mincer earning function using a sample of twins</h3>
<p>In <a href="non_spherical.html#sec-error_component_gls"><span>Section&nbsp;6.4.3</span></a>, we’ve estimated a Mincer earning function using the <code>twins</code> data set. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">twins</span> <span class="op">&lt;-</span> <span class="va">twins</span> <span class="op">%&gt;%</span> <span class="fu">mutate</span><span class="op">(</span>age2 <span class="op">=</span> <span class="va">age</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">age</span> <span class="op">+</span> <span class="va">age2</span>, <span class="va">twins</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Estimate Std. Error t value Pr(&gt;|t|)
educ   0.0768     0.0106    7.25    2e-12
age    0.0778     0.0214    3.64  0.00031
age2  -0.0968     0.0266   -3.64  0.00031</code></pre>
</div>
</div>
<p>The estimated return of education is about 7.7%, but it may be biased if the error is correlated with education. In particular, “abilities” are unobserved and may be correlated with education. If this correlation is positive, then the OLS estimator is upward-biased. The solution here is to consider that abilities should be similar for the pair of twins, as they are genetically identical and had the same familial environment. In this case, the fixed effects model can be performed. It can be obtained either by: estimating coefficients for all family dummies, using OLS on the within transformed variables or using OLS on the differences. Note that identical twins have obviously the same age, so this covariate disappears in the fixed effects model. Let’s start with the LSDV estimator: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lsdv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">family</span><span class="op">)</span>, <span class="va">twins</span><span class="op">)</span></span>
<span><span class="va">lsdv</span> <span class="op">%&gt;%</span> <span class="va">coef</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">##     (Intercept)            educ factor(family)2 factor(family)3 </span></span>
<span><span class="co">##         1.57897         0.03935        -0.20232        -0.43463</span></span>
<span><span class="va">lsdv</span> <span class="op">%&gt;%</span> <span class="va">coef</span> <span class="op">%&gt;%</span> <span class="va">length</span></span>
<span><span class="co">## [1] 215</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The estimated return of education is much lower (3.9%) than the OLS estimator. Note that numerous parameters are estimated (215), and the computation can be unfeasible if the number of entities is very large. Whatever the size of the sample, it is simpler and more efficient to use OLS on the transformed data, either using the within or the first-difference estimator. This can easily be done using: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">twins</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">family</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">mutate</span><span class="op">(</span>learning <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span><span class="op">)</span>,</span>
<span>           educ <span class="op">=</span> <span class="va">educ</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">educ</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">learning</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">##    educ </span></span>
<span><span class="co">## 0.03935</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>or: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">twins</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">family</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">summarise</span><span class="op">(</span>learning <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">earning</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              educ <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="va">educ</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">learning</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">coef</span></span>
<span><span class="co">##    educ </span></span>
<span><span class="co">## 0.03935</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In both cases, we grouped the rows by family, i.e., there are 214 groups of two lines, one for each pair of twins. In the first case, we use <code>mutate</code> to remove the individual mean; for example, <code>mean(educ)</code> is the mean of education computed for every family and is repeated two times, and we therefore have 428 observations (one for each individual). In the second case, we use <code>summarise</code> and we therefore get 214 observations (one for each family), the response and the covariate being the twin difference of earning and education. </p>
</section><section id="application-testing-tobins-q-theory-of-investment-using-panel-data" class="level3"><h3 class="anchored" data-anchor-id="application-testing-tobins-q-theory-of-investment-using-panel-data">Application: Testing Tobin’s Q theory of investment using panel data</h3>
<p><span class="citation" data-cites="SCHA:90">Schaller (<a href="#ref-SCHA:90" role="doc-biblioref">1990</a>)</span> tested the relevance of Tobin’s Q theory of investment by regressing the investment rate (the ratio of the investment and the stock of capital) to Tobin’s Q, which is the ratio of the value of the firm and the stock of capital. This data set has already been presented in <a href="non_spherical.html#sec-error_component_gls"><span>Section&nbsp;6.4.3</span></a>, where we’ve estimated by GLS an investment equation using the Tobin’s Q theory of investment. We consider now the estimation of a fixed effects model. It is obtained using <code>plm</code> (of the <strong>plm</strong> package) by setting the <code>model</code> argument to <code>"within"</code> (which is actually the default value). </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://cran.r-project.org/package=plm">plm</a></span><span class="op">)</span></span>
<span><span class="va">qw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/plm/man/plm.html">plm</a></span><span class="op">(</span><span class="va">ikn</span> <span class="op">~</span> <span class="va">qn</span>, <span class="va">tobinq</span><span class="op">)</span></span>
<span><span class="va">qw</span> <span class="op">%&gt;%</span> <span class="va">summary</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Oneway (individual) effect Within Model

Call:
plm(formula = ikn ~ qn, data = tobinq)

Balanced Panel: n = 188, T = 35, N = 6580

Residuals:
    Min.  1st Qu.   Median  3rd Qu.     Max. 
-0.21631 -0.04525 -0.00849  0.03365  0.61844 

Coefficients:
   Estimate Std. Error t-value Pr(&gt;|t|)    
qn 0.003792   0.000173      22   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Total Sum of Squares:    36.7
Residual Sum of Squares: 34.1
R-Squared:      0.0702
Adj. R-Squared: 0.0428
F-statistic: 482.412 on 1 and 6391 DF, p-value: &lt;2e-16</code></pre>
</div>
</div>
<p>The individual effects are not estimated because the estimator uses the within transformation and then performs OLS on transformed data. However, the individual effects can be computed easily because: <span class="math inline">\(\hat{\eta}_n = \bar{y}_{n.} - \hat{\beta} ^ \top \bar{x}_{n.}\)</span>. The <code>fixef</code> method for <code>plm</code> objects retrieves the fixed effects and a <code>type</code> argument can be set to:</p>
<ul>
<li>
<code>"level"</code>: the effects are then the same as those obtained by LSDV without intercept,</li>
<li>
<code>"dfirst"</code>: only <span class="math inline">\(N - 1\)</span> effects are estimated, the first one being set to 0; these are the effects obtained by LSDV with an intercept,</li>
<li>
<code>"dmean"</code>: <span class="math inline">\(N\)</span> effects and an overall intercept are estimated, but the <span class="math inline">\(N\)</span> effects have a 0 mean. </li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">qw</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"level"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">head</span></span>
<span><span class="co">##   2824   6284   9158  13716  17372  19411 </span></span>
<span><span class="co">## 0.1453 0.1281 0.2581 0.1100 0.1267 0.1695</span></span>
<span><span class="va">qw</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"dfirst"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">head</span></span>
<span><span class="co">##     6284     9158    13716    17372    19411    19519 </span></span>
<span><span class="co">## -0.01723  0.11279 -0.03528 -0.01856  0.02420 -0.01038</span></span>
<span><span class="va">qw</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"dmean"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">head</span></span>
<span><span class="co">##      2824      6284      9158     13716     17372     19411 </span></span>
<span><span class="co">## -0.014213 -0.031448  0.098581 -0.049492 -0.032778  0.009986</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There is a <code>summary</code> method that reports the usual table of coefficients for the effects: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">qw</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"dmean"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">summary</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Estimate Std. Error t-value  Pr(&gt;|t|)
2824 -0.01421    0.01240  -1.146 2.519e-01
6284 -0.03145    0.01234  -2.548 1.087e-02
9158  0.09858    0.01235   7.984 1.673e-15</code></pre>
</div>
</div>
</section></section><section id="sec-tests_endog" class="level2" data-number="7.6"><h2 data-number="7.6" class="anchored" data-anchor-id="sec-tests_endog">
<span class="header-section-number">7.6</span> Specification tests</h2>
<section id="hausman-test" class="level3"><h3 class="anchored" data-anchor-id="hausman-test">Hausman test</h3>
<p></p>
<p>Models estimated in this chapter, either using the IV or the fixed effects estimator, treat the endogeneity of some covariates, either by using instruments or by removing individual effects. These estimates are consistent if some covariates are actually endogenous, although other estimators like OLS or GLS are not. However, if endogeneity is actually not a problem, these later estimators are also consistent and are moreover more efficient. The Hausman test is based on the comparison of these two estimators and on the null hypothesis of the absence of endogeneity:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_0\)</span>, with variance <span class="math inline">\(\hat{V}_0\)</span> is only consistent if the hypothesis is true and is in this case efficient,</li>
<li>
<span class="math inline">\(\hat{\beta}_1\)</span>, with variance <span class="math inline">\(\hat{V}_1\)</span> is always consistent, but is less efficient than <span class="math inline">\(\hat{\beta}_0\)</span> if the hypothesis is true.</li>
</ul>
<p>Hausman’s test <span class="citation" data-cites="HAUS:78">(<a href="#ref-HAUS:78" role="doc-biblioref">Hausman 1978</a>)</span> test is based on <span class="math inline">\(\hat{q} = \hat{\beta}_1 - \hat{\beta}_0\)</span>, for which the variance is: <span id="eq-var_difference"><span class="math display">\[
\mbox{V}(\hat{q}) =
\mbox{V}(\hat{\beta}_1) + \mbox{V}(\hat{\beta}_0) - 2
\mbox{cov}(\hat{\beta}_1, \hat{\beta}_0)
\tag{7.13}\]</span></span></p>
<p>Hausman showed that the covariance between <span class="math inline">\(\hat{q}\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span> is 0. Therefore:</p>
<p><span class="math display">\[
\mbox{cov}(\hat{\beta}_1 - \hat{\beta}_0, \hat{\beta}_0) =
\mbox{cov}(\hat{\beta}_1, \hat{\beta}_0) + \mbox{V}(\hat{\beta}_0) = 0
\]</span></p>
<p>and therefore <a href="#eq-var_difference">Equation&nbsp;<span>7.13</span></a> simplifies to: <span class="math inline">\(\mbox{V}(\hat{q}) = \mbox{V}(\hat{\beta}_1) - \mbox{V}(\hat{\beta}_0)\)</span>. The asymptotic distribution of the difference of the two vectors of estimates is, under H<sub>0</sub>:</p>
<p><span class="math display">\[
\hat{\beta}_1 - \hat{\beta}_0 \overset{a}{\sim} \mathcal{N} \left(0, \mbox{V}(\hat{\beta}_1) - \mbox{V}(\hat{\beta}_0)\right)
\]</span></p>
<p>and therefore <span class="math inline">\((\hat{\beta}_1 - \hat{\beta}_0) ^ \top \left(\mbox{V}(\hat{\beta}_1) - \mbox{V}(\hat{\beta}_0)\right) ^ {-1} (\hat{\beta}_1 - \hat{\beta}_0)\)</span> is a <span class="math inline">\(\chi ^ 2\)</span> with degrees of freedom equal to the number of estimated parameters. Note that the length of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> may be different. This is the case for panel data with individual specific covariates which are estimated using OLS or GLS but which are not with the within estimator. In this case, the test should be performed on the subset of common parameters. For tests involving OLS and IV estimator, the test can also be performed only on the subset of parameters associated with covariates that are suspected to be endogenous. </p>
</section><section id="weak-instruments" class="level3"><h3 class="anchored" data-anchor-id="weak-instruments">Weak instruments</h3>
<p>Instruments should be not only uncorrelated with the error of the model, but they should also be correlated with the covariates. We have seen in <a href="#sec-ssprop_iv"><span>Section&nbsp;7.2.2</span></a> that the expected value of the IV estimator doesn’t exist when there is only one instrument. It exists if the number of instruments is at least 2 and in this case, it can be shown that the bias of the IV estimator is approximately inversely proportional to the F statistic of the regression of the endogenous variable on the instruments.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Therefore, if the correlation between the endogenous variable and the instruments is weak, i.e., if the IV is performed using <strong>weak instruments</strong>, the estimator will not only be highly imprecise, but it will also be seriously biased, in the direction of the OLS estimator. While performing IV estimation it is therefore important to check that the instruments are sufficiently correlated with the endogenous covariate. This can be performed using an F test for the first stage regression, comparing the fit for the regression of the endogenous covariates on the set of the exogenous covariates, on the set of the exogenous covariates, and on the external instruments. A rule of thumb often used is that the F statistic should be at least equal to 10 (a less strict rule is <span class="math inline">\(F &gt; 5\)</span>). <!-- to unsure that the maximal bias of the IV estimator is no --> <!-- more than 10% that of OLS.  --> </p>
</section><section id="sargan-test" class="level3"><h3 class="anchored" data-anchor-id="sargan-test">Sargan test</h3>
<p></p>
<p>The IV can be obtained as a moment estimator, using moment conditions <span class="math inline">\(m = W ^ \top \epsilon / N\)</span>. If the instruments are relevant, they are uncorrelated with the errors, so that <span class="math inline">\(m \overset{a}{\sim} \mathcal{N}(0, V)\)</span> and <span class="math inline">\(S = m ^ \top V ^ {-1} m\)</span> is a <span class="math inline">\(\chi^2\)</span> with a degree of freedom equal to the number of instruments. In the just-identified case, <span class="math inline">\(m=0\)</span>, but in the over-identified case, this statistic is positive and the hypothesis of orthogonal instruments implies that <span class="math inline">\(m\)</span> is close enough to a vector of 0 or that <span class="math inline">\(S\)</span> is sufficiently small.</p>
</section><section id="individual-effects" class="level3"><h3 class="anchored" data-anchor-id="individual-effects">Individual effects</h3>
<p></p>
<p>In the fixed effects model, the absence of individual effects can be tested using a standard <span class="math inline">\(F\)</span> test that all the estimated individual effects are zero. More simply, it can be obtained by comparing the sum of squares residuals of the OLS and the fixed effects models.</p>
<p><span class="math display">\[
\frac{SCR_{\mbox{OLS}} - SCR_{\mbox{FE}}}{SCR_{\mbox{FE}}} \times \frac{N (T - 1) - K }{N -
1} \sim F_{N-1, N (T - 1) - K}
\]</span></p>
</section><section id="panel-application-testing-tobins-q-theory-of-investment" class="level3"><h3 class="anchored" data-anchor-id="panel-application-testing-tobins-q-theory-of-investment">Panel application: Testing Tobin’s Q theory of investment</h3>
<p>The presence of individual effects was already tested using a score test based on the OLS residuals. We compute here an F test, using <code><a href="https://rdrr.io/pkg/plm/man/pFtest.html">plm::pFtest</a></code>: </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/pFtest.html">pFtest</a></span><span class="op">(</span><span class="va">ikn</span> <span class="op">~</span> <span class="va">qn</span>, <span class="va">tobinq</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## F = 14.322, df: 187-6391, pval = 0.000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and the hypothesis is strongly rejected. Once we have concluded that there are individual effects, the question is whether these effects are correlated with the covariates or not. If this is the case, the fixed effects model should be used because the GLS model is inconsistent. On the contrary, both models are consistent, and the GLS estimator, which is more efficient, should be used. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/plm/man/phtest.html">phtest</a></span><span class="op">(</span><span class="va">ikn</span> <span class="op">~</span> <span class="va">qn</span>, <span class="va">tobinq</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">gaze</span></span>
<span><span class="co">## chisq = 3.304, df: 1, pval = 0.069</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The hypothesis of uncorrelated individual effects is not rejected at the 5%, which leads to the choice of the GLS estimator. </p>
</section><section id="sec-test_sltr" class="level3"><h3 class="anchored" data-anchor-id="sec-test_sltr">Instrumental variable application: slave trade</h3>
<p>The IV estimator for the <code>slave_trade</code> data set was presented in <a href="#sec-test_sltr"><span>Section&nbsp;7.6.6</span></a>. We use this time the <code><a href="https://zeileis.github.io/ivreg/reference/ivreg.html">ivreg::ivreg</a></code> functions, which computes all the relevant specification tests. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sltd_iv</span> <span class="op">&lt;-</span> <span class="fu">ivreg</span><span class="fu">::</span><span class="fu"><a href="https://zeileis.github.io/ivreg/reference/ivreg.html">ivreg</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">gdp</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">slavesarea</span><span class="op">)</span> <span class="op">+</span> <span class="va">colony</span> <span class="op">|</span> </span>
<span>                          <span class="va">colony</span> <span class="op">+</span> <span class="va">redsea</span> <span class="op">+</span> <span class="va">atlantic</span> <span class="op">+</span> <span class="va">sahara</span> <span class="op">+</span> <span class="va">indian</span>, </span>
<span>                        data <span class="op">=</span> <span class="va">sltd</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">sltd_iv</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
ivreg::ivreg(formula = log(gdp) ~ log(slavesarea) + colony | 
    colony + redsea + atlantic + sahara + indian, data = sltd)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9262 -0.4487  0.0698  0.4580  1.3327 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       7.8498     0.2238   35.08   &lt;2e-16 ***
log(slavesarea)  -0.1960     0.0461   -4.25   0.0001 ***
colonyfrance     -0.0101     0.2281   -0.04   0.9649    
colonyportugal   -0.1119     0.3581   -0.31   0.7561    
colonybelgium    -1.3941     0.4479   -3.11   0.0032 ** 
colonyOther       0.1865     0.3615    0.52   0.6084    

Diagnostic tests:
                 df1 df2 statistic p-value   
Weak instruments   4  43      4.89  0.0024 **
Wu-Hausman         1  45      4.76  0.0344 * 
Sargan             3  NA      3.63  0.3042   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.707 on 46 degrees of freedom
Multiple R-Squared: 0.338,  Adjusted R-squared: 0.266 
Wald test: 5.34 on 5 and 46 DF,  p-value: 0.000592 </code></pre>
</div>
</div>
<p>The F statistic for the first stage regression is only <span class="math inline">\(4.89\)</span>, so that the instruments can be considered as weak and we can suspect the IV estimator to be severely biased. Anyway, remind that the bias is in the direction of the OLS estimator so that the effect of slave trades on current GDP would be underestimated. The p-value for the Hausman test is <span class="math inline">\(0.03\)</span> so that the exogeneity hypothesis of the covariate is rejected at the 5% level, but not at the 1% level. Finally, as there are 4 instruments, the Sargan statistic, which is the quadratic form of the 4 moment conditions with the inverse of its variance is <span class="math inline">\(3.63\)</span> and is a <span class="math inline">\(\chi ^ 2\)</span> with 3 degrees of freedom if the instruments are valid. This hypothesis is not rejected, even at the 10% level. </p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-ANAN:11" class="csl-entry" role="doc-biblioentry">
Ananat, Elizabeth Oltmans. 2011. <span>“The Wrong Side(s) of the Tracks: The Causal Effects of Racial Segregation on Urban Poverty and Inequality.”</span> <em>American Economic Journal: Applied Economics</em> 3 (2): 34–66. <a href="https://doi.org/10.1257/app.3.2.34">https://doi.org/10.1257/app.3.2.34</a>.
</div>
<div id="ref-ANGR:90" class="csl-entry" role="doc-biblioentry">
Angrist, Joshua D. 1990. <span>“Lifetime Earnings and the Vietnam Era Draft Lottery: Evidence from Social Security Administrative Records.”</span> <em>The American Economic Review</em> 80 (3): 313–36. <a href="http://www.jstor.org/stable/2006669">http://www.jstor.org/stable/2006669</a>.
</div>
<div id="ref-BOUN:JAEG:BAKE:95" class="csl-entry" role="doc-biblioentry">
Bound, John, David A. Jaeger, and Regina M. Baker. 1995. <span>“Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogeneous Explanatory Variable Is Weak.”</span> <em>Journal of the American Statistical Association</em> 90 (430): 443–50. <a href="http://www.jstor.org/stable/2291055">http://www.jstor.org/stable/2291055</a>.
</div>
<div id="ref-CAME:TRIV:05" class="csl-entry" role="doc-biblioentry">
Cameron, A. Colin, and Pravin K. Trivedi. 2005. <em>Microeconometrics</em>. Cambridge University Press. <a href="https://EconPapers.repec.org/RePEc:cup:cbooks:9780521848053">https://EconPapers.repec.org/RePEc:cup:cbooks:9780521848053</a>.
</div>
<div id="ref-DAVI:MACK:04" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, and James G. MacKinnon. 2004. <em>Econometric Theory and Methods</em>. Oxford University Press.
</div>
<div id="ref-HAUS:78" class="csl-entry" role="doc-biblioentry">
Hausman, Jerry. 1978. <span>“Specification Tests in Econometrics.”</span> <em>Econometrica</em> 46 (6): 1251–71. <a href="http://www.jstor.org/stable/1913827">http://www.jstor.org/stable/1913827</a>.
</div>
<div id="ref-HECK:00" class="csl-entry" role="doc-biblioentry">
Heckman, James J. 2000. <span>“Causal Parameters and Policy Analysis in Economics: A Twentieth Century Retrospective.”</span> <em>The Quarterly Journal of Economics</em> 115 (1): 45–97. <a href="http://www.jstor.org/stable/2586935">http://www.jstor.org/stable/2586935</a>.
</div>
<div id="ref-KOOP:POIR:TOBI:05" class="csl-entry" role="doc-biblioentry">
Koop, Gary, Dale J. Poirier, and Justin Tobias. 2005. <span>“Semiparametric Bayesian Inference in Multiple Equation Models.”</span> <em>Journal of Applied Econometrics</em> 20 (6): 723–47. <a href="https://doi.org/10.1002/jae.810">https://doi.org/10.1002/jae.810</a>.
</div>
<div id="ref-MADD:LAHI:09" class="csl-entry" role="doc-biblioentry">
Maddala, G. S., and Kajal Lahiri. 2009. <em>Introduction to Econometrics</em>. 4th ed. Wiley.
</div>
<div id="ref-NUNN:08" class="csl-entry" role="doc-biblioentry">
Nunn, Nathan. 2008. <span>“The Long-Term Effects of Africa’s Slave Trades.”</span> <em>The Quarterly Journal of Economics</em> 123 (1): 139–76. <a href="https://EconPapers.repec.org/RePEc:oup:qjecon:v:123:y:2008:i:1:p:139-176.">https://EconPapers.repec.org/RePEc:oup:qjecon:v:123:y:2008:i:1:p:139-176.</a>
</div>
<div id="ref-SCHA:90" class="csl-entry" role="doc-biblioentry">
Schaller, Huntley. 1990. <span>“A Re-Examination of the q Theory of Investment Using u.s. Firm Data.”</span> <em>Journal of Applied Econometrics</em> 5 (4): 309–25. <a href="http://www.jstor.org/stable/2096476">http://www.jstor.org/stable/2096476</a>.
</div>
<div id="ref-STEW:19" class="csl-entry" role="doc-biblioentry">
Stewart, Kenneth. 2019. <span>“Suits’ Watermelon Model: The Missing Simultaneous Equations Empirical Application.”</span> <em>Journal of Economics Teaching</em> 4 (2): 115–39. <a href="https://EconPapers.repec.org/RePEc:jtc:journl:v:4:y:2019:i:2:p:115-139">https://EconPapers.repec.org/RePEc:jtc:journl:v:4:y:2019:i:2:p:115-139</a>.
</div>
<div id="ref-STOC:WATS:15" class="csl-entry" role="doc-biblioentry">
Stock, James H., and Mark W. Watson. 2015. <em>Introduction to Econometrics</em>. Pearson.
</div>
<div id="ref-SUIT:55" class="csl-entry" role="doc-biblioentry">
Suits, Daniel B. 1955. <span>“An Econometric Model of the Watermelon Market.”</span> <em>American Journal of Agricultural Economics</em> 37 (2): 237–51. <a href="https://doi.org/10.2307/1233923">https://doi.org/10.2307/1233923</a>.
</div>
<div id="ref-WOLD:58" class="csl-entry" role="doc-biblioentry">
Wold, H. O. A. 1958. <span>“A Case Study of Interdependent Versus Causal Chain Systems.”</span> <em>Revue de l’Institut International de Statistique / Review of the International Statistical Institute</em> 26 (1/3): 5–25. <a href="http://www.jstor.org/stable/1401568">http://www.jstor.org/stable/1401568</a>.
</div>
<div id="ref-ZELL:THEI:62" class="csl-entry" role="doc-biblioentry">
Zellner, Arnold, and H. Theil. 1962. <span>“Three-Stage Least Squares: Simultaneous Estimation of Simultaneous Equations.”</span> <em>Econometrica</em> 30 (1): 54–78. <a href="http://www.jstor.org/stable/1911287">http://www.jstor.org/stable/1911287</a>.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>The terminology used to distinguish variables that are uncorrelated (exogenous) or correlated (endogenous) with the error of the model comes from the literature on system of equations estimation where variables determined within and outside the model are respectively called endogenous and exogenous variables <span class="citation" data-cites="STOC:WATS:15">(see <a href="#ref-STOC:WATS:15" role="doc-biblioref">Stock and Watson 2015, 423</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Needless to say, the hypothesis that abilities in all their dimensions can be measured by the AFQT test is very strong.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See <span class="citation" data-cites="HECK:00">Heckman (<a href="#ref-HECK:00" role="doc-biblioref">2000</a>)</span>, page 58 and <span class="citation" data-cites="CAME:TRIV:05">Cameron and Trivedi (<a href="#ref-CAME:TRIV:05" role="doc-biblioref">2005</a>)</span>, page 98.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See <span class="citation" data-cites="DAVI:MACK:04">Davidson and MacKinnon (<a href="#ref-DAVI:MACK:04" role="doc-biblioref">2004</a>)</span>, pages 326-327.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><code>lm.fit</code> is used internally by <code>lm</code>, and its two first arguments are a matrix of covariates and the response.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>See <span class="citation" data-cites="ANGR:90">Angrist (<a href="#ref-ANGR:90" role="doc-biblioref">1990</a>)</span>, note 7, pp.&nbsp;321-322.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="citation" data-cites="ANGR:90">Angrist (<a href="#ref-ANGR:90" role="doc-biblioref">1990</a>)</span>, p.&nbsp;313.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><span class="citation" data-cites="CAME:TRIV:05">Cameron and Trivedi (<a href="#ref-CAME:TRIV:05" role="doc-biblioref">2005</a>)</span>, page 187.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>We’ve already presented briefly this package in <a href="multiple_regression.html#sec-sys_eq_ols"><span>Section&nbsp;3.7.1</span></a>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><span class="citation" data-cites="ZELL:THEI:62">Zellner and Theil (<a href="#ref-ZELL:THEI:62" role="doc-biblioref">1962</a>)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>As for the SUR model estimated in <a href="non_spherical.html#sec-sur"><span>Section&nbsp;6.4.4</span></a>, we set the <code>methodResidCov</code> argument to <code>"noDfCor"</code>, so that no degrees of freedom correction is used for the estimation of the covariance matrix of the errors.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>See <span class="citation" data-cites="BOUN:JAEG:BAKE:95">Bound, Jaeger, and Baker (<a href="#ref-BOUN:JAEG:BAKE:95" role="doc-biblioref">1995</a>)</span> and <span class="citation" data-cites="CAME:TRIV:05">Cameron and Trivedi (<a href="#ref-CAME:TRIV:05" role="doc-biblioref">2005</a>)</span> pages 108-109 for a discussion and further references.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../chapters/non_spherical.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Non-spherical disturbances</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/treateff.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Treatment effect</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb72" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Endogeneity {#sec-endogeneity}</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setup_endogeneity</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../_commonR.R"</span>)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>The unbiasedness and the consistency of the OLS estimator rest on the</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>hypothesis that the conditional expectation of the error is constant</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>(and can safely be set to zero if the model contains an</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>intercept). Namely, starting with the simple linear model: $y_n = \alpha + \beta x_n + \epsilon_n$, $\mbox{E}(\epsilon \mid x) = 0$, or equivalently $\mbox{E}(y \mid x) = \alpha + \beta x_n$. The same property can also be described using the covariance between the covariate and the error that can be written, using the rule of repeated expectation:</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(x, \epsilon) = \mbox{E}\left((x - \mu_x)\epsilon\right) = </span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>\mbox{E}_x\left[\mbox{E}_\epsilon\left((x - \mu_x)\epsilon\mid</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>x\right)\right]</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>= \mbox{E}_x\left[(x - \mu_x)\mbox{E}_\epsilon\left(\epsilon\mid</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>x\right)\right]</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>If the conditional expectation of $\epsilon$ is a constant</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>$\mbox{E}_\epsilon(\epsilon\mid x) = \mu_\epsilon$ (not necessarily 0),</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>the covariance is $\mbox{cov}(x, \epsilon) = \mu_\epsilon \mbox{E}_x(x -</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a>\mu_x) = 0$. Stated differently, $x$ is supposed to be exogenous, or $x$ is</span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>assumed to be uncorrelated with $\epsilon$. This is a reasonable</span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a>assumption in an experimental setting, when the values of $x$ in the</span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a>sample are set by the researcher. Unfortunately, most of the data used in microeconometrics are not experimental, so the problem of endogeneity is severe.^<span class="co">[</span><span class="ot">The terminology used to distinguish variables that are uncorrelated (exogenous) or correlated (endogenous) with the error of the model comes from the literature on system of equations estimation where variables determined within and outside the model are respectively called endogenous and exogenous variables [see @STOC:WATS:15, p. 423].</span><span class="co">]</span></span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a>@sec-source_endog presents the circumstances where some covariates are endogenous. @sec-simple_iv presents the simple instrumental variable estimator (one endogenous covariate and one instrument), @sec-general_iv extends this model to the case where there may be more than one endogenous covariate and several instruments and @sec-three_sls to the estimation of systems of equations. @sec-fixed_effects presents the fixed effects model. Finally, several testing procedures are presented in @sec-tests_endog.</span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sources of endogeneity {#sec-source_endog}</span></span>
<span id="cb72-33"><a href="#cb72-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-34"><a href="#cb72-34" aria-hidden="true" tabindex="-1"></a>In economics, the problem of endogenous covariates happens mainly in three circumstances: errors in variables, omitted variables and simultaneity.</span>
<span id="cb72-35"><a href="#cb72-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-36"><a href="#cb72-36" aria-hidden="true" tabindex="-1"></a><span class="fu">### Errors in variables</span></span>
<span id="cb72-37"><a href="#cb72-37" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{errors in variables|(}</span>
<span id="cb72-38"><a href="#cb72-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-39"><a href="#cb72-39" aria-hidden="true" tabindex="-1"></a>Data used in economics, especially micro-data, are prone to errors</span>
<span id="cb72-40"><a href="#cb72-40" aria-hidden="true" tabindex="-1"></a>of measurement. This problem can affect either the response or some of</span>
<span id="cb72-41"><a href="#cb72-41" aria-hidden="true" tabindex="-1"></a>the covariates. Suppose that the model that we seek to estimate is:</span>
<span id="cb72-42"><a href="#cb72-42" aria-hidden="true" tabindex="-1"></a>$y^*_n = \alpha + \beta x^*_n + \epsilon^*_n$, where the covariate is exogenous, which implies that $\mbox{cov}(x^*,</span>
<span id="cb72-43"><a href="#cb72-43" aria-hidden="true" tabindex="-1"></a>\epsilon^*)=0$. Suppose that the response is observed with error,</span>
<span id="cb72-44"><a href="#cb72-44" aria-hidden="true" tabindex="-1"></a>namely that the observed value of the response is $y_n = y ^*_n +</span>
<span id="cb72-45"><a href="#cb72-45" aria-hidden="true" tabindex="-1"></a>\nu_n$, where $\nu_n$ is the measurement error of the response. In</span>
<span id="cb72-46"><a href="#cb72-46" aria-hidden="true" tabindex="-1"></a>terms of the observed response, the model is now:</span>
<span id="cb72-47"><a href="#cb72-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-48"><a href="#cb72-48" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-49"><a href="#cb72-49" aria-hidden="true" tabindex="-1"></a>y_n = \alpha + \beta x_n ^ * + (\epsilon^*_n + \nu_n)</span>
<span id="cb72-50"><a href="#cb72-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-51"><a href="#cb72-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-52"><a href="#cb72-52" aria-hidden="true" tabindex="-1"></a>The error of the estimable model is then $\epsilon_n = \epsilon^*_n +</span>
<span id="cb72-53"><a href="#cb72-53" aria-hidden="true" tabindex="-1"></a>\nu_n$ which is still uncorrelated with $x$ if $\nu$ is uncorrelated</span>
<span id="cb72-54"><a href="#cb72-54" aria-hidden="true" tabindex="-1"></a>with $x$, which means that the error of measurement of the response is</span>
<span id="cb72-55"><a href="#cb72-55" aria-hidden="true" tabindex="-1"></a>uncorrelated with the covariate. In this case, the measurement error</span>
<span id="cb72-56"><a href="#cb72-56" aria-hidden="true" tabindex="-1"></a>only increases the size of the error, which implies that the</span>
<span id="cb72-57"><a href="#cb72-57" aria-hidden="true" tabindex="-1"></a>coefficients are estimated less precisely and that the R^2^ is lower</span>
<span id="cb72-58"><a href="#cb72-58" aria-hidden="true" tabindex="-1"></a>compared to a model with a correctly measured response. </span>
<span id="cb72-59"><a href="#cb72-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-60"><a href="#cb72-60" aria-hidden="true" tabindex="-1"></a>Now consider that the covariate is measured with error and that the</span>
<span id="cb72-61"><a href="#cb72-61" aria-hidden="true" tabindex="-1"></a>observable values of the covariate is $x_n = x_n ^ * + \nu_n$. If the</span>
<span id="cb72-62"><a href="#cb72-62" aria-hidden="true" tabindex="-1"></a>measurement error is uncorrelated with the value of the covariate, the</span>
<span id="cb72-63"><a href="#cb72-63" aria-hidden="true" tabindex="-1"></a>variance of the observed covariate is therefore $\sigma_x ^ 2 =</span>
<span id="cb72-64"><a href="#cb72-64" aria-hidden="true" tabindex="-1"></a>\sigma_x ^ {*2} + \sigma_\nu ^ 2$. Moreover, the covariance between the observed covariate and the</span>
<span id="cb72-65"><a href="#cb72-65" aria-hidden="true" tabindex="-1"></a>measurement error is equal to the variance of the measurement error:</span>
<span id="cb72-66"><a href="#cb72-66" aria-hidden="true" tabindex="-1"></a>$\sigma_{x\nu} = \mbox{E} \left((x ^ * + \nu - \mu_x)</span>
<span id="cb72-67"><a href="#cb72-67" aria-hidden="true" tabindex="-1"></a>\nu\right) = \sigma_\nu ^ 2$, </span>
<span id="cb72-68"><a href="#cb72-68" aria-hidden="true" tabindex="-1"></a>because the measurement error is uncorrelated with the covariate.</span>
<span id="cb72-69"><a href="#cb72-69" aria-hidden="true" tabindex="-1"></a>Rewriting the model in terms of $x$, we get:</span>
<span id="cb72-70"><a href="#cb72-70" aria-hidden="true" tabindex="-1"></a>$y_n = \alpha + \beta x_n + \epsilon_n$, with $\epsilon_n = \epsilon_n  ^ *- \beta \nu_n$.</span>
<span id="cb72-71"><a href="#cb72-71" aria-hidden="true" tabindex="-1"></a>The error of this model is now correlated with $x$, as $\mbox{cov}(x, \epsilon_n) = \mbox{cov}(x^* + \nu,\epsilon ^ * - \beta \nu) = - \beta \sigma_\nu ^ 2$. The OLS estimator can be written as usual as:</span>
<span id="cb72-72"><a href="#cb72-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-73"><a href="#cb72-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-74"><a href="#cb72-74" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = </span>
<span id="cb72-75"><a href="#cb72-75" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n (x_n - \bar{x})(y_n - \bar{y})}{\sum_n (x_n - \bar{x}) ^</span>
<span id="cb72-76"><a href="#cb72-76" aria-hidden="true" tabindex="-1"></a>2} =</span>
<span id="cb72-77"><a href="#cb72-77" aria-hidden="true" tabindex="-1"></a>\beta + \frac{\sum_n (x_n - \bar{x})\epsilon_n}{\sum_n (x_n -</span>
<span id="cb72-78"><a href="#cb72-78" aria-hidden="true" tabindex="-1"></a>\bar{x}) ^ 2}</span>
<span id="cb72-79"><a href="#cb72-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-80"><a href="#cb72-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-81"><a href="#cb72-81" aria-hidden="true" tabindex="-1"></a>Taking the expectations, we have $\mbox{E}\left[(x - \bar{x})</span>
<span id="cb72-82"><a href="#cb72-82" aria-hidden="true" tabindex="-1"></a>\epsilon\right] = -\beta \sigma_\nu ^ 2$ and the expected value of the</span>
<span id="cb72-83"><a href="#cb72-83" aria-hidden="true" tabindex="-1"></a>estimator is then:</span>
<span id="cb72-84"><a href="#cb72-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-85"><a href="#cb72-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-86"><a href="#cb72-86" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\beta}) = \beta\left(1 - \frac{\sigma_\nu ^ 2}{\sum (x_n</span>
<span id="cb72-87"><a href="#cb72-87" aria-hidden="true" tabindex="-1"></a>-\bar{x}) ^ 2 / N}\right)</span>
<span id="cb72-88"><a href="#cb72-88" aria-hidden="true" tabindex="-1"></a>= \beta\left(1 - \frac{\sigma_\nu ^ 2}{\hat{\sigma}_x ^ 2}\right)</span>
<span id="cb72-89"><a href="#cb72-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-90"><a href="#cb72-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-91"><a href="#cb72-91" aria-hidden="true" tabindex="-1"></a>Therefore, the OLS estimator is biased and the term in brackets is the minus the share of the variance of $x$ that is due to measurement errors.Therefore</span>
<span id="cb72-92"><a href="#cb72-92" aria-hidden="true" tabindex="-1"></a>$\mid\hat{\beta}\mid &lt; \beta$. This kind of bias is called an</span>
<span id="cb72-93"><a href="#cb72-93" aria-hidden="true" tabindex="-1"></a>**attenuation bias** (the absolute value of the estimator is lower</span>
<span id="cb72-94"><a href="#cb72-94" aria-hidden="true" tabindex="-1"></a>than the true value), which can be either a lower or an upper bias</span>
<span id="cb72-95"><a href="#cb72-95" aria-hidden="true" tabindex="-1"></a>depending on the sign of $\beta$.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{attenuation bias}\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!errors in variables}</span>
<span id="cb72-96"><a href="#cb72-96" aria-hidden="true" tabindex="-1"></a>This bias clearly doesn't attenuate in large samples. As $N$ grows, the</span>
<span id="cb72-97"><a href="#cb72-97" aria-hidden="true" tabindex="-1"></a>empirical variances/covariances converge to the population ones, and</span>
<span id="cb72-98"><a href="#cb72-98" aria-hidden="true" tabindex="-1"></a>the estimator therefore converges to: $\mbox{plim} \;\hat{\beta} = \beta \left(1 - \sigma_\nu ^ 2 / \sigma_x ^ 2\right)$. For example, if the measurement error accounts for 20% of the total</span>
<span id="cb72-99"><a href="#cb72-99" aria-hidden="true" tabindex="-1"></a>variance of $x$, $\hat{\beta}$ converges to 80% of the true parameter.</span>
<span id="cb72-100"><a href="#cb72-100" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{errors in variables|)}</span>
<span id="cb72-101"><a href="#cb72-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-102"><a href="#cb72-102" aria-hidden="true" tabindex="-1"></a><span class="fu">### Omitted variable bias</span></span>
<span id="cb72-103"><a href="#cb72-103" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{omitted variable bias|(}</span>
<span id="cb72-104"><a href="#cb72-104" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!omitted variable|(}</span>
<span id="cb72-105"><a href="#cb72-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-106"><a href="#cb72-106" aria-hidden="true" tabindex="-1"></a>Suppose that the true model is: $y_n = \alpha + \beta_x x_n + \beta_z</span>
<span id="cb72-107"><a href="#cb72-107" aria-hidden="true" tabindex="-1"></a>z_n + \epsilon_n$, where the conditional expectation of $\epsilon$</span>
<span id="cb72-108"><a href="#cb72-108" aria-hidden="true" tabindex="-1"></a>with respect to $x$ and $z$ is 0. Therefore, this model can be</span>
<span id="cb72-109"><a href="#cb72-109" aria-hidden="true" tabindex="-1"></a>consistently estimated by least squares. Consider now that $z$ is</span>
<span id="cb72-110"><a href="#cb72-110" aria-hidden="true" tabindex="-1"></a>unobserved. Therefore, the model to be estimated is $y_n = \alpha +</span>
<span id="cb72-111"><a href="#cb72-111" aria-hidden="true" tabindex="-1"></a>\beta_x x_n + \eta_n$, with $\eta_n = \beta_z z_n + \epsilon_n$. The omission of</span>
<span id="cb72-112"><a href="#cb72-112" aria-hidden="true" tabindex="-1"></a>a relevant ($\beta_z \neq 0$) covariate has two consequences:</span>
<span id="cb72-113"><a href="#cb72-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-114"><a href="#cb72-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the variance of the error is now $\sigma_\eta ^ 2 = \beta_z ^ 2</span>
<span id="cb72-115"><a href="#cb72-115" aria-hidden="true" tabindex="-1"></a>\sigma_z^2 + \sigma_\epsilon^2$, and is therefore greater than the one</span>
<span id="cb72-116"><a href="#cb72-116" aria-hidden="true" tabindex="-1"></a>of the initial model for which $z$ is observed and used as a</span>
<span id="cb72-117"><a href="#cb72-117" aria-hidden="true" tabindex="-1"></a>covariate,</span>
<span id="cb72-118"><a href="#cb72-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the covariance between the error and $x$ is $\mbox{cov}(x, \eta) = \beta_z</span>
<span id="cb72-119"><a href="#cb72-119" aria-hidden="true" tabindex="-1"></a>  \mbox{cov}(x, z)$; therefore, if the covariate is correlated with the</span>
<span id="cb72-120"><a href="#cb72-120" aria-hidden="true" tabindex="-1"></a>  omitted variable, the covariate and the error of the model are</span>
<span id="cb72-121"><a href="#cb72-121" aria-hidden="true" tabindex="-1"></a>  correlated.</span>
<span id="cb72-122"><a href="#cb72-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-123"><a href="#cb72-123" aria-hidden="true" tabindex="-1"></a>As the variance of the OLS estimator is proportional to the variance of</span>
<span id="cb72-124"><a href="#cb72-124" aria-hidden="true" tabindex="-1"></a>the errors, omission of a relevant covariate will always induce a less</span>
<span id="cb72-125"><a href="#cb72-125" aria-hidden="true" tabindex="-1"></a>precise estimation of the slopes and a lower R^2^. Moreover, if the omitted covariate is correlated with the covariate used in the</span>
<span id="cb72-126"><a href="#cb72-126" aria-hidden="true" tabindex="-1"></a>regression, the estimation will be biased and inconsistent. This</span>
<span id="cb72-127"><a href="#cb72-127" aria-hidden="true" tabindex="-1"></a>**omitted variable bias** can be computed as follows:</span>
<span id="cb72-128"><a href="#cb72-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-129"><a href="#cb72-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-130"><a href="#cb72-130" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_x = \beta_x + \frac{\sum_n (x_n - \bar{x})(\beta_z z_n +</span>
<span id="cb72-131"><a href="#cb72-131" aria-hidden="true" tabindex="-1"></a>\epsilon_n)}{\sum_n (x_n - \bar{x}) ^ 2}=</span>
<span id="cb72-132"><a href="#cb72-132" aria-hidden="true" tabindex="-1"></a>\beta_x + \beta_z \frac{\sum_n (x_n - \bar{x}) (z_n - \bar{z})}{\sum_n</span>
<span id="cb72-133"><a href="#cb72-133" aria-hidden="true" tabindex="-1"></a>(x_n - \bar{x}) ^ 2} + </span>
<span id="cb72-134"><a href="#cb72-134" aria-hidden="true" tabindex="-1"></a>\frac{\sum_n (x_n - \bar{x}) \epsilon_n}{\sum_n (x_n - \bar{x}) ^ 2}</span>
<span id="cb72-135"><a href="#cb72-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-136"><a href="#cb72-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-137"><a href="#cb72-137" aria-hidden="true" tabindex="-1"></a>Taking the conditional expectation, the last term disappears, so that:</span>
<span id="cb72-138"><a href="#cb72-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-139"><a href="#cb72-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-140"><a href="#cb72-140" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\beta}_x\mid x, z) = \beta_x + \beta_z</span>
<span id="cb72-141"><a href="#cb72-141" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{xz}}{\hat{\sigma}_x ^ 2}</span>
<span id="cb72-142"><a href="#cb72-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-143"><a href="#cb72-143" aria-hidden="true" tabindex="-1"></a>There is an upper bias if the signs of the covariance between $x$ and</span>
<span id="cb72-144"><a href="#cb72-144" aria-hidden="true" tabindex="-1"></a>$z$ and $\beta_z$ are the same, and a lower bias if they have opposite</span>
<span id="cb72-145"><a href="#cb72-145" aria-hidden="true" tabindex="-1"></a>signs.</span>
<span id="cb72-146"><a href="#cb72-146" aria-hidden="true" tabindex="-1"></a>As $N$ tends to infinity, the OLS estimator converges to:</span>
<span id="cb72-147"><a href="#cb72-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-148"><a href="#cb72-148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-149"><a href="#cb72-149" aria-hidden="true" tabindex="-1"></a>\mbox{plim} \;\hat{\beta}_x = \beta_x + \beta_z \frac{\sigma_{xz}}{\sigma_x ^ 2}=</span>
<span id="cb72-150"><a href="#cb72-150" aria-hidden="true" tabindex="-1"></a>\beta_x + \beta_z \beta_{z / x}</span>
<span id="cb72-151"><a href="#cb72-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-152"><a href="#cb72-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-153"><a href="#cb72-153" aria-hidden="true" tabindex="-1"></a>where $\beta_{z/x}$ is the true value of the slope of the regression</span>
<span id="cb72-154"><a href="#cb72-154" aria-hidden="true" tabindex="-1"></a>of $z$ on $x$. This formula makes clear what $\hat{\beta}_x$ really</span>
<span id="cb72-155"><a href="#cb72-155" aria-hidden="true" tabindex="-1"></a>estimates in a linear regression:</span>
<span id="cb72-156"><a href="#cb72-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-157"><a href="#cb72-157" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the direct effect of $x$ on $y$ which is $\beta_x$,</span>
<span id="cb72-158"><a href="#cb72-158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the indirect effect of $x$ on $y$ which is the product of the effect</span>
<span id="cb72-159"><a href="#cb72-159" aria-hidden="true" tabindex="-1"></a>  of $x$ on $z$ ($\beta_{z/x}$) times the effect of $z$ on $\eta$</span>
<span id="cb72-160"><a href="#cb72-160" aria-hidden="true" tabindex="-1"></a>  and therefore on $y$ ($\beta_z$).</span>
<span id="cb72-161"><a href="#cb72-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-162"><a href="#cb72-162" aria-hidden="true" tabindex="-1"></a>A classic example of omitted variable bias occurs in the estimation of a Mincer</span>
<span id="cb72-163"><a href="#cb72-163" aria-hidden="true" tabindex="-1"></a>earning function which relates wage ($w$), education ($e$ in years) and experience</span>
<span id="cb72-164"><a href="#cb72-164" aria-hidden="true" tabindex="-1"></a>($s$ in weeks):</span>
<span id="cb72-165"><a href="#cb72-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-166"><a href="#cb72-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-167"><a href="#cb72-167" aria-hidden="true" tabindex="-1"></a>\ln w = \beta_o + \beta_e e + \beta_s s + \beta_{ss} s ^ 2 + \epsilon</span>
<span id="cb72-168"><a href="#cb72-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-169"><a href="#cb72-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-170"><a href="#cb72-170" aria-hidden="true" tabindex="-1"></a>$\beta_e = \frac{d\ln w}{de} = \frac{d w / w}{de}$ is the percentage</span>
<span id="cb72-171"><a href="#cb72-171" aria-hidden="true" tabindex="-1"></a>increase of the wage for one more year of education.  To illustrate</span>
<span id="cb72-172"><a href="#cb72-172" aria-hidden="true" tabindex="-1"></a>the estimation of a Mincer function, we use the data of</span>
<span id="cb72-173"><a href="#cb72-173" aria-hidden="true" tabindex="-1"></a>@KOOP:POIR:TOBI:05,\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Koop}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Poirier}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Tobias} which is a sample of 303 white males</span>
<span id="cb72-174"><a href="#cb72-174" aria-hidden="true" tabindex="-1"></a>taken from the National Longitudinal Survey of Youth and is available</span>
<span id="cb72-175"><a href="#cb72-175" aria-hidden="true" tabindex="-1"></a>as <span class="in">`sibling_educ`</span>.</span>
<span id="cb72-176"><a href="#cb72-176" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{sibling<span class="sc">\_</span>educ}{micsr.data}\idxfun{mutate}{dplyr}\idxfun{gaze}{micsr}\idxfun{lm}{stats}\idxfun{poly}{stats}</span>
<span id="cb72-177"><a href="#cb72-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-178"><a href="#cb72-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-179"><a href="#cb72-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: mincer_sibling</span></span>
<span id="cb72-180"><a href="#cb72-180" aria-hidden="true" tabindex="-1"></a>sibling_educ <span class="ot">&lt;-</span> sibling_educ <span class="sc">%&gt;%</span></span>
<span id="cb72-181"><a href="#cb72-181" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">experience =</span> experience <span class="sc">/</span> <span class="dv">52</span>)</span>
<span id="cb72-182"><a href="#cb72-182" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> educ <span class="sc">+</span> <span class="fu">poly</span>(experience, <span class="dv">2</span>), sibling_educ) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-183"><a href="#cb72-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-184"><a href="#cb72-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-185"><a href="#cb72-185" aria-hidden="true" tabindex="-1"></a>Results indicate that one more year of education increases on average</span>
<span id="cb72-186"><a href="#cb72-186" aria-hidden="true" tabindex="-1"></a>the wage by 10%. One concern about this kind of estimation is that</span>
<span id="cb72-187"><a href="#cb72-187" aria-hidden="true" tabindex="-1"></a>individuals have different abilities ($a$), and that more abilities have a</span>
<span id="cb72-188"><a href="#cb72-188" aria-hidden="true" tabindex="-1"></a>positive effect on wage, but may also have a positive effect on</span>
<span id="cb72-189"><a href="#cb72-189" aria-hidden="true" tabindex="-1"></a>education. If this is the case, $\beta_{a} &gt; 0$, $\beta_{a/e} &gt; 0$ and</span>
<span id="cb72-190"><a href="#cb72-190" aria-hidden="true" tabindex="-1"></a>therefore $\mbox{plim} \;\hat{\beta}_e = \beta_e + \beta_{a} \beta_{a/e} &gt;</span>
<span id="cb72-191"><a href="#cb72-191" aria-hidden="true" tabindex="-1"></a>\beta_e$ and the OLS estimator is upward biased. This is the case</span>
<span id="cb72-192"><a href="#cb72-192" aria-hidden="true" tabindex="-1"></a>because more education:</span>
<span id="cb72-193"><a href="#cb72-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-194"><a href="#cb72-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>increases, for a given level of ability, the expected wage by</span>
<span id="cb72-195"><a href="#cb72-195" aria-hidden="true" tabindex="-1"></a>  $\beta_e$,</span>
<span id="cb72-196"><a href="#cb72-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>means that, on average, the level of ability is higher, this effect</span>
<span id="cb72-197"><a href="#cb72-197" aria-hidden="true" tabindex="-1"></a>  being $\beta_{a/e}$ (which in this case doesn't imply a causality</span>
<span id="cb72-198"><a href="#cb72-198" aria-hidden="true" tabindex="-1"></a>  of $e$ on $a$, but simply a correlation), so the wage will also be</span>
<span id="cb72-199"><a href="#cb72-199" aria-hidden="true" tabindex="-1"></a>  higher ($\beta_a &gt; 0$).</span>
<span id="cb72-200"><a href="#cb72-200" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-201"><a href="#cb72-201" aria-hidden="true" tabindex="-1"></a>Numerous studies of the Mincer function deal with this problem of</span>
<span id="cb72-202"><a href="#cb72-202" aria-hidden="true" tabindex="-1"></a>endogeneity of the education level. But in the data set we used, there</span>
<span id="cb72-203"><a href="#cb72-203" aria-hidden="true" tabindex="-1"></a>is a measure of the ability, which is the standardized AFQT test</span>
<span id="cb72-204"><a href="#cb72-204" aria-hidden="true" tabindex="-1"></a>score. If we introduce ability in the regression, education is no more</span>
<span id="cb72-205"><a href="#cb72-205" aria-hidden="true" tabindex="-1"></a>endogenous and least squares will give a consistent estimation of the</span>
<span id="cb72-206"><a href="#cb72-206" aria-hidden="true" tabindex="-1"></a>effect of education on wage.^<span class="co">[</span><span class="ot">Needless to say, the hypothesis that abilities in all their dimensions can be measured by the AFQT test is very strong.</span><span class="co">]</span> We first check that education and</span>
<span id="cb72-207"><a href="#cb72-207" aria-hidden="true" tabindex="-1"></a>ability are positively correlated:</span>
<span id="cb72-208"><a href="#cb72-208" aria-hidden="true" tabindex="-1"></a>\idxfun{summarise}{dplyr}\idxfun{cor}{stats}\idxfun{pull}{dplyr}</span>
<span id="cb72-209"><a href="#cb72-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-210"><a href="#cb72-210" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-211"><a href="#cb72-211" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-212"><a href="#cb72-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: corr_educ_ability</span></span>
<span id="cb72-213"><a href="#cb72-213" aria-hidden="true" tabindex="-1"></a>sibling_educ <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="fu">cor</span>(educ, ability)) <span class="sc">%&gt;%</span> pull</span>
<span id="cb72-214"><a href="#cb72-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-215"><a href="#cb72-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-216"><a href="#cb72-216" aria-hidden="true" tabindex="-1"></a>Therefore, adding ability as a covariate in the previous regression should decrease the coefficient on</span>
<span id="cb72-217"><a href="#cb72-217" aria-hidden="true" tabindex="-1"></a>education:</span>
<span id="cb72-218"><a href="#cb72-218" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{poly}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-219"><a href="#cb72-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-220"><a href="#cb72-220" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-221"><a href="#cb72-221" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: mincer_ability</span></span>
<span id="cb72-222"><a href="#cb72-222" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-223"><a href="#cb72-223" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> educ <span class="sc">+</span> <span class="fu">poly</span>(experience, <span class="dv">2</span>) <span class="sc">+</span> ability, sibling_educ) <span class="sc">%&gt;%</span> </span>
<span id="cb72-224"><a href="#cb72-224" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gaze</span>(<span class="at">coef =</span> <span class="st">"educ"</span>)</span>
<span id="cb72-225"><a href="#cb72-225" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-226"><a href="#cb72-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-227"><a href="#cb72-227" aria-hidden="true" tabindex="-1"></a>The effect of one more year of education is now an increase of 8.7% of the wage (compared to 10% previously).</span>
<span id="cb72-228"><a href="#cb72-228" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{sibling<span class="sc">\_</span>educ}{micsr.data}</span>
<span id="cb72-229"><a href="#cb72-229" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{omitted variable bias|)}</span>
<span id="cb72-230"><a href="#cb72-230" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!omitted variable|)}</span>
<span id="cb72-231"><a href="#cb72-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-232"><a href="#cb72-232" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simultaneity bias</span></span>
<span id="cb72-233"><a href="#cb72-233" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simultaneity bias|(}</span>
<span id="cb72-234"><a href="#cb72-234" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!simultaneity|(}</span>
<span id="cb72-235"><a href="#cb72-235" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation!simultaneity bias|(}</span>
<span id="cb72-236"><a href="#cb72-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-237"><a href="#cb72-237" aria-hidden="true" tabindex="-1"></a>Often in economics, the phenomenon of interest is not described by a</span>
<span id="cb72-238"><a href="#cb72-238" aria-hidden="true" tabindex="-1"></a>single equation, but by a system of equations. Consider for example a</span>
<span id="cb72-239"><a href="#cb72-239" aria-hidden="true" tabindex="-1"></a>market equilibrium. The two equations relate the quantity demanded /</span>
<span id="cb72-240"><a href="#cb72-240" aria-hidden="true" tabindex="-1"></a>supplied ($q ^ d$ and $q ^ o$) to the unit price and to some specific</span>
<span id="cb72-241"><a href="#cb72-241" aria-hidden="true" tabindex="-1"></a>covariates to the demand and to the supply side of the market. @MADD:LAHI:09\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Maddala}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Lahiri}, pp.</span>
<span id="cb72-242"><a href="#cb72-242" aria-hidden="true" tabindex="-1"></a>376-377 studied the market for commercial loans using monthly US data</span>
<span id="cb72-243"><a href="#cb72-243" aria-hidden="true" tabindex="-1"></a>for 1979-1984. The data set is available as</span>
<span id="cb72-244"><a href="#cb72-244" aria-hidden="true" tabindex="-1"></a><span class="in">`loan_market`</span>. Total commercial loans (<span class="in">`loans`</span>) are in billions</span>
<span id="cb72-245"><a href="#cb72-245" aria-hidden="true" tabindex="-1"></a>of dollars; the price is the average prime rate charged by banks</span>
<span id="cb72-246"><a href="#cb72-246" aria-hidden="true" tabindex="-1"></a>(<span class="in">`prime_rate`</span>). <span class="in">`aaa_rate`</span> is the AAA corporate bond rate, which is</span>
<span id="cb72-247"><a href="#cb72-247" aria-hidden="true" tabindex="-1"></a>the price of an alternative financing to firms. Therefore, it enters</span>
<span id="cb72-248"><a href="#cb72-248" aria-hidden="true" tabindex="-1"></a>only the demand equation, with an expected positive sign. <span class="in">`treas_rate`</span></span>
<span id="cb72-249"><a href="#cb72-249" aria-hidden="true" tabindex="-1"></a>is the treasure bill rate. As it is a substitute to commercial loans from a</span>
<span id="cb72-250"><a href="#cb72-250" aria-hidden="true" tabindex="-1"></a>bank, it should enter only the supply equation, with an expected</span>
<span id="cb72-251"><a href="#cb72-251" aria-hidden="true" tabindex="-1"></a>negative sign. For the sake of simplicity, we denote $q$ and $p$ as the quantity (in logarithm) and</span>
<span id="cb72-252"><a href="#cb72-252" aria-hidden="true" tabindex="-1"></a>the price for this loan market model and, $d$ and $s$ as</span>
<span id="cb72-253"><a href="#cb72-253" aria-hidden="true" tabindex="-1"></a>the two rates that enter only, respectively, the demand and the supply</span>
<span id="cb72-254"><a href="#cb72-254" aria-hidden="true" tabindex="-1"></a>equation:</span>
<span id="cb72-255"><a href="#cb72-255" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{loan<span class="sc">\_</span>market}{micsr.data}\idxfun{transmute}{dplyr}</span>
<span id="cb72-256"><a href="#cb72-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-257"><a href="#cb72-257" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-258"><a href="#cb72-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: loan_market_covariates</span></span>
<span id="cb72-259"><a href="#cb72-259" aria-hidden="true" tabindex="-1"></a>loan <span class="ot">&lt;-</span> loan_market <span class="sc">%&gt;%</span></span>
<span id="cb72-260"><a href="#cb72-260" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(<span class="at">q =</span> <span class="fu">log</span>(loans),  <span class="at">p =</span> prime_rate,</span>
<span id="cb72-261"><a href="#cb72-261" aria-hidden="true" tabindex="-1"></a>              <span class="at">d =</span> aaa_rate, <span class="at">s =</span> treas_rate)</span>
<span id="cb72-262"><a href="#cb72-262" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-263"><a href="#cb72-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-264"><a href="#cb72-264" aria-hidden="true" tabindex="-1"></a>The equilibrium on the loan market is then defined by a system of</span>
<span id="cb72-265"><a href="#cb72-265" aria-hidden="true" tabindex="-1"></a>three equations:</span>
<span id="cb72-266"><a href="#cb72-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-267"><a href="#cb72-267" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb72-268"><a href="#cb72-268" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-269"><a href="#cb72-269" aria-hidden="true" tabindex="-1"></a>  \begin{array}{rcl}</span>
<span id="cb72-270"><a href="#cb72-270" aria-hidden="true" tabindex="-1"></a>    q_d &amp;=&amp; \alpha_d + \beta_d p + \gamma_d d + \epsilon_d <span class="sc">\\</span></span>
<span id="cb72-271"><a href="#cb72-271" aria-hidden="true" tabindex="-1"></a>    q_s &amp;=&amp; \alpha_s + \beta_s p + \gamma_s s + \epsilon_s <span class="sc">\\</span></span>
<span id="cb72-272"><a href="#cb72-272" aria-hidden="true" tabindex="-1"></a>    q_d &amp;=&amp; q_s</span>
<span id="cb72-273"><a href="#cb72-273" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-274"><a href="#cb72-274" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-275"><a href="#cb72-275" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb72-276"><a href="#cb72-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-277"><a href="#cb72-277" aria-hidden="true" tabindex="-1"></a>The last equation is non-stochastic and states that the market should</span>
<span id="cb72-278"><a href="#cb72-278" aria-hidden="true" tabindex="-1"></a>be in equilibrium. The demand curve should be decreasing ($\beta_d</span>
<span id="cb72-279"><a href="#cb72-279" aria-hidden="true" tabindex="-1"></a>&lt; 0$) and the supply curve increasing ($\beta_s &gt; 0$). The equilibrium is depicted in @fig-market_equilibrium.</span>
<span id="cb72-280"><a href="#cb72-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-281"><a href="#cb72-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-282"><a href="#cb72-282" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Market equilibrium"</span></span>
<span id="cb72-283"><a href="#cb72-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-market_equilibrium</span></span>
<span id="cb72-284"><a href="#cb72-284" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb72-285"><a href="#cb72-285" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"./tikz/fig/equilibre.png"</span>, <span class="at">auto_pdf =</span> <span class="cn">TRUE</span>)</span>
<span id="cb72-286"><a href="#cb72-286" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-287"><a href="#cb72-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-288"><a href="#cb72-288" aria-hidden="true" tabindex="-1"></a>The OLS estimation of the demand and the supply equations is given below:</span>
<span id="cb72-289"><a href="#cb72-289" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-290"><a href="#cb72-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-291"><a href="#cb72-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-292"><a href="#cb72-292" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: supply_demand</span></span>
<span id="cb72-293"><a href="#cb72-293" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-294"><a href="#cb72-294" aria-hidden="true" tabindex="-1"></a>ols_d <span class="ot">&lt;-</span> <span class="fu">lm</span>(q <span class="sc">~</span> p <span class="sc">+</span> d, <span class="at">data =</span> loan)</span>
<span id="cb72-295"><a href="#cb72-295" aria-hidden="true" tabindex="-1"></a>ols_s <span class="ot">&lt;-</span> <span class="fu">lm</span>(q <span class="sc">~</span> p <span class="sc">+</span> s, <span class="at">data =</span> loan)</span>
<span id="cb72-296"><a href="#cb72-296" aria-hidden="true" tabindex="-1"></a>ols_d <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-297"><a href="#cb72-297" aria-hidden="true" tabindex="-1"></a>ols_s <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-298"><a href="#cb72-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-299"><a href="#cb72-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-300"><a href="#cb72-300" aria-hidden="true" tabindex="-1"></a>The two coefficients of the demand equation have the predicted sign and are</span>
<span id="cb72-301"><a href="#cb72-301" aria-hidden="true" tabindex="-1"></a>highly significant: a 1 point of percentage increase of the prime</span>
<span id="cb72-302"><a href="#cb72-302" aria-hidden="true" tabindex="-1"></a>rate decreases loans by 4.3%, and a one point of</span>
<span id="cb72-303"><a href="#cb72-303" aria-hidden="true" tabindex="-1"></a>percentage increase of the corporate bond rate increases loans by 10.3%. The</span>
<span id="cb72-304"><a href="#cb72-304" aria-hidden="true" tabindex="-1"></a>fit of the supply equation is very bad and, even if the two coefficients</span>
<span id="cb72-305"><a href="#cb72-305" aria-hidden="true" tabindex="-1"></a>have the predicted sign, the values are very low and insignificant. </span>
<span id="cb72-306"><a href="#cb72-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-307"><a href="#cb72-307" aria-hidden="true" tabindex="-1"></a>What is actually observed for each observation in the sample is a</span>
<span id="cb72-308"><a href="#cb72-308" aria-hidden="true" tabindex="-1"></a>price-quantity combination at an equilibrium. A positive shock on the</span>
<span id="cb72-309"><a href="#cb72-309" aria-hidden="true" tabindex="-1"></a>demand equation will move upward the demand curve and will lead</span>
<span id="cb72-310"><a href="#cb72-310" aria-hidden="true" tabindex="-1"></a>to a new equilibrium with a higher equilibrium quantity $q'$ and also a</span>
<span id="cb72-311"><a href="#cb72-311" aria-hidden="true" tabindex="-1"></a>higher equilibrium price $p'$ (except in the special case where the supply</span>
<span id="cb72-312"><a href="#cb72-312" aria-hidden="true" tabindex="-1"></a>curve is vertical, which means that the price elasticity of supply is infinite). This means that $p$ is correlated with $\epsilon_ d$,</span>
<span id="cb72-313"><a href="#cb72-313" aria-hidden="true" tabindex="-1"></a>which leads to a bias in the estimation of $\beta_d$ by OLS. The same</span>
<span id="cb72-314"><a href="#cb72-314" aria-hidden="true" tabindex="-1"></a>reasoning applies of course to the supply curve.</span>
<span id="cb72-315"><a href="#cb72-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-316"><a href="#cb72-316" aria-hidden="true" tabindex="-1"></a>One solution would be to use the equilibrium condition and then to solve for $p$, and then for $q$. This **reduced-form** system of two equations:</span>
<span id="cb72-317"><a href="#cb72-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-318"><a href="#cb72-318" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-319"><a href="#cb72-319" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-320"><a href="#cb72-320" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcccccccccc}</span>
<span id="cb72-321"><a href="#cb72-321" aria-hidden="true" tabindex="-1"></a>p &amp;=&amp; \displaystyle \frac{\alpha_d - \alpha_s}{\beta_s - \beta_d} &amp;+&amp;</span>
<span id="cb72-322"><a href="#cb72-322" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\gamma_d}{\beta_s - \beta_d} d &amp;-&amp; </span>
<span id="cb72-323"><a href="#cb72-323" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\gamma_s}{\beta_s - \beta_d} s &amp;+&amp; </span>
<span id="cb72-324"><a href="#cb72-324" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\epsilon_d - \epsilon_s}{\beta_s - \beta_d} <span class="sc">\\</span></span>
<span id="cb72-325"><a href="#cb72-325" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \pi_o &amp;+&amp; \pi_d d &amp;+&amp; \pi_s s &amp;+&amp; \nu_p <span class="sc">\\</span></span>
<span id="cb72-326"><a href="#cb72-326" aria-hidden="true" tabindex="-1"></a>q &amp;=&amp; \displaystyle \frac{\alpha_d \beta_s - \beta_d \alpha_s}{\beta_s - \beta_d} &amp;+&amp;</span>
<span id="cb72-327"><a href="#cb72-327" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\beta_s \gamma_d}{\beta_s - \beta_d}d &amp;-&amp; </span>
<span id="cb72-328"><a href="#cb72-328" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\beta_d \gamma_s}{\beta_s - \beta_d} s &amp;+&amp; </span>
<span id="cb72-329"><a href="#cb72-329" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{- \beta_d \epsilon_s + \beta_s</span>
<span id="cb72-330"><a href="#cb72-330" aria-hidden="true" tabindex="-1"></a>\epsilon_d}{\beta_s - \beta_d} <span class="sc">\\</span></span>
<span id="cb72-331"><a href="#cb72-331" aria-hidden="true" tabindex="-1"></a>&amp;=&amp;  \delta_o &amp;+&amp; \delta_d d &amp;+&amp; \delta_s s &amp;+&amp; \nu_q <span class="sc">\\</span></span>
<span id="cb72-332"><a href="#cb72-332" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-333"><a href="#cb72-333" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-334"><a href="#cb72-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-335"><a href="#cb72-335" aria-hidden="true" tabindex="-1"></a>makes clear that both $p$ and $q$ depend on $\epsilon_s$ and</span>
<span id="cb72-336"><a href="#cb72-336" aria-hidden="true" tabindex="-1"></a>$\epsilon_d$. More precisely, as $\beta_s - \beta_d &gt; 0$, $\epsilon_d$</span>
<span id="cb72-337"><a href="#cb72-337" aria-hidden="true" tabindex="-1"></a>and $\epsilon_s$ are respectively positively and negatively correlated</span>
<span id="cb72-338"><a href="#cb72-338" aria-hidden="true" tabindex="-1"></a>with the price. $\Delta \epsilon_d &gt; 0$ will shift the demand curve</span>
<span id="cb72-339"><a href="#cb72-339" aria-hidden="true" tabindex="-1"></a>upward and therefore will increase the equilibrium price. Therefore,</span>
<span id="cb72-340"><a href="#cb72-340" aria-hidden="true" tabindex="-1"></a>the OLS estimator of the slope of the demand curve is biased upward which means, as it is</span>
<span id="cb72-341"><a href="#cb72-341" aria-hidden="true" tabindex="-1"></a>negative, that it is biased downward in absolute value. A positive shock on the supply side</span>
<span id="cb72-342"><a href="#cb72-342" aria-hidden="true" tabindex="-1"></a>($\Delta \epsilon_s &gt; 0$) will move the supply curve upward and therefore</span>
<span id="cb72-343"><a href="#cb72-343" aria-hidden="true" tabindex="-1"></a>will decrease the equilibrium price. The OLS estimator of the</span>
<span id="cb72-344"><a href="#cb72-344" aria-hidden="true" tabindex="-1"></a>slope of the supply curve is then downward biased.</span>
<span id="cb72-345"><a href="#cb72-345" aria-hidden="true" tabindex="-1"></a>The parameters of these two equations can be</span>
<span id="cb72-346"><a href="#cb72-346" aria-hidden="true" tabindex="-1"></a>consistently estimated by least squares, as $d$ and $s$ are</span>
<span id="cb72-347"><a href="#cb72-347" aria-hidden="true" tabindex="-1"></a>uncorrelated with the two error terms. </span>
<span id="cb72-348"><a href="#cb72-348" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-349"><a href="#cb72-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-350"><a href="#cb72-350" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-351"><a href="#cb72-351" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: reduced_form_market</span></span>
<span id="cb72-352"><a href="#cb72-352" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-353"><a href="#cb72-353" aria-hidden="true" tabindex="-1"></a>ols_q <span class="ot">&lt;-</span> <span class="fu">lm</span>(q <span class="sc">~</span> d <span class="sc">+</span> s, <span class="at">data =</span> loan)</span>
<span id="cb72-354"><a href="#cb72-354" aria-hidden="true" tabindex="-1"></a>ols_p <span class="ot">&lt;-</span> <span class="fu">lm</span>(p <span class="sc">~</span> d <span class="sc">+</span> s, <span class="at">data =</span> loan)</span>
<span id="cb72-355"><a href="#cb72-355" aria-hidden="true" tabindex="-1"></a>ols_q <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-356"><a href="#cb72-356" aria-hidden="true" tabindex="-1"></a>ols_p <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-357"><a href="#cb72-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-358"><a href="#cb72-358" aria-hidden="true" tabindex="-1"></a>The reduced form coefficients are not meaningful by themselves, but</span>
<span id="cb72-359"><a href="#cb72-359" aria-hidden="true" tabindex="-1"></a>only if they enable to retrieve the structural parameters. This is</span>
<span id="cb72-360"><a href="#cb72-360" aria-hidden="true" tabindex="-1"></a>actually the case here as</span>
<span id="cb72-361"><a href="#cb72-361" aria-hidden="true" tabindex="-1"></a>$\frac{\delta_s}{\pi_s}=\beta_d$ and</span>
<span id="cb72-362"><a href="#cb72-362" aria-hidden="true" tabindex="-1"></a>$\frac{\delta_d}{\pi_d}=\beta_s$.</span>
<span id="cb72-363"><a href="#cb72-363" aria-hidden="true" tabindex="-1"></a>\idxfun{unname}{base}</span>
<span id="cb72-364"><a href="#cb72-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-365"><a href="#cb72-365" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-366"><a href="#cb72-366" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: retrieve_struct_par</span></span>
<span id="cb72-367"><a href="#cb72-367" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-368"><a href="#cb72-368" aria-hidden="true" tabindex="-1"></a>price_coefs <span class="ot">&lt;-</span> <span class="fu">unname</span>(<span class="fu">coef</span>(ols_q) <span class="sc">/</span> <span class="fu">coef</span>(ols_p))[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb72-369"><a href="#cb72-369" aria-hidden="true" tabindex="-1"></a>price_coefs</span>
<span id="cb72-370"><a href="#cb72-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-371"><a href="#cb72-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-372"><a href="#cb72-372" aria-hidden="true" tabindex="-1"></a>The price coefficients are as expected higher in absolute values than</span>
<span id="cb72-373"><a href="#cb72-373" aria-hidden="true" tabindex="-1"></a>those obtained using OLS on the demand and on the supply</span>
<span id="cb72-374"><a href="#cb72-374" aria-hidden="true" tabindex="-1"></a>equation. It is particularly the case for $\beta_s$ </span>
<span id="cb72-375"><a href="#cb72-375" aria-hidden="true" tabindex="-1"></a>(<span class="in">`r round(price_coefs[1], 3)`</span> vs. <span class="in">`r round(coef(ols_s)["p"], 3)`</span>).</span>
<span id="cb72-376"><a href="#cb72-376" aria-hidden="true" tabindex="-1"></a>On the contrary, the absolute value of $\beta_d$ increases very slightly </span>
<span id="cb72-377"><a href="#cb72-377" aria-hidden="true" tabindex="-1"></a>($<span class="in">`r round(price_coefs[2], 3)`</span>$ vs. $<span class="in">`r round(coef(ols_d)["p"], 3)`</span>$).</span>
<span id="cb72-378"><a href="#cb72-378" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{loan<span class="sc">\_</span>market}{micsr.data}</span>
<span id="cb72-379"><a href="#cb72-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-380"><a href="#cb72-380" aria-hidden="true" tabindex="-1"></a>Actually, the case we have considered is special because there is one</span>
<span id="cb72-381"><a href="#cb72-381" aria-hidden="true" tabindex="-1"></a>and only one extra covariate in both equations. Consider as an example</span>
<span id="cb72-382"><a href="#cb72-382" aria-hidden="true" tabindex="-1"></a>the following system of equations:</span>
<span id="cb72-383"><a href="#cb72-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-384"><a href="#cb72-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-385"><a href="#cb72-385" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-386"><a href="#cb72-386" aria-hidden="true" tabindex="-1"></a>  \begin{array}{rcl}</span>
<span id="cb72-387"><a href="#cb72-387" aria-hidden="true" tabindex="-1"></a>    y_d &amp;=&amp; \alpha_d + \beta_d p + \gamma_1 d_1 + \gamma_2 d_2 + \epsilon_d <span class="sc">\\</span></span>
<span id="cb72-388"><a href="#cb72-388" aria-hidden="true" tabindex="-1"></a>    y_s &amp;=&amp; \alpha_s + \beta_s p + \epsilon_s <span class="sc">\\</span></span>
<span id="cb72-389"><a href="#cb72-389" aria-hidden="true" tabindex="-1"></a>    y_d &amp;=&amp; y_s</span>
<span id="cb72-390"><a href="#cb72-390" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-391"><a href="#cb72-391" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-392"><a href="#cb72-392" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-393"><a href="#cb72-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-394"><a href="#cb72-394" aria-hidden="true" tabindex="-1"></a>where there are two extra covariates in the demand equation and no</span>
<span id="cb72-395"><a href="#cb72-395" aria-hidden="true" tabindex="-1"></a>covariates (except the price) in the supply equation. Solving for the</span>
<span id="cb72-396"><a href="#cb72-396" aria-hidden="true" tabindex="-1"></a>two endogenous variables $p$ and $q$, we get in this case:</span>
<span id="cb72-397"><a href="#cb72-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-398"><a href="#cb72-398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-399"><a href="#cb72-399" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-400"><a href="#cb72-400" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcccccccccc}</span>
<span id="cb72-401"><a href="#cb72-401" aria-hidden="true" tabindex="-1"></a>p &amp;=&amp; \displaystyle \frac{\alpha_d - \alpha_s}{\beta_s - \beta_d} &amp;+&amp;</span>
<span id="cb72-402"><a href="#cb72-402" aria-hidden="true" tabindex="-1"></a>\displaystyle\frac{\gamma_1}{\beta_s - \beta_d} d_1 &amp;+&amp; </span>
<span id="cb72-403"><a href="#cb72-403" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\gamma_2}{\beta_s - \beta_d} d_2 &amp;+&amp; </span>
<span id="cb72-404"><a href="#cb72-404" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\epsilon_d - \epsilon_s}{\beta_s - \beta_d} <span class="sc">\\</span></span>
<span id="cb72-405"><a href="#cb72-405" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \pi_s &amp;+&amp; \pi_1 d_1 &amp;+&amp; \pi_2 d_2 &amp;+&amp; \nu_p <span class="sc">\\</span></span>
<span id="cb72-406"><a href="#cb72-406" aria-hidden="true" tabindex="-1"></a>q &amp;=&amp; \displaystyle \frac{\beta_s \alpha_d - \beta_d \alpha_s}{\beta_s - \beta_d} &amp;+&amp;</span>
<span id="cb72-407"><a href="#cb72-407" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\beta_s \gamma_1}{\beta_s - \beta d}d_1 &amp;+&amp; </span>
<span id="cb72-408"><a href="#cb72-408" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\beta_s \gamma_2}{\beta_s - \beta_d} d_2 &amp;+&amp; </span>
<span id="cb72-409"><a href="#cb72-409" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\beta_s \epsilon_d - \beta_d \epsilon_s}{\beta_s - \beta_d} <span class="sc">\\</span></span>
<span id="cb72-410"><a href="#cb72-410" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \delta_s &amp;+&amp; \delta_1 d_1 &amp;+&amp; \delta_2 d_2 &amp;+&amp; \nu_q <span class="sc">\\</span></span>
<span id="cb72-411"><a href="#cb72-411" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-412"><a href="#cb72-412" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-413"><a href="#cb72-413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-414"><a href="#cb72-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-415"><a href="#cb72-415" aria-hidden="true" tabindex="-1"></a>we now have $\frac{\delta_1}{\pi_1} = \frac{\delta_2}{\pi_2} =</span>
<span id="cb72-416"><a href="#cb72-416" aria-hidden="true" tabindex="-1"></a>\beta_s$; there are two ratios of the reduced parameters that give the</span>
<span id="cb72-417"><a href="#cb72-417" aria-hidden="true" tabindex="-1"></a>value of the slope of the supply curve, but it is very implausible that these two values will be equal. On the contrary, there is no way to</span>
<span id="cb72-418"><a href="#cb72-418" aria-hidden="true" tabindex="-1"></a>retrieve the slope of the demand curve from the reduced form</span>
<span id="cb72-419"><a href="#cb72-419" aria-hidden="true" tabindex="-1"></a>parameters. Therefore, this **indirect least squares** \index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{indirect least squares}</span>
<span id="cb72-420"><a href="#cb72-420" aria-hidden="true" tabindex="-1"></a>approach is of</span>
<span id="cb72-421"><a href="#cb72-421" aria-hidden="true" tabindex="-1"></a>limited interest. In the general case, as we have seen, some</span>
<span id="cb72-422"><a href="#cb72-422" aria-hidden="true" tabindex="-1"></a>coefficients like $\beta_s$ in our example may be **over-identified**</span>
<span id="cb72-423"><a href="#cb72-423" aria-hidden="true" tabindex="-1"></a>and some other like $\beta_d$ are **under-identified**.\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{identification}</span>
<span id="cb72-424"><a href="#cb72-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-425"><a href="#cb72-425" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{simultaneity bias|)}</span>
<span id="cb72-426"><a href="#cb72-426" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation!simultaneity bias|)}</span>
<span id="cb72-427"><a href="#cb72-427" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!simultaneity|)}</span>
<span id="cb72-428"><a href="#cb72-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-429"><a href="#cb72-429" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator|(}</span>
<span id="cb72-430"><a href="#cb72-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-431"><a href="#cb72-431" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simple instrumental variable estimator {#sec-simple_iv}</span></span>
<span id="cb72-432"><a href="#cb72-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-433"><a href="#cb72-433" aria-hidden="true" tabindex="-1"></a>The general idea of the **instrumental variable** (**IV**) estimator is to find</span>
<span id="cb72-434"><a href="#cb72-434" aria-hidden="true" tabindex="-1"></a>variables which are correlated with the endogenous covariates and</span>
<span id="cb72-435"><a href="#cb72-435" aria-hidden="true" tabindex="-1"></a>uncorrelated with the error of the structural equation. This</span>
<span id="cb72-436"><a href="#cb72-436" aria-hidden="true" tabindex="-1"></a>means that the instruments don't have a direct effect on the response,</span>
<span id="cb72-437"><a href="#cb72-437" aria-hidden="true" tabindex="-1"></a>but only an indirect effect because of their correlation with the</span>
<span id="cb72-438"><a href="#cb72-438" aria-hidden="true" tabindex="-1"></a>endogenous covariates. These instruments allow to get an exogenous</span>
<span id="cb72-439"><a href="#cb72-439" aria-hidden="true" tabindex="-1"></a>source of variation of the covariate, i.e., a source of variation that</span>
<span id="cb72-440"><a href="#cb72-440" aria-hidden="true" tabindex="-1"></a>has nothing to do with the process of interest. </span>
<span id="cb72-441"><a href="#cb72-441" aria-hidden="true" tabindex="-1"></a>We'll start with the simple case where the number of</span>
<span id="cb72-442"><a href="#cb72-442" aria-hidden="true" tabindex="-1"></a>instruments equals the number of endogenous covariates, i.e., the **just-identified** case.</span>
<span id="cb72-443"><a href="#cb72-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-444"><a href="#cb72-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the simple instrumental variable estimator</span></span>
<span id="cb72-445"><a href="#cb72-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-446"><a href="#cb72-446" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!just identified|(}</span>
<span id="cb72-447"><a href="#cb72-447" aria-hidden="true" tabindex="-1"></a>Consider a simple linear regression: $y_n = \alpha + \beta  x_n + \epsilon_n$, with $\mbox{E}(\epsilon \mid x) \neq 0$. From @eq-normal_equation, the first-order conditions for the minimization of the sum of square residuals can be written as:</span>
<span id="cb72-448"><a href="#cb72-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-449"><a href="#cb72-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-450"><a href="#cb72-450" aria-hidden="true" tabindex="-1"></a>\frac{1}{N} \sum_n \left<span class="co">[</span><span class="ot"> (y_n - \bar{y}) - \hat{\beta} (x_n - \bar{x})\right</span><span class="co">]</span></span>
<span id="cb72-451"><a href="#cb72-451" aria-hidden="true" tabindex="-1"></a>(x_n - \bar{x}) = \frac{1}{N} \sum_n \hat{\epsilon}_n (x_n - \bar{x}) =</span>
<span id="cb72-452"><a href="#cb72-452" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\epsilon} x}= 0</span>
<span id="cb72-453"><a href="#cb72-453" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-normal_eq2}</span>
<span id="cb72-454"><a href="#cb72-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-455"><a href="#cb72-455" aria-hidden="true" tabindex="-1"></a>@eq-normal_eq2 states that the OLS estimator is obtained by imposing</span>
<span id="cb72-456"><a href="#cb72-456" aria-hidden="true" tabindex="-1"></a>the sample equivalent to the moment condition $\mbox{E}(\epsilon \mid</span>
<span id="cb72-457"><a href="#cb72-457" aria-hidden="true" tabindex="-1"></a>x) = 0$, i.e., that the covariance between the residuals and the</span>
<span id="cb72-458"><a href="#cb72-458" aria-hidden="true" tabindex="-1"></a>covariate is exactly 0 in the sample. This of course leads to a biased</span>
<span id="cb72-459"><a href="#cb72-459" aria-hidden="true" tabindex="-1"></a>and inconsistent estimator if $\mbox{E}(\epsilon \mid x)\neq 0$. Now</span>
<span id="cb72-460"><a href="#cb72-460" aria-hidden="true" tabindex="-1"></a>suppose that a variable $w$ exists, which is correlated with the</span>
<span id="cb72-461"><a href="#cb72-461" aria-hidden="true" tabindex="-1"></a>covariate, but not with the error of the model (this latter hypothesis is called the **exclusion restriction**)\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{exclusion restriction}. Such a variable has </span>
<span id="cb72-462"><a href="#cb72-462" aria-hidden="true" tabindex="-1"></a>no direct effect on the response, but only an indirect effect due to</span>
<span id="cb72-463"><a href="#cb72-463" aria-hidden="true" tabindex="-1"></a>its correlation with the covariate. As by</span>
<span id="cb72-464"><a href="#cb72-464" aria-hidden="true" tabindex="-1"></a>hypothesis, $\mbox{E}(\epsilon \mid w) = 0$, a consistent estimator</span>
<span id="cb72-465"><a href="#cb72-465" aria-hidden="true" tabindex="-1"></a>can be obtained by imposing the sample equivalent to this moment</span>
<span id="cb72-466"><a href="#cb72-466" aria-hidden="true" tabindex="-1"></a>condition, i.e., by setting the estimator to a value such that the</span>
<span id="cb72-467"><a href="#cb72-467" aria-hidden="true" tabindex="-1"></a>covariance between the residuals and the instrument is 0:</span>
<span id="cb72-468"><a href="#cb72-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-469"><a href="#cb72-469" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-470"><a href="#cb72-470" aria-hidden="true" tabindex="-1"></a>\frac{1}{N} \sum_n \left<span class="co">[</span><span class="ot"> (y_n - \bar{y}) - \hat{\beta} (x_n - \bar{x})\right</span><span class="co">]</span></span>
<span id="cb72-471"><a href="#cb72-471" aria-hidden="true" tabindex="-1"></a>(w_n - \bar{w}) = \frac{1}{N} \sum_n \hat{\epsilon}_n (w_n - \bar{w}) =</span>
<span id="cb72-472"><a href="#cb72-472" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\epsilon} w}= 0</span>
<span id="cb72-473"><a href="#cb72-473" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-normal_inst_eq}</span>
<span id="cb72-474"><a href="#cb72-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-475"><a href="#cb72-475" aria-hidden="true" tabindex="-1"></a>Solving @eq-normal_inst_eq for $\hat{\beta}$, we get the</span>
<span id="cb72-476"><a href="#cb72-476" aria-hidden="true" tabindex="-1"></a>instrumental variable estimator:</span>
<span id="cb72-477"><a href="#cb72-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-478"><a href="#cb72-478" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-479"><a href="#cb72-479" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_n (w_n - \bar{w})(y_n - \bar{y})}{\sum_n (w_n - \bar{w})(x_n - \bar{x})} = \frac{\hat{\sigma}_{wy}}{\hat{\sigma}_{wx}}</span>
<span id="cb72-480"><a href="#cb72-480" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-iv_simple}</span>
<span id="cb72-481"><a href="#cb72-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-482"><a href="#cb72-482" aria-hidden="true" tabindex="-1"></a>which is the ratio of the empirical covariances of $w$ with $y$ and with</span>
<span id="cb72-483"><a href="#cb72-483" aria-hidden="true" tabindex="-1"></a>$x$. Dividing both sides of the ratio by the empirical variance of $w$</span>
<span id="cb72-484"><a href="#cb72-484" aria-hidden="true" tabindex="-1"></a>($\hat{\sigma}_w ^ 2$), @eq-iv_simple can also be written as:</span>
<span id="cb72-485"><a href="#cb72-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-486"><a href="#cb72-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-487"><a href="#cb72-487" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-488"><a href="#cb72-488" aria-hidden="true" tabindex="-1"></a>\hat{\beta}  = \frac{\hat{\sigma}_{wy} / \hat{\sigma}_w ^</span>
<span id="cb72-489"><a href="#cb72-489" aria-hidden="true" tabindex="-1"></a>2}{\hat{\sigma}_{wx} / \hat{\sigma}_w ^2} </span>
<span id="cb72-490"><a href="#cb72-490" aria-hidden="true" tabindex="-1"></a>= \frac{\hat{\beta}_{y/w}}{\hat{\beta}_{x/w}} = \frac{dy /</span>
<span id="cb72-491"><a href="#cb72-491" aria-hidden="true" tabindex="-1"></a>dw}{dx / dw}</span>
<span id="cb72-492"><a href="#cb72-492" aria-hidden="true" tabindex="-1"></a>$$ {#eq-iv_ratio_me}</span>
<span id="cb72-493"><a href="#cb72-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-494"><a href="#cb72-494" aria-hidden="true" tabindex="-1"></a>where $\hat{\beta}_{y/w}$ and $\hat{\beta}_{x/w}$ are the slopes of the regressions of $y$ and $x$ on $w$. Therefore, the IV estimator is also the ratio of the marginal effects of $w$ on $y$ and on $x$.^<span class="co">[</span><span class="ot">See @HECK:00\index[author]{Heckman}, page 58 and @CAME:TRIV:05\index[author]{Cameron}\index[author]{Trivedi}, page 98.</span><span class="co">]</span></span>
<span id="cb72-495"><a href="#cb72-495" aria-hidden="true" tabindex="-1"></a>Replacing $y_n - \bar{y}$ by $\beta(x_n - \bar{x}) + \epsilon_n$ in</span>
<span id="cb72-496"><a href="#cb72-496" aria-hidden="true" tabindex="-1"></a>@eq-iv_simple, we get:</span>
<span id="cb72-497"><a href="#cb72-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-498"><a href="#cb72-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-499"><a href="#cb72-499" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \beta + \frac{\sum_n (w_n - \bar{w}) \epsilon_n}{\sum_n</span>
<span id="cb72-500"><a href="#cb72-500" aria-hidden="true" tabindex="-1"></a>(w_n - \bar{w}) (x_n - \bar{x})} = \beta +</span>
<span id="cb72-501"><a href="#cb72-501" aria-hidden="true" tabindex="-1"></a>\frac{\hat{\sigma}_{w\epsilon}}{\hat{\sigma}_{wx}}</span>
<span id="cb72-502"><a href="#cb72-502" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-503"><a href="#cb72-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-504"><a href="#cb72-504" aria-hidden="true" tabindex="-1"></a>As $N$ grows, the two empirical covariances converge to the population</span>
<span id="cb72-505"><a href="#cb72-505" aria-hidden="true" tabindex="-1"></a>covariances and, as by hypothesis, the instrument is uncorrelated with</span>
<span id="cb72-506"><a href="#cb72-506" aria-hidden="true" tabindex="-1"></a>the error ($\sigma_{w\epsilon} = 0$) and is correlated with the</span>
<span id="cb72-507"><a href="#cb72-507" aria-hidden="true" tabindex="-1"></a>covariate ($\sigma_{wx} \neq0$), the instrumental variable</span>
<span id="cb72-508"><a href="#cb72-508" aria-hidden="true" tabindex="-1"></a>estimator is consistent ($\mbox{plim} \;\hat{\beta} = \beta + \frac{\sigma_{w\epsilon}}{\sigma_{wx}} = \beta$).</span>
<span id="cb72-509"><a href="#cb72-509" aria-hidden="true" tabindex="-1"></a>Assuming spherical disturbances, the variance of the IV estimator</span>
<span id="cb72-510"><a href="#cb72-510" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb72-511"><a href="#cb72-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-512"><a href="#cb72-512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-513"><a href="#cb72-513" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}) = \mbox{E}\left<span class="co">[</span><span class="ot">(\hat{\beta}-\beta)^2\right</span><span class="co">]</span> = </span>
<span id="cb72-514"><a href="#cb72-514" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon ^ 2 \sum_n (w_n - \bar{w}) ^ 2}{\left[\sum_n</span>
<span id="cb72-515"><a href="#cb72-515" aria-hidden="true" tabindex="-1"></a>(w_n - \bar{w})(x_n - \bar{x})\right] ^ 2} = \frac{\sigma_\epsilon ^ 2</span>
<span id="cb72-516"><a href="#cb72-516" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_w ^ 2}{N \hat{\sigma}_{wx} ^ 2}</span>
<span id="cb72-517"><a href="#cb72-517" aria-hidden="true" tabindex="-1"></a>= \frac{\sigma_\epsilon ^ 2}{N \hat{\sigma}_x ^ 2 \hat{\rho}_{wx} ^ 2}</span>
<span id="cb72-518"><a href="#cb72-518" aria-hidden="true" tabindex="-1"></a>=\left(\frac{\sigma_\epsilon}{\sqrt{N} \hat{\sigma}_x \mid \hat{\rho}_{wx}\mid}\right) ^ 2</span>
<span id="cb72-519"><a href="#cb72-519" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-520"><a href="#cb72-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-521"><a href="#cb72-521" aria-hidden="true" tabindex="-1"></a>The last expression, which introduces the coefficient of correlation</span>
<span id="cb72-522"><a href="#cb72-522" aria-hidden="true" tabindex="-1"></a>between $x$ and $z$, is particularly appealing as it shows that,</span>
<span id="cb72-523"><a href="#cb72-523" aria-hidden="true" tabindex="-1"></a>compared to the standard error of the OLS estimator, the standard error of the IV estimator is inflated by $1 / \mid \hat{\rho}_{wx}\mid$. This term</span>
<span id="cb72-524"><a href="#cb72-524" aria-hidden="true" tabindex="-1"></a>is close to 1 if the correlation between $x$ and $w$ is high and, in this</span>
<span id="cb72-525"><a href="#cb72-525" aria-hidden="true" tabindex="-1"></a>case, the loss of precision implied by the use of the IV estimator</span>
<span id="cb72-526"><a href="#cb72-526" aria-hidden="true" tabindex="-1"></a>is low. On the contrary, if the correlation between $x$ and $w$ is</span>
<span id="cb72-527"><a href="#cb72-527" aria-hidden="true" tabindex="-1"></a>low, the IV estimator will be very imprecisely estimated.</span>
<span id="cb72-528"><a href="#cb72-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-529"><a href="#cb72-529" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{two-stage least squares estimator|(}</span>
<span id="cb72-530"><a href="#cb72-530" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!two-stage least squares|(}</span>
<span id="cb72-531"><a href="#cb72-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-532"><a href="#cb72-532" aria-hidden="true" tabindex="-1"></a>The IV estimator can also be obtained by first regressing $x$ on</span>
<span id="cb72-533"><a href="#cb72-533" aria-hidden="true" tabindex="-1"></a>$w$ and then by regressing $y$ on the fitted values of the previous</span>
<span id="cb72-534"><a href="#cb72-534" aria-hidden="true" tabindex="-1"></a>regression. Consider the linear model that relates the instrument to the covariate: $x_n = \gamma + \delta w_n + \nu_n$. We estimate this model by OLS and then we express the fitted values ($\hat{x}$) as a function of $\hat{\delta}$: $\hat{x}_n - \bar{x} = \hat{\delta}(w_n - \bar{w})$.</span>
<span id="cb72-535"><a href="#cb72-535" aria-hidden="true" tabindex="-1"></a>We can therefore rewrite the numerator and the</span>
<span id="cb72-536"><a href="#cb72-536" aria-hidden="true" tabindex="-1"></a>denominator of the IV estimator given by @eq-iv_simple as:</span>
<span id="cb72-537"><a href="#cb72-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-538"><a href="#cb72-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sum_n (w_n - \bar{w})(y_n - \bar{y}) = \sum_n (\hat{x}_n - \bar{x}) (y_n - \bar{y})/ \hat{\delta}$,</span>
<span id="cb72-539"><a href="#cb72-539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sum_n (w_n - \bar{w})(x_n - \bar{x})  = \sum_n (w_n - \bar{w})(\hat{x}_n + \hat{\epsilon}_n- \bar{x}) = \sum_n (\hat{x}_n -\bar{x}) ^ 2/ \hat{\delta}$ (as $\sum_n (w_n - \bar{w})</span>
<span id="cb72-540"><a href="#cb72-540" aria-hidden="true" tabindex="-1"></a>\hat{\epsilon}_n=0$), </span>
<span id="cb72-541"><a href="#cb72-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-542"><a href="#cb72-542" aria-hidden="true" tabindex="-1"></a>so that:</span>
<span id="cb72-543"><a href="#cb72-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-544"><a href="#cb72-544" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-545"><a href="#cb72-545" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_n(\hat{x}_n - \bar{x})(y_n -</span>
<span id="cb72-546"><a href="#cb72-546" aria-hidden="true" tabindex="-1"></a>\bar{y})}{\sum_n(\hat{x}_n - \bar{x}) ^ 2}</span>
<span id="cb72-547"><a href="#cb72-547" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-548"><a href="#cb72-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-549"><a href="#cb72-549" aria-hidden="true" tabindex="-1"></a>which is the OLS estimator of $y$ on $\hat{x}$, $\hat{x}$ being</span>
<span id="cb72-550"><a href="#cb72-550" aria-hidden="true" tabindex="-1"></a>the fitted values of the regression of $x$ on $w$. Therefore, the</span>
<span id="cb72-551"><a href="#cb72-551" aria-hidden="true" tabindex="-1"></a>IV estimator can be obtained by running two OLS regressions</span>
<span id="cb72-552"><a href="#cb72-552" aria-hidden="true" tabindex="-1"></a>and is for this reason also called the **two-stage least square**</span>
<span id="cb72-553"><a href="#cb72-553" aria-hidden="true" tabindex="-1"></a>(**2SLS**) estimator. As $x$ is correlated with $\epsilon$,</span>
<span id="cb72-554"><a href="#cb72-554" aria-hidden="true" tabindex="-1"></a>but $w$ is not, the idea of the 2SLS estimator is to replace $x$</span>
<span id="cb72-555"><a href="#cb72-555" aria-hidden="true" tabindex="-1"></a>by a linear transformation of $w$ which is as close as possible to</span>
<span id="cb72-556"><a href="#cb72-556" aria-hidden="true" tabindex="-1"></a>$x$, and this is simply the fitted values of the regression of $x$ on</span>
<span id="cb72-557"><a href="#cb72-557" aria-hidden="true" tabindex="-1"></a>$w$.</span>
<span id="cb72-558"><a href="#cb72-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-559"><a href="#cb72-559" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{two-stage least squares estimator|)}</span>
<span id="cb72-560"><a href="#cb72-560" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!two-stage least squares|)}</span>
<span id="cb72-561"><a href="#cb72-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-562"><a href="#cb72-562" aria-hidden="true" tabindex="-1"></a>The extension of the case when there are $K &gt; 1$ covariates and $K$</span>
<span id="cb72-563"><a href="#cb72-563" aria-hidden="true" tabindex="-1"></a>instruments is straightforward. The two sets of variables $x$ and $w$</span>
<span id="cb72-564"><a href="#cb72-564" aria-hidden="true" tabindex="-1"></a>can overlap because, if some of the covariates are exogenous, they</span>
<span id="cb72-565"><a href="#cb72-565" aria-hidden="true" tabindex="-1"></a>should also be used as instruments. Denoting $w$ as the vector of instruments, the moment conditions are</span>
<span id="cb72-566"><a href="#cb72-566" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon \mid w) = 0$ and the sample equivalent is:</span>
<span id="cb72-567"><a href="#cb72-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-568"><a href="#cb72-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-569"><a href="#cb72-569" aria-hidden="true" tabindex="-1"></a>\frac{1}{N} W ^ \top (y - Z \gamma) = 0</span>
<span id="cb72-570"><a href="#cb72-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-571"><a href="#cb72-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-572"><a href="#cb72-572" aria-hidden="true" tabindex="-1"></a>where $Z$ and $W$ are $N\times (K+1)$ matrix containing respectively the covariates and the instruments (with a vector of 1). Solving for $\gamma$, we get the instrumental variable estimator:</span>
<span id="cb72-573"><a href="#cb72-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-574"><a href="#cb72-574" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-575"><a href="#cb72-575" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = (W^\top X) ^ {-1} W^\top y</span>
<span id="cb72-576"><a href="#cb72-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-577"><a href="#cb72-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-578"><a href="#cb72-578" aria-hidden="true" tabindex="-1"></a><span class="fu">### Small sample properties of the **IV** estimator {#sec-ssprop_iv}</span></span>
<span id="cb72-579"><a href="#cb72-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-580"><a href="#cb72-580" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{small sample properties!instrumental variable estimator|(}</span>
<span id="cb72-581"><a href="#cb72-581" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!instrumental variable estimator|(}</span>
<span id="cb72-582"><a href="#cb72-582" aria-hidden="true" tabindex="-1"></a>Although it is consistent, the instrumental variable estimator isn't</span>
<span id="cb72-583"><a href="#cb72-583" aria-hidden="true" tabindex="-1"></a>unbiased. Actually, it doesn't even have an expected value in the just-identified case. This result can be easily shown starting with the</span>
<span id="cb72-584"><a href="#cb72-584" aria-hidden="true" tabindex="-1"></a>following system of equations:^<span class="co">[</span><span class="ot">See @DAVI:MACK:04\index[author]{Davidson}\index[author]{McKinnon}, pages 326-327.</span><span class="co">]</span></span>
<span id="cb72-585"><a href="#cb72-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-586"><a href="#cb72-586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-587"><a href="#cb72-587" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-588"><a href="#cb72-588" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-589"><a href="#cb72-589" aria-hidden="true" tabindex="-1"></a>y &amp;=&amp; \alpha + \beta x + \sigma_\epsilon \epsilon<span class="sc">\\</span></span>
<span id="cb72-590"><a href="#cb72-590" aria-hidden="true" tabindex="-1"></a>x &amp;=&amp; \gamma + \delta w + \sigma_\nu \nu<span class="sc">\\</span></span>
<span id="cb72-591"><a href="#cb72-591" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-592"><a href="#cb72-592" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-593"><a href="#cb72-593" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-594"><a href="#cb72-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-595"><a href="#cb72-595" aria-hidden="true" tabindex="-1"></a>where, for convenience, the two error terms are written as standard</span>
<span id="cb72-596"><a href="#cb72-596" aria-hidden="true" tabindex="-1"></a>normal deviates. Moreover, we can write $\epsilon = \rho \nu + \iota$,</span>
<span id="cb72-597"><a href="#cb72-597" aria-hidden="true" tabindex="-1"></a>so $\rho$ is the coefficient of correlation between the two error</span>
<span id="cb72-598"><a href="#cb72-598" aria-hidden="true" tabindex="-1"></a>terms and $\iota$ is by construction uncorrelated with $\nu$. The IV</span>
<span id="cb72-599"><a href="#cb72-599" aria-hidden="true" tabindex="-1"></a>estimator is then:</span>
<span id="cb72-600"><a href="#cb72-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-601"><a href="#cb72-601" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-602"><a href="#cb72-602" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_n (w_n - \bar{w})(y_n - \bar{y})}</span>
<span id="cb72-603"><a href="#cb72-603" aria-hidden="true" tabindex="-1"></a>{\sum_n (w_n - \bar{w})(x_n - \bar{x})} = \beta + \frac{\sigma_{\epsilon}\sum_n (w_n -</span>
<span id="cb72-604"><a href="#cb72-604" aria-hidden="true" tabindex="-1"></a>\bar{w})(\rho \nu_n + \iota_n)}{\sum_n (w_n - \bar{w})(x_n - \bar{x})}</span>
<span id="cb72-605"><a href="#cb72-605" aria-hidden="true" tabindex="-1"></a>$$ {#eq-simpleiv_bias1}</span>
<span id="cb72-606"><a href="#cb72-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-607"><a href="#cb72-607" aria-hidden="true" tabindex="-1"></a>The denominator is closely linked to the OLS estimator of $\gamma$,</span>
<span id="cb72-608"><a href="#cb72-608" aria-hidden="true" tabindex="-1"></a>which is:</span>
<span id="cb72-609"><a href="#cb72-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-610"><a href="#cb72-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-611"><a href="#cb72-611" aria-hidden="true" tabindex="-1"></a>\hat{\delta} = \frac{\sum_n (w_n - \bar{w})(x_n - \bar{x})}{\sum_n</span>
<span id="cb72-612"><a href="#cb72-612" aria-hidden="true" tabindex="-1"></a>(w_n - \bar{w}) ^ 2} = \delta + </span>
<span id="cb72-613"><a href="#cb72-613" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\nu\sum_n (w_n - \bar{w})\nu_n}{\sum_n (w_n - \bar{w}) ^ 2}</span>
<span id="cb72-614"><a href="#cb72-614" aria-hidden="true" tabindex="-1"></a>$$ {#eq-simpleiv_bias2}</span>
<span id="cb72-615"><a href="#cb72-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-616"><a href="#cb72-616" aria-hidden="true" tabindex="-1"></a>From @eq-simpleiv_bias1 and @eq-simpleiv_bias2, we get:</span>
<span id="cb72-617"><a href="#cb72-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-618"><a href="#cb72-618" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-619"><a href="#cb72-619" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \beta + \frac{\sigma_\epsilon \rho \sum_n (w_n - \bar{w})\nu_n +</span>
<span id="cb72-620"><a href="#cb72-620" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon \sum_n (w_n - \bar{w}) \iota_n}</span>
<span id="cb72-621"><a href="#cb72-621" aria-hidden="true" tabindex="-1"></a>{\delta \sum_n(w_n - \bar{w}) ^ 2 + \sigma_\nu\sum_n(w_n - \bar{w}) \nu_n}</span>
<span id="cb72-622"><a href="#cb72-622" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-623"><a href="#cb72-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-624"><a href="#cb72-624" aria-hidden="true" tabindex="-1"></a>Denoting $c_n = \frac{w_n - \bar{w}}{\sqrt{\sum_n (w_n - \bar{w}) ^</span>
<span id="cb72-625"><a href="#cb72-625" aria-hidden="true" tabindex="-1"></a>2}}$, with $\sum_n c_n ^ 2 = 1$, we get:</span>
<span id="cb72-626"><a href="#cb72-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-627"><a href="#cb72-627" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-628"><a href="#cb72-628" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \beta + \frac{\sigma_\epsilon \rho \sum_n c_n\nu_n +</span>
<span id="cb72-629"><a href="#cb72-629" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon \sum_n c_n\iota_n}</span>
<span id="cb72-630"><a href="#cb72-630" aria-hidden="true" tabindex="-1"></a>{\gamma \sqrt{\sum_n(w_n - \bar{w}) ^ 2} + \sigma_\nu\sum_n c_n \nu_n}</span>
<span id="cb72-631"><a href="#cb72-631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-632"><a href="#cb72-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-633"><a href="#cb72-633" aria-hidden="true" tabindex="-1"></a>As, by construction, $\mbox{E}(\iota \mid \nu) = 0$, denoting $\omega =</span>
<span id="cb72-634"><a href="#cb72-634" aria-hidden="true" tabindex="-1"></a>\sum_n c_n \nu_n$, which is a standard normal deviate and $a = \delta \sqrt{\sum_n (w_n - \bar{w}) ^ 2} / \sigma_\nu$ we finally get:</span>
<span id="cb72-635"><a href="#cb72-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-636"><a href="#cb72-636" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-637"><a href="#cb72-637" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \beta + \frac{\sigma_\epsilon</span>
<span id="cb72-638"><a href="#cb72-638" aria-hidden="true" tabindex="-1"></a>\rho}{\sigma_\nu}\frac{\omega}{\omega + a}</span>
<span id="cb72-639"><a href="#cb72-639" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-640"><a href="#cb72-640" aria-hidden="true" tabindex="-1"></a>Then the expected value of $\hat{\beta}$ is obtained by integrating</span>
<span id="cb72-641"><a href="#cb72-641" aria-hidden="true" tabindex="-1"></a>out this expression with respect to $\omega$:</span>
<span id="cb72-642"><a href="#cb72-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-643"><a href="#cb72-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-644"><a href="#cb72-644" aria-hidden="true" tabindex="-1"></a>\mbox{E}(\hat{\beta}) = \beta  + \frac{\sigma_\epsilon</span>
<span id="cb72-645"><a href="#cb72-645" aria-hidden="true" tabindex="-1"></a>\rho}{\sigma_\nu}\int_{-\infty}^{\infty} \frac{\omega}{\omega + a}\phi(\omega)d\omega</span>
<span id="cb72-646"><a href="#cb72-646" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-647"><a href="#cb72-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-648"><a href="#cb72-648" aria-hidden="true" tabindex="-1"></a>but this integral is divergent as $\omega / (a + \omega)$ tends to</span>
<span id="cb72-649"><a href="#cb72-649" aria-hidden="true" tabindex="-1"></a>infinity as $\omega$ approach  $-a$. Therefore, $\hat{\beta}$ has no expected value. The 2SLS derivation of the IV estimator also gives an intuition of the reason why the IV estimator is not unbiased. It would be if, for the second OLS</span>
<span id="cb72-650"><a href="#cb72-650" aria-hidden="true" tabindex="-1"></a>estimation, $\mbox{E}(x_n\mid w_n) = \gamma + \delta w_n$ were used as the</span>
<span id="cb72-651"><a href="#cb72-651" aria-hidden="true" tabindex="-1"></a>regressor. But actually, $\hat{x}_n = \hat{\gamma} + \hat{\delta} w_n$</span>
<span id="cb72-652"><a href="#cb72-652" aria-hidden="true" tabindex="-1"></a>is used and, as the OLS estimator over-fits, the fitted values of $x$ will be</span>
<span id="cb72-653"><a href="#cb72-653" aria-hidden="true" tabindex="-1"></a>partly correlated with $\epsilon$. Of course, when the sample size</span>
<span id="cb72-654"><a href="#cb72-654" aria-hidden="true" tabindex="-1"></a>grows, as the OLS estimator is consistent, $\hat{x}_n$ converges to</span>
<span id="cb72-655"><a href="#cb72-655" aria-hidden="true" tabindex="-1"></a>$\gamma + \delta z_n$ and the asymptotic bias vanishes.</span>
<span id="cb72-656"><a href="#cb72-656" aria-hidden="true" tabindex="-1"></a>This can be usefully illustrated by simulation. The </span>
<span id="cb72-657"><a href="#cb72-657" aria-hidden="true" tabindex="-1"></a><span class="in">`iv_data`</span> function draws a sample of $y$, $x$ and one instrument $w$:</span>
<span id="cb72-658"><a href="#cb72-658" aria-hidden="true" tabindex="-1"></a>\idxfun{names}{base}\idxfun{matrix}{base}\idxfun{rnorm}{stats}\idxfun{chol}{base}\idxfun{colnames}{base}\idxfun{as<span class="sc">\_</span>tibble}{tibble}\idxfun{add<span class="sc">\_</span>column}{tibble}\idxfun{factor}{base}\idxfun{mutate}{dplyr}\idxfun{matrix}{base}</span>
<span id="cb72-659"><a href="#cb72-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-660"><a href="#cb72-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-661"><a href="#cb72-661" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-662"><a href="#cb72-662" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: iv_data</span></span>
<span id="cb72-663"><a href="#cb72-663" aria-hidden="true" tabindex="-1"></a>iv_data <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">N =</span> <span class="fl">5E01</span>, <span class="at">R =</span> <span class="fl">1E03</span>, </span>
<span id="cb72-664"><a href="#cb72-664" aria-hidden="true" tabindex="-1"></a>                      <span class="at">r_xe =</span> <span class="fl">0.5</span>, <span class="at">r_xw =</span> <span class="fl">0.2</span>, <span class="at">r_we =</span> <span class="dv">0</span>,</span>
<span id="cb72-665"><a href="#cb72-665" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">beta =</span> <span class="dv">1</span>,</span>
<span id="cb72-666"><a href="#cb72-666" aria-hidden="true" tabindex="-1"></a>                      <span class="at">sds =</span> <span class="fu">c</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">e =</span> <span class="dv">1</span>, <span class="at">w =</span> <span class="dv">1</span>),</span>
<span id="cb72-667"><a href="#cb72-667" aria-hidden="true" tabindex="-1"></a>                      <span class="at">mns =</span> <span class="fu">c</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">w =</span> <span class="dv">0</span>)){</span>
<span id="cb72-668"><a href="#cb72-668" aria-hidden="true" tabindex="-1"></a>    nms <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"e"</span>, <span class="st">"w"</span>)</span>
<span id="cb72-669"><a href="#cb72-669" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(sds) <span class="ot">&lt;-</span> nms ;  <span class="fu">names</span>(mns) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"w"</span>)</span>
<span id="cb72-670"><a href="#cb72-670" aria-hidden="true" tabindex="-1"></a>    b_wx <span class="ot">&lt;-</span> r_xw <span class="sc">*</span> sds[<span class="st">"x"</span>] <span class="sc">/</span> sds[<span class="st">"w"</span>]</span>
<span id="cb72-671"><a href="#cb72-671" aria-hidden="true" tabindex="-1"></a>    a_wx <span class="ot">&lt;-</span> mns[<span class="st">"x"</span>] <span class="sc">-</span> b_wx <span class="sc">*</span> mns[<span class="st">"w"</span>]</span>
<span id="cb72-672"><a href="#cb72-672" aria-hidden="true" tabindex="-1"></a>    cors <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, r_xe, r_xw, r_xe, <span class="dv">1</span>, r_we, r_xw, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb72-673"><a href="#cb72-673" aria-hidden="true" tabindex="-1"></a>    XEW <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> R <span class="sc">*</span> <span class="dv">3</span>), <span class="at">nrow =</span> N <span class="sc">*</span> R) <span class="sc">%*%</span></span>
<span id="cb72-674"><a href="#cb72-674" aria-hidden="true" tabindex="-1"></a>        <span class="fu">chol</span>(cors) </span>
<span id="cb72-675"><a href="#cb72-675" aria-hidden="true" tabindex="-1"></a>    <span class="fu">colnames</span>(XEW) <span class="ot">&lt;-</span> nms</span>
<span id="cb72-676"><a href="#cb72-676" aria-hidden="true" tabindex="-1"></a>    XEW <span class="sc">%&gt;%</span></span>
<span id="cb72-677"><a href="#cb72-677" aria-hidden="true" tabindex="-1"></a>        as_tibble <span class="sc">%&gt;%</span></span>
<span id="cb72-678"><a href="#cb72-678" aria-hidden="true" tabindex="-1"></a>        <span class="fu">mutate</span>(<span class="at">x =</span> x <span class="sc">*</span> sds[<span class="st">"x"</span>] <span class="sc">+</span> mns[<span class="st">"x"</span>],</span>
<span id="cb72-679"><a href="#cb72-679" aria-hidden="true" tabindex="-1"></a>               <span class="at">w =</span> w <span class="sc">*</span> sds[<span class="st">"w"</span>] <span class="sc">+</span> mns[<span class="st">"w"</span>],</span>
<span id="cb72-680"><a href="#cb72-680" aria-hidden="true" tabindex="-1"></a>               <span class="at">e =</span> e <span class="sc">*</span> sds[<span class="st">"e"</span>],</span>
<span id="cb72-681"><a href="#cb72-681" aria-hidden="true" tabindex="-1"></a>               <span class="at">Exw =</span> a_wx <span class="sc">+</span> b_wx <span class="sc">*</span> w,</span>
<span id="cb72-682"><a href="#cb72-682" aria-hidden="true" tabindex="-1"></a>               <span class="at">y =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> e) <span class="sc">%&gt;%</span></span>
<span id="cb72-683"><a href="#cb72-683" aria-hidden="true" tabindex="-1"></a>        <span class="fu">add_column</span>(<span class="at">id =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>R, <span class="at">each =</span> N)), <span class="at">.before =</span> <span class="dv">1</span>)</span>
<span id="cb72-684"><a href="#cb72-684" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-685"><a href="#cb72-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-686"><a href="#cb72-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-687"><a href="#cb72-687" aria-hidden="true" tabindex="-1"></a>The arguments of the function are the sample size (<span class="in">`N`</span>), the number of samples (<span class="in">`R`</span>),</span>
<span id="cb72-688"><a href="#cb72-688" aria-hidden="true" tabindex="-1"></a>the correlations between</span>
<span id="cb72-689"><a href="#cb72-689" aria-hidden="true" tabindex="-1"></a>$x$ and $\epsilon$ (the default is 0.5), between $x$ and $w$ (0.2 by</span>
<span id="cb72-690"><a href="#cb72-690" aria-hidden="true" tabindex="-1"></a>default) and between $w$ and $\epsilon$. The default value for</span>
<span id="cb72-691"><a href="#cb72-691" aria-hidden="true" tabindex="-1"></a>this last correlation is 0, which is a necessary condition for the IV</span>
<span id="cb72-692"><a href="#cb72-692" aria-hidden="true" tabindex="-1"></a>estimator to be consistent. $x$, $\epsilon$ and $w$ are assumed by</span>
<span id="cb72-693"><a href="#cb72-693" aria-hidden="true" tabindex="-1"></a>default to be standard normal deviates, but the means of $x$ and $w$</span>
<span id="cb72-694"><a href="#cb72-694" aria-hidden="true" tabindex="-1"></a>and the standard deviations of $x$, $w$ and $\epsilon$ can be</span>
<span id="cb72-695"><a href="#cb72-695" aria-hidden="true" tabindex="-1"></a>customized using the <span class="in">`mns`</span> and <span class="in">`sds`</span> argument. Finally, the</span>
<span id="cb72-696"><a href="#cb72-696" aria-hidden="true" tabindex="-1"></a>coefficients of the linear relation between $y$ and $x$ are <span class="in">`alpha`</span></span>
<span id="cb72-697"><a href="#cb72-697" aria-hidden="true" tabindex="-1"></a>and <span class="in">`beta`</span> and these two values are set by default to 1. First, a</span>
<span id="cb72-698"><a href="#cb72-698" aria-hidden="true" tabindex="-1"></a>matrix of normal standard deviates <span class="in">`XEW`</span> is constructed. This matrix</span>
<span id="cb72-699"><a href="#cb72-699" aria-hidden="true" tabindex="-1"></a>is post-multiplied by the Cholesky decomposition of the matrix of</span>
<span id="cb72-700"><a href="#cb72-700" aria-hidden="true" tabindex="-1"></a>correlation, which introduces the desired correlation between $x$,</span>
<span id="cb72-701"><a href="#cb72-701" aria-hidden="true" tabindex="-1"></a>$\epsilon$ and $w$, which are then adjusted for</span>
<span id="cb72-702"><a href="#cb72-702" aria-hidden="true" tabindex="-1"></a>non-zero means and non-unity standard deviations if necessary. Finally the vector</span>
<span id="cb72-703"><a href="#cb72-703" aria-hidden="true" tabindex="-1"></a>of response is computed, along with the conditional expectation of $x$: $\mbox{E}(x\mid w) = \gamma + \delta w$. The <span class="in">`iv_coefs`</span> function computes the IV estimator using the</span>
<span id="cb72-704"><a href="#cb72-704" aria-hidden="true" tabindex="-1"></a>2SLS approach.</span>
<span id="cb72-705"><a href="#cb72-705" aria-hidden="true" tabindex="-1"></a>\idxfun{lm.fit}{stats}\idxfun{coef}{stats}\idxfun{unname}{base}\idxfun{tibble}{tibble}</span>
<span id="cb72-706"><a href="#cb72-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-707"><a href="#cb72-707" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-708"><a href="#cb72-708" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: iv_coefs</span></span>
<span id="cb72-709"><a href="#cb72-709" aria-hidden="true" tabindex="-1"></a>iv_coefs <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb72-710"><a href="#cb72-710" aria-hidden="true" tabindex="-1"></a>    xh <span class="ot">&lt;-</span> <span class="fu">lm.fit</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, i<span class="sc">$</span>w), i<span class="sc">$</span>x)<span class="sc">$</span>fitted.values</span>
<span id="cb72-711"><a href="#cb72-711" aria-hidden="true" tabindex="-1"></a>    ols <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm.fit</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, i<span class="sc">$</span>x), i<span class="sc">$</span>y))[<span class="dv">2</span>] <span class="sc">%&gt;%</span> unname</span>
<span id="cb72-712"><a href="#cb72-712" aria-hidden="true" tabindex="-1"></a>    ivo <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm.fit</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, i<span class="sc">$</span>Exw), i<span class="sc">$</span>y))[<span class="dv">2</span>] <span class="sc">%&gt;%</span> unname</span>
<span id="cb72-713"><a href="#cb72-713" aria-hidden="true" tabindex="-1"></a>    iv <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm.fit</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, xh), i<span class="sc">$</span>y))[<span class="dv">2</span>] <span class="sc">%&gt;%</span> unname</span>
<span id="cb72-714"><a href="#cb72-714" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tibble</span>(<span class="at">ols =</span> ols, <span class="at">ivo =</span> ivo, <span class="at">iv =</span> iv)</span>
<span id="cb72-715"><a href="#cb72-715" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-716"><a href="#cb72-716" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-717"><a href="#cb72-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-718"><a href="#cb72-718" aria-hidden="true" tabindex="-1"></a>It uses <span class="in">`lm.fit`</span> to regress $x$ on $w$.^<span class="co">[</span><span class="ot">`lm.fit` is used internally by `lm`, and its two first arguments are a matrix of covariates and the response.</span><span class="co">]</span> We compute <span class="in">`xh`</span> which is</span>
<span id="cb72-719"><a href="#cb72-719" aria-hidden="true" tabindex="-1"></a>$\hat{x} = \hat{\gamma} + \hat{\delta}  w_n$, the fitted values</span>
<span id="cb72-720"><a href="#cb72-720" aria-hidden="true" tabindex="-1"></a>of the regression of $x$ on $w$. <span class="in">`iv_coefs`</span> computes three estimators:</span>
<span id="cb72-721"><a href="#cb72-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-722"><a href="#cb72-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`ols`</span>, which is the OLS estimator of $y$ on $x$,</span>
<span id="cb72-723"><a href="#cb72-723" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`iv`</span>, which is the 2SLS estimator of $y$ on $x$ using $w$ as</span>
<span id="cb72-724"><a href="#cb72-724" aria-hidden="true" tabindex="-1"></a>  instruments,</span>
<span id="cb72-725"><a href="#cb72-725" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`ivo`</span>, which is an estimator that can only be computed in the context of simulations and</span>
<span id="cb72-726"><a href="#cb72-726" aria-hidden="true" tabindex="-1"></a>  uses $\mbox{E}(x_n\mid w_n)$ instead of $\hat{x}_n$ as the regressor in</span>
<span id="cb72-727"><a href="#cb72-727" aria-hidden="true" tabindex="-1"></a>  the second OLS estimation.</span>
<span id="cb72-728"><a href="#cb72-728" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-729"><a href="#cb72-729" aria-hidden="true" tabindex="-1"></a>We first generate a unique sample of 100 observations and we compute the three estimators.</span>
<span id="cb72-730"><a href="#cb72-730" aria-hidden="true" tabindex="-1"></a>\idxfun{set.seed}{base}</span>
<span id="cb72-731"><a href="#cb72-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-732"><a href="#cb72-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-733"><a href="#cb72-733" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: iv_data_one_sample</span></span>
<span id="cb72-734"><a href="#cb72-734" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-735"><a href="#cb72-735" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb72-736"><a href="#cb72-736" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">iv_data</span>(<span class="at">R =</span> <span class="dv">1</span>, <span class="at">N =</span> <span class="fl">1E02</span>)</span>
<span id="cb72-737"><a href="#cb72-737" aria-hidden="true" tabindex="-1"></a><span class="fu">iv_coefs</span>(d)</span>
<span id="cb72-738"><a href="#cb72-738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-739"><a href="#cb72-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-740"><a href="#cb72-740" aria-hidden="true" tabindex="-1"></a>To empirically analyze the distribution of these three estimators, we then</span>
<span id="cb72-741"><a href="#cb72-741" aria-hidden="true" tabindex="-1"></a>generate several samples by setting the <span class="in">`R`</span> argument, </span>
<span id="cb72-742"><a href="#cb72-742" aria-hidden="true" tabindex="-1"></a>compute the estimators for every sample and analyze the empirical</span>
<span id="cb72-743"><a href="#cb72-743" aria-hidden="true" tabindex="-1"></a>distribution of the estimators. This can be easily done using</span>
<span id="cb72-744"><a href="#cb72-744" aria-hidden="true" tabindex="-1"></a><span class="in">`tidyr::nests`</span> and <span class="in">`tidyr::unnests`</span> functions, as in @sec-random_numbers_simulations. </span>
<span id="cb72-745"><a href="#cb72-745" aria-hidden="true" tabindex="-1"></a>\idxfun{set.seed}{base}\idxfun{nest}{tidyr}\idxfun{transmute}{dplyr}\idxfun{unnest}{tidyr}\idxfun{map}{purrr}</span>
<span id="cb72-746"><a href="#cb72-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-747"><a href="#cb72-747" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-748"><a href="#cb72-748" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simul_nest_unnest</span></span>
<span id="cb72-749"><a href="#cb72-749" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb72-750"><a href="#cb72-750" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb72-751"><a href="#cb72-751" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">iv_data</span>(<span class="at">R =</span> <span class="fl">1E04</span>, <span class="at">N =</span> <span class="dv">50</span>) <span class="sc">%&gt;%</span></span>
<span id="cb72-752"><a href="#cb72-752" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nest</span>(<span class="at">.by =</span> id) <span class="sc">%&gt;%</span></span>
<span id="cb72-753"><a href="#cb72-753" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(<span class="at">model =</span> <span class="fu">map</span>(data, iv_coefs)) <span class="sc">%&gt;%</span></span>
<span id="cb72-754"><a href="#cb72-754" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unnest</span>(<span class="at">cols =</span> model)</span>
<span id="cb72-755"><a href="#cb72-755" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb72-756"><a href="#cb72-756" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-757"><a href="#cb72-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-758"><a href="#cb72-758" aria-hidden="true" tabindex="-1"></a>We then compute the mean, the median and the standard deviation for the three</span>
<span id="cb72-759"><a href="#cb72-759" aria-hidden="true" tabindex="-1"></a>estimators:</span>
<span id="cb72-760"><a href="#cb72-760" aria-hidden="true" tabindex="-1"></a>\idxfun{summarise}{dplyr}\idxfun{everything}{dplyr}\idxfun{list}{base}\idxfun{pivot<span class="sc">\_</span>longer}{tidyr}\idxfun{separate}{tidyr}\idxfun{pivot<span class="sc">\_</span>wider}{tidyr}</span>
<span id="cb72-761"><a href="#cb72-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-762"><a href="#cb72-762" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-763"><a href="#cb72-763" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: dist_iv_estimator</span></span>
<span id="cb72-764"><a href="#cb72-764" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="fu">across</span>(<span class="fu">everything</span>(),</span>
<span id="cb72-765"><a href="#cb72-765" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">list</span>(<span class="at">mean =</span> mean, <span class="at">median =</span> median, </span>
<span id="cb72-766"><a href="#cb72-766" aria-hidden="true" tabindex="-1"></a>                            <span class="at">sd =</span> sd))) <span class="sc">%&gt;%</span></span>
<span id="cb72-767"><a href="#cb72-767" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_longer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>) <span class="sc">%&gt;%</span></span>
<span id="cb72-768"><a href="#cb72-768" aria-hidden="true" tabindex="-1"></a>    <span class="fu">separate</span>(name, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">"model"</span>, <span class="st">"stat"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb72-769"><a href="#cb72-769" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> stat, <span class="at">values_from =</span> value)</span>
<span id="cb72-770"><a href="#cb72-770" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-771"><a href="#cb72-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-772"><a href="#cb72-772" aria-hidden="true" tabindex="-1"></a>The distribution of the three estimators is presented in @fig-dist_iv.</span>
<span id="cb72-773"><a href="#cb72-773" aria-hidden="true" tabindex="-1"></a>\idxfun{as<span class="sc">\_</span>tibble}{tibble}\idxfun{pivot<span class="sc">\_</span>longer}{tidyr}\idxfun{ggplot}{ggplot2}\idxfun{aes}{ggplot2}\idxfun{geom<span class="sc">\_</span>density}{ggplot2}\idxfun{scale<span class="sc">\_</span>x<span class="sc">\_</span>continuous}{ggplot2}</span>
<span id="cb72-774"><a href="#cb72-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-775"><a href="#cb72-775" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-776"><a href="#cb72-776" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb72-777"><a href="#cb72-777" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb72-778"><a href="#cb72-778" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of the IV estimator"</span></span>
<span id="cb72-779"><a href="#cb72-779" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dist_iv</span></span>
<span id="cb72-780"><a href="#cb72-780" aria-hidden="true" tabindex="-1"></a>dsties <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span> as_tibble <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span></span>
<span id="cb72-781"><a href="#cb72-781" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">linetype =</span> name)) <span class="sc">+</span></span>
<span id="cb72-782"><a href="#cb72-782" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb72-783"><a href="#cb72-783" aria-hidden="true" tabindex="-1"></a>dsties</span>
<span id="cb72-784"><a href="#cb72-784" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-785"><a href="#cb72-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-786"><a href="#cb72-786" aria-hidden="true" tabindex="-1"></a>The OLS estimator is severely biased, as the central value of its</span>
<span id="cb72-787"><a href="#cb72-787" aria-hidden="true" tabindex="-1"></a>distribution is about 1.5 and it has a small variance. The "pseudo-IV" estimator seems unbiased, as its mean (and median) is very close</span>
<span id="cb72-788"><a href="#cb72-788" aria-hidden="true" tabindex="-1"></a>to one. The standard deviation is about 10 times larger than that</span>
<span id="cb72-789"><a href="#cb72-789" aria-hidden="true" tabindex="-1"></a>of the OLS estimator, so the density curve is extremely flat.  The</span>
<span id="cb72-790"><a href="#cb72-790" aria-hidden="true" tabindex="-1"></a>mode of the density curve of the IV estimator is slightly larger</span>
<span id="cb72-791"><a href="#cb72-791" aria-hidden="true" tabindex="-1"></a>than 1. Moreover, it has extremely fat tails (much more than the</span>
<span id="cb72-792"><a href="#cb72-792" aria-hidden="true" tabindex="-1"></a>ones of the pseudo-IV estimator), which explains why the expected</span>
<span id="cb72-793"><a href="#cb72-793" aria-hidden="true" tabindex="-1"></a>value and the variance don't exist. This feature becomes obvious if we</span>
<span id="cb72-794"><a href="#cb72-794" aria-hidden="true" tabindex="-1"></a>zoom in extreme values of the estimator, for example on the $(-4,-3)$</span>
<span id="cb72-795"><a href="#cb72-795" aria-hidden="true" tabindex="-1"></a>range (see @fig-dist_iv_zoom).</span>
<span id="cb72-796"><a href="#cb72-796" aria-hidden="true" tabindex="-1"></a>\idxfun{coord<span class="sc">\_</span>cartesian}{ggplot2}</span>
<span id="cb72-797"><a href="#cb72-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-798"><a href="#cb72-798" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-799"><a href="#cb72-799" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb72-800"><a href="#cb72-800" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of the IV estimator (zoom)"</span></span>
<span id="cb72-801"><a href="#cb72-801" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dist_iv_zoom</span></span>
<span id="cb72-802"><a href="#cb72-802" aria-hidden="true" tabindex="-1"></a>dsties <span class="sc">+</span> <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="sc">-</span><span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.015</span>))</span>
<span id="cb72-803"><a href="#cb72-803" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-804"><a href="#cb72-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-805"><a href="#cb72-805" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{small sample properties!instrumental variable estimator|(}</span>
<span id="cb72-806"><a href="#cb72-806" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{bias!instrumental variable estimator|(}</span>
<span id="cb72-807"><a href="#cb72-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-808"><a href="#cb72-808" aria-hidden="true" tabindex="-1"></a><span class="fu">### An example: Segregation effects on urban poverty and inequality</span></span>
<span id="cb72-809"><a href="#cb72-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-810"><a href="#cb72-810" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{tracks<span class="sc">\_</span>side}{micsr.data}</span>
<span id="cb72-811"><a href="#cb72-811" aria-hidden="true" tabindex="-1"></a>@ANAN:11\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Ananat} investigates the causal effect of segregation on urban poverty</span>
<span id="cb72-812"><a href="#cb72-812" aria-hidden="true" tabindex="-1"></a>and inequality. Several responses are used, especially the Gini index and </span>
<span id="cb72-813"><a href="#cb72-813" aria-hidden="true" tabindex="-1"></a>the poverty rate for the black populations of 121 American cities. The</span>
<span id="cb72-814"><a href="#cb72-814" aria-hidden="true" tabindex="-1"></a>data set is called <span class="in">`tracks_side`</span>. The level of segregation is</span>
<span id="cb72-815"><a href="#cb72-815" aria-hidden="true" tabindex="-1"></a>measured by the following dissimilarity index:</span>
<span id="cb72-816"><a href="#cb72-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-817"><a href="#cb72-817" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-818"><a href="#cb72-818" aria-hidden="true" tabindex="-1"></a>\frac{1}{2}\sum_{n=1} ^ N \left| b_n - w_n \right|</span>
<span id="cb72-819"><a href="#cb72-819" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-820"><a href="#cb72-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-821"><a href="#cb72-821" aria-hidden="true" tabindex="-1"></a>where $b_n$ and $w_n$ are respectively the share of the black and white</span>
<span id="cb72-822"><a href="#cb72-822" aria-hidden="true" tabindex="-1"></a>population of the whole city that lives in census track $n$ of the</span>
<span id="cb72-823"><a href="#cb72-823" aria-hidden="true" tabindex="-1"></a>city. This index ranges from 0 (no segregation) to 1 (perfect</span>
<span id="cb72-824"><a href="#cb72-824" aria-hidden="true" tabindex="-1"></a>segregation). In the sample of 121 cities used, the segregation index ranges</span>
<span id="cb72-825"><a href="#cb72-825" aria-hidden="true" tabindex="-1"></a>from <span class="in">`r round(min(tracks_side$segregation), 2)`</span> to </span>
<span id="cb72-826"><a href="#cb72-826" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(max(tracks_side$segregation), 2)`</span>, </span>
<span id="cb72-827"><a href="#cb72-827" aria-hidden="true" tabindex="-1"></a>with a median value of <span class="in">`r round(median(tracks_side$segregation), 2)`</span>.</span>
<span id="cb72-828"><a href="#cb72-828" aria-hidden="true" tabindex="-1"></a>We'll focus on the effect of segregation on the poverty rate of black</span>
<span id="cb72-829"><a href="#cb72-829" aria-hidden="true" tabindex="-1"></a>people:</span>
<span id="cb72-830"><a href="#cb72-830" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-831"><a href="#cb72-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-832"><a href="#cb72-832" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-833"><a href="#cb72-833" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ols_tracks_side</span></span>
<span id="cb72-834"><a href="#cb72-834" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-835"><a href="#cb72-835" aria-hidden="true" tabindex="-1"></a>lm_yx <span class="ot">&lt;-</span> <span class="fu">lm</span>(povb <span class="sc">~</span> segregation, tracks_side)</span>
<span id="cb72-836"><a href="#cb72-836" aria-hidden="true" tabindex="-1"></a>lm_yx <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-837"><a href="#cb72-837" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-838"><a href="#cb72-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-839"><a href="#cb72-839" aria-hidden="true" tabindex="-1"></a>The coefficient of segregation is positive and highly significant. It</span>
<span id="cb72-840"><a href="#cb72-840" aria-hidden="true" tabindex="-1"></a>indicates that a 1 point increase of the segregation index raises the</span>
<span id="cb72-841"><a href="#cb72-841" aria-hidden="true" tabindex="-1"></a>poverty rate of black people by about <span class="in">`r round(coef(lm_yx)[2], 2)`</span> point. </span>
<span id="cb72-842"><a href="#cb72-842" aria-hidden="true" tabindex="-1"></a>The correlation between segregation and bad economic outcome for black people is well</span>
<span id="cb72-843"><a href="#cb72-843" aria-hidden="true" tabindex="-1"></a>established but, according to the author, the OLS estimator cannot easily be</span>
<span id="cb72-844"><a href="#cb72-844" aria-hidden="true" tabindex="-1"></a>considered as a measure of the causal relationship of segregation on</span>
<span id="cb72-845"><a href="#cb72-845" aria-hidden="true" tabindex="-1"></a>income, as there are some other variables that both influence</span>
<span id="cb72-846"><a href="#cb72-846" aria-hidden="true" tabindex="-1"></a>segregation and outcome for black people. As an example, the situation of Detroit is described, which is a highly segregated city</span>
<span id="cb72-847"><a href="#cb72-847" aria-hidden="true" tabindex="-1"></a>with poor economic outcomes, but other characteristics of the city</span>
<span id="cb72-848"><a href="#cb72-848" aria-hidden="true" tabindex="-1"></a>(political corruption, legacy of a manufacturing economy) can be the</span>
<span id="cb72-849"><a href="#cb72-849" aria-hidden="true" tabindex="-1"></a>cause of these two phenomena <span class="co">[</span><span class="ot">@ANAN:11 p. 35</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Ananat}.</span>
<span id="cb72-850"><a href="#cb72-850" aria-hidden="true" tabindex="-1"></a>Therefore, the OLS estimator is suspected to be biased and</span>
<span id="cb72-851"><a href="#cb72-851" aria-hidden="true" tabindex="-1"></a>inconsistent because of the omitted variable bias. The instrumental</span>
<span id="cb72-852"><a href="#cb72-852" aria-hidden="true" tabindex="-1"></a>variable estimator can be used in this context, but it requires the</span>
<span id="cb72-853"><a href="#cb72-853" aria-hidden="true" tabindex="-1"></a>use of a good instrumental variable, i.e., a variable which is correlated</span>
<span id="cb72-854"><a href="#cb72-854" aria-hidden="true" tabindex="-1"></a>with the endogenous covariate (segregation), but not directly with the</span>
<span id="cb72-855"><a href="#cb72-855" aria-hidden="true" tabindex="-1"></a>response (the poverty rate). The author suggests that the way cities</span>
<span id="cb72-856"><a href="#cb72-856" aria-hidden="true" tabindex="-1"></a>were subdivided by railroads into a large number of neighborhoods can</span>
<span id="cb72-857"><a href="#cb72-857" aria-hidden="true" tabindex="-1"></a>be used as an instrument. Moreover, the tracks were mostly built</span>
<span id="cb72-858"><a href="#cb72-858" aria-hidden="true" tabindex="-1"></a>during the nineteenth century, prior to the great migration (between 1915</span>
<span id="cb72-859"><a href="#cb72-859" aria-hidden="true" tabindex="-1"></a>to 1950) of African Americans from the south. More</span>
<span id="cb72-860"><a href="#cb72-860" aria-hidden="true" tabindex="-1"></a>precisely, the index is defined as follow:</span>
<span id="cb72-861"><a href="#cb72-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-862"><a href="#cb72-862" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-863"><a href="#cb72-863" aria-hidden="true" tabindex="-1"></a>1 - \sum_n \left(\frac{a_n}{A}\right) ^ 2</span>
<span id="cb72-864"><a href="#cb72-864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-865"><a href="#cb72-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-866"><a href="#cb72-866" aria-hidden="true" tabindex="-1"></a>where $a_n$ is the area of neighborhood $n$ defined by the rail</span>
<span id="cb72-867"><a href="#cb72-867" aria-hidden="true" tabindex="-1"></a>tracks and $A$ is the total area of the city. The index is 0 if the</span>
<span id="cb72-868"><a href="#cb72-868" aria-hidden="true" tabindex="-1"></a>city is completely undivided and tends to 1 if the number of</span>
<span id="cb72-869"><a href="#cb72-869" aria-hidden="true" tabindex="-1"></a>neighborhoods tends to infinity. This index </span>
<span id="cb72-870"><a href="#cb72-870" aria-hidden="true" tabindex="-1"></a>ranges from <span class="in">`r round(min(tracks_side$raildiv), 2)`</span> to </span>
<span id="cb72-871"><a href="#cb72-871" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(max(tracks_side$raildiv), 2)`</span> in the sample, </span>
<span id="cb72-872"><a href="#cb72-872" aria-hidden="true" tabindex="-1"></a>with a median value of <span class="in">`r round(median(tracks_side$raildiv), 2)`</span>.</span>
<span id="cb72-873"><a href="#cb72-873" aria-hidden="true" tabindex="-1"></a>The regression of $x$ (<span class="in">`segregation`</span>) on $w$ (<span class="in">`raildiv`</span>) gives:</span>
<span id="cb72-874"><a href="#cb72-874" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-875"><a href="#cb72-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-876"><a href="#cb72-876" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-877"><a href="#cb72-877" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: first_step_tracks_side</span></span>
<span id="cb72-878"><a href="#cb72-878" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-879"><a href="#cb72-879" aria-hidden="true" tabindex="-1"></a>lm_xw <span class="ot">&lt;-</span> <span class="fu">lm</span>(segregation <span class="sc">~</span> raildiv, tracks_side)</span>
<span id="cb72-880"><a href="#cb72-880" aria-hidden="true" tabindex="-1"></a>lm_xw <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-881"><a href="#cb72-881" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-882"><a href="#cb72-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-883"><a href="#cb72-883" aria-hidden="true" tabindex="-1"></a>In the 2SLS interpretation of the IV estimator, this is the</span>
<span id="cb72-884"><a href="#cb72-884" aria-hidden="true" tabindex="-1"></a>**first stage regression**\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{first stage regression}. The coefficient of <span class="in">`raildiv`</span> is, as expected, positive and highly significant. It is important to check that the</span>
<span id="cb72-885"><a href="#cb72-885" aria-hidden="true" tabindex="-1"></a>correlation between the covariate and the instrument is strong enough</span>
<span id="cb72-886"><a href="#cb72-886" aria-hidden="true" tabindex="-1"></a>to get a precise IV estimator. This can be performed by computing</span>
<span id="cb72-887"><a href="#cb72-887" aria-hidden="true" tabindex="-1"></a>their coefficient of correlation, or using the R^2^ or the $F$</span>
<span id="cb72-888"><a href="#cb72-888" aria-hidden="true" tabindex="-1"></a>statistic of the first stage regression:</span>
<span id="cb72-889"><a href="#cb72-889" aria-hidden="true" tabindex="-1"></a>\idxfun{summarise}{dplyr}\idxfun{cor}{stats}\idxfun{pull}{dplyr}\idxfun{ftest}{micsr}\idxfun{rsq}{micsr}\idxfun{gaze}{micsr}</span>
<span id="cb72-890"><a href="#cb72-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-891"><a href="#cb72-891" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-892"><a href="#cb72-892" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: cor_segr_rail</span></span>
<span id="cb72-893"><a href="#cb72-893" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-894"><a href="#cb72-894" aria-hidden="true" tabindex="-1"></a>tracks_side <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="fu">cor</span>(segregation, raildiv)) <span class="sc">%&gt;%</span> pull</span>
<span id="cb72-895"><a href="#cb72-895" aria-hidden="true" tabindex="-1"></a>lm_xw <span class="sc">%&gt;%</span> ftest <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-896"><a href="#cb72-896" aria-hidden="true" tabindex="-1"></a>lm_xw <span class="sc">%&gt;%</span> rsq</span>
<span id="cb72-897"><a href="#cb72-897" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-898"><a href="#cb72-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-899"><a href="#cb72-899" aria-hidden="true" tabindex="-1"></a>The IV estimator can be obtained by regressing the response on the</span>
<span id="cb72-900"><a href="#cb72-900" aria-hidden="true" tabindex="-1"></a>fitted values of the first stage regression:</span>
<span id="cb72-901"><a href="#cb72-901" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-902"><a href="#cb72-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-903"><a href="#cb72-903" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-904"><a href="#cb72-904" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: second_step_tracks_side</span></span>
<span id="cb72-905"><a href="#cb72-905" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-906"><a href="#cb72-906" aria-hidden="true" tabindex="-1"></a>lm_yhx <span class="ot">&lt;-</span> <span class="fu">lm</span>(povb <span class="sc">~</span> <span class="fu">fitted</span>(lm_xw), tracks_side)</span>
<span id="cb72-907"><a href="#cb72-907" aria-hidden="true" tabindex="-1"></a>lm_yhx <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-908"><a href="#cb72-908" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-909"><a href="#cb72-909" aria-hidden="true" tabindex="-1"></a>As expected, the IV estimator is larger than the OLS estimator (<span class="in">`r round(coef(lm_yhx)[2], 2)`</span> vs. <span class="in">`r round(coef(lm_yx)[2], 2)`</span>).</span>
<span id="cb72-910"><a href="#cb72-910" aria-hidden="true" tabindex="-1"></a>It can also be obtained by dividing the</span>
<span id="cb72-911"><a href="#cb72-911" aria-hidden="true" tabindex="-1"></a>OLS coefficients of the regressions of $y$ on $w$ and of $x$ on</span>
<span id="cb72-912"><a href="#cb72-912" aria-hidden="true" tabindex="-1"></a>$w$. The latter has already been computed, the former is:</span>
<span id="cb72-913"><a href="#cb72-913" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-914"><a href="#cb72-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-915"><a href="#cb72-915" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-916"><a href="#cb72-916" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-917"><a href="#cb72-917" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: iv_ratio_margeffects</span></span>
<span id="cb72-918"><a href="#cb72-918" aria-hidden="true" tabindex="-1"></a>lm_yw <span class="ot">&lt;-</span> <span class="fu">lm</span>(povb <span class="sc">~</span> raildiv, tracks_side)</span>
<span id="cb72-919"><a href="#cb72-919" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm_yw)[<span class="dv">2</span>]</span>
<span id="cb72-920"><a href="#cb72-920" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm_yw)[<span class="dv">2</span>] <span class="sc">/</span> <span class="fu">coef</span>(lm_xw)[<span class="dv">2</span>]</span>
<span id="cb72-921"><a href="#cb72-921" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-922"><a href="#cb72-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-923"><a href="#cb72-923" aria-hidden="true" tabindex="-1"></a>A 1 point increase of <span class="in">`raildiv`</span> is associated with a</span>
<span id="cb72-924"><a href="#cb72-924" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(coef(lm_xw)[2], 2)`</span> point increase of the discrimination index and</span>
<span id="cb72-925"><a href="#cb72-925" aria-hidden="true" tabindex="-1"></a>with a <span class="in">`r round(coef(lm_yw)[2], 2)`</span> point of the poverty</span>
<span id="cb72-926"><a href="#cb72-926" aria-hidden="true" tabindex="-1"></a>rate. Therefore, the </span>
<span id="cb72-927"><a href="#cb72-927" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(coef(lm_xw)[2], 2)`</span> point increase of <span class="in">`segregation`</span> </span>
<span id="cb72-928"><a href="#cb72-928" aria-hidden="true" tabindex="-1"></a>increases <span class="in">`povb`</span> by <span class="in">`r round(coef(lm_yw)[2], 2)`</span>, which means that an</span>
<span id="cb72-929"><a href="#cb72-929" aria-hidden="true" tabindex="-1"></a>increase of 1 point  of <span class="in">`segregation`</span> would increase <span class="in">`povb`</span> by </span>
<span id="cb72-930"><a href="#cb72-930" aria-hidden="true" tabindex="-1"></a>$<span class="in">`r round(coef(lm_yw)[2], 2)`</span> / <span class="in">`r round(coef(lm_xw)[2], 2)`</span> = </span>
<span id="cb72-931"><a href="#cb72-931" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(coef(lm_yw)[2] / coef(lm_xw)[2], 2)`</span>$, which is the value of</span>
<span id="cb72-932"><a href="#cb72-932" aria-hidden="true" tabindex="-1"></a>the IV estimator.</span>
<span id="cb72-933"><a href="#cb72-933" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{tracks<span class="sc">\_</span>side}{micsr.data}</span>
<span id="cb72-934"><a href="#cb72-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-935"><a href="#cb72-935" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!just identified|)}</span>
<span id="cb72-936"><a href="#cb72-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-937"><a href="#cb72-937" aria-hidden="true" tabindex="-1"></a><span class="fu">### Wald estimator {#sec-wald_estimator}</span></span>
<span id="cb72-938"><a href="#cb72-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-939"><a href="#cb72-939" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!Wald estimator|(}</span>
<span id="cb72-940"><a href="#cb72-940" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald estimator|(}</span>
<span id="cb72-941"><a href="#cb72-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-942"><a href="#cb72-942" aria-hidden="true" tabindex="-1"></a>The Wald estimator is the special case of the instrumental variable</span>
<span id="cb72-943"><a href="#cb72-943" aria-hidden="true" tabindex="-1"></a>estimator where the instrument $w$ is a binary variable and therefore</span>
<span id="cb72-944"><a href="#cb72-944" aria-hidden="true" tabindex="-1"></a>defines two groups ($w=0$ and $w=1$). In this case, the slope of the regression</span>
<span id="cb72-945"><a href="#cb72-945" aria-hidden="true" tabindex="-1"></a>of $y$ on $z$ is $\hat{\beta}_{y/w} = \bar{y}_1 - \bar{y}_0$, where</span>
<span id="cb72-946"><a href="#cb72-946" aria-hidden="true" tabindex="-1"></a>$\bar{y}_i$ is the sample mean of $y$ in group $i=0,1$ and, similarly,</span>
<span id="cb72-947"><a href="#cb72-947" aria-hidden="true" tabindex="-1"></a>the regression of $x$ on $w$ is $\hat{\beta}_{x/w} = \bar{x}_1 -</span>
<span id="cb72-948"><a href="#cb72-948" aria-hidden="true" tabindex="-1"></a>\bar{x}_0$. The Wald estimator is then:</span>
<span id="cb72-949"><a href="#cb72-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-950"><a href="#cb72-950" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-951"><a href="#cb72-951" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\bar{y}_1 - \bar{y}_0}{\bar{x}_1 - \bar{x}_0} = \frac{\hat{\beta}_{yw}}{\hat{\beta}_{xw}}</span>
<span id="cb72-952"><a href="#cb72-952" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-953"><a href="#cb72-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-954"><a href="#cb72-954" aria-hidden="true" tabindex="-1"></a>As $(\bar{x}_1 - \bar{x}_0)$ converges to a constant, the asymptotic</span>
<span id="cb72-955"><a href="#cb72-955" aria-hidden="true" tabindex="-1"></a>standard deviation of $\hat{\beta}$ is:^<span class="co">[</span><span class="ot">See @ANGR:90\index[author]{Angrist}, note 7, pp. 321-322.</span><span class="co">]</span></span>
<span id="cb72-956"><a href="#cb72-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-957"><a href="#cb72-957" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-958"><a href="#cb72-958" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}_{\hat{\beta}} = \frac{\hat{\sigma}_{\bar{y}_1 - \bar{y}_0}}{\bar{x}_1 - \bar{x}_0}</span>
<span id="cb72-959"><a href="#cb72-959" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-std_wald}</span>
<span id="cb72-960"><a href="#cb72-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-961"><a href="#cb72-961" aria-hidden="true" tabindex="-1"></a>@ANGR:90\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Angrist} studied whether veterans (in his article the Vietnam war)</span>
<span id="cb72-962"><a href="#cb72-962" aria-hidden="true" tabindex="-1"></a>experience a long-term loss of income, and therefore should legitimately</span>
<span id="cb72-963"><a href="#cb72-963" aria-hidden="true" tabindex="-1"></a>receive a benefit to compensate this loss. An obvious way to detect a</span>
<span id="cb72-964"><a href="#cb72-964" aria-hidden="true" tabindex="-1"></a>loss would be to consider a sample with two groups (veterans and</span>
<span id="cb72-965"><a href="#cb72-965" aria-hidden="true" tabindex="-1"></a>non-veterans) and to compare the mean income in these two</span>
<span id="cb72-966"><a href="#cb72-966" aria-hidden="true" tabindex="-1"></a>groups. However, enrollment in the army is not random, and therefore it</span>
<span id="cb72-967"><a href="#cb72-967" aria-hidden="true" tabindex="-1"></a>is probable that "certain types of men are more likely to serve in the</span>
<span id="cb72-968"><a href="#cb72-968" aria-hidden="true" tabindex="-1"></a>armed forces than others".^<span class="co">[</span><span class="ot">@ANGR:90\index[author]{Angrist}, p. 313.</span><span class="co">]</span> In this situation,</span>
<span id="cb72-969"><a href="#cb72-969" aria-hidden="true" tabindex="-1"></a>income difference is likely to be a biased estimator of the effect of military</span>
<span id="cb72-970"><a href="#cb72-970" aria-hidden="true" tabindex="-1"></a>enrollment. To overcome this difficulty, @ANGR:90\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Angrist} used the fact that a</span>
<span id="cb72-971"><a href="#cb72-971" aria-hidden="true" tabindex="-1"></a>draft lottery was used during the Vietnam war to select the young men</span>
<span id="cb72-972"><a href="#cb72-972" aria-hidden="true" tabindex="-1"></a>who were enrolled in the army. More precisely, the 365 possible days</span>
<span id="cb72-973"><a href="#cb72-973" aria-hidden="true" tabindex="-1"></a>of birth were randomly drawn and ordered and, later on, the army</span>
<span id="cb72-974"><a href="#cb72-974" aria-hidden="true" tabindex="-1"></a>announced the number of days of birth that would lead to an</span>
<span id="cb72-975"><a href="#cb72-975" aria-hidden="true" tabindex="-1"></a>enrollment (depending on the year, this number was between 95 and</span>
<span id="cb72-976"><a href="#cb72-976" aria-hidden="true" tabindex="-1"></a>195). All the lottery-eligible men didn't go to the war and some that</span>
<span id="cb72-977"><a href="#cb72-977" aria-hidden="true" tabindex="-1"></a>were not eligible fought, but the lottery produces an exogenous</span>
<span id="cb72-978"><a href="#cb72-978" aria-hidden="true" tabindex="-1"></a>variation of the probability to be enrolled.</span>
<span id="cb72-979"><a href="#cb72-979" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - the 1970 lottery concerns men borned from 1944 to 1950 and the --&gt;</span></span>
<span id="cb72-980"><a href="#cb72-980" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   maximum eligible rank is 195, --&gt;</span></span>
<span id="cb72-981"><a href="#cb72-981" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - the 1971 lottery concerns men borned in 1951 and the maximum --&gt;</span></span>
<span id="cb72-982"><a href="#cb72-982" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   eligible rank is 125, --&gt;</span></span>
<span id="cb72-983"><a href="#cb72-983" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - the 1972 lottery concerns men borned in 1952-53 and the maximum --&gt;</span></span>
<span id="cb72-984"><a href="#cb72-984" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   eligible rank is 95. --&gt;</span></span>
<span id="cb72-985"><a href="#cb72-985" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{vietnam}{micsr.data}</span>
<span id="cb72-986"><a href="#cb72-986" aria-hidden="true" tabindex="-1"></a>Two data sources are used and contained in the <span class="in">`vietnam`</span> data set. The first one is a 1% sample of all the</span>
<span id="cb72-987"><a href="#cb72-987" aria-hidden="true" tabindex="-1"></a>social security numbers and indicates the yearly income. A subset of</span>
<span id="cb72-988"><a href="#cb72-988" aria-hidden="true" tabindex="-1"></a>the <span class="in">`vietnam`</span> data set (defined by <span class="in">`variable == income`</span>) contains the</span>
<span id="cb72-989"><a href="#cb72-989" aria-hidden="true" tabindex="-1"></a>average and the standard deviation of income (the FICA definition) for</span>
<span id="cb72-990"><a href="#cb72-990" aria-hidden="true" tabindex="-1"></a>every combination of birth year (<span class="in">`birth`</span>, from 1950 to 1953), draft</span>
<span id="cb72-991"><a href="#cb72-991" aria-hidden="true" tabindex="-1"></a>eligibility (<span class="in">`eligible`</span> a factor with levels <span class="in">`yes`</span> or <span class="in">`no`</span>), race</span>
<span id="cb72-992"><a href="#cb72-992" aria-hidden="true" tabindex="-1"></a>(<span class="in">`white`</span> or <span class="in">`nonwhite`</span>) and year (from 1966 to 1984 for men born in</span>
<span id="cb72-993"><a href="#cb72-993" aria-hidden="true" tabindex="-1"></a>1950 and starting in 1967, 1968 and 1969 for men born respectively</span>
<span id="cb72-994"><a href="#cb72-994" aria-hidden="true" tabindex="-1"></a>in 1951, 1952 and 1953).</span>
<span id="cb72-995"><a href="#cb72-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-996"><a href="#cb72-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-997"><a href="#cb72-997" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-998"><a href="#cb72-998" aria-hidden="true" tabindex="-1"></a><span class="co">#|  label: viet_nam</span></span>
<span id="cb72-999"><a href="#cb72-999" aria-hidden="true" tabindex="-1"></a>vietnam <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-1000"><a href="#cb72-1000" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1001"><a href="#cb72-1001" aria-hidden="true" tabindex="-1"></a>We divide the mean income and its standard deviation by the</span>
<span id="cb72-1002"><a href="#cb72-1002" aria-hidden="true" tabindex="-1"></a>consumer price index (<span class="in">`cpi`</span>), we then create two columns for eligible and</span>
<span id="cb72-1003"><a href="#cb72-1003" aria-hidden="true" tabindex="-1"></a>non-eligible men and we compute the mean difference (<span class="in">`dmean`</span>) and its</span>
<span id="cb72-1004"><a href="#cb72-1004" aria-hidden="true" tabindex="-1"></a>standard deviation (<span class="in">`sd_dmean`</span>):</span>
<span id="cb72-1005"><a href="#cb72-1005" aria-hidden="true" tabindex="-1"></a>\idxfun{filter}{dplyr}\idxfun{mutate}{dplyr}\idxfun{select}{dplyr}\idxfun{pivot<span class="sc">\_</span>wider}{tidyr}</span>
<span id="cb72-1006"><a href="#cb72-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1007"><a href="#cb72-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1008"><a href="#cb72-1008" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: dinc_elig</span></span>
<span id="cb72-1009"><a href="#cb72-1009" aria-hidden="true" tabindex="-1"></a>dinc_elig <span class="ot">&lt;-</span> vietnam <span class="sc">%&gt;%</span></span>
<span id="cb72-1010"><a href="#cb72-1010" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(variable <span class="sc">==</span> <span class="st">"income"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb72-1011"><a href="#cb72-1011" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">mean =</span> mean <span class="sc">/</span> cpi <span class="sc">*</span> <span class="dv">100</span>, <span class="at">sd =</span> sd <span class="sc">/</span> cpi <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb72-1012"><a href="#cb72-1012" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span> variable, <span class="sc">-</span> cpi) <span class="sc">%&gt;%</span></span>
<span id="cb72-1013"><a href="#cb72-1013" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> eligible, <span class="at">values_from =</span> <span class="fu">c</span>(mean, sd)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1014"><a href="#cb72-1014" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">dmean =</span> mean_yes <span class="sc">-</span> mean_no, </span>
<span id="cb72-1015"><a href="#cb72-1015" aria-hidden="true" tabindex="-1"></a>           <span class="at">sd_dmean =</span> <span class="fu">sqrt</span>(sd_no <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> sd_yes <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1016"><a href="#cb72-1016" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span>(mean_no<span class="sc">:</span>sd_yes))</span>
<span id="cb72-1017"><a href="#cb72-1017" aria-hidden="true" tabindex="-1"></a>dinc_elig <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-1018"><a href="#cb72-1018" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1019"><a href="#cb72-1019" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- This income difference between eligible and ineligible young men to --&gt;</span></span>
<span id="cb72-1020"><a href="#cb72-1020" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- the draft lottery can be plotted for every year of birth and for income --&gt;</span></span>
<span id="cb72-1021"><a href="#cb72-1021" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- from 1966 to 1984. We can see that eligible men experience a loss of --&gt;</span></span>
<span id="cb72-1022"><a href="#cb72-1022" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- income by the time they can be enrolled and that this loss is still --&gt;</span></span>
<span id="cb72-1023"><a href="#cb72-1023" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- present more than ten years after the potential enrollement. Note the --&gt;</span></span>
<span id="cb72-1024"><a href="#cb72-1024" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- use of `ggplot2::geom_ribbon` to draw a confidence interval for the --&gt;</span></span>
<span id="cb72-1025"><a href="#cb72-1025" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- mean income difference. --&gt;</span></span>
<span id="cb72-1026"><a href="#cb72-1026" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1027"><a href="#cb72-1027" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb72-1028"><a href="#cb72-1028" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb72-1029"><a href="#cb72-1029" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb72-1030"><a href="#cb72-1030" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb72-1031"><a href="#cb72-1031" aria-hidden="true" tabindex="-1"></a>dinc_elig  <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(year, dmean)) <span class="sc">+</span> </span>
<span id="cb72-1032"><a href="#cb72-1032" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> dmean <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> sd_dmean, <span class="at">ymax =</span> dmean <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> sd_dmean),</span>
<span id="cb72-1033"><a href="#cb72-1033" aria-hidden="true" tabindex="-1"></a>                <span class="at">fill =</span> <span class="st">"lightgrey"</span>) <span class="sc">+</span></span>
<span id="cb72-1034"><a href="#cb72-1034" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">span =</span> .<span class="dv">2</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb72-1035"><a href="#cb72-1035" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb72-1036"><a href="#cb72-1036" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(birth <span class="sc">~</span> race)        </span>
<span id="cb72-1037"><a href="#cb72-1037" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1038"><a href="#cb72-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1039"><a href="#cb72-1039" aria-hidden="true" tabindex="-1"></a>The results match table 1 of @ANGR:90\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Angrist} page 318 (except that, in the</span>
<span id="cb72-1040"><a href="#cb72-1040" aria-hidden="true" tabindex="-1"></a>table, the income is not divided by the cpi).</span>
<span id="cb72-1041"><a href="#cb72-1041" aria-hidden="true" tabindex="-1"></a>The mean income difference is the numerator of the Wald</span>
<span id="cb72-1042"><a href="#cb72-1042" aria-hidden="true" tabindex="-1"></a>estimator. The denominator is the difference of enrollment probability</span>
<span id="cb72-1043"><a href="#cb72-1043" aria-hidden="true" tabindex="-1"></a>between draft eligible and ineligible young men. The author estimates</span>
<span id="cb72-1044"><a href="#cb72-1044" aria-hidden="true" tabindex="-1"></a>this difference using the data of the 1984 Survey of Income and</span>
<span id="cb72-1045"><a href="#cb72-1045" aria-hidden="true" tabindex="-1"></a>Program Participation (SIPP), which are available in the subset of the</span>
<span id="cb72-1046"><a href="#cb72-1046" aria-hidden="true" tabindex="-1"></a><span class="in">`vietnam`</span> data set obtained with <span class="in">`variable == "veteran"`</span> and contains the</span>
<span id="cb72-1047"><a href="#cb72-1047" aria-hidden="true" tabindex="-1"></a>share of veterans and the standard deviation of this share</span>
<span id="cb72-1048"><a href="#cb72-1048" aria-hidden="true" tabindex="-1"></a>for eligible and non-eligible young men by year of</span>
<span id="cb72-1049"><a href="#cb72-1049" aria-hidden="true" tabindex="-1"></a>birth and race (<span class="in">`birth`</span> and <span class="in">`race`</span>):</span>
<span id="cb72-1050"><a href="#cb72-1050" aria-hidden="true" tabindex="-1"></a>\idxfun{filter}{dplyr}\idxfun{select}{dplyr}</span>
<span id="cb72-1051"><a href="#cb72-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1052"><a href="#cb72-1052" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1053"><a href="#cb72-1053" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: veterans</span></span>
<span id="cb72-1054"><a href="#cb72-1054" aria-hidden="true" tabindex="-1"></a>veterans <span class="ot">&lt;-</span> vietnam <span class="sc">%&gt;%</span></span>
<span id="cb72-1055"><a href="#cb72-1055" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(variable <span class="sc">==</span> <span class="st">"veteran"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb72-1056"><a href="#cb72-1056" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span> year, <span class="sc">-</span> cpi, <span class="sc">-</span> variable)</span>
<span id="cb72-1057"><a href="#cb72-1057" aria-hidden="true" tabindex="-1"></a>veterans <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb72-1058"><a href="#cb72-1058" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1059"><a href="#cb72-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1060"><a href="#cb72-1060" aria-hidden="true" tabindex="-1"></a>We then create one column for eligible and non-eligible young men, and</span>
<span id="cb72-1061"><a href="#cb72-1061" aria-hidden="true" tabindex="-1"></a>we compute the difference of shares of enrollment and its standard</span>
<span id="cb72-1062"><a href="#cb72-1062" aria-hidden="true" tabindex="-1"></a>deviation:</span>
<span id="cb72-1063"><a href="#cb72-1063" aria-hidden="true" tabindex="-1"></a>\idxfun{pivot<span class="sc">\_</span>wider}{tidyr}\idxfun{mutate}{dplyr}\idxfun{select}{dplyr}</span>
<span id="cb72-1064"><a href="#cb72-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1065"><a href="#cb72-1065" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1066"><a href="#cb72-1066" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: dshare_elig</span></span>
<span id="cb72-1067"><a href="#cb72-1067" aria-hidden="true" tabindex="-1"></a>dshare_elig <span class="ot">&lt;-</span> veterans <span class="sc">%&gt;%</span></span>
<span id="cb72-1068"><a href="#cb72-1068" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> eligible, <span class="at">values_from =</span> <span class="fu">c</span>(mean, sd)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1069"><a href="#cb72-1069" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">dshare =</span> mean_yes <span class="sc">-</span> mean_no, </span>
<span id="cb72-1070"><a href="#cb72-1070" aria-hidden="true" tabindex="-1"></a>           <span class="at">sd_dshare =</span> <span class="fu">sqrt</span>(sd_yes <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> sd_no <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1071"><a href="#cb72-1071" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span> (mean_no<span class="sc">:</span>sd_total))</span>
<span id="cb72-1072"><a href="#cb72-1072" aria-hidden="true" tabindex="-1"></a>dshare_elig <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-1073"><a href="#cb72-1073" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1074"><a href="#cb72-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1075"><a href="#cb72-1075" aria-hidden="true" tabindex="-1"></a>Finally, we join the two tables by race and year of birth; we</span>
<span id="cb72-1076"><a href="#cb72-1076" aria-hidden="true" tabindex="-1"></a>compute the Wald estimator of the income difference, its standard</span>
<span id="cb72-1077"><a href="#cb72-1077" aria-hidden="true" tabindex="-1"></a>deviation (using @eq-std_wald) and the Student statistic, which is</span>
<span id="cb72-1078"><a href="#cb72-1078" aria-hidden="true" tabindex="-1"></a>their ratio:</span>
<span id="cb72-1079"><a href="#cb72-1079" aria-hidden="true" tabindex="-1"></a>\idxfun{left<span class="sc">\_</span>join}{dplyr}\idxfun{mutate}{dplyr}\idxfun{select}{dplyr}</span>
<span id="cb72-1080"><a href="#cb72-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1081"><a href="#cb72-1081" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1082"><a href="#cb72-1082" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: wald_estimator</span></span>
<span id="cb72-1083"><a href="#cb72-1083" aria-hidden="true" tabindex="-1"></a>wald <span class="ot">&lt;-</span> dinc_elig <span class="sc">%&gt;%</span></span>
<span id="cb72-1084"><a href="#cb72-1084" aria-hidden="true" tabindex="-1"></a>    <span class="fu">left_join</span>(dshare_elig, <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"birth"</span>, <span class="st">"race"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1085"><a href="#cb72-1085" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">wald =</span> dmean <span class="sc">/</span> dshare,</span>
<span id="cb72-1086"><a href="#cb72-1086" aria-hidden="true" tabindex="-1"></a>           <span class="at">sd =</span> sd_dmean <span class="sc">/</span> dshare,</span>
<span id="cb72-1087"><a href="#cb72-1087" aria-hidden="true" tabindex="-1"></a>           <span class="at">z =</span> wald <span class="sc">/</span> sd) <span class="sc">%&gt;%</span></span>
<span id="cb72-1088"><a href="#cb72-1088" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(birth, race, year, wald, sd, z)</span>
<span id="cb72-1089"><a href="#cb72-1089" aria-hidden="true" tabindex="-1"></a>wald <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-1090"><a href="#cb72-1090" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1091"><a href="#cb72-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1092"><a href="#cb72-1092" aria-hidden="true" tabindex="-1"></a>The income differentials are depicted in @fig-veterans; note the use of <span class="in">`geom_ribbon`</span> to draw a confidence interval for the mean income difference.</span>
<span id="cb72-1093"><a href="#cb72-1093" aria-hidden="true" tabindex="-1"></a>\idxfun{filter}{dplyr}\idxfun{aes}{ggplot2}\idxfun{geom<span class="sc">\_</span>ribbon}{ggplot2}\idxfun{geom<span class="sc">\_</span>hline}{ggplot2}\idxfun{geom<span class="sc">\_</span>smooth}{ggplot2}\idxfun{facet<span class="sc">\_</span>grid}{ggplot2}</span>
<span id="cb72-1094"><a href="#cb72-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1095"><a href="#cb72-1095" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1096"><a href="#cb72-1096" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-veterans</span></span>
<span id="cb72-1097"><a href="#cb72-1097" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Income differentials between veterans and non-veterans</span></span>
<span id="cb72-1098"><a href="#cb72-1098" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb72-1099"><a href="#cb72-1099" aria-hidden="true" tabindex="-1"></a>wald <span class="sc">%&gt;%</span></span>
<span id="cb72-1100"><a href="#cb72-1100" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(race <span class="sc">==</span> <span class="st">"white"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb72-1101"><a href="#cb72-1101" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(year, wald)) <span class="sc">+</span></span>
<span id="cb72-1102"><a href="#cb72-1102" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> wald <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> sd, <span class="at">ymax =</span> wald <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> sd), </span>
<span id="cb72-1103"><a href="#cb72-1103" aria-hidden="true" tabindex="-1"></a>                <span class="at">fill =</span> <span class="st">"lightgrey"</span>) <span class="sc">+</span></span>
<span id="cb72-1104"><a href="#cb72-1104" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb72-1105"><a href="#cb72-1105" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">span =</span> <span class="fl">0.2</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb72-1106"><a href="#cb72-1106" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(<span class="sc">~</span> birth)</span>
<span id="cb72-1107"><a href="#cb72-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1108"><a href="#cb72-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1109"><a href="#cb72-1109" aria-hidden="true" tabindex="-1"></a>The income differential between veterans and non-veterans is substantial and persistent (about $1000 of 1984) and is most of the time significant at the 5% level (except for men born in 1951).\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{vietnam}{micsr.data}</span>
<span id="cb72-1110"><a href="#cb72-1110" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!Wald estimator|)}</span>
<span id="cb72-1111"><a href="#cb72-1111" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Wald estimator|)}</span>
<span id="cb72-1112"><a href="#cb72-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1113"><a href="#cb72-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1114"><a href="#cb72-1114" aria-hidden="true" tabindex="-1"></a><span class="fu">## General IV estimator {#sec-general_iv}</span></span>
<span id="cb72-1115"><a href="#cb72-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1116"><a href="#cb72-1116" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!general|(}</span>
<span id="cb72-1117"><a href="#cb72-1117" aria-hidden="true" tabindex="-1"></a>Consider now the general case. Among the covariates, some of them are</span>
<span id="cb72-1118"><a href="#cb72-1118" aria-hidden="true" tabindex="-1"></a>endogenous and other are not and should be included in the instrument</span>
<span id="cb72-1119"><a href="#cb72-1119" aria-hidden="true" tabindex="-1"></a>list. Moreover, there may be more instruments than endogenous covariates.</span>
<span id="cb72-1120"><a href="#cb72-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1121"><a href="#cb72-1121" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the estimator</span></span>
<span id="cb72-1122"><a href="#cb72-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1123"><a href="#cb72-1123" aria-hidden="true" tabindex="-1"></a>There are $K$ covariates, $J$ endogenous covariates, $K-J$ exogenous covariates and $G$ external instruments.</span>
<span id="cb72-1124"><a href="#cb72-1124" aria-hidden="true" tabindex="-1"></a>We denote $Z$ and $W$ the matrix of covariates and instruments. The</span>
<span id="cb72-1125"><a href="#cb72-1125" aria-hidden="true" tabindex="-1"></a>number of columns of these two matrices are respectively $K + 1$ and</span>
<span id="cb72-1126"><a href="#cb72-1126" aria-hidden="true" tabindex="-1"></a>$G + K - J + 1$. For the model to be identified, we must have $G + K -</span>
<span id="cb72-1127"><a href="#cb72-1127" aria-hidden="true" tabindex="-1"></a>J + 1 \geq K + 1$ or $G \geq J$. Therefore, there must be at least as</span>
<span id="cb72-1128"><a href="#cb72-1128" aria-hidden="true" tabindex="-1"></a>many external instruments as there are endogenous covariates. We'll now</span>
<span id="cb72-1129"><a href="#cb72-1129" aria-hidden="true" tabindex="-1"></a>denote $L + 1= G + K - J + 1$ the number of columns of $W$. The $L+1$ moment</span>
<span id="cb72-1130"><a href="#cb72-1130" aria-hidden="true" tabindex="-1"></a>conditions are $\mbox{E}(\epsilon | w) = 0$ which implies</span>
<span id="cb72-1131"><a href="#cb72-1131" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon w) = 0$. The sample</span>
<span id="cb72-1132"><a href="#cb72-1132" aria-hidden="true" tabindex="-1"></a>equivalent is the vector of $L + 1$ empirical moments:</span>
<span id="cb72-1133"><a href="#cb72-1133" aria-hidden="true" tabindex="-1"></a>$m = \frac{1}{N}W^\top \hat{\epsilon}$. As $\hat{\epsilon} = y - Z</span>
<span id="cb72-1134"><a href="#cb72-1134" aria-hidden="true" tabindex="-1"></a>\hat{\gamma}$, this is a system of $L + 1$ equation with $K + 1$</span>
<span id="cb72-1135"><a href="#cb72-1135" aria-hidden="true" tabindex="-1"></a>unknown parameter. The system is over-identified if $L &gt; K$ and in</span>
<span id="cb72-1136"><a href="#cb72-1136" aria-hidden="true" tabindex="-1"></a>this case it is not possible to find a vector of estimates</span>
<span id="cb72-1137"><a href="#cb72-1137" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}$ for which all the empirical moments are 0. The</span>
<span id="cb72-1138"><a href="#cb72-1138" aria-hidden="true" tabindex="-1"></a>instrumental variable estimator is, in this setting, the vector of</span>
<span id="cb72-1139"><a href="#cb72-1139" aria-hidden="true" tabindex="-1"></a>parameters that makes the vector of empirical moments as close as</span>
<span id="cb72-1140"><a href="#cb72-1140" aria-hidden="true" tabindex="-1"></a>possible to a vector of 0. If the errors are spherical, the variance of the vector of empirical</span>
<span id="cb72-1141"><a href="#cb72-1141" aria-hidden="true" tabindex="-1"></a>moments is:</span>
<span id="cb72-1142"><a href="#cb72-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1143"><a href="#cb72-1143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1144"><a href="#cb72-1144" aria-hidden="true" tabindex="-1"></a>\mbox{V}(m) = \mbox{V}\left(\frac{1}{N}W^\top \epsilon\right)= </span>
<span id="cb72-1145"><a href="#cb72-1145" aria-hidden="true" tabindex="-1"></a>\frac{1}{N ^ 2} \mbox{E}\left(W^\top \epsilon \epsilon ^ \top</span>
<span id="cb72-1146"><a href="#cb72-1146" aria-hidden="true" tabindex="-1"></a>W\right)=</span>
<span id="cb72-1147"><a href="#cb72-1147" aria-hidden="true" tabindex="-1"></a>\frac{\sigma_\epsilon ^ 2}{N ^ 2}W^\top W</span>
<span id="cb72-1148"><a href="#cb72-1148" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-var_homosc}</span>
<span id="cb72-1149"><a href="#cb72-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1150"><a href="#cb72-1150" aria-hidden="true" tabindex="-1"></a>and the IV estimator minimizes the quadratic form of the vector of</span>
<span id="cb72-1151"><a href="#cb72-1151" aria-hidden="true" tabindex="-1"></a>moments with the inverse of its covariance matrix:</span>
<span id="cb72-1152"><a href="#cb72-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1153"><a href="#cb72-1153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1154"><a href="#cb72-1154" aria-hidden="true" tabindex="-1"></a>\frac{N ^ 2}{\sigma_\epsilon^2}m^\top (W ^\top W) ^ {-1} m</span>
<span id="cb72-1155"><a href="#cb72-1155" aria-hidden="true" tabindex="-1"></a>=\frac{1}{\sigma_\epsilon ^ 2} \epsilon^\top W (W ^\top W) ^{-1} W ^</span>
<span id="cb72-1156"><a href="#cb72-1156" aria-hidden="true" tabindex="-1"></a>\top \epsilon</span>
<span id="cb72-1157"><a href="#cb72-1157" aria-hidden="true" tabindex="-1"></a>=\frac{1}{\sigma_\epsilon ^ 2} \epsilon^\top P_W \epsilon</span>
<span id="cb72-1158"><a href="#cb72-1158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1159"><a href="#cb72-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1160"><a href="#cb72-1160" aria-hidden="true" tabindex="-1"></a>where $P_W$ is the projection matrix of $W$, i.e., $P_Wz$</span>
<span id="cb72-1161"><a href="#cb72-1161" aria-hidden="true" tabindex="-1"></a>is the vector of fitted values of $z$ obtained by regressing $z$ on</span>
<span id="cb72-1162"><a href="#cb72-1162" aria-hidden="true" tabindex="-1"></a>$W$. Therefore, the IV estimator minimizes:</span>
<span id="cb72-1163"><a href="#cb72-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1164"><a href="#cb72-1164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1165"><a href="#cb72-1165" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma_\epsilon ^ 2} (y - Z\gamma)^\top P_W (y - Z\gamma) =</span>
<span id="cb72-1166"><a href="#cb72-1166" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma_\epsilon ^ 2} (P_W y - P_W Z\gamma)^\top(P_Wy - P_WZ\gamma)</span>
<span id="cb72-1167"><a href="#cb72-1167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1168"><a href="#cb72-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1169"><a href="#cb72-1169" aria-hidden="true" tabindex="-1"></a>and therefore (because $P_W$ is idempotent):</span>
<span id="cb72-1170"><a href="#cb72-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1171"><a href="#cb72-1171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1172"><a href="#cb72-1172" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = (Z^\top P_W Z) ^ {-1} (Z^\top P_W y)</span>
<span id="cb72-1173"><a href="#cb72-1173" aria-hidden="true" tabindex="-1"></a>$$ {<span class="sc">\#</span>eq-overidentified_iv}</span>
<span id="cb72-1174"><a href="#cb72-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1175"><a href="#cb72-1175" aria-hidden="true" tabindex="-1"></a>The 2SLS interpretation of the IV estimator is clear, as it is</span>
<span id="cb72-1176"><a href="#cb72-1176" aria-hidden="true" tabindex="-1"></a>the OLS estimator of a model with $y$ or ($P_W y$) as the response and</span>
<span id="cb72-1177"><a href="#cb72-1177" aria-hidden="true" tabindex="-1"></a>$P_W Z$ the covariate. Therefore, it can be obtained in a first step by regressing all the covariates on the instruments and in a second steps by regressing the response on the fitted values of all the</span>
<span id="cb72-1178"><a href="#cb72-1178" aria-hidden="true" tabindex="-1"></a>covariates obtained in the first step.</span>
<span id="cb72-1179"><a href="#cb72-1179" aria-hidden="true" tabindex="-1"></a>Replacing $y$ by $Z\gamma + \epsilon$ in @eq-overidentified_iv, we get:</span>
<span id="cb72-1180"><a href="#cb72-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1181"><a href="#cb72-1181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1182"><a href="#cb72-1182" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = \gamma + \left(\frac{1}{N}Z^\top P_W Z\right) ^ {-1} \left(\frac{1}{N}Z^\top P_W \epsilon\right)</span>
<span id="cb72-1183"><a href="#cb72-1183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1184"><a href="#cb72-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1185"><a href="#cb72-1185" aria-hidden="true" tabindex="-1"></a>and the IV estimator is consistent if $\mbox{plim} \frac{1}{N}</span>
<span id="cb72-1186"><a href="#cb72-1186" aria-hidden="true" tabindex="-1"></a>Z^\top P_W\epsilon = 0$, or $\mbox{plim} \frac{1}N W ^ \top \epsilon = 0$. With spherical disturbances, the variance of the IV estimator is:</span>
<span id="cb72-1187"><a href="#cb72-1187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1188"><a href="#cb72-1188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1189"><a href="#cb72-1189" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\gamma}) = \left(\frac{1}{N}Z^\top P_WZ\right) ^ {-1}</span>
<span id="cb72-1190"><a href="#cb72-1190" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N ^ 2}Z^\top P_W \epsilon \epsilon ^ \top P_W Z\right)  </span>
<span id="cb72-1191"><a href="#cb72-1191" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{N}Z^\top P_W Z\right) ^ {-1} =</span>
<span id="cb72-1192"><a href="#cb72-1192" aria-hidden="true" tabindex="-1"></a>\sigma_\epsilon ^ 2\left(Z^\top P_W Z\right) ^ {-1}</span>
<span id="cb72-1193"><a href="#cb72-1193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1194"><a href="#cb72-1194" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{general method of moments estimator|(}</span>
<span id="cb72-1195"><a href="#cb72-1195" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!general method of moments estimator|(}</span>
<span id="cb72-1196"><a href="#cb72-1196" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!two stage|(}</span>
<span id="cb72-1197"><a href="#cb72-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1198"><a href="#cb72-1198" aria-hidden="true" tabindex="-1"></a>If the errors are heteroskedastic (or correlated), @eq-var_homosc is a</span>
<span id="cb72-1199"><a href="#cb72-1199" aria-hidden="true" tabindex="-1"></a>biased estimator of the variance of the moments, as</span>
<span id="cb72-1200"><a href="#cb72-1200" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\epsilon\epsilon^\top) \neq \sigma_\epsilon ^ 2 I$. In</span>
<span id="cb72-1201"><a href="#cb72-1201" aria-hidden="true" tabindex="-1"></a>case of heteroskedasticity, $\mbox{E}(W^\top\epsilon\epsilon^\top W)$ can be consistently</span>
<span id="cb72-1202"><a href="#cb72-1202" aria-hidden="true" tabindex="-1"></a>estimated by $\hat{S} = \sum_n \hat{\epsilon}_n ^ 2 w_n w_n^\top$</span>
<span id="cb72-1203"><a href="#cb72-1203" aria-hidden="true" tabindex="-1"></a>where $\hat{\epsilon}$ are the residuals of a consistent estimation,</span>
<span id="cb72-1204"><a href="#cb72-1204" aria-hidden="true" tabindex="-1"></a>for example the residuals of the IV estimator previously</span>
<span id="cb72-1205"><a href="#cb72-1205" aria-hidden="true" tabindex="-1"></a>described. Then, the objective function is:</span>
<span id="cb72-1206"><a href="#cb72-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1207"><a href="#cb72-1207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1208"><a href="#cb72-1208" aria-hidden="true" tabindex="-1"></a>(y - Z\gamma)^\top W \hat{S}^{-1} W^\top (y - Z\gamma)</span>
<span id="cb72-1209"><a href="#cb72-1209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1210"><a href="#cb72-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1211"><a href="#cb72-1211" aria-hidden="true" tabindex="-1"></a>Minimizing this quadratic form leads to the **general method of moments** (**GMM**) estimator, also called the **two-stage IV** estimator.^<span class="co">[</span><span class="ot">@CAME:TRIV:05\index[author]{Cameron}\index[author]{Trivedi}, page 187.</span><span class="co">]</span></span>
<span id="cb72-1212"><a href="#cb72-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1213"><a href="#cb72-1213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1214"><a href="#cb72-1214" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}Z^\top W \hat{S}^{-1} W ^ \top y</span>
<span id="cb72-1215"><a href="#cb72-1215" aria-hidden="true" tabindex="-1"></a>$$ {#eq-two_steps_iv}</span>
<span id="cb72-1216"><a href="#cb72-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1217"><a href="#cb72-1217" aria-hidden="true" tabindex="-1"></a>Replacing $y$ in @eq-two_steps_iv by $Z\gamma + \epsilon$, we get:</span>
<span id="cb72-1218"><a href="#cb72-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1219"><a href="#cb72-1219" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1220"><a href="#cb72-1220" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = \gamma + \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}Z^\top W \hat{S}^{-1} W ^ \top \epsilon</span>
<span id="cb72-1221"><a href="#cb72-1221" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-1222"><a href="#cb72-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1223"><a href="#cb72-1223" aria-hidden="true" tabindex="-1"></a>which leads to the following covariance matrix:</span>
<span id="cb72-1224"><a href="#cb72-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1225"><a href="#cb72-1225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1226"><a href="#cb72-1226" aria-hidden="true" tabindex="-1"></a>\hat{\mbox{V}}(\hat{\gamma}) = \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}</span>
<span id="cb72-1227"><a href="#cb72-1227" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1228"><a href="#cb72-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1229"><a href="#cb72-1229" aria-hidden="true" tabindex="-1"></a>To estimate this covariance matrix, one can use an estimation of $S$</span>
<span id="cb72-1230"><a href="#cb72-1230" aria-hidden="true" tabindex="-1"></a>based on the residuals of the regression of the second step.</span>
<span id="cb72-1231"><a href="#cb72-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1232"><a href="#cb72-1232" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{general method of moments estimator|)}</span>
<span id="cb72-1233"><a href="#cb72-1233" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!general method of moments estimator|)}</span>
<span id="cb72-1234"><a href="#cb72-1234" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!two stage|)}</span>
<span id="cb72-1235"><a href="#cb72-1235" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!general|)}</span>
<span id="cb72-1236"><a href="#cb72-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1237"><a href="#cb72-1237" aria-hidden="true" tabindex="-1"></a><span class="fu">### An example: long-term effects of slave trade</span></span>
<span id="cb72-1238"><a href="#cb72-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1239"><a href="#cb72-1239" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{slave<span class="sc">\_</span>trade}{necountries}</span>
<span id="cb72-1240"><a href="#cb72-1240" aria-hidden="true" tabindex="-1"></a>Africa experienced poor economic performances during the second half of</span>
<span id="cb72-1241"><a href="#cb72-1241" aria-hidden="true" tabindex="-1"></a>the twentieth century, which can be explained by its experience of</span>
<span id="cb72-1242"><a href="#cb72-1242" aria-hidden="true" tabindex="-1"></a>slave trade and colonialism. In particular, slave trade may induce</span>
<span id="cb72-1243"><a href="#cb72-1243" aria-hidden="true" tabindex="-1"></a>long-term negative effects on the economic development of African</span>
<span id="cb72-1244"><a href="#cb72-1244" aria-hidden="true" tabindex="-1"></a>countries because of induced corruption, ethnic fragmentation and</span>
<span id="cb72-1245"><a href="#cb72-1245" aria-hidden="true" tabindex="-1"></a>weakening of established states. Africa experienced, between 1400</span>
<span id="cb72-1246"><a href="#cb72-1246" aria-hidden="true" tabindex="-1"></a>and 1900 four slave trades: the trans-Atlantic slave trade (the most</span>
<span id="cb72-1247"><a href="#cb72-1247" aria-hidden="true" tabindex="-1"></a>important), but also the trans-Saharan, the Red Sea and the Indian Ocean slave</span>
<span id="cb72-1248"><a href="#cb72-1248" aria-hidden="true" tabindex="-1"></a>trades. Not including those who died during the slave trade process,</span>
<span id="cb72-1249"><a href="#cb72-1249" aria-hidden="true" tabindex="-1"></a>about 18 millions slaves were exported from Africa. @NUNN:08\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Nunn} conducted</span>
<span id="cb72-1250"><a href="#cb72-1250" aria-hidden="true" tabindex="-1"></a>a quantitative analysis of the effects of slave trade on economic</span>
<span id="cb72-1251"><a href="#cb72-1251" aria-hidden="true" tabindex="-1"></a>performances, by regressing the 2000 GDP per capita of 52 African</span>
<span id="cb72-1252"><a href="#cb72-1252" aria-hidden="true" tabindex="-1"></a>countries on a measure of the level of slave extraction. The</span>
<span id="cb72-1253"><a href="#cb72-1253" aria-hidden="true" tabindex="-1"></a><span class="in">`slave_trade`</span> data set is provided by the **necountries** package:</span>
<span id="cb72-1254"><a href="#cb72-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1255"><a href="#cb72-1255" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1256"><a href="#cb72-1256" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: slave_trades</span></span>
<span id="cb72-1257"><a href="#cb72-1257" aria-hidden="true" tabindex="-1"></a>sltd <span class="ot">&lt;-</span> necountries<span class="sc">::</span>slave_trade</span>
<span id="cb72-1258"><a href="#cb72-1258" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1259"><a href="#cb72-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1260"><a href="#cb72-1260" aria-hidden="true" tabindex="-1"></a>The response is <span class="in">`gdp`</span> and the main covariate is a measure of the level of</span>
<span id="cb72-1261"><a href="#cb72-1261" aria-hidden="true" tabindex="-1"></a>slave extraction, which is the number of slaves normalized by the</span>
<span id="cb72-1262"><a href="#cb72-1262" aria-hidden="true" tabindex="-1"></a>area of the country. In @fig-gdp_slaves, we first use a scatterplot,</span>
<span id="cb72-1263"><a href="#cb72-1263" aria-hidden="true" tabindex="-1"></a>using log scales for both variables, which clearly indicates a</span>
<span id="cb72-1264"><a href="#cb72-1264" aria-hidden="true" tabindex="-1"></a>negative relationship between slaves extraction and per capita GDP in</span>
<span id="cb72-1265"><a href="#cb72-1265" aria-hidden="true" tabindex="-1"></a><span class="ss">2000. </span>Note the use of <span class="in">`ggrepel::geom_label_repel`</span>: **ggplot2** provides two geoms to draw labels (<span class="in">`geom_text`</span> and <span class="in">`geom_label`</span>), but the labels may overlap. <span class="in">`geom_label_repel`</span> computes a position for the labels that prevent this overlapping.</span>
<span id="cb72-1266"><a href="#cb72-1266" aria-hidden="true" tabindex="-1"></a>\idxfun{ggplot}{ggplot2}\idxfun{aes}{ggplot2}\idxfun{scale<span class="sc">\_</span>x<span class="sc">\_</span>log10}{ggplot2}\idxfun{scale<span class="sc">\_</span>y<span class="sc">\_</span>log10}{ggplot2}\idxfun{geom<span class="sc">\_</span>smooth}{ggplot2}\idxfun{geom<span class="sc">\_</span>label<span class="sc">\_</span>repel}{ggrepel}</span>
<span id="cb72-1267"><a href="#cb72-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1268"><a href="#cb72-1268" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1269"><a href="#cb72-1269" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb72-1270"><a href="#cb72-1270" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-gdp_slaves</span></span>
<span id="cb72-1271"><a href="#cb72-1271" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Per capita GDP and slave extraction"</span></span>
<span id="cb72-1272"><a href="#cb72-1272" aria-hidden="true" tabindex="-1"></a>sltd <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(slavesarea, gdp)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb72-1273"><a href="#cb72-1273" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">trans =</span> <span class="st">"log10"</span>, <span class="at">expand =</span> <span class="fu">expansion</span>(<span class="at">mult =</span> <span class="fu">c</span>(.<span class="dv">1</span>))) <span class="sc">+</span> </span>
<span id="cb72-1274"><a href="#cb72-1274" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span> </span>
<span id="cb72-1275"><a href="#cb72-1275" aria-hidden="true" tabindex="-1"></a>  ggrepel<span class="sc">::</span><span class="fu">geom_label_repel</span>(<span class="fu">aes</span>(<span class="at">label =</span> country),</span>
<span id="cb72-1276"><a href="#cb72-1276" aria-hidden="true" tabindex="-1"></a>                            <span class="at">size =</span> <span class="dv">2</span>, <span class="at">max.overlaps =</span> <span class="cn">Inf</span>) </span>
<span id="cb72-1277"><a href="#cb72-1277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1278"><a href="#cb72-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1279"><a href="#cb72-1279" aria-hidden="true" tabindex="-1"></a>@NUNN:08\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Nunn} in table 2 presents a series of linear regressions, with</span>
<span id="cb72-1280"><a href="#cb72-1280" aria-hidden="true" tabindex="-1"></a>different sets of controls. We just consider Nunn's first specification,</span>
<span id="cb72-1281"><a href="#cb72-1281" aria-hidden="true" tabindex="-1"></a>which includes only dummies for the colonizer as supplementary covariates. <span class="in">`colony`</span> is a factor with eight levels: we use <span class="in">`forcats::fct_lump_min`</span> to merge the most infrequent levels: <span class="in">`"none"`</span> (2 countries), <span class="in">`"spain"`</span>, <span class="in">`"germany"`</span> and <span class="in">`"italy"`</span> (1 country).</span>
<span id="cb72-1282"><a href="#cb72-1282" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}\idxfun{fct<span class="sc">\_</span>lump<span class="sc">\_</span>min}{forcats}</span>
<span id="cb72-1283"><a href="#cb72-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1284"><a href="#cb72-1284" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1285"><a href="#cb72-1285" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: slave_trade_ols</span></span>
<span id="cb72-1286"><a href="#cb72-1286" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1287"><a href="#cb72-1287" aria-hidden="true" tabindex="-1"></a>sltd <span class="ot">&lt;-</span> sltd <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">colony =</span> <span class="fu">fct_lump_min</span>(colony, <span class="dv">3</span>))</span>
<span id="cb72-1288"><a href="#cb72-1288" aria-hidden="true" tabindex="-1"></a>slaves_ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(gdp) <span class="sc">~</span> <span class="fu">log</span>(slavesarea) <span class="sc">+</span> colony, sltd)</span>
<span id="cb72-1289"><a href="#cb72-1289" aria-hidden="true" tabindex="-1"></a>slaves_ols <span class="sc">%&gt;%</span> <span class="fu">gaze</span>(<span class="at">coef =</span> <span class="st">"log(slavesarea)"</span>)</span>
<span id="cb72-1290"><a href="#cb72-1290" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1291"><a href="#cb72-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1292"><a href="#cb72-1292" aria-hidden="true" tabindex="-1"></a>The coefficient is negative and highly significant; it implies that a 10% increase of slave extraction induces a reduction of 1% of GDP per capita. </span>
<span id="cb72-1293"><a href="#cb72-1293" aria-hidden="true" tabindex="-1"></a>As noticed by @NUNN:08\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Nunn}, the estimation of the effect of slave trade</span>
<span id="cb72-1294"><a href="#cb72-1294" aria-hidden="true" tabindex="-1"></a>on GDP can be inconsistent for two reasons:</span>
<span id="cb72-1295"><a href="#cb72-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1296"><a href="#cb72-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the level of slave extraction, which is based on information of the</span>
<span id="cb72-1297"><a href="#cb72-1297" aria-hidden="true" tabindex="-1"></a>  ethnicity of individual slaves and then aggregated at the current countries' level can be prone to error of measurement; moreover, for countries</span>
<span id="cb72-1298"><a href="#cb72-1298" aria-hidden="true" tabindex="-1"></a>  inside the continent (compared to coastal countries), a lot of</span>
<span id="cb72-1299"><a href="#cb72-1299" aria-hidden="true" tabindex="-1"></a>  slaves died during the journey to the coastal port of export, so</span>
<span id="cb72-1300"><a href="#cb72-1300" aria-hidden="true" tabindex="-1"></a>  the level of extraction may be underestimated for these</span>
<span id="cb72-1301"><a href="#cb72-1301" aria-hidden="true" tabindex="-1"></a>  countries,</span>
<span id="cb72-1302"><a href="#cb72-1302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the average economic conditions may be different for countries that</span>
<span id="cb72-1303"><a href="#cb72-1303" aria-hidden="true" tabindex="-1"></a>  suffered a large extraction, compared to the others; in particular,</span>
<span id="cb72-1304"><a href="#cb72-1304" aria-hidden="true" tabindex="-1"></a>  if countries where the trade was particularly important were poor,</span>
<span id="cb72-1305"><a href="#cb72-1305" aria-hidden="true" tabindex="-1"></a>  their current poor economic conditions</span>
<span id="cb72-1306"><a href="#cb72-1306" aria-hidden="true" tabindex="-1"></a>  can be explained by their poor economic conditions 600 years ago and</span>
<span id="cb72-1307"><a href="#cb72-1307" aria-hidden="true" tabindex="-1"></a>  not by slave trades.</span>
<span id="cb72-1308"><a href="#cb72-1308" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-1309"><a href="#cb72-1309" aria-hidden="true" tabindex="-1"></a>Measurement error induces an attenuation bias, which means that without</span>
<span id="cb72-1310"><a href="#cb72-1310" aria-hidden="true" tabindex="-1"></a>measurement error, the negative effect of slave trades on GDP per</span>
<span id="cb72-1311"><a href="#cb72-1311" aria-hidden="true" tabindex="-1"></a>capita would be stronger. The second effect would induce an upward-bias</span>
<span id="cb72-1312"><a href="#cb72-1312" aria-hidden="true" tabindex="-1"></a>(in absolute value) of the coefficient on slave trades. But, actually,</span>
<span id="cb72-1313"><a href="#cb72-1313" aria-hidden="true" tabindex="-1"></a>@NUNN:08\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Nunn} showed that areas of Africa that suffered the most slave trade were in general not the poorest areas, but the most developed ones. In</span>
<span id="cb72-1314"><a href="#cb72-1314" aria-hidden="true" tabindex="-1"></a>this case, the OLS estimator would underestimate the effect of</span>
<span id="cb72-1315"><a href="#cb72-1315" aria-hidden="true" tabindex="-1"></a>slave trades on GDP per capita. </span>
<span id="cb72-1316"><a href="#cb72-1316" aria-hidden="true" tabindex="-1"></a>@NUNN:08\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Nunn} then performs instrumental variable regressions, using as</span>
<span id="cb72-1317"><a href="#cb72-1317" aria-hidden="true" tabindex="-1"></a>instruments the distance between the centroid of the countries and the</span>
<span id="cb72-1318"><a href="#cb72-1318" aria-hidden="true" tabindex="-1"></a>closest major market for the four slave trades (for example Mauritius and</span>
<span id="cb72-1319"><a href="#cb72-1319" aria-hidden="true" tabindex="-1"></a>Oman for the Indian Ocean slave trade and Massawa, Suakin and</span>
<span id="cb72-1320"><a href="#cb72-1320" aria-hidden="true" tabindex="-1"></a>Djibouti for the Red Sea slave trade).</span>
<span id="cb72-1321"><a href="#cb72-1321" aria-hidden="true" tabindex="-1"></a>The IV regression can be performed by first regressing the</span>
<span id="cb72-1322"><a href="#cb72-1322" aria-hidden="true" tabindex="-1"></a>endogenous covariate on the external instruments (<span class="in">`atlantic`</span>,</span>
<span id="cb72-1323"><a href="#cb72-1323" aria-hidden="true" tabindex="-1"></a><span class="in">`indian`</span>, <span class="in">`redsea`</span> and <span class="in">`sahara`</span>) and on the exogenous covariates (here</span>
<span id="cb72-1324"><a href="#cb72-1324" aria-hidden="true" tabindex="-1"></a><span class="in">`colony`</span>, the factor indicating the previous colonizer). This is the</span>
<span id="cb72-1325"><a href="#cb72-1325" aria-hidden="true" tabindex="-1"></a>so-called **first-stage regression**:</span>
<span id="cb72-1326"><a href="#cb72-1326" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{gaze}{micsr}\idxfun{rsq}{micsr}</span>
<span id="cb72-1327"><a href="#cb72-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1328"><a href="#cb72-1328" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1329"><a href="#cb72-1329" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: first_stage_slave</span></span>
<span id="cb72-1330"><a href="#cb72-1330" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1331"><a href="#cb72-1331" aria-hidden="true" tabindex="-1"></a>slaves_first <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(slavesarea) <span class="sc">~</span> colony <span class="sc">+</span> atlantic <span class="sc">+</span> indian <span class="sc">+</span></span>
<span id="cb72-1332"><a href="#cb72-1332" aria-hidden="true" tabindex="-1"></a>                       redsea <span class="sc">+</span> sahara, sltd)</span>
<span id="cb72-1333"><a href="#cb72-1333" aria-hidden="true" tabindex="-1"></a>slaves_first <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-1334"><a href="#cb72-1334" aria-hidden="true" tabindex="-1"></a>slaves_first <span class="sc">%&gt;%</span> rsq</span>
<span id="cb72-1335"><a href="#cb72-1335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1336"><a href="#cb72-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1337"><a href="#cb72-1337" aria-hidden="true" tabindex="-1"></a>Except for the <span class="in">`redsea`</span> variable, the coefficients are highly</span>
<span id="cb72-1338"><a href="#cb72-1338" aria-hidden="true" tabindex="-1"></a>significant, and the four instruments and the exogenous covariate explain more than one-fourth of the variance of slave extraction. The second stage is obtained by</span>
<span id="cb72-1339"><a href="#cb72-1339" aria-hidden="true" tabindex="-1"></a>regressing the response on the fitted values of the first-step estimation:</span>
<span id="cb72-1340"><a href="#cb72-1340" aria-hidden="true" tabindex="-1"></a>\idxfun{add<span class="sc">\_</span>column}{tibble}\idxfun{fitted}{stats}\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-1341"><a href="#cb72-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1342"><a href="#cb72-1342" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1343"><a href="#cb72-1343" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: second_stage_slave</span></span>
<span id="cb72-1344"><a href="#cb72-1344" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1345"><a href="#cb72-1345" aria-hidden="true" tabindex="-1"></a>sltd <span class="ot">&lt;-</span> sltd <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">hlslarea =</span> <span class="fu">fitted</span>(slaves_first))</span>
<span id="cb72-1346"><a href="#cb72-1346" aria-hidden="true" tabindex="-1"></a>slaves_second <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(gdp) <span class="sc">~</span> hlslarea, sltd)</span>
<span id="cb72-1347"><a href="#cb72-1347" aria-hidden="true" tabindex="-1"></a>slaves_second <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-1348"><a href="#cb72-1348" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1349"><a href="#cb72-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1350"><a href="#cb72-1350" aria-hidden="true" tabindex="-1"></a>The coefficient has almost doubled, compared to the OLS estimator,</span>
<span id="cb72-1351"><a href="#cb72-1351" aria-hidden="true" tabindex="-1"></a>which confirms that this latter estimator is biased, with an</span>
<span id="cb72-1352"><a href="#cb72-1352" aria-hidden="true" tabindex="-1"></a>attenuation bias due to measurement error and a downward-bias caused by the fact that the most developed African regions were more affected by slave trade. </span>
<span id="cb72-1353"><a href="#cb72-1353" aria-hidden="true" tabindex="-1"></a>The two-stage IV estimator is then computed. We use the **Formula** package</span>
<span id="cb72-1354"><a href="#cb72-1354" aria-hidden="true" tabindex="-1"></a>which enables to write complex formulas with multiple set of</span>
<span id="cb72-1355"><a href="#cb72-1355" aria-hidden="true" tabindex="-1"></a>variables, separated by the <span class="in">`|`</span> operator:^<span class="co">[</span><span class="ot">We've already presented briefly this package in @sec-sys_eq_ols.</span><span class="co">]</span></span>
<span id="cb72-1356"><a href="#cb72-1356" aria-hidden="true" tabindex="-1"></a>\idxfun{Formula}{Formula}</span>
<span id="cb72-1357"><a href="#cb72-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1358"><a href="#cb72-1358" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1359"><a href="#cb72-1359" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: Formula_slave</span></span>
<span id="cb72-1360"><a href="#cb72-1360" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Formula)</span>
<span id="cb72-1361"><a href="#cb72-1361" aria-hidden="true" tabindex="-1"></a>form <span class="ot">&lt;-</span> <span class="fu">Formula</span>(<span class="fu">log</span>(gdp) <span class="sc">~</span> <span class="fu">log</span>(slavesarea) <span class="sc">+</span> colony<span class="sc">|</span> colony <span class="sc">+</span> </span>
<span id="cb72-1362"><a href="#cb72-1362" aria-hidden="true" tabindex="-1"></a>                   redsea <span class="sc">+</span> atlantic <span class="sc">+</span> sahara <span class="sc">+</span> indian)</span>
<span id="cb72-1363"><a href="#cb72-1363" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1364"><a href="#cb72-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1365"><a href="#cb72-1365" aria-hidden="true" tabindex="-1"></a>The first part contains the covariates and the second part the</span>
<span id="cb72-1366"><a href="#cb72-1366" aria-hidden="true" tabindex="-1"></a>instruments. We then compute the model frame and we extract the covariates, the instruments and the response:</span>
<span id="cb72-1367"><a href="#cb72-1367" aria-hidden="true" tabindex="-1"></a>\idxfun{model.frame}{stats}\idxfun{model.matrix}{stats}\idxfun{model.part}{Formula}\idxfun{pull}{dplyr}</span>
<span id="cb72-1368"><a href="#cb72-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1369"><a href="#cb72-1369" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1370"><a href="#cb72-1370" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_matrices_slave</span></span>
<span id="cb72-1371"><a href="#cb72-1371" aria-hidden="true" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">model.frame</span>(form, sltd)</span>
<span id="cb72-1372"><a href="#cb72-1372" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(form, sltd, <span class="at">rhs =</span> <span class="dv">2</span>)</span>
<span id="cb72-1373"><a href="#cb72-1373" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(form, sltd, <span class="at">rhs =</span> <span class="dv">1</span>)</span>
<span id="cb72-1374"><a href="#cb72-1374" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">model.part</span>(form, mf, <span class="at">lhs =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> pull</span>
<span id="cb72-1375"><a href="#cb72-1375" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1376"><a href="#cb72-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1377"><a href="#cb72-1377" aria-hidden="true" tabindex="-1"></a>We then compute the cross-products of the instruments and the covariates</span>
<span id="cb72-1378"><a href="#cb72-1378" aria-hidden="true" tabindex="-1"></a>($Z^\top W$) and $\hat{S}$</span>
<span id="cb72-1379"><a href="#cb72-1379" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}\idxfun{resid}{stats}\idxfun{solve}{base}\idxfun{drop}{base}\idxfun{cbind}{base}\idxfun{stder}{micsr}</span>
<span id="cb72-1380"><a href="#cb72-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1381"><a href="#cb72-1381" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1382"><a href="#cb72-1382" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: two_stage_iv</span></span>
<span id="cb72-1383"><a href="#cb72-1383" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1384"><a href="#cb72-1384" aria-hidden="true" tabindex="-1"></a>ZPW <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(Z, W)</span>
<span id="cb72-1385"><a href="#cb72-1385" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">abs</span>(<span class="fu">resid</span>(slaves_second)) <span class="sc">*</span> W)</span>
<span id="cb72-1386"><a href="#cb72-1386" aria-hidden="true" tabindex="-1"></a>vcov_1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(ZPW <span class="sc">%*%</span> <span class="fu">solve</span>(S) <span class="sc">%*%</span> <span class="fu">t</span>(ZPW))</span>
<span id="cb72-1387"><a href="#cb72-1387" aria-hidden="true" tabindex="-1"></a>iv2s <span class="ot">&lt;-</span> vcov_1 <span class="sc">%*%</span> (ZPW <span class="sc">%*%</span> <span class="fu">solve</span>(S) <span class="sc">%*%</span> <span class="fu">crossprod</span>(W, y)) <span class="sc">%&gt;%</span> drop</span>
<span id="cb72-1388"><a href="#cb72-1388" aria-hidden="true" tabindex="-1"></a>resid2 <span class="ot">&lt;-</span> (y <span class="sc">-</span> Z <span class="sc">%*%</span> iv2s) <span class="sc">%&gt;%</span> drop</span>
<span id="cb72-1389"><a href="#cb72-1389" aria-hidden="true" tabindex="-1"></a>S2 <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">abs</span>(resid2) <span class="sc">*</span> W)</span>
<span id="cb72-1390"><a href="#cb72-1390" aria-hidden="true" tabindex="-1"></a>vcov_2 <span class="ot">&lt;-</span> <span class="fu">solve</span>(ZPW <span class="sc">%*%</span> <span class="fu">solve</span>(S2) <span class="sc">%*%</span> <span class="fu">t</span>(ZPW))</span>
<span id="cb72-1391"><a href="#cb72-1391" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="at">coef =</span> iv2s, <span class="at">sd1 =</span> <span class="fu">stder</span>(vcov_1),</span>
<span id="cb72-1392"><a href="#cb72-1392" aria-hidden="true" tabindex="-1"></a>      <span class="at">sd2 =</span> <span class="fu">stder</span>(vcov_2))[<span class="dv">2</span>, ] <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">digits =</span> <span class="dv">2</span>)</span>
<span id="cb72-1393"><a href="#cb72-1393" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1394"><a href="#cb72-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1395"><a href="#cb72-1395" aria-hidden="true" tabindex="-1"></a>The results are very similar to those of the one-step IV estimator. The</span>
<span id="cb72-1396"><a href="#cb72-1396" aria-hidden="true" tabindex="-1"></a>IV estimator can also be computed using the <span class="in">`ivreg::ivreg`</span></span>
<span id="cb72-1397"><a href="#cb72-1397" aria-hidden="true" tabindex="-1"></a>function. The main argument is a two-part formula, the first part</span>
<span id="cb72-1398"><a href="#cb72-1398" aria-hidden="true" tabindex="-1"></a>containing the covariates and the second part the instruments:</span>
<span id="cb72-1399"><a href="#cb72-1399" aria-hidden="true" tabindex="-1"></a>\idxfun{ivreg}{ivreg}</span>
<span id="cb72-1400"><a href="#cb72-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1401"><a href="#cb72-1401" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1402"><a href="#cb72-1402" aria-hidden="true" tabindex="-1"></a><span class="co">#| results: false</span></span>
<span id="cb72-1403"><a href="#cb72-1403" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb72-1404"><a href="#cb72-1404" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ivreg_slave_silent</span></span>
<span id="cb72-1405"><a href="#cb72-1405" aria-hidden="true" tabindex="-1"></a>ivreg<span class="sc">::</span><span class="fu">ivreg</span>(<span class="fu">log</span>(gdp) <span class="sc">~</span> <span class="fu">log</span>(slavesarea) <span class="sc">+</span> colony<span class="sc">|</span> colony <span class="sc">+</span> </span>
<span id="cb72-1406"><a href="#cb72-1406" aria-hidden="true" tabindex="-1"></a>                 redsea <span class="sc">+</span> atlantic <span class="sc">+</span> sahara <span class="sc">+</span> indian, <span class="at">data =</span> sltd)</span>
<span id="cb72-1407"><a href="#cb72-1407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1408"><a href="#cb72-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1409"><a href="#cb72-1409" aria-hidden="true" tabindex="-1"></a>The output of the <span class="in">`ivreg`</span> function will be presented in @sec-test_sltr.</span>
<span id="cb72-1410"><a href="#cb72-1410" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{slave<span class="sc">\_</span>trade}{necountries}</span>
<span id="cb72-1411"><a href="#cb72-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1412"><a href="#cb72-1412" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator|)}</span>
<span id="cb72-1413"><a href="#cb72-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1414"><a href="#cb72-1414" aria-hidden="true" tabindex="-1"></a><span class="fu">## Three-stage least squares {#sec-three_sls}</span></span>
<span id="cb72-1415"><a href="#cb72-1415" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{three-stage least squares|(}</span>
<span id="cb72-1416"><a href="#cb72-1416" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!three-stage least squares|(}</span>
<span id="cb72-1417"><a href="#cb72-1417" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation!three-stage least squares|(}</span>
<span id="cb72-1418"><a href="#cb72-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1419"><a href="#cb72-1419" aria-hidden="true" tabindex="-1"></a>Consider now the case where the model is defined by a system of</span>
<span id="cb72-1420"><a href="#cb72-1420" aria-hidden="true" tabindex="-1"></a>equations, some of the covariates entering these equations being</span>
<span id="cb72-1421"><a href="#cb72-1421" aria-hidden="true" tabindex="-1"></a>endogenous.</span>
<span id="cb72-1422"><a href="#cb72-1422" aria-hidden="true" tabindex="-1"></a>We consider therefore a system of $L$ equations denoted by</span>
<span id="cb72-1423"><a href="#cb72-1423" aria-hidden="true" tabindex="-1"></a>$y_l=Z_l\gamma_l+\epsilon_l$, with $l=1\ldots L$. This situation has already</span>
<span id="cb72-1424"><a href="#cb72-1424" aria-hidden="true" tabindex="-1"></a>encountered in @sec-sys_eq_ols, @sec-bptest_system and @sec-sur. In this latter section, we</span>
<span id="cb72-1425"><a href="#cb72-1425" aria-hidden="true" tabindex="-1"></a>considered that all the covariates were exogenous and we presented the</span>
<span id="cb72-1426"><a href="#cb72-1426" aria-hidden="true" tabindex="-1"></a>seemingly unrelated regression estimator, which is a GLS estimator</span>
<span id="cb72-1427"><a href="#cb72-1427" aria-hidden="true" tabindex="-1"></a>that takes into account the correlation between the errors of the</span>
<span id="cb72-1428"><a href="#cb72-1428" aria-hidden="true" tabindex="-1"></a>different equations. Remember that, in matrix form, the system can be written as</span>
<span id="cb72-1429"><a href="#cb72-1429" aria-hidden="true" tabindex="-1"></a>follows:</span>
<span id="cb72-1430"><a href="#cb72-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1431"><a href="#cb72-1431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1432"><a href="#cb72-1432" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1433"><a href="#cb72-1433" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb72-1434"><a href="#cb72-1434" aria-hidden="true" tabindex="-1"></a>    y_1 <span class="sc">\\</span> y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> y_L</span>
<span id="cb72-1435"><a href="#cb72-1435" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1436"><a href="#cb72-1436" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1437"><a href="#cb72-1437" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb72-1438"><a href="#cb72-1438" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1439"><a href="#cb72-1439" aria-hidden="true" tabindex="-1"></a>  \begin{array}{ccccc}</span>
<span id="cb72-1440"><a href="#cb72-1440" aria-hidden="true" tabindex="-1"></a>    Z_1 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1441"><a href="#cb72-1441" aria-hidden="true" tabindex="-1"></a>    0 &amp; Z_2 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1442"><a href="#cb72-1442" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1443"><a href="#cb72-1443" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; \ldots &amp; Z_L</span>
<span id="cb72-1444"><a href="#cb72-1444" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1445"><a href="#cb72-1445" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1446"><a href="#cb72-1446" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1447"><a href="#cb72-1447" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb72-1448"><a href="#cb72-1448" aria-hidden="true" tabindex="-1"></a>    \gamma_1 <span class="sc">\\</span> \gamma_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \gamma_L</span>
<span id="cb72-1449"><a href="#cb72-1449" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1450"><a href="#cb72-1450" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1451"><a href="#cb72-1451" aria-hidden="true" tabindex="-1"></a>+</span>
<span id="cb72-1452"><a href="#cb72-1452" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1453"><a href="#cb72-1453" aria-hidden="true" tabindex="-1"></a>  \begin{array}{c}</span>
<span id="cb72-1454"><a href="#cb72-1454" aria-hidden="true" tabindex="-1"></a>    \epsilon_1 <span class="sc">\\</span> \epsilon_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \epsilon_L</span>
<span id="cb72-1455"><a href="#cb72-1455" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1456"><a href="#cb72-1456" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1457"><a href="#cb72-1457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1458"><a href="#cb72-1458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1459"><a href="#cb72-1459" aria-hidden="true" tabindex="-1"></a>And the covariance of the error vector for the whole system is assumed</span>
<span id="cb72-1460"><a href="#cb72-1460" aria-hidden="true" tabindex="-1"></a>to be:</span>
<span id="cb72-1461"><a href="#cb72-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1462"><a href="#cb72-1462" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1463"><a href="#cb72-1463" aria-hidden="true" tabindex="-1"></a>\Omega=</span>
<span id="cb72-1464"><a href="#cb72-1464" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1465"><a href="#cb72-1465" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb72-1466"><a href="#cb72-1466" aria-hidden="true" tabindex="-1"></a>  \sigma_{11} I &amp; \sigma_{12} I &amp; \ldots &amp;\sigma_{1L} I <span class="sc">\\</span></span>
<span id="cb72-1467"><a href="#cb72-1467" aria-hidden="true" tabindex="-1"></a>  \sigma_{12} I &amp; \sigma_{22} I &amp; \ldots &amp;\sigma_{2L} I <span class="sc">\\</span></span>
<span id="cb72-1468"><a href="#cb72-1468" aria-hidden="true" tabindex="-1"></a>  \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1469"><a href="#cb72-1469" aria-hidden="true" tabindex="-1"></a>  \sigma_{1L} I &amp; \sigma_{2L} I &amp; \ldots &amp; \sigma_{LL} I</span>
<span id="cb72-1470"><a href="#cb72-1470" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1471"><a href="#cb72-1471" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1472"><a href="#cb72-1472" aria-hidden="true" tabindex="-1"></a>= \Sigma \otimes I</span>
<span id="cb72-1473"><a href="#cb72-1473" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1474"><a href="#cb72-1474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1475"><a href="#cb72-1475" aria-hidden="true" tabindex="-1"></a>where $\otimes$ is the Kronecker product\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Kronecker product}</span>
<span id="cb72-1476"><a href="#cb72-1476" aria-hidden="true" tabindex="-1"></a>and $\Sigma$ is a symmetric</span>
<span id="cb72-1477"><a href="#cb72-1477" aria-hidden="true" tabindex="-1"></a>matrix of dimensions $L$ for which the diagonal elements are the</span>
<span id="cb72-1478"><a href="#cb72-1478" aria-hidden="true" tabindex="-1"></a>variance of the errors for a given equation and the off-diagonal</span>
<span id="cb72-1479"><a href="#cb72-1479" aria-hidden="true" tabindex="-1"></a>elements the covariances between the pairs of errors of two different equations. </span>
<span id="cb72-1480"><a href="#cb72-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1481"><a href="#cb72-1481" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the three-stage least square estimator</span></span>
<span id="cb72-1482"><a href="#cb72-1482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1483"><a href="#cb72-1483" aria-hidden="true" tabindex="-1"></a>If some covariates are endogenous, we should consider, for each</span>
<span id="cb72-1484"><a href="#cb72-1484" aria-hidden="true" tabindex="-1"></a>equation, a matrix of instruments:</span>
<span id="cb72-1485"><a href="#cb72-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1486"><a href="#cb72-1486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1487"><a href="#cb72-1487" aria-hidden="true" tabindex="-1"></a>W = </span>
<span id="cb72-1488"><a href="#cb72-1488" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1489"><a href="#cb72-1489" aria-hidden="true" tabindex="-1"></a>  \begin{array}{ccccc}</span>
<span id="cb72-1490"><a href="#cb72-1490" aria-hidden="true" tabindex="-1"></a>    W_1 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1491"><a href="#cb72-1491" aria-hidden="true" tabindex="-1"></a>    0 &amp; W_2 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1492"><a href="#cb72-1492" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1493"><a href="#cb72-1493" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; \ldots &amp; W_L</span>
<span id="cb72-1494"><a href="#cb72-1494" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb72-1495"><a href="#cb72-1495" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1496"><a href="#cb72-1496" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1497"><a href="#cb72-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1498"><a href="#cb72-1498" aria-hidden="true" tabindex="-1"></a>The moment conditions for the whole system are then:</span>
<span id="cb72-1499"><a href="#cb72-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1500"><a href="#cb72-1500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1501"><a href="#cb72-1501" aria-hidden="true" tabindex="-1"></a>m = </span>
<span id="cb72-1502"><a href="#cb72-1502" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}W^\top \epsilon=</span>
<span id="cb72-1503"><a href="#cb72-1503" aria-hidden="true" tabindex="-1"></a>\frac{1}{N}\left(</span>
<span id="cb72-1504"><a href="#cb72-1504" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1505"><a href="#cb72-1505" aria-hidden="true" tabindex="-1"></a>W_1 ^ \top \epsilon_1 <span class="sc">\\</span></span>
<span id="cb72-1506"><a href="#cb72-1506" aria-hidden="true" tabindex="-1"></a>W_2 ^ \top \epsilon_2 <span class="sc">\\</span></span>
<span id="cb72-1507"><a href="#cb72-1507" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb72-1508"><a href="#cb72-1508" aria-hidden="true" tabindex="-1"></a>W_L^\top \epsilon_L</span>
<span id="cb72-1509"><a href="#cb72-1509" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1510"><a href="#cb72-1510" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1511"><a href="#cb72-1511" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1512"><a href="#cb72-1512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1513"><a href="#cb72-1513" aria-hidden="true" tabindex="-1"></a>and the variance of the vector of moments is:</span>
<span id="cb72-1514"><a href="#cb72-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1515"><a href="#cb72-1515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1516"><a href="#cb72-1516" aria-hidden="true" tabindex="-1"></a>\mbox{V}(m) = \frac{1}{N ^ 2}\mbox{E}\left(m m ^ \top\right) =</span>
<span id="cb72-1517"><a href="#cb72-1517" aria-hidden="true" tabindex="-1"></a>\frac{1}{N ^ 2}W ^ \top \mbox{E}\left(\epsilon \epsilon ^ \top\right) W =</span>
<span id="cb72-1518"><a href="#cb72-1518" aria-hidden="true" tabindex="-1"></a>\frac{1}{N ^ 2}W ^ \top \Omega W = \frac{1}{N ^ 2}W ^ \top (\Sigma \otimes I) W</span>
<span id="cb72-1519"><a href="#cb72-1519" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1520"><a href="#cb72-1520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1521"><a href="#cb72-1521" aria-hidden="true" tabindex="-1"></a>The **three-stage least squares** (**3SLS**) estimator^<span class="co">[</span><span class="ot">@ZELL:THEI:62.\index[author]{Zellner}\index[author]{Theil}</span><span class="co">]</span>  minimizes the quadratic form</span>
<span id="cb72-1522"><a href="#cb72-1522" aria-hidden="true" tabindex="-1"></a>of the moments with the inverse of this variance matrix:</span>
<span id="cb72-1523"><a href="#cb72-1523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1524"><a href="#cb72-1524" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1525"><a href="#cb72-1525" aria-hidden="true" tabindex="-1"></a>m ^ \top (W ^ \top \Omega W) ^ {-1} m = (y - Z \gamma) ^ \top (W ^ \top</span>
<span id="cb72-1526"><a href="#cb72-1526" aria-hidden="true" tabindex="-1"></a>\Omega W) ^ {-1} (y - Z \gamma)</span>
<span id="cb72-1527"><a href="#cb72-1527" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1528"><a href="#cb72-1528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1529"><a href="#cb72-1529" aria-hidden="true" tabindex="-1"></a>which leads to the following estimator:</span>
<span id="cb72-1530"><a href="#cb72-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1531"><a href="#cb72-1531" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1532"><a href="#cb72-1532" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} = \left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top</span>
<span id="cb72-1533"><a href="#cb72-1533" aria-hidden="true" tabindex="-1"></a>Z\right) ^ {-1}</span>
<span id="cb72-1534"><a href="#cb72-1534" aria-hidden="true" tabindex="-1"></a>\left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top y\right)</span>
<span id="cb72-1535"><a href="#cb72-1535" aria-hidden="true" tabindex="-1"></a>$$ {#eq-threesls}</span>
<span id="cb72-1536"><a href="#cb72-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1537"><a href="#cb72-1537" aria-hidden="true" tabindex="-1"></a>This estimator can actually be computed using least squares on</span>
<span id="cb72-1538"><a href="#cb72-1538" aria-hidden="true" tabindex="-1"></a>transformed data. Denote $\Psi = \Sigma ^ {- 0.5} \otimes I$ the</span>
<span id="cb72-1539"><a href="#cb72-1539" aria-hidden="true" tabindex="-1"></a>matrix such that $\Psi ^ \top \Psi = \Sigma ^ {-1} \otimes I = \Omega</span>
<span id="cb72-1540"><a href="#cb72-1540" aria-hidden="true" tabindex="-1"></a>^ {-1}$. Then, premultiply the covariates and the response by $\Psi$</span>
<span id="cb72-1541"><a href="#cb72-1541" aria-hidden="true" tabindex="-1"></a>and the instruments by $(\Psi^{-1}) ^ \top$. Then the projection</span>
<span id="cb72-1542"><a href="#cb72-1542" aria-hidden="true" tabindex="-1"></a>matrix of $\tilde{W} = {\Psi^{-1}} ^ \top W$ is:</span>
<span id="cb72-1543"><a href="#cb72-1543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1544"><a href="#cb72-1544" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1545"><a href="#cb72-1545" aria-hidden="true" tabindex="-1"></a>P_{\tilde{W}} = (\Psi^{-1}) ^ \top W\left(W ^ \top \Psi ^ {-1} (\Psi ^ {-1})</span>
<span id="cb72-1546"><a href="#cb72-1546" aria-hidden="true" tabindex="-1"></a>^ \top W\right) ^ {-1} W ^ \top \Psi^{-1}</span>
<span id="cb72-1547"><a href="#cb72-1547" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1548"><a href="#cb72-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1549"><a href="#cb72-1549" aria-hidden="true" tabindex="-1"></a>but $\Psi ^ {-1} (\Psi ^ {-1}) ^ \top = \Psi ^ {-1} (\Psi ^ \top ) ^</span>
<span id="cb72-1550"><a href="#cb72-1550" aria-hidden="true" tabindex="-1"></a>{-1} = (\Psi ^ \top \Psi) ^ {-1} = \Omega$. Therefore:</span>
<span id="cb72-1551"><a href="#cb72-1551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1552"><a href="#cb72-1552" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1553"><a href="#cb72-1553" aria-hidden="true" tabindex="-1"></a>P_{\tilde{W}} = (\Psi^{-1}) ^ \top W \left(W ^ \top \Omega W\right) ^ {-1} W ^ \top \Psi^{-1}</span>
<span id="cb72-1554"><a href="#cb72-1554" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1555"><a href="#cb72-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1556"><a href="#cb72-1556" aria-hidden="true" tabindex="-1"></a>The transformed covariates and response are $\tilde{Z} = \Psi Z$ and</span>
<span id="cb72-1557"><a href="#cb72-1557" aria-hidden="true" tabindex="-1"></a>$\tilde{y} = \Psi y$, so that performing the instrumental variable</span>
<span id="cb72-1558"><a href="#cb72-1558" aria-hidden="true" tabindex="-1"></a>estimator on the transformed data, we get:</span>
<span id="cb72-1559"><a href="#cb72-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1560"><a href="#cb72-1560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1561"><a href="#cb72-1561" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-1562"><a href="#cb72-1562" aria-hidden="true" tabindex="-1"></a>\hat{\gamma} &amp;=&amp; \left(\tilde{Z}^\top P_{\tilde{W}} \tilde{Z}\right) ^</span>
<span id="cb72-1563"><a href="#cb72-1563" aria-hidden="true" tabindex="-1"></a>{-1} \left(\tilde{Z}^\top P_{\tilde{W}} \tilde{y}\right) <span class="sc">\\</span></span>
<span id="cb72-1564"><a href="#cb72-1564" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \left(Z ^ \top \Psi ^ \top (\Psi^{-1}) ^ \top W \left(W ^ \top \Omega</span>
<span id="cb72-1565"><a href="#cb72-1565" aria-hidden="true" tabindex="-1"></a>W\right) ^ {-1} W ^ \top \Psi^{-1} \Psi Z\right) ^ {-1} <span class="sc">\\</span></span>
<span id="cb72-1566"><a href="#cb72-1566" aria-hidden="true" tabindex="-1"></a>&amp;\times &amp; \left(Z ^ \top \Psi ^ \top (\Psi^{-1}) ^ \top W \left(W ^ \top \Omega</span>
<span id="cb72-1567"><a href="#cb72-1567" aria-hidden="true" tabindex="-1"></a>W\right) ^ {-1} W ^ \top \Psi^{-1} \Psi y\right) <span class="sc">\\</span></span>
<span id="cb72-1568"><a href="#cb72-1568" aria-hidden="true" tabindex="-1"></a>&amp;=&amp; \left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top Z\right)^ {-1}</span>
<span id="cb72-1569"><a href="#cb72-1569" aria-hidden="true" tabindex="-1"></a>\left(Z ^ \top W (W ^ \top \Omega W) ^ {-1} W ^ \top y\right)</span>
<span id="cb72-1570"><a href="#cb72-1570" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1571"><a href="#cb72-1571" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1572"><a href="#cb72-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1573"><a href="#cb72-1573" aria-hidden="true" tabindex="-1"></a>which is @eq-threesls. Therefore, the 3SLS estimator can be</span>
<span id="cb72-1574"><a href="#cb72-1574" aria-hidden="true" tabindex="-1"></a>computed in the following way:</span>
<span id="cb72-1575"><a href="#cb72-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1576"><a href="#cb72-1576" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>First compute the 2SLS estimator and retrieve the vectors of</span>
<span id="cb72-1577"><a href="#cb72-1577" aria-hidden="true" tabindex="-1"></a>   residuals ($\hat{\Xi} = (\hat{\epsilon}_1, \ldots, \hat{\epsilon}_L)$).</span>
<span id="cb72-1578"><a href="#cb72-1578" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Estimate $\Sigma$ using the cross-products of the vectors of residuals:</span>
<span id="cb72-1579"><a href="#cb72-1579" aria-hidden="true" tabindex="-1"></a>   $\hat{\Sigma} = \hat{\Xi} ^ \top \hat{\Xi} / N$,</span>
<span id="cb72-1580"><a href="#cb72-1580" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Use the Cholesky decomposition of $\hat{\Sigma} ^ {-1}$ to get $V$</span>
<span id="cb72-1581"><a href="#cb72-1581" aria-hidden="true" tabindex="-1"></a>    such that $V ^ \top V = \Sigma ^ {-1}$,</span>
<span id="cb72-1582"><a href="#cb72-1582" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Premultiply the covariates and the response by $\hat{\Psi} = V \otimes</span>
<span id="cb72-1583"><a href="#cb72-1583" aria-hidden="true" tabindex="-1"></a>   I$ and the instruments by $\left(\hat{\Psi} ^ {-1}\right) ^</span>
<span id="cb72-1584"><a href="#cb72-1584" aria-hidden="true" tabindex="-1"></a>   {\top} = (V ^ {-1}) ^ \top \otimes I$,</span>
<span id="cb72-1585"><a href="#cb72-1585" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Regress the transformed covariates on the transformed instruments</span>
<span id="cb72-1586"><a href="#cb72-1586" aria-hidden="true" tabindex="-1"></a>   and retrieve the fitted values,</span>
<span id="cb72-1587"><a href="#cb72-1587" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Regress the transformed response on the fitted values of the</span>
<span id="cb72-1588"><a href="#cb72-1588" aria-hidden="true" tabindex="-1"></a>   previous regression.</span>
<span id="cb72-1589"><a href="#cb72-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1590"><a href="#cb72-1590" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{three-stage least squares|)}</span>
<span id="cb72-1591"><a href="#cb72-1591" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{instrumental variable estimator!three-stage least squares|)}</span>
<span id="cb72-1592"><a href="#cb72-1592" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{system estimation!three-stage least squares|)}</span>
<span id="cb72-1593"><a href="#cb72-1593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1594"><a href="#cb72-1594" aria-hidden="true" tabindex="-1"></a><span class="fu">### An example: the watermelon market</span></span>
<span id="cb72-1595"><a href="#cb72-1595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1596"><a href="#cb72-1596" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{watermelon}{micsr.data}</span>
<span id="cb72-1597"><a href="#cb72-1597" aria-hidden="true" tabindex="-1"></a>@SUIT:55\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Suit} built an econometric model of the watermelon market using a</span>
<span id="cb72-1598"><a href="#cb72-1598" aria-hidden="true" tabindex="-1"></a>time series for the United States, and his study was complemented by</span>
<span id="cb72-1599"><a href="#cb72-1599" aria-hidden="true" tabindex="-1"></a>@WOLD:58\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Wold} who rebuilt the data set. This data set (called <span class="in">`watermelon`</span>) is a good example of the use of system estimation with endogeneity, and its use for teaching purposes is advocated by @STEW:19\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Stewart}:</span>
<span id="cb72-1600"><a href="#cb72-1600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1601"><a href="#cb72-1601" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1602"><a href="#cb72-1602" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: watermelon</span></span>
<span id="cb72-1603"><a href="#cb72-1603" aria-hidden="true" tabindex="-1"></a>watermelon <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb72-1604"><a href="#cb72-1604" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1605"><a href="#cb72-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1606"><a href="#cb72-1606" aria-hidden="true" tabindex="-1"></a>On the supply side, two quantities of watermelons are distinguished: <span class="in">`q`</span> are crop of watermelons available for harvest (millions) and <span class="in">`h`</span> are watermelons actually harvested (millions). </span>
<span id="cb72-1607"><a href="#cb72-1607" aria-hidden="true" tabindex="-1"></a><span class="in">`q`</span> depends on planting decisions made on information of the previous</span>
<span id="cb72-1608"><a href="#cb72-1608" aria-hidden="true" tabindex="-1"></a>season; more specifically, <span class="in">`q`</span> depends on lag values of the average farm price of watermelon <span class="in">`p`</span> (in  dollars per thousand), the average annual net farm price per pound of cottons <span class="in">`pc`</span> (in dollars), the average farm price of vegetables <span class="in">`pv`</span> (an index) and on two dummy variables for government cotton acreage allotment program <span class="in">`d1`</span> (one for the 1934-1951 period) and for World War 2 <span class="in">`d2`</span> (one for 1943-1946 period).</span>
<span id="cb72-1609"><a href="#cb72-1609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1610"><a href="#cb72-1610" aria-hidden="true" tabindex="-1"></a>The amount of watermelons actually harvested <span class="in">`h`</span> depends on current</span>
<span id="cb72-1611"><a href="#cb72-1611" aria-hidden="true" tabindex="-1"></a>price farm of watermelons <span class="in">`p`</span>, wages <span class="in">`w`</span> (the major cost of</span>
<span id="cb72-1612"><a href="#cb72-1612" aria-hidden="true" tabindex="-1"></a>harvesting) and is of course bounded by <span class="in">`q`</span>, the amount of watermelon</span>
<span id="cb72-1613"><a href="#cb72-1613" aria-hidden="true" tabindex="-1"></a>available for harvest. More specifically, the relative price of</span>
<span id="cb72-1614"><a href="#cb72-1614" aria-hidden="true" tabindex="-1"></a>watermelon and wage is considered. </span>
<span id="cb72-1615"><a href="#cb72-1615" aria-hidden="true" tabindex="-1"></a>On the demand side, farm price depends on per capita harvest, per</span>
<span id="cb72-1616"><a href="#cb72-1616" aria-hidden="true" tabindex="-1"></a>capita income (<span class="in">`yn`</span>) and transportation cost (<span class="in">`pf`</span>). An inverse</span>
<span id="cb72-1617"><a href="#cb72-1617" aria-hidden="true" tabindex="-1"></a>demand function is estimated, of the form:</span>
<span id="cb72-1618"><a href="#cb72-1618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1619"><a href="#cb72-1619" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1620"><a href="#cb72-1620" aria-hidden="true" tabindex="-1"></a>p = \alpha + \beta_q q + \beta_r r + \ldots</span>
<span id="cb72-1621"><a href="#cb72-1621" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1622"><a href="#cb72-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1623"><a href="#cb72-1623" aria-hidden="true" tabindex="-1"></a>and, all the variables being in logarithm, the price and income elasticities are therefore respectively $1 / \beta_q$ and $- \beta_r / \beta_q$. We compute the relative price of</span>
<span id="cb72-1624"><a href="#cb72-1624" aria-hidden="true" tabindex="-1"></a>watermelons in terms of wage (<span class="in">`pw`</span>), the income per capita (<span class="in">`yn`</span>) and</span>
<span id="cb72-1625"><a href="#cb72-1625" aria-hidden="true" tabindex="-1"></a>harvest (<span class="in">`hn`</span>) and the first lags for <span class="in">`p`</span>, <span class="in">`pc`</span> and <span class="in">`pv`</span>. We also</span>
<span id="cb72-1626"><a href="#cb72-1626" aria-hidden="true" tabindex="-1"></a>remove the first and the last observation (because of the lag for the</span>
<span id="cb72-1627"><a href="#cb72-1627" aria-hidden="true" tabindex="-1"></a>first one and because of the missing value for <span class="in">`pv`</span> and <span class="in">`w`</span> for the last</span>
<span id="cb72-1628"><a href="#cb72-1628" aria-hidden="true" tabindex="-1"></a>one).</span>
<span id="cb72-1629"><a href="#cb72-1629" aria-hidden="true" tabindex="-1"></a>\idxfun{mutate}{dplyr}\idxfun{lag}{dplyr}\idxfun{filter}{dplyr}</span>
<span id="cb72-1630"><a href="#cb72-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1631"><a href="#cb72-1631" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1632"><a href="#cb72-1632" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: waltermelon_covariates</span></span>
<span id="cb72-1633"><a href="#cb72-1633" aria-hidden="true" tabindex="-1"></a>wm <span class="ot">&lt;-</span> watermelon <span class="sc">%&gt;%</span></span>
<span id="cb72-1634"><a href="#cb72-1634" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">yn =</span> y <span class="sc">-</span> n, <span class="at">hn =</span> h <span class="sc">-</span> n,</span>
<span id="cb72-1635"><a href="#cb72-1635" aria-hidden="true" tabindex="-1"></a>           <span class="at">pw =</span> p <span class="sc">-</span> w, <span class="at">lp =</span> <span class="fu">lag</span>(p),</span>
<span id="cb72-1636"><a href="#cb72-1636" aria-hidden="true" tabindex="-1"></a>           <span class="at">lpc =</span> <span class="fu">lag</span>(pc), <span class="at">lpv =</span> <span class="fu">lag</span>(pv)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1637"><a href="#cb72-1637" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(<span class="sc">!</span> year <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1930</span>, <span class="dv">1951</span>))</span>
<span id="cb72-1638"><a href="#cb72-1638" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1639"><a href="#cb72-1639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1640"><a href="#cb72-1640" aria-hidden="true" tabindex="-1"></a>We can now define the set of three equations:</span>
<span id="cb72-1641"><a href="#cb72-1641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1642"><a href="#cb72-1642" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1643"><a href="#cb72-1643" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: equations_watermelon</span></span>
<span id="cb72-1644"><a href="#cb72-1644" aria-hidden="true" tabindex="-1"></a>eq_c <span class="ot">&lt;-</span> q <span class="sc">~</span> lp <span class="sc">+</span> lpc <span class="sc">+</span> lpv <span class="sc">+</span> d1 <span class="sc">+</span> d2</span>
<span id="cb72-1645"><a href="#cb72-1645" aria-hidden="true" tabindex="-1"></a>eq_s <span class="ot">&lt;-</span> h <span class="sc">~</span> pw <span class="sc">+</span> q</span>
<span id="cb72-1646"><a href="#cb72-1646" aria-hidden="true" tabindex="-1"></a>eq_d <span class="ot">&lt;-</span> p <span class="sc">~</span> hn <span class="sc">+</span> yn <span class="sc">+</span> pf</span>
<span id="cb72-1647"><a href="#cb72-1647" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1648"><a href="#cb72-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1649"><a href="#cb72-1649" aria-hidden="true" tabindex="-1"></a>The exogenous variables are <span class="in">`w`</span>, <span class="in">`n`</span>, <span class="in">`yn`</span>, <span class="in">`pf`</span>, <span class="in">`d1`</span>, <span class="in">`d2`</span> and the</span>
<span id="cb72-1650"><a href="#cb72-1650" aria-hidden="true" tabindex="-1"></a>lagged values of the price of watermelons (<span class="in">`lp`</span>), cotton (<span class="in">`lpc`</span>) and</span>
<span id="cb72-1651"><a href="#cb72-1651" aria-hidden="true" tabindex="-1"></a>vegetables (<span class="in">`lpv`</span>). We form a one-sided formula for this set of</span>
<span id="cb72-1652"><a href="#cb72-1652" aria-hidden="true" tabindex="-1"></a>instruments:</span>
<span id="cb72-1653"><a href="#cb72-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1654"><a href="#cb72-1654" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1655"><a href="#cb72-1655" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: eq_inst_watermelon</span></span>
<span id="cb72-1656"><a href="#cb72-1656" aria-hidden="true" tabindex="-1"></a>eq_inst <span class="ot">&lt;-</span> <span class="er">~</span> w <span class="sc">+</span> n <span class="sc">+</span> yn <span class="sc">+</span> lp <span class="sc">+</span> pf <span class="sc">+</span> d1 <span class="sc">+</span> d2 <span class="sc">+</span> lpc <span class="sc">+</span> lpv</span>
<span id="cb72-1657"><a href="#cb72-1657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1658"><a href="#cb72-1658" aria-hidden="true" tabindex="-1"></a>We then extract the three matrices of covariates, the matrix of</span>
<span id="cb72-1659"><a href="#cb72-1659" aria-hidden="true" tabindex="-1"></a>instruments and the matrix of responses:</span>
<span id="cb72-1660"><a href="#cb72-1660" aria-hidden="true" tabindex="-1"></a>\idxfun{model.matrix}{stats}\idxfun{nrow}{base}\idxfun{select}{dplyr}\idxfun{as.matrix}{base}</span>
<span id="cb72-1661"><a href="#cb72-1661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1662"><a href="#cb72-1662" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1663"><a href="#cb72-1663" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model_matrices_watermelon</span></span>
<span id="cb72-1664"><a href="#cb72-1664" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_inst, wm)</span>
<span id="cb72-1665"><a href="#cb72-1665" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_c, wm)</span>
<span id="cb72-1666"><a href="#cb72-1666" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_s, wm)</span>
<span id="cb72-1667"><a href="#cb72-1667" aria-hidden="true" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(eq_d, wm)</span>
<span id="cb72-1668"><a href="#cb72-1668" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(W)</span>
<span id="cb72-1669"><a href="#cb72-1669" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">select</span>(wm, q, h, p) <span class="sc">%&gt;%</span> as.matrix</span>
<span id="cb72-1670"><a href="#cb72-1670" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1671"><a href="#cb72-1671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1672"><a href="#cb72-1672" aria-hidden="true" tabindex="-1"></a>We first compute the 2SLS estimator, using the **systemfit** package:</span>
<span id="cb72-1673"><a href="#cb72-1673" aria-hidden="true" tabindex="-1"></a>\idxfun{systemfit}{systemfit}</span>
<span id="cb72-1674"><a href="#cb72-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1675"><a href="#cb72-1675" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1676"><a href="#cb72-1676" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: twosls_watermelon</span></span>
<span id="cb72-1677"><a href="#cb72-1677" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb72-1678"><a href="#cb72-1678" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(systemfit)</span>
<span id="cb72-1679"><a href="#cb72-1679" aria-hidden="true" tabindex="-1"></a>twosls <span class="ot">&lt;-</span> <span class="fu">systemfit</span>(<span class="fu">list</span>(<span class="at">crop =</span> eq_c, <span class="at">supply =</span> eq_s, <span class="at">demand =</span> eq_d),</span>
<span id="cb72-1680"><a href="#cb72-1680" aria-hidden="true" tabindex="-1"></a>                    <span class="at">inst =</span> eq_inst, <span class="at">method =</span> <span class="st">"2SLS"</span>, <span class="at">data =</span> wm)</span>
<span id="cb72-1681"><a href="#cb72-1681" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1682"><a href="#cb72-1682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1683"><a href="#cb72-1683" aria-hidden="true" tabindex="-1"></a>From this consistent, but inefficient estimator, we extract the data frame of residuals (one column per equation, one line per observation), we coerce it to a matrix and</span>
<span id="cb72-1684"><a href="#cb72-1684" aria-hidden="true" tabindex="-1"></a>we estimate the matrix of covariance for the system of equation ($\Sigma$):</span>
<span id="cb72-1685"><a href="#cb72-1685" aria-hidden="true" tabindex="-1"></a>\idxfun{crossprod}{base}\idxfun{as.matrix}{base}\idxfun{resid}{stats}</span>
<span id="cb72-1686"><a href="#cb72-1686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1687"><a href="#cb72-1687" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1688"><a href="#cb72-1688" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: estimation_sigma</span></span>
<span id="cb72-1689"><a href="#cb72-1689" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(<span class="fu">as.matrix</span>(<span class="fu">resid</span>(twosls))) <span class="sc">/</span> N</span>
<span id="cb72-1690"><a href="#cb72-1690" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1691"><a href="#cb72-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1692"><a href="#cb72-1692" aria-hidden="true" tabindex="-1"></a>We then compute <span class="in">`V`</span> using the Cholesky decomposition of the inverse of</span>
<span id="cb72-1693"><a href="#cb72-1693" aria-hidden="true" tabindex="-1"></a>$\Sigma$:</span>
<span id="cb72-1694"><a href="#cb72-1694" aria-hidden="true" tabindex="-1"></a>\idxfun{solve}{base}\idxfun{chol}{base}</span>
<span id="cb72-1695"><a href="#cb72-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1696"><a href="#cb72-1696" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1697"><a href="#cb72-1697" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: cholesky_sigma</span></span>
<span id="cb72-1698"><a href="#cb72-1698" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> Sigma <span class="sc">%&gt;%</span> solve <span class="sc">%&gt;%</span> chol</span>
<span id="cb72-1699"><a href="#cb72-1699" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1700"><a href="#cb72-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1701"><a href="#cb72-1701" aria-hidden="true" tabindex="-1"></a>Using $V$, we apply the relevant transformation for the response and</span>
<span id="cb72-1702"><a href="#cb72-1702" aria-hidden="true" tabindex="-1"></a>for the covariate:</span>
<span id="cb72-1703"><a href="#cb72-1703" aria-hidden="true" tabindex="-1"></a>\idxfun{rbind}{base}\idxfun{cbind}{base}\idxfun{as.numeric}{base}</span>
<span id="cb72-1704"><a href="#cb72-1704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1705"><a href="#cb72-1705" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1706"><a href="#cb72-1706" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: transformed_variables_watermelon</span></span>
<span id="cb72-1707"><a href="#cb72-1707" aria-hidden="true" tabindex="-1"></a>Xt <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">cbind</span>(V[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> X1, V[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">*</span> X2, V[<span class="dv">1</span>, <span class="dv">3</span>] <span class="sc">*</span> X3),</span>
<span id="cb72-1708"><a href="#cb72-1708" aria-hidden="true" tabindex="-1"></a>            <span class="fu">cbind</span>(V[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span> X1, V[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">*</span> X2, V[<span class="dv">2</span>, <span class="dv">3</span>] <span class="sc">*</span> X3),</span>
<span id="cb72-1709"><a href="#cb72-1709" aria-hidden="true" tabindex="-1"></a>            <span class="fu">cbind</span>(V[<span class="dv">3</span>, <span class="dv">1</span>] <span class="sc">*</span> X1, V[<span class="dv">3</span>, <span class="dv">2</span>] <span class="sc">*</span> X2, V[<span class="dv">3</span>, <span class="dv">3</span>] <span class="sc">*</span> X3))</span>
<span id="cb72-1710"><a href="#cb72-1710" aria-hidden="true" tabindex="-1"></a>yt <span class="ot">&lt;-</span> Y <span class="sc">%*%</span> <span class="fu">t</span>(V) <span class="sc">%&gt;%</span> as.numeric</span>
<span id="cb72-1711"><a href="#cb72-1711" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1712"><a href="#cb72-1712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1713"><a href="#cb72-1713" aria-hidden="true" tabindex="-1"></a>We then apply the transformation for the instruments, using $\left(V ^</span>
<span id="cb72-1714"><a href="#cb72-1714" aria-hidden="true" tabindex="-1"></a>{-1}\right) ^ \top$. The matrix of instruments being the same for all</span>
<span id="cb72-1715"><a href="#cb72-1715" aria-hidden="true" tabindex="-1"></a>the equations, the transformation can be obtained more simply using</span>
<span id="cb72-1716"><a href="#cb72-1716" aria-hidden="true" tabindex="-1"></a>a Kronecker product:</span>
<span id="cb72-1717"><a href="#cb72-1717" aria-hidden="true" tabindex="-1"></a>\idxfun{solve}{base}</span>
<span id="cb72-1718"><a href="#cb72-1718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1719"><a href="#cb72-1719" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1720"><a href="#cb72-1720" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: instrument_matrix_watermelon</span></span>
<span id="cb72-1721"><a href="#cb72-1721" aria-hidden="true" tabindex="-1"></a>Wt <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">solve</span>(V)) <span class="sc">%x%</span> W</span>
<span id="cb72-1722"><a href="#cb72-1722" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1723"><a href="#cb72-1723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1724"><a href="#cb72-1724" aria-hidden="true" tabindex="-1"></a>Then, 2SLS is performed by first regressing <span class="in">`Zt`</span> on <span class="in">`Xt`</span>:</span>
<span id="cb72-1725"><a href="#cb72-1725" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}</span>
<span id="cb72-1726"><a href="#cb72-1726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1727"><a href="#cb72-1727" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1728"><a href="#cb72-1728" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: first_stage_watermelon</span></span>
<span id="cb72-1729"><a href="#cb72-1729" aria-hidden="true" tabindex="-1"></a>first <span class="ot">&lt;-</span> <span class="fu">lm</span>(Xt <span class="sc">~</span> Wt <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb72-1730"><a href="#cb72-1730" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1731"><a href="#cb72-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1732"><a href="#cb72-1732" aria-hidden="true" tabindex="-1"></a>and then by regressing the fitted values of this first step regression</span>
<span id="cb72-1733"><a href="#cb72-1733" aria-hidden="true" tabindex="-1"></a>on the transformed response:</span>
<span id="cb72-1734"><a href="#cb72-1734" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{fitted}{stats}</span>
<span id="cb72-1735"><a href="#cb72-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1736"><a href="#cb72-1736" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1737"><a href="#cb72-1737" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: second_stage_watermelon</span></span>
<span id="cb72-1738"><a href="#cb72-1738" aria-hidden="true" tabindex="-1"></a>second <span class="ot">&lt;-</span> <span class="fu">lm</span>(yt <span class="sc">~</span> <span class="fu">fitted</span>(first) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb72-1739"><a href="#cb72-1739" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1740"><a href="#cb72-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1741"><a href="#cb72-1741" aria-hidden="true" tabindex="-1"></a>Identical results are obtained using <span class="in">`systemfit`</span> and setting <span class="in">`method`</span></span>
<span id="cb72-1742"><a href="#cb72-1742" aria-hidden="true" tabindex="-1"></a>to <span class="in">`"3SLS"`</span>:^<span class="co">[</span><span class="ot">As for the SUR model estimated in @sec-sur, we set the `methodResidCov` argument to `"noDfCor"`, so that no degrees of freedom correction is used for the estimation of the covariance matrix of the errors.</span><span class="co">]</span></span>
<span id="cb72-1743"><a href="#cb72-1743" aria-hidden="true" tabindex="-1"></a>\idxfun{coef}{stats}\idxfun{systemfit}{systemfit}</span>
<span id="cb72-1744"><a href="#cb72-1744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1745"><a href="#cb72-1745" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1746"><a href="#cb72-1746" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: threesls_watermelon</span></span>
<span id="cb72-1747"><a href="#cb72-1747" aria-hidden="true" tabindex="-1"></a>threesls <span class="ot">&lt;-</span> <span class="fu">systemfit</span>(<span class="fu">list</span>(<span class="at">crop =</span> eq_c, <span class="at">supply =</span> eq_s,</span>
<span id="cb72-1748"><a href="#cb72-1748" aria-hidden="true" tabindex="-1"></a>                           <span class="at">demand =</span> eq_d), <span class="at">inst =</span> eq_inst,</span>
<span id="cb72-1749"><a href="#cb72-1749" aria-hidden="true" tabindex="-1"></a>                      <span class="at">method =</span> <span class="st">"3SLS"</span>, <span class="at">data =</span> wm,</span>
<span id="cb72-1750"><a href="#cb72-1750" aria-hidden="true" tabindex="-1"></a>                      <span class="at">methodResidCov=</span>  <span class="st">"noDfCor"</span>)</span>
<span id="cb72-1751"><a href="#cb72-1751" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(threesls)</span>
<span id="cb72-1752"><a href="#cb72-1752" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1753"><a href="#cb72-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1756"><a href="#cb72-1756" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb72-1757"><a href="#cb72-1757" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: threesls_hide</span></span>
<span id="cb72-1758"><a href="#cb72-1758" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb72-1759"><a href="#cb72-1759" aria-hidden="true" tabindex="-1"></a>s_p <span class="ot">&lt;-</span> <span class="fu">coef</span>(threesls)[<span class="st">"supply_pw"</span>]</span>
<span id="cb72-1760"><a href="#cb72-1760" aria-hidden="true" tabindex="-1"></a>d_q <span class="ot">&lt;-</span> <span class="fu">coef</span>(threesls)[<span class="st">"demand_hn"</span>]</span>
<span id="cb72-1761"><a href="#cb72-1761" aria-hidden="true" tabindex="-1"></a>d_y <span class="ot">&lt;-</span> <span class="fu">coef</span>(threesls)[<span class="st">"demand_yn"</span>]</span>
<span id="cb72-1762"><a href="#cb72-1762" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1763"><a href="#cb72-1763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1764"><a href="#cb72-1764" aria-hidden="true" tabindex="-1"></a>The relative price of watermelon is significantly positive in</span>
<span id="cb72-1765"><a href="#cb72-1765" aria-hidden="true" tabindex="-1"></a>the supply equation, with a value of <span class="in">`r round(s_p, 2)`</span> which is the</span>
<span id="cb72-1766"><a href="#cb72-1766" aria-hidden="true" tabindex="-1"></a>price elasticity of supply. In the inverse demand function, the</span>
<span id="cb72-1767"><a href="#cb72-1767" aria-hidden="true" tabindex="-1"></a>coefficients of per capita quantity of watermelons and of per capita</span>
<span id="cb72-1768"><a href="#cb72-1768" aria-hidden="true" tabindex="-1"></a>income have the expected sign (respectively negative and positive) and</span>
<span id="cb72-1769"><a href="#cb72-1769" aria-hidden="true" tabindex="-1"></a>are highly significant. The estimated price and income elasticities</span>
<span id="cb72-1770"><a href="#cb72-1770" aria-hidden="true" tabindex="-1"></a>are $1 / <span class="in">`r round(d_q, 2)`</span> = <span class="in">`r round(1 / d_q, 2)`</span>$ and $- <span class="in">`r round(d_y, 2)`</span> / <span class="in">`r round(d_q, 2)`</span> = <span class="in">`r round(- d_y / d_q, 2)`</span>$.</span>
<span id="cb72-1771"><a href="#cb72-1771" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{watermelon}{micsr.data}</span>
<span id="cb72-1772"><a href="#cb72-1772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1773"><a href="#cb72-1773" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fixed effects model {#sec-fixed_effects}</span></span>
<span id="cb72-1774"><a href="#cb72-1774" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{fixed effects model|(}</span>
<span id="cb72-1775"><a href="#cb72-1775" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!fixed effects model|(}</span>
<span id="cb72-1776"><a href="#cb72-1776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1777"><a href="#cb72-1777" aria-hidden="true" tabindex="-1"></a>In @sec-error_component, we developed the error component model for panel or pseudo-panel data sets. Remember that when we have several observations ($t$) for the same entities ($n$), the simple linear model can be written as:</span>
<span id="cb72-1778"><a href="#cb72-1778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1779"><a href="#cb72-1779" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1780"><a href="#cb72-1780" aria-hidden="true" tabindex="-1"></a>y_{nt} = \alpha + \beta x_{nt} + \epsilon_{nt} = \alpha + \beta x_{nt} + \eta_n + \nu_{nt}</span>
<span id="cb72-1781"><a href="#cb72-1781" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1782"><a href="#cb72-1782" aria-hidden="true" tabindex="-1"></a>the error term being the sum of two components: an entity / individual effect $\eta_n$ and an idiosyncratic effect $\nu_{nt}$. In </span>
<span id="cb72-1783"><a href="#cb72-1783" aria-hidden="true" tabindex="-1"></a>@sec-error_component, we assumed that $x$ was uncorrelated with $\epsilon_{nt}$. OLS was still a consistent estimator, but the BLUE estimator was GLS which takes into account the correlation between errors of the observations of the same entity / individual caused by the presence of a common individual effect $\eta_n$. Consider now that $x$ is endogenous; the OLS and GLS estimators are then biased and inconsistent. If $x$ is correlated with $\eta$:</span>
<span id="cb72-1784"><a href="#cb72-1784" aria-hidden="true" tabindex="-1"></a>$\mbox{E}(\eta \mid x) \neq 0$ but not with $\nu$: $\mbox{E}(\nu \mid x) = 0$, unbiased and consistent OLS estimators can be obtained when the individual effect $\eta$ is either estimated or if the</span>
<span id="cb72-1785"><a href="#cb72-1785" aria-hidden="true" tabindex="-1"></a>estimation is performed on a transformation of the covariate and the</span>
<span id="cb72-1786"><a href="#cb72-1786" aria-hidden="true" tabindex="-1"></a>response that removes the individual effect. This is called the **fixed</span>
<span id="cb72-1787"><a href="#cb72-1787" aria-hidden="true" tabindex="-1"></a>effects** estimator. </span>
<span id="cb72-1788"><a href="#cb72-1788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1789"><a href="#cb72-1789" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computation of the fixed effects estimator</span></span>
<span id="cb72-1790"><a href="#cb72-1790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1791"><a href="#cb72-1791" aria-hidden="true" tabindex="-1"></a>The linear model: $y_{nt} = \beta^\top x_{nt} + \eta_n + \nu_{nt}$ can be written in matrix form as:</span>
<span id="cb72-1792"><a href="#cb72-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1793"><a href="#cb72-1793" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1794"><a href="#cb72-1794" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1795"><a href="#cb72-1795" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1796"><a href="#cb72-1796" aria-hidden="true" tabindex="-1"></a>y_{11}<span class="sc">\\</span></span>
<span id="cb72-1797"><a href="#cb72-1797" aria-hidden="true" tabindex="-1"></a>y_{12}<span class="sc">\\</span></span>
<span id="cb72-1798"><a href="#cb72-1798" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1799"><a href="#cb72-1799" aria-hidden="true" tabindex="-1"></a>y_{1T} <span class="sc">\\</span></span>
<span id="cb72-1800"><a href="#cb72-1800" aria-hidden="true" tabindex="-1"></a>y_{21}<span class="sc">\\</span></span>
<span id="cb72-1801"><a href="#cb72-1801" aria-hidden="true" tabindex="-1"></a>y_{22}<span class="sc">\\</span></span>
<span id="cb72-1802"><a href="#cb72-1802" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1803"><a href="#cb72-1803" aria-hidden="true" tabindex="-1"></a>y_{2T}<span class="sc">\\</span></span>
<span id="cb72-1804"><a href="#cb72-1804" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1805"><a href="#cb72-1805" aria-hidden="true" tabindex="-1"></a>y_{N1}<span class="sc">\\</span></span>
<span id="cb72-1806"><a href="#cb72-1806" aria-hidden="true" tabindex="-1"></a>y_{N2}<span class="sc">\\</span></span>
<span id="cb72-1807"><a href="#cb72-1807" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1808"><a href="#cb72-1808" aria-hidden="true" tabindex="-1"></a>y_{NT}</span>
<span id="cb72-1809"><a href="#cb72-1809" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1810"><a href="#cb72-1810" aria-hidden="true" tabindex="-1"></a>\right) = </span>
<span id="cb72-1811"><a href="#cb72-1811" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1812"><a href="#cb72-1812" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1813"><a href="#cb72-1813" aria-hidden="true" tabindex="-1"></a>x_{11}^\top<span class="sc">\\</span></span>
<span id="cb72-1814"><a href="#cb72-1814" aria-hidden="true" tabindex="-1"></a>x_{12}^\top<span class="sc">\\</span></span>
<span id="cb72-1815"><a href="#cb72-1815" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1816"><a href="#cb72-1816" aria-hidden="true" tabindex="-1"></a>x_{1T}^\top<span class="sc">\\</span></span>
<span id="cb72-1817"><a href="#cb72-1817" aria-hidden="true" tabindex="-1"></a>x_{21}^\top<span class="sc">\\</span></span>
<span id="cb72-1818"><a href="#cb72-1818" aria-hidden="true" tabindex="-1"></a>x_{22}^\top<span class="sc">\\</span></span>
<span id="cb72-1819"><a href="#cb72-1819" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1820"><a href="#cb72-1820" aria-hidden="true" tabindex="-1"></a>x_{2T}^\top<span class="sc">\\</span></span>
<span id="cb72-1821"><a href="#cb72-1821" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1822"><a href="#cb72-1822" aria-hidden="true" tabindex="-1"></a>x_{N1}^\top<span class="sc">\\</span></span>
<span id="cb72-1823"><a href="#cb72-1823" aria-hidden="true" tabindex="-1"></a>x_{N2}^\top<span class="sc">\\</span></span>
<span id="cb72-1824"><a href="#cb72-1824" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1825"><a href="#cb72-1825" aria-hidden="true" tabindex="-1"></a>x_{NT}^\top</span>
<span id="cb72-1826"><a href="#cb72-1826" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1827"><a href="#cb72-1827" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1828"><a href="#cb72-1828" aria-hidden="true" tabindex="-1"></a>\beta</span>
<span id="cb72-1829"><a href="#cb72-1829" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span></span>
<span id="cb72-1830"><a href="#cb72-1830" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1831"><a href="#cb72-1831" aria-hidden="true" tabindex="-1"></a>\begin{array}{cccc}</span>
<span id="cb72-1832"><a href="#cb72-1832" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1833"><a href="#cb72-1833" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1834"><a href="#cb72-1834" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1835"><a href="#cb72-1835" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1836"><a href="#cb72-1836" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1837"><a href="#cb72-1837" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1838"><a href="#cb72-1838" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1839"><a href="#cb72-1839" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; \ldots &amp; 0 <span class="sc">\\</span></span>
<span id="cb72-1840"><a href="#cb72-1840" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1841"><a href="#cb72-1841" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; \ldots &amp; 1 <span class="sc">\\</span></span>
<span id="cb72-1842"><a href="#cb72-1842" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; \ldots &amp; 1 <span class="sc">\\</span></span>
<span id="cb72-1843"><a href="#cb72-1843" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb72-1844"><a href="#cb72-1844" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; \ldots &amp; 1 <span class="sc">\\</span></span>
<span id="cb72-1845"><a href="#cb72-1845" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1846"><a href="#cb72-1846" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1847"><a href="#cb72-1847" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1848"><a href="#cb72-1848" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1849"><a href="#cb72-1849" aria-hidden="true" tabindex="-1"></a>\eta_1 <span class="sc">\\</span></span>
<span id="cb72-1850"><a href="#cb72-1850" aria-hidden="true" tabindex="-1"></a>\eta_2 <span class="sc">\\</span></span>
<span id="cb72-1851"><a href="#cb72-1851" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb72-1852"><a href="#cb72-1852" aria-hidden="true" tabindex="-1"></a>\eta_N</span>
<span id="cb72-1853"><a href="#cb72-1853" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1854"><a href="#cb72-1854" aria-hidden="true" tabindex="-1"></a>\right)+</span>
<span id="cb72-1855"><a href="#cb72-1855" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb72-1856"><a href="#cb72-1856" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1857"><a href="#cb72-1857" aria-hidden="true" tabindex="-1"></a>\nu_{11}<span class="sc">\\</span></span>
<span id="cb72-1858"><a href="#cb72-1858" aria-hidden="true" tabindex="-1"></a>\nu_{12}<span class="sc">\\</span></span>
<span id="cb72-1859"><a href="#cb72-1859" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1860"><a href="#cb72-1860" aria-hidden="true" tabindex="-1"></a>\nu_{1T} <span class="sc">\\</span></span>
<span id="cb72-1861"><a href="#cb72-1861" aria-hidden="true" tabindex="-1"></a>\nu_{21}<span class="sc">\\</span></span>
<span id="cb72-1862"><a href="#cb72-1862" aria-hidden="true" tabindex="-1"></a>\nu_{22}<span class="sc">\\</span></span>
<span id="cb72-1863"><a href="#cb72-1863" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1864"><a href="#cb72-1864" aria-hidden="true" tabindex="-1"></a>\nu_{2T}<span class="sc">\\</span></span>
<span id="cb72-1865"><a href="#cb72-1865" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1866"><a href="#cb72-1866" aria-hidden="true" tabindex="-1"></a>\nu_{N1}<span class="sc">\\</span></span>
<span id="cb72-1867"><a href="#cb72-1867" aria-hidden="true" tabindex="-1"></a>\nu_{N2}<span class="sc">\\</span></span>
<span id="cb72-1868"><a href="#cb72-1868" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb72-1869"><a href="#cb72-1869" aria-hidden="true" tabindex="-1"></a>\nu_{NT}</span>
<span id="cb72-1870"><a href="#cb72-1870" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1871"><a href="#cb72-1871" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1872"><a href="#cb72-1872" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1873"><a href="#cb72-1873" aria-hidden="true" tabindex="-1"></a>or:</span>
<span id="cb72-1874"><a href="#cb72-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1875"><a href="#cb72-1875" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1876"><a href="#cb72-1876" aria-hidden="true" tabindex="-1"></a>y = X \beta + D \eta + \nu</span>
<span id="cb72-1877"><a href="#cb72-1877" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1878"><a href="#cb72-1878" aria-hidden="true" tabindex="-1"></a>Note that this model doesn't contain an intercept: as $\eta$ are considered as parameters to be estimated, with an intercept $\alpha$, only $N$ parameters like $\alpha + \eta_n$ can be estimated and therefore, $\alpha$ can safely be set to 0.</span>
<span id="cb72-1879"><a href="#cb72-1879" aria-hidden="true" tabindex="-1"></a>$D = j_T \otimes I_N$ is a $NT\times N$ matrix where each column contains a dummy variable for one individual. The **least squares dummy variable** (**LSDV**) estimator </span>
<span id="cb72-1880"><a href="#cb72-1880" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!least squares dummy variable}</span>
<span id="cb72-1881"><a href="#cb72-1881" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{least squares dummy variable estimator}</span>
<span id="cb72-1882"><a href="#cb72-1882" aria-hidden="true" tabindex="-1"></a>consists of estimating by least squares $\beta$ and $\eta$. Instead of estimating the $N$ $\eta$ parameters, it is simpler and more efficient to use the Frisch-Waugh theorem\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Frisch-Waugh theorem}: first $X$ and $y$ are regressed on $D$, then the residuals of the first regression of $y$ are regressed on those of $X$.</span>
<span id="cb72-1883"><a href="#cb72-1883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1884"><a href="#cb72-1884" aria-hidden="true" tabindex="-1"></a>The OLS estimate of a variable $z$ on $D$ is: $\hat{\delta} = (D ^ \top D) ^ {-1} D^\top z$. But $D^\top D =  T I$, so that $(D ^ \top D) ^ {-1} = I / T$. Moreover:</span>
<span id="cb72-1885"><a href="#cb72-1885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1886"><a href="#cb72-1886" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1887"><a href="#cb72-1887" aria-hidden="true" tabindex="-1"></a>D^\top z = \left(</span>
<span id="cb72-1888"><a href="#cb72-1888" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb72-1889"><a href="#cb72-1889" aria-hidden="true" tabindex="-1"></a>\sum_t z_{1t} <span class="sc">\\</span></span>
<span id="cb72-1890"><a href="#cb72-1890" aria-hidden="true" tabindex="-1"></a>\sum_t z_{2t} <span class="sc">\\</span></span>
<span id="cb72-1891"><a href="#cb72-1891" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb72-1892"><a href="#cb72-1892" aria-hidden="true" tabindex="-1"></a>\sum_t z_{Nt} <span class="sc">\\</span></span>
<span id="cb72-1893"><a href="#cb72-1893" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1894"><a href="#cb72-1894" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb72-1895"><a href="#cb72-1895" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1896"><a href="#cb72-1896" aria-hidden="true" tabindex="-1"></a>and therefore $\hat{\delta}$ is a vector of individual mean of $z$, with typical element $\hat{\delta}_n = \bar{z}_{n.} = \sum_t z_{nt} / T$. Therefore, applying the Frisch-Waugh theorem implies that regressing $y$ on $X$ and $D$ is equivalent to regressing $y$ on $X$ with both the response and the covariates measured in deviations from their individual means. For one observation, we have: $y_{nt} = \beta^\top x_{nt} + \eta_n + \nu_{nt}$. Taking the individual mean of this equation, we get: $\bar{y}_{n.} = \beta^\top \bar{x}_{n.} + \eta_n + \bar{\nu}_{n.}$. Taking deviations from the individual means, the model to estimate is then:</span>
<span id="cb72-1897"><a href="#cb72-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1898"><a href="#cb72-1898" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1899"><a href="#cb72-1899" aria-hidden="true" tabindex="-1"></a>y_{nt} - \bar{y}_{n.} = \beta ^ \top (x_{nt} - \bar{x}_{n.}) + (\nu_{nt} - \bar{\nu}_{n.})</span>
<span id="cb72-1900"><a href="#cb72-1900" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1901"><a href="#cb72-1901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1902"><a href="#cb72-1902" aria-hidden="true" tabindex="-1"></a>Therefore, if $x$ is correlated with $\eta$, but not with $\nu$, the OLS estimator of this model is consistent because the error doesn't contain $\eta$ anymore and is therefore uncorrelated with $x$. This model is called the fixed effects estimator and also the **within** estimator in the panel data literature. With a single regressor, the estimator of the unique slope is:</span>
<span id="cb72-1903"><a href="#cb72-1903" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!within estimator}</span>
<span id="cb72-1904"><a href="#cb72-1904" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{within estimator}</span>
<span id="cb72-1905"><a href="#cb72-1905" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1906"><a href="#cb72-1906" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_{n=1} ^ N \sum_{t = 1} ^ T (y_{nt} - \bar{y}_{n.})(x_{nt} - \bar{x}_{n.})}</span>
<span id="cb72-1907"><a href="#cb72-1907" aria-hidden="true" tabindex="-1"></a>{\sum_{n=1} ^ N \sum_{t = 1} ^ T (x_{nt} - \bar{x}_{n.}) ^ 2}</span>
<span id="cb72-1908"><a href="#cb72-1908" aria-hidden="true" tabindex="-1"></a>$$ {#eq-within_est}</span>
<span id="cb72-1909"><a href="#cb72-1909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1910"><a href="#cb72-1910" aria-hidden="true" tabindex="-1"></a>The deviation from individual means is obviously not the only transformation that enables to get rid of the entity effects. Consider the special case where $T = 2$ for all individuals. An interesting example of this particular case is samples of twins. In this case:</span>
<span id="cb72-1911"><a href="#cb72-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1912"><a href="#cb72-1912" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1913"><a href="#cb72-1913" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb72-1914"><a href="#cb72-1914" aria-hidden="true" tabindex="-1"></a>\begin{array}{rcl}</span>
<span id="cb72-1915"><a href="#cb72-1915" aria-hidden="true" tabindex="-1"></a>y_{n1} = \beta x_{n1} + \eta_n + \nu_{n1} <span class="sc">\\</span></span>
<span id="cb72-1916"><a href="#cb72-1916" aria-hidden="true" tabindex="-1"></a>y_{n2} = \beta x_{n2} + \eta_n + \nu_{n2} <span class="sc">\\</span></span>
<span id="cb72-1917"><a href="#cb72-1917" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb72-1918"><a href="#cb72-1918" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb72-1919"><a href="#cb72-1919" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1920"><a href="#cb72-1920" aria-hidden="true" tabindex="-1"></a>And the difference between the two equations enables to get rid of the entity effect:</span>
<span id="cb72-1921"><a href="#cb72-1921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1922"><a href="#cb72-1922" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1923"><a href="#cb72-1923" aria-hidden="true" tabindex="-1"></a>y_{n1} - y_{n2} = \beta(x_{n1} -x_{n2}) + \nu_{n1} - \nu_{n2}</span>
<span id="cb72-1924"><a href="#cb72-1924" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1925"><a href="#cb72-1925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1926"><a href="#cb72-1926" aria-hidden="true" tabindex="-1"></a>The least squares estimator of the slope is the following **first-difference** estimator:</span>
<span id="cb72-1927"><a href="#cb72-1927" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{first-difference estimator}</span>
<span id="cb72-1928"><a href="#cb72-1928" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!first-difference estimator}</span>
<span id="cb72-1929"><a href="#cb72-1929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1930"><a href="#cb72-1930" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1931"><a href="#cb72-1931" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = \frac{\sum_n(y_{n1} - y_{n2})(x_{n1} - x_{n2})}{\sum_n(x_{n1} - x_{n2})}</span>
<span id="cb72-1932"><a href="#cb72-1932" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1933"><a href="#cb72-1933" aria-hidden="true" tabindex="-1"></a>which is in this case identical to the within estimator. This result can be established by remarking that the numerator of @eq-within_est is a sum of $N$ terms of the form: </span>
<span id="cb72-1934"><a href="#cb72-1934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1935"><a href="#cb72-1935" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1936"><a href="#cb72-1936" aria-hidden="true" tabindex="-1"></a>(y_{n1} - \bar{y}_{n.})(x_{n1} - \bar{x}_{n.})+(y_{n2} - \bar{y}_{n.})(x_{n2} - \bar{x}_{n.})</span>
<span id="cb72-1937"><a href="#cb72-1937" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-1938"><a href="#cb72-1938" aria-hidden="true" tabindex="-1"></a>But $z_{n1} - \bar{z}_{n.} = z_{n1} - \frac{1}{2}(z_{n1} + z_{n2}) = \frac{1}{2}(z_{n1} - z_{n2})$. Similarly, $z_{n2} - \bar{z}_{n.} = -\frac{1}{2}(z_{n1} - z_{n2})$. Therefore, each term of the numerator of @eq-within_est reduces to $\frac{1}{2} (y_{n1} - y_{n2})(x_{n1} - x_{n2})$ and similarly, each term of the denominator reduces to $\frac{1}{2}(x_{n1} - x_{n2}) ^ 2$ which proves the equivalence between the first-difference and the within estimators. When $T &gt; 2$ the within and the first-difference estimators differ:</span>
<span id="cb72-1939"><a href="#cb72-1939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1940"><a href="#cb72-1940" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the within estimator is more efficient, as it uses all the observations; on the contrary, while performing the first difference, the first observation for every individual is lost,</span>
<span id="cb72-1941"><a href="#cb72-1941" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the error of the within estimator is $\nu_{nt} - \bar{\nu}_{n.}$ and therefore contains the whole series of $\nu_n$; on the contrary, the error of the first difference is $\nu_{nt} - \nu_{n(t-1)}$ and therefore contains the values of $\nu_n$ only for the current and the previous period.</span>
<span id="cb72-1942"><a href="#cb72-1942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1943"><a href="#cb72-1943" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{fixed effects model|)}</span>
<span id="cb72-1944"><a href="#cb72-1944" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{panel data!fixed effects model|)}</span>
<span id="cb72-1945"><a href="#cb72-1945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1946"><a href="#cb72-1946" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application: Mincer earning function using a sample of twins</span></span>
<span id="cb72-1947"><a href="#cb72-1947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1948"><a href="#cb72-1948" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb72-1949"><a href="#cb72-1949" aria-hidden="true" tabindex="-1"></a>In @sec-error_component_gls, we've estimated a Mincer earning function using the <span class="in">`twins`</span> data set.</span>
<span id="cb72-1950"><a href="#cb72-1950" aria-hidden="true" tabindex="-1"></a>\idxfun{mutate}{dplyr}\idxfun{lm}{stats}\idxfun{gaze}{micsr}</span>
<span id="cb72-1951"><a href="#cb72-1951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1952"><a href="#cb72-1952" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1953"><a href="#cb72-1953" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: twins_bonjour</span></span>
<span id="cb72-1954"><a href="#cb72-1954" aria-hidden="true" tabindex="-1"></a>twins <span class="ot">&lt;-</span> twins <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">age2 =</span> age <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> <span class="dv">100</span>)</span>
<span id="cb72-1955"><a href="#cb72-1955" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> educ <span class="sc">+</span> age <span class="sc">+</span> age2, twins) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-1956"><a href="#cb72-1956" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1957"><a href="#cb72-1957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1958"><a href="#cb72-1958" aria-hidden="true" tabindex="-1"></a>The estimated return of education is about 7.7%, but it may be biased if</span>
<span id="cb72-1959"><a href="#cb72-1959" aria-hidden="true" tabindex="-1"></a>the error is correlated with education. In particular, "abilities" are</span>
<span id="cb72-1960"><a href="#cb72-1960" aria-hidden="true" tabindex="-1"></a>unobserved and may be correlated with education. If this correlation</span>
<span id="cb72-1961"><a href="#cb72-1961" aria-hidden="true" tabindex="-1"></a>is positive, then the OLS estimator is upward-biased. The solution</span>
<span id="cb72-1962"><a href="#cb72-1962" aria-hidden="true" tabindex="-1"></a>here is to consider that abilities should be similar for the pair of twins,</span>
<span id="cb72-1963"><a href="#cb72-1963" aria-hidden="true" tabindex="-1"></a>as they are genetically identical and had the same familial</span>
<span id="cb72-1964"><a href="#cb72-1964" aria-hidden="true" tabindex="-1"></a>environment. In this case, the fixed effects model can be performed. It</span>
<span id="cb72-1965"><a href="#cb72-1965" aria-hidden="true" tabindex="-1"></a>can be obtained either by: estimating coefficients for all family dummies, using OLS on the within transformed variables or using OLS on the differences.</span>
<span id="cb72-1966"><a href="#cb72-1966" aria-hidden="true" tabindex="-1"></a>Note that identical twins have obviously the same age, so this</span>
<span id="cb72-1967"><a href="#cb72-1967" aria-hidden="true" tabindex="-1"></a>covariate disappears in the fixed effects model. Let's start with the</span>
<span id="cb72-1968"><a href="#cb72-1968" aria-hidden="true" tabindex="-1"></a>LSDV estimator:</span>
<span id="cb72-1969"><a href="#cb72-1969" aria-hidden="true" tabindex="-1"></a>\idxfun{lm}{stats}\idxfun{coef}{stats}\idxfun{head}{utils}\idxfun{factor}{base}</span>
<span id="cb72-1970"><a href="#cb72-1970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1971"><a href="#cb72-1971" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1972"><a href="#cb72-1972" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lsdv_twins</span></span>
<span id="cb72-1973"><a href="#cb72-1973" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1974"><a href="#cb72-1974" aria-hidden="true" tabindex="-1"></a>lsdv <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(earning) <span class="sc">~</span> educ <span class="sc">+</span> <span class="fu">factor</span>(family), twins)</span>
<span id="cb72-1975"><a href="#cb72-1975" aria-hidden="true" tabindex="-1"></a>lsdv <span class="sc">%&gt;%</span> coef <span class="sc">%&gt;%</span> <span class="fu">head</span>(<span class="dv">4</span>)</span>
<span id="cb72-1976"><a href="#cb72-1976" aria-hidden="true" tabindex="-1"></a>lsdv <span class="sc">%&gt;%</span> coef <span class="sc">%&gt;%</span> length</span>
<span id="cb72-1977"><a href="#cb72-1977" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1978"><a href="#cb72-1978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1979"><a href="#cb72-1979" aria-hidden="true" tabindex="-1"></a>The estimated return of education is much lower (3.9%) than the</span>
<span id="cb72-1980"><a href="#cb72-1980" aria-hidden="true" tabindex="-1"></a>OLS estimator. Note that numerous parameters are estimated (215),</span>
<span id="cb72-1981"><a href="#cb72-1981" aria-hidden="true" tabindex="-1"></a>and the computation can be unfeasible if the number of entities is very</span>
<span id="cb72-1982"><a href="#cb72-1982" aria-hidden="true" tabindex="-1"></a>large. Whatever the size of the sample, it is simpler and more efficient to use OLS on the</span>
<span id="cb72-1983"><a href="#cb72-1983" aria-hidden="true" tabindex="-1"></a>transformed data, either using the within or the first-difference</span>
<span id="cb72-1984"><a href="#cb72-1984" aria-hidden="true" tabindex="-1"></a>estimator. This can easily be done using:</span>
<span id="cb72-1985"><a href="#cb72-1985" aria-hidden="true" tabindex="-1"></a>\idxfun{group<span class="sc">\_</span>by}{dplyr}\idxfun{mutate}{dplyr}\idxfun{lm}{stats}</span>
<span id="cb72-1986"><a href="#cb72-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1987"><a href="#cb72-1987" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-1988"><a href="#cb72-1988" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: within_twins_manual </span></span>
<span id="cb72-1989"><a href="#cb72-1989" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-1990"><a href="#cb72-1990" aria-hidden="true" tabindex="-1"></a>twins <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(family) <span class="sc">%&gt;%</span></span>
<span id="cb72-1991"><a href="#cb72-1991" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">learning =</span> <span class="fu">log</span>(earning) <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">log</span>(earning)),</span>
<span id="cb72-1992"><a href="#cb72-1992" aria-hidden="true" tabindex="-1"></a>           <span class="at">educ =</span> educ <span class="sc">-</span> <span class="fu">mean</span>(educ)) <span class="sc">%&gt;%</span></span>
<span id="cb72-1993"><a href="#cb72-1993" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lm</span>(<span class="at">formula =</span> learning <span class="sc">~</span> educ <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-1994"><a href="#cb72-1994" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-1995"><a href="#cb72-1995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1996"><a href="#cb72-1996" aria-hidden="true" tabindex="-1"></a>or:</span>
<span id="cb72-1997"><a href="#cb72-1997" aria-hidden="true" tabindex="-1"></a>\idxfun{summarise}{dplyr}\idxfun{group<span class="sc">\_</span>by}{dplyr}\idxfun{diff}{base}\idxfun{lm}{stats}\idxfun{coef}{stats}</span>
<span id="cb72-1998"><a href="#cb72-1998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-1999"><a href="#cb72-1999" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2000"><a href="#cb72-2000" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: first_diff_twins_manual</span></span>
<span id="cb72-2001"><a href="#cb72-2001" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-2002"><a href="#cb72-2002" aria-hidden="true" tabindex="-1"></a>twins <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(family) <span class="sc">%&gt;%</span></span>
<span id="cb72-2003"><a href="#cb72-2003" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">learning =</span> <span class="fu">diff</span>(<span class="fu">log</span>(earning)),</span>
<span id="cb72-2004"><a href="#cb72-2004" aria-hidden="true" tabindex="-1"></a>              <span class="at">educ =</span> <span class="fu">diff</span>(educ)) <span class="sc">%&gt;%</span></span>
<span id="cb72-2005"><a href="#cb72-2005" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lm</span>(<span class="at">formula =</span> learning <span class="sc">~</span> educ <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> coef</span>
<span id="cb72-2006"><a href="#cb72-2006" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2007"><a href="#cb72-2007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2008"><a href="#cb72-2008" aria-hidden="true" tabindex="-1"></a>In both cases, we grouped the rows by family, i.e., there are 214 groups</span>
<span id="cb72-2009"><a href="#cb72-2009" aria-hidden="true" tabindex="-1"></a>of two lines, one for each pair of twins. In the first case, we use</span>
<span id="cb72-2010"><a href="#cb72-2010" aria-hidden="true" tabindex="-1"></a><span class="in">`mutate`</span> to remove the individual mean; for example, <span class="in">`mean(educ)`</span> is</span>
<span id="cb72-2011"><a href="#cb72-2011" aria-hidden="true" tabindex="-1"></a>the mean of education computed for every family and is repeated two</span>
<span id="cb72-2012"><a href="#cb72-2012" aria-hidden="true" tabindex="-1"></a>times, and we therefore have 428 observations (one for each</span>
<span id="cb72-2013"><a href="#cb72-2013" aria-hidden="true" tabindex="-1"></a>individual). In the second case, we use <span class="in">`summarise`</span> and we therefore</span>
<span id="cb72-2014"><a href="#cb72-2014" aria-hidden="true" tabindex="-1"></a>get 214 observations (one for each family), the response and the</span>
<span id="cb72-2015"><a href="#cb72-2015" aria-hidden="true" tabindex="-1"></a>covariate being the twin difference of earning and education.</span>
<span id="cb72-2016"><a href="#cb72-2016" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{twins}{micsr.data}</span>
<span id="cb72-2017"><a href="#cb72-2017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2018"><a href="#cb72-2018" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application: Testing Tobin's Q theory of investment using panel data</span></span>
<span id="cb72-2019"><a href="#cb72-2019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2020"><a href="#cb72-2020" aria-hidden="true" tabindex="-1"></a>@SCHA:90\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Schaller} tested the relevance of Tobin's Q theory of investment by</span>
<span id="cb72-2021"><a href="#cb72-2021" aria-hidden="true" tabindex="-1"></a>regressing the investment rate (the ratio of the investment and the</span>
<span id="cb72-2022"><a href="#cb72-2022" aria-hidden="true" tabindex="-1"></a>stock of capital) to Tobin's Q, which is the ratio of the value of the</span>
<span id="cb72-2023"><a href="#cb72-2023" aria-hidden="true" tabindex="-1"></a>firm and the stock of capital. This data set has already been presented</span>
<span id="cb72-2024"><a href="#cb72-2024" aria-hidden="true" tabindex="-1"></a>in @sec-error_component_gls, where we've estimated by GLS an investment equation using the Tobin's Q theory of investment. We consider now the estimation of a fixed effects model. It is obtained using <span class="in">`plm`</span> (of the **plm** package)</span>
<span id="cb72-2025"><a href="#cb72-2025" aria-hidden="true" tabindex="-1"></a>by setting the <span class="in">`model`</span> argument to <span class="in">`"within"`</span> (which is actually the</span>
<span id="cb72-2026"><a href="#cb72-2026" aria-hidden="true" tabindex="-1"></a>default value). </span>
<span id="cb72-2027"><a href="#cb72-2027" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb72-2028"><a href="#cb72-2028" aria-hidden="true" tabindex="-1"></a>\idxfun{plm}{plm}</span>
<span id="cb72-2029"><a href="#cb72-2029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2030"><a href="#cb72-2030" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2031"><a href="#cb72-2031" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tobinq_within</span></span>
<span id="cb72-2032"><a href="#cb72-2032" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb72-2033"><a href="#cb72-2033" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb72-2034"><a href="#cb72-2034" aria-hidden="true" tabindex="-1"></a>qw <span class="ot">&lt;-</span> <span class="fu">plm</span>(ikn <span class="sc">~</span> qn, tobinq)</span>
<span id="cb72-2035"><a href="#cb72-2035" aria-hidden="true" tabindex="-1"></a>qw <span class="sc">%&gt;%</span> summary</span>
<span id="cb72-2036"><a href="#cb72-2036" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2037"><a href="#cb72-2037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2038"><a href="#cb72-2038" aria-hidden="true" tabindex="-1"></a>The individual effects are not estimated because the estimator</span>
<span id="cb72-2039"><a href="#cb72-2039" aria-hidden="true" tabindex="-1"></a>uses the within transformation and then performs OLS on transformed</span>
<span id="cb72-2040"><a href="#cb72-2040" aria-hidden="true" tabindex="-1"></a>data. However, the individual effects can be computed easily because:</span>
<span id="cb72-2041"><a href="#cb72-2041" aria-hidden="true" tabindex="-1"></a>$\hat{\eta}_n = \bar{y}_{n.} - \hat{\beta} ^ \top \bar{x}_{n.}$. The</span>
<span id="cb72-2042"><a href="#cb72-2042" aria-hidden="true" tabindex="-1"></a><span class="in">`fixef`</span> method for <span class="in">`plm`</span> objects retrieves the fixed effects and a</span>
<span id="cb72-2043"><a href="#cb72-2043" aria-hidden="true" tabindex="-1"></a><span class="in">`type`</span> argument can be set to:</span>
<span id="cb72-2044"><a href="#cb72-2044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2045"><a href="#cb72-2045" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`"level"`</span>: the effects are then the same as those obtained</span>
<span id="cb72-2046"><a href="#cb72-2046" aria-hidden="true" tabindex="-1"></a>  by LSDV without intercept,</span>
<span id="cb72-2047"><a href="#cb72-2047" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`"dfirst"`</span>: only $N - 1$ effects are estimated, the first one being</span>
<span id="cb72-2048"><a href="#cb72-2048" aria-hidden="true" tabindex="-1"></a>  set to 0; these are the effects obtained by LSDV with an</span>
<span id="cb72-2049"><a href="#cb72-2049" aria-hidden="true" tabindex="-1"></a>  intercept,</span>
<span id="cb72-2050"><a href="#cb72-2050" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`"dmean"`</span>: $N$ effects and an overall intercept are estimated, but</span>
<span id="cb72-2051"><a href="#cb72-2051" aria-hidden="true" tabindex="-1"></a>  the $N$ effects have a 0 mean.</span>
<span id="cb72-2052"><a href="#cb72-2052" aria-hidden="true" tabindex="-1"></a>\idxfun{fixef}{plm}\idxfun{head}{utils}</span>
<span id="cb72-2053"><a href="#cb72-2053" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-2054"><a href="#cb72-2054" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2055"><a href="#cb72-2055" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fixef_tobinq</span></span>
<span id="cb72-2056"><a href="#cb72-2056" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-2057"><a href="#cb72-2057" aria-hidden="true" tabindex="-1"></a>qw <span class="sc">%&gt;%</span> <span class="fu">fixef</span>(<span class="at">type =</span> <span class="st">"level"</span>) <span class="sc">%&gt;%</span> head</span>
<span id="cb72-2058"><a href="#cb72-2058" aria-hidden="true" tabindex="-1"></a>qw <span class="sc">%&gt;%</span> <span class="fu">fixef</span>(<span class="at">type =</span> <span class="st">"dfirst"</span>) <span class="sc">%&gt;%</span> head</span>
<span id="cb72-2059"><a href="#cb72-2059" aria-hidden="true" tabindex="-1"></a>qw <span class="sc">%&gt;%</span> <span class="fu">fixef</span>(<span class="at">type =</span> <span class="st">"dmean"</span>) <span class="sc">%&gt;%</span> head</span>
<span id="cb72-2060"><a href="#cb72-2060" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2061"><a href="#cb72-2061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2062"><a href="#cb72-2062" aria-hidden="true" tabindex="-1"></a>There is a <span class="in">`summary`</span> method that reports the usual table of</span>
<span id="cb72-2063"><a href="#cb72-2063" aria-hidden="true" tabindex="-1"></a>coefficients for the effects:</span>
<span id="cb72-2064"><a href="#cb72-2064" aria-hidden="true" tabindex="-1"></a>\idxfun{fixef}{plm}\idxfun{head}{utils}</span>
<span id="cb72-2065"><a href="#cb72-2065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2066"><a href="#cb72-2066" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2067"><a href="#cb72-2067" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: summary_fixef_tobinq</span></span>
<span id="cb72-2068"><a href="#cb72-2068" aria-hidden="true" tabindex="-1"></a>qw <span class="sc">%&gt;%</span> <span class="fu">fixef</span>(<span class="at">type =</span> <span class="st">"dmean"</span>) <span class="sc">%&gt;%</span> summary <span class="sc">%&gt;%</span> <span class="fu">head</span>(<span class="dv">3</span>)</span>
<span id="cb72-2069"><a href="#cb72-2069" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2070"><a href="#cb72-2070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2071"><a href="#cb72-2071" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb72-2072"><a href="#cb72-2072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2073"><a href="#cb72-2073" aria-hidden="true" tabindex="-1"></a><span class="fu">## Specification tests {#sec-tests_endog}</span></span>
<span id="cb72-2074"><a href="#cb72-2074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2075"><a href="#cb72-2075" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hausman test</span></span>
<span id="cb72-2076"><a href="#cb72-2076" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Hausman test|(} </span>
<span id="cb72-2077"><a href="#cb72-2077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2078"><a href="#cb72-2078" aria-hidden="true" tabindex="-1"></a>Models estimated in this chapter, either using the IV or the fixed effects</span>
<span id="cb72-2079"><a href="#cb72-2079" aria-hidden="true" tabindex="-1"></a>estimator, treat the endogeneity of some covariates, either by using</span>
<span id="cb72-2080"><a href="#cb72-2080" aria-hidden="true" tabindex="-1"></a>instruments or by removing individual effects. These estimates are</span>
<span id="cb72-2081"><a href="#cb72-2081" aria-hidden="true" tabindex="-1"></a>consistent if some covariates are actually endogenous, although other</span>
<span id="cb72-2082"><a href="#cb72-2082" aria-hidden="true" tabindex="-1"></a>estimators like OLS or GLS are not. However, if endogeneity is</span>
<span id="cb72-2083"><a href="#cb72-2083" aria-hidden="true" tabindex="-1"></a>actually not a problem, these later estimators are also consistent and</span>
<span id="cb72-2084"><a href="#cb72-2084" aria-hidden="true" tabindex="-1"></a>are moreover more efficient. The Hausman test is based on the</span>
<span id="cb72-2085"><a href="#cb72-2085" aria-hidden="true" tabindex="-1"></a>comparison of these two estimators and on the null hypothesis of the absence of endogeneity:</span>
<span id="cb72-2086"><a href="#cb72-2086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2087"><a href="#cb72-2087" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\beta}_0$, with variance $\hat{V}_0$ is only consistent if the hypothesis is</span>
<span id="cb72-2088"><a href="#cb72-2088" aria-hidden="true" tabindex="-1"></a>true and is in this case efficient,</span>
<span id="cb72-2089"><a href="#cb72-2089" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\beta}_1$, with variance $\hat{V}_1$ is always consistent, but</span>
<span id="cb72-2090"><a href="#cb72-2090" aria-hidden="true" tabindex="-1"></a>  is less efficient than $\hat{\beta}_0$ if the hypothesis is true.</span>
<span id="cb72-2091"><a href="#cb72-2091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2092"><a href="#cb72-2092" aria-hidden="true" tabindex="-1"></a>Hausman's test <span class="co">[</span><span class="ot">@HAUS:78</span><span class="co">]</span>\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Hausman} test is based on $\hat{q} = \hat{\beta}_1 - \hat{\beta}_0$,</span>
<span id="cb72-2093"><a href="#cb72-2093" aria-hidden="true" tabindex="-1"></a>for which the variance is: </span>
<span id="cb72-2094"><a href="#cb72-2094" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2095"><a href="#cb72-2095" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{q}) =</span>
<span id="cb72-2096"><a href="#cb72-2096" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}_1) + \mbox{V}(\hat{\beta}_0) - 2</span>
<span id="cb72-2097"><a href="#cb72-2097" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(\hat{\beta}_1, \hat{\beta}_0)</span>
<span id="cb72-2098"><a href="#cb72-2098" aria-hidden="true" tabindex="-1"></a>$$ {#eq-var_difference}</span>
<span id="cb72-2099"><a href="#cb72-2099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2100"><a href="#cb72-2100" aria-hidden="true" tabindex="-1"></a>Hausman showed that the covariance between $\hat{q}$ and</span>
<span id="cb72-2101"><a href="#cb72-2101" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_0$ is 0. Therefore:</span>
<span id="cb72-2102"><a href="#cb72-2102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2103"><a href="#cb72-2103" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb72-2104"><a href="#cb72-2104" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(\hat{\beta}_1 - \hat{\beta}_0, \hat{\beta}_0) =</span>
<span id="cb72-2105"><a href="#cb72-2105" aria-hidden="true" tabindex="-1"></a>\mbox{cov}(\hat{\beta}_1, \hat{\beta}_0) + \mbox{V}(\hat{\beta}_0) = 0</span>
<span id="cb72-2106"><a href="#cb72-2106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2107"><a href="#cb72-2107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2108"><a href="#cb72-2108" aria-hidden="true" tabindex="-1"></a>and therefore @eq-var_difference simplifies to: $\mbox{V}(\hat{q}) =</span>
<span id="cb72-2109"><a href="#cb72-2109" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}_1) - \mbox{V}(\hat{\beta}_0)$. The asymptotic</span>
<span id="cb72-2110"><a href="#cb72-2110" aria-hidden="true" tabindex="-1"></a>distribution of the difference of the two vectors of estimates is,</span>
<span id="cb72-2111"><a href="#cb72-2111" aria-hidden="true" tabindex="-1"></a>under H~0~:</span>
<span id="cb72-2112"><a href="#cb72-2112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2113"><a href="#cb72-2113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2114"><a href="#cb72-2114" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1 - \hat{\beta}_0 \overset{a}{\sim} \mathcal{N} \left(0, \mbox{V}(\hat{\beta}_1) - \mbox{V}(\hat{\beta}_0)\right)</span>
<span id="cb72-2115"><a href="#cb72-2115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2116"><a href="#cb72-2116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2117"><a href="#cb72-2117" aria-hidden="true" tabindex="-1"></a>and therefore $(\hat{\beta}_1 - \hat{\beta}_0) ^ \top \left(\mbox{V}(\hat{\beta}_1) -</span>
<span id="cb72-2118"><a href="#cb72-2118" aria-hidden="true" tabindex="-1"></a>\mbox{V}(\hat{\beta}_0)\right) ^ {-1} (\hat{\beta}_1 - \hat{\beta}_0)$</span>
<span id="cb72-2119"><a href="#cb72-2119" aria-hidden="true" tabindex="-1"></a>is a $\chi ^ 2$ with degrees of freedom equal to the number of</span>
<span id="cb72-2120"><a href="#cb72-2120" aria-hidden="true" tabindex="-1"></a>estimated parameters. Note that the length of $\hat{\beta}_0$ and</span>
<span id="cb72-2121"><a href="#cb72-2121" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_1$ may be different. This is the case for panel data with</span>
<span id="cb72-2122"><a href="#cb72-2122" aria-hidden="true" tabindex="-1"></a>individual specific covariates which are estimated using OLS or</span>
<span id="cb72-2123"><a href="#cb72-2123" aria-hidden="true" tabindex="-1"></a>GLS but which are not with the within estimator. In this case, the</span>
<span id="cb72-2124"><a href="#cb72-2124" aria-hidden="true" tabindex="-1"></a>test should be performed on the subset of common parameters. For tests</span>
<span id="cb72-2125"><a href="#cb72-2125" aria-hidden="true" tabindex="-1"></a>involving OLS and IV estimator, the test can also be performed</span>
<span id="cb72-2126"><a href="#cb72-2126" aria-hidden="true" tabindex="-1"></a>only on the subset of parameters associated with covariates that are</span>
<span id="cb72-2127"><a href="#cb72-2127" aria-hidden="true" tabindex="-1"></a>suspected to be endogenous.</span>
<span id="cb72-2128"><a href="#cb72-2128" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Hausman test|)} </span>
<span id="cb72-2129"><a href="#cb72-2129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2130"><a href="#cb72-2130" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weak instruments</span></span>
<span id="cb72-2131"><a href="#cb72-2131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2132"><a href="#cb72-2132" aria-hidden="true" tabindex="-1"></a>Instruments should be not only uncorrelated with the error of the</span>
<span id="cb72-2133"><a href="#cb72-2133" aria-hidden="true" tabindex="-1"></a>model, but they should also be correlated with the covariates. We have</span>
<span id="cb72-2134"><a href="#cb72-2134" aria-hidden="true" tabindex="-1"></a>seen in @sec-ssprop_iv that the expected value of the IV estimator</span>
<span id="cb72-2135"><a href="#cb72-2135" aria-hidden="true" tabindex="-1"></a>doesn't exist when there is only one instrument. It exists if the</span>
<span id="cb72-2136"><a href="#cb72-2136" aria-hidden="true" tabindex="-1"></a>number of instruments is at least 2 and in this case, it can be shown</span>
<span id="cb72-2137"><a href="#cb72-2137" aria-hidden="true" tabindex="-1"></a>that the bias of the IV estimator is approximately inversely</span>
<span id="cb72-2138"><a href="#cb72-2138" aria-hidden="true" tabindex="-1"></a>proportional to the F statistic of the regression of the endogenous</span>
<span id="cb72-2139"><a href="#cb72-2139" aria-hidden="true" tabindex="-1"></a>variable on the instruments.^<span class="co">[</span><span class="ot">See @BOUN:JAEG:BAKE:95\index[author</span><span class="co">]</span>{Bound}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Jaeger}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Baker} and @CAME:TRIV:05\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Cameron}\index<span class="co">[</span><span class="ot">author</span><span class="co">]</span>{Trivedi} pages 108-109 for a</span>
<span id="cb72-2140"><a href="#cb72-2140" aria-hidden="true" tabindex="-1"></a>discussion and further references.] Therefore, if the correlation between the</span>
<span id="cb72-2141"><a href="#cb72-2141" aria-hidden="true" tabindex="-1"></a>endogenous variable and the instruments is weak, i.e., if the IV is</span>
<span id="cb72-2142"><a href="#cb72-2142" aria-hidden="true" tabindex="-1"></a>performed using **weak instruments**, the estimator will not only be</span>
<span id="cb72-2143"><a href="#cb72-2143" aria-hidden="true" tabindex="-1"></a>highly imprecise, but it will also be seriously biased, in</span>
<span id="cb72-2144"><a href="#cb72-2144" aria-hidden="true" tabindex="-1"></a>the direction of the OLS estimator. While performing IV</span>
<span id="cb72-2145"><a href="#cb72-2145" aria-hidden="true" tabindex="-1"></a>estimation it is therefore important to check that the instruments are</span>
<span id="cb72-2146"><a href="#cb72-2146" aria-hidden="true" tabindex="-1"></a>sufficiently correlated with the endogenous covariate. This can be</span>
<span id="cb72-2147"><a href="#cb72-2147" aria-hidden="true" tabindex="-1"></a>performed using an F test for the first stage regression, comparing</span>
<span id="cb72-2148"><a href="#cb72-2148" aria-hidden="true" tabindex="-1"></a>the fit for the regression of the endogenous covariates on the set of the exogenous covariates, on the set of the exogenous covariates, and on the external instruments.</span>
<span id="cb72-2149"><a href="#cb72-2149" aria-hidden="true" tabindex="-1"></a>A rule of thumb often used is that the F statistic should be at least</span>
<span id="cb72-2150"><a href="#cb72-2150" aria-hidden="true" tabindex="-1"></a>equal to 10 (a less strict rule is $F &gt; 5$).</span>
<span id="cb72-2151"><a href="#cb72-2151" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- to unsure that the maximal bias of the IV estimator is no --&gt;</span></span>
<span id="cb72-2152"><a href="#cb72-2152" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- more than 10% that of OLS.  --&gt;</span></span>
<span id="cb72-2153"><a href="#cb72-2153" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{weak instruments test}</span>
<span id="cb72-2154"><a href="#cb72-2154" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{F statistic!weak instruments}</span>
<span id="cb72-2155"><a href="#cb72-2155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2156"><a href="#cb72-2156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2157"><a href="#cb72-2157" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sargan test</span></span>
<span id="cb72-2158"><a href="#cb72-2158" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{Sargan test}</span>
<span id="cb72-2159"><a href="#cb72-2159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2160"><a href="#cb72-2160" aria-hidden="true" tabindex="-1"></a>The IV can be obtained as a moment estimator, using moment</span>
<span id="cb72-2161"><a href="#cb72-2161" aria-hidden="true" tabindex="-1"></a>conditions $m = W ^ \top \epsilon / N$. If the instruments are relevant,</span>
<span id="cb72-2162"><a href="#cb72-2162" aria-hidden="true" tabindex="-1"></a>they are uncorrelated with the errors, so that $m \overset{a}{\sim}</span>
<span id="cb72-2163"><a href="#cb72-2163" aria-hidden="true" tabindex="-1"></a>\mathcal{N}(0, V)$ and $S = m ^ \top V ^ {-1} m$  is a $\chi^2$ with a</span>
<span id="cb72-2164"><a href="#cb72-2164" aria-hidden="true" tabindex="-1"></a>degree of freedom equal to the number of instruments. In the</span>
<span id="cb72-2165"><a href="#cb72-2165" aria-hidden="true" tabindex="-1"></a>just-identified case, $m=0$, but in the over-identified case, this</span>
<span id="cb72-2166"><a href="#cb72-2166" aria-hidden="true" tabindex="-1"></a>statistic is positive and the hypothesis of orthogonal instruments</span>
<span id="cb72-2167"><a href="#cb72-2167" aria-hidden="true" tabindex="-1"></a>implies that $m$ is close enough to a vector of 0 or that $S$ is</span>
<span id="cb72-2168"><a href="#cb72-2168" aria-hidden="true" tabindex="-1"></a>sufficiently small.</span>
<span id="cb72-2169"><a href="#cb72-2169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2170"><a href="#cb72-2170" aria-hidden="true" tabindex="-1"></a><span class="fu">### Individual effects</span></span>
<span id="cb72-2171"><a href="#cb72-2171" aria-hidden="true" tabindex="-1"></a>\index<span class="co">[</span><span class="ot">general</span><span class="co">]</span>{F statistic!individual effects}</span>
<span id="cb72-2172"><a href="#cb72-2172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2173"><a href="#cb72-2173" aria-hidden="true" tabindex="-1"></a>In the fixed effects model, the absence of individual effects can be</span>
<span id="cb72-2174"><a href="#cb72-2174" aria-hidden="true" tabindex="-1"></a>tested using a standard $F$ test that all the estimated individual</span>
<span id="cb72-2175"><a href="#cb72-2175" aria-hidden="true" tabindex="-1"></a>effects are zero. More simply, it can be obtained by comparing the</span>
<span id="cb72-2176"><a href="#cb72-2176" aria-hidden="true" tabindex="-1"></a>sum of squares residuals of the OLS and the fixed effects models.</span>
<span id="cb72-2177"><a href="#cb72-2177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2178"><a href="#cb72-2178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2179"><a href="#cb72-2179" aria-hidden="true" tabindex="-1"></a>\frac{SCR_{\mbox{OLS}} - SCR_{\mbox{FE}}}{SCR_{\mbox{FE}}} \times \frac{N (T - 1) - K }{N -</span>
<span id="cb72-2180"><a href="#cb72-2180" aria-hidden="true" tabindex="-1"></a>1} \sim F_{N-1, N (T - 1) - K}</span>
<span id="cb72-2181"><a href="#cb72-2181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb72-2182"><a href="#cb72-2182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2183"><a href="#cb72-2183" aria-hidden="true" tabindex="-1"></a><span class="fu">### Panel application: Testing Tobin's Q theory of investment</span></span>
<span id="cb72-2184"><a href="#cb72-2184" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb72-2185"><a href="#cb72-2185" aria-hidden="true" tabindex="-1"></a>The presence of individual effects was already tested using a score</span>
<span id="cb72-2186"><a href="#cb72-2186" aria-hidden="true" tabindex="-1"></a>test based on the OLS residuals. We compute here an F test, using <span class="in">`plm::pFtest`</span>:</span>
<span id="cb72-2187"><a href="#cb72-2187" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb72-2188"><a href="#cb72-2188" aria-hidden="true" tabindex="-1"></a>\idxfun{pFtest}{plm}\idxfun{gaze}{micsr}</span>
<span id="cb72-2189"><a href="#cb72-2189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2190"><a href="#cb72-2190" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2191"><a href="#cb72-2191" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: pFtest_tobinq</span></span>
<span id="cb72-2192"><a href="#cb72-2192" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-2193"><a href="#cb72-2193" aria-hidden="true" tabindex="-1"></a><span class="fu">pFtest</span>(ikn <span class="sc">~</span> qn, tobinq) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-2194"><a href="#cb72-2194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2195"><a href="#cb72-2195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2196"><a href="#cb72-2196" aria-hidden="true" tabindex="-1"></a>and the hypothesis is strongly rejected. Once we have concluded that there are individual effects, the question is whether these effects</span>
<span id="cb72-2197"><a href="#cb72-2197" aria-hidden="true" tabindex="-1"></a>are correlated with the covariates or not. If this is the case, the fixed</span>
<span id="cb72-2198"><a href="#cb72-2198" aria-hidden="true" tabindex="-1"></a>effects model should be used because the GLS model is</span>
<span id="cb72-2199"><a href="#cb72-2199" aria-hidden="true" tabindex="-1"></a>inconsistent. On the contrary, both models are consistent, and the</span>
<span id="cb72-2200"><a href="#cb72-2200" aria-hidden="true" tabindex="-1"></a>GLS estimator, which is more efficient, should be used.</span>
<span id="cb72-2201"><a href="#cb72-2201" aria-hidden="true" tabindex="-1"></a>\idxfun{phtest}{plm}\idxfun{gaze}{micsr}</span>
<span id="cb72-2202"><a href="#cb72-2202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2203"><a href="#cb72-2203" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2204"><a href="#cb72-2204" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: phtest_tobinq</span></span>
<span id="cb72-2205"><a href="#cb72-2205" aria-hidden="true" tabindex="-1"></a><span class="co">#| collapse: true</span></span>
<span id="cb72-2206"><a href="#cb72-2206" aria-hidden="true" tabindex="-1"></a><span class="fu">phtest</span>(ikn <span class="sc">~</span> qn, tobinq) <span class="sc">%&gt;%</span> gaze</span>
<span id="cb72-2207"><a href="#cb72-2207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2208"><a href="#cb72-2208" aria-hidden="true" tabindex="-1"></a>The hypothesis of uncorrelated individual effects is not rejected at</span>
<span id="cb72-2209"><a href="#cb72-2209" aria-hidden="true" tabindex="-1"></a>the 5%, which leads to the choice of the GLS estimator.</span>
<span id="cb72-2210"><a href="#cb72-2210" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{tobinq}{micsr.data}</span>
<span id="cb72-2211"><a href="#cb72-2211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2212"><a href="#cb72-2212" aria-hidden="true" tabindex="-1"></a><span class="fu">### Instrumental variable application: slave trade {#sec-test_sltr}</span></span>
<span id="cb72-2213"><a href="#cb72-2213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2214"><a href="#cb72-2214" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">(</span><span class="co">]</span>{slave<span class="sc">\_</span>trade}{necountries}</span>
<span id="cb72-2215"><a href="#cb72-2215" aria-hidden="true" tabindex="-1"></a>The IV estimator for the <span class="in">`slave_trade`</span> data set was presented in @sec-test_sltr. We use</span>
<span id="cb72-2216"><a href="#cb72-2216" aria-hidden="true" tabindex="-1"></a>this time the <span class="in">`ivreg::ivreg`</span> functions, which computes all the</span>
<span id="cb72-2217"><a href="#cb72-2217" aria-hidden="true" tabindex="-1"></a>relevant specification tests.</span>
<span id="cb72-2218"><a href="#cb72-2218" aria-hidden="true" tabindex="-1"></a>\idxfun{ivreg}{ivreg}</span>
<span id="cb72-2219"><a href="#cb72-2219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2220"><a href="#cb72-2220" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb72-2221"><a href="#cb72-2221" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ivreg_slave</span></span>
<span id="cb72-2222"><a href="#cb72-2222" aria-hidden="true" tabindex="-1"></a>sltd_iv <span class="ot">&lt;-</span> ivreg<span class="sc">::</span><span class="fu">ivreg</span>(<span class="fu">log</span>(gdp) <span class="sc">~</span> <span class="fu">log</span>(slavesarea) <span class="sc">+</span> colony <span class="sc">|</span> </span>
<span id="cb72-2223"><a href="#cb72-2223" aria-hidden="true" tabindex="-1"></a>                          colony <span class="sc">+</span> redsea <span class="sc">+</span> atlantic <span class="sc">+</span> sahara <span class="sc">+</span> indian, </span>
<span id="cb72-2224"><a href="#cb72-2224" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> sltd)</span>
<span id="cb72-2225"><a href="#cb72-2225" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(sltd_iv)</span>
<span id="cb72-2226"><a href="#cb72-2226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb72-2227"><a href="#cb72-2227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2228"><a href="#cb72-2228" aria-hidden="true" tabindex="-1"></a>The F statistic for the first stage regression is only $4.89$, so that</span>
<span id="cb72-2229"><a href="#cb72-2229" aria-hidden="true" tabindex="-1"></a>the instruments can be considered as weak and we can suspect the IV</span>
<span id="cb72-2230"><a href="#cb72-2230" aria-hidden="true" tabindex="-1"></a>estimator to be severely biased. Anyway, remind that the bias is in</span>
<span id="cb72-2231"><a href="#cb72-2231" aria-hidden="true" tabindex="-1"></a>the direction of the OLS estimator so that the effect of slave trades</span>
<span id="cb72-2232"><a href="#cb72-2232" aria-hidden="true" tabindex="-1"></a>on current GDP would be underestimated. The p-value for the Hausman</span>
<span id="cb72-2233"><a href="#cb72-2233" aria-hidden="true" tabindex="-1"></a>test is $0.03$ so that the exogeneity hypothesis of the covariate is</span>
<span id="cb72-2234"><a href="#cb72-2234" aria-hidden="true" tabindex="-1"></a>rejected at the 5% level, but not at the 1% level. Finally, as there</span>
<span id="cb72-2235"><a href="#cb72-2235" aria-hidden="true" tabindex="-1"></a>are 4 instruments, the Sargan statistic, which is the quadratic form</span>
<span id="cb72-2236"><a href="#cb72-2236" aria-hidden="true" tabindex="-1"></a>of the 4 moment conditions with the inverse of its variance is $3.63$</span>
<span id="cb72-2237"><a href="#cb72-2237" aria-hidden="true" tabindex="-1"></a>and is a $\chi ^ 2$ with 3 degrees of freedom if the instruments are</span>
<span id="cb72-2238"><a href="#cb72-2238" aria-hidden="true" tabindex="-1"></a>valid. This hypothesis is not rejected, even at the 10% level.</span>
<span id="cb72-2239"><a href="#cb72-2239" aria-hidden="true" tabindex="-1"></a>\idxdata<span class="co">[</span><span class="ot">)</span><span class="co">]</span>{slave<span class="sc">\_</span>trade}{necountries}</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>