```{r }
#| include: false
#| label: setup_simple_regression
source("../_commonR.R")
```

<!-- see zeileis Kleiber on structural models -->
<!-- the adoptees data set is quite heavy -->
<!-- introduce the epsilon_O notation ? -->

# Simple linear regression model

## Introduction

In a regression model we consider a variable $y$, called the **response**
or the endogenous or the explained variable and the model seeks to
explain the variations of this variable from one observation to
another. To achieve this goal, a set of other variables $x$ called the
**covariates** or explanatory or exogenous variables are
introduced. These covariates are not only correlated to the response,
but they are assumed to have a causal effect on the response. This
means that a variation of $x$ causes a change on the value of $y$, as
the opposite is not true. For example, wages are correlated with
education levels. More precisely, a variation of education has a
causal effect on wage, which means that wage is the response and
education is a covariate.

### Conditional expectation

Obviously there will be rarely a deterministic relationship between
a response and one or several covariates. As education has a positive
causal effect on wage, there are many other variables that affect
wages: sex, ethnicity, experience, abilities to name a few. Even if a
large set of relevant covariates are observed, some are not (this
is in particular the case for abilities). Therefore, we won't try to
modelize the value of $y$ as a function of covariates $x$ and some
unknown parameters $\gamma$: $y = f(x, \gamma)$ but, more modestly,
the **conditional expectation** of $y$:

$$
\mbox{E}(y | x) = f(x, \gamma)
$$ {#eq-condexp}

Back to our education / wage model, the conditional expectation is the
mean value of the wage in the population for a given value of education. It is therefore
a function of $x$ which takes as many values as there are distinct
values of $x$. 

In this chapter, we discuss the simplest econometric model, which is
the simple linear regression model. This is a **simple** model because
there is only one covariate $x$. This is a **linear** model
because the $f$ function is assumed to be linear. @eq-condexp can then
be rewritten as follow:

$$
\mbox{E}(y | x) = \alpha + \beta x
$$ {#eq-lincondexp}

To estimate the two unknown parameters $\alpha$ and $\beta$, we need a
data set that contains different individuals for which $x$ and $y$ are
observed. Typically, we'll consider a random sample, which is a small
subset of the whole population consisting of a set of individuals
randomly drawn from this population.  To understand what the
hypothesis of the simple linear model implies, we consider two data
sets. The first one is called `birthwt` [@MULL:97] and contains birth weights
(`birthwt` in ounces) of babies and smoking habits of mothers. The
`cigarettes` variable is the number of cigarettes smoked per day
during pregnancy (for about 85% of the mothers, it is equal to 0). We first compute a binary variable for smoking mothers:


```{r }
#| label: smoke_computation
birthwt <- birthwt %>% mutate(smoke = ifelse(cigarettes > 0, 1, 0))
```

and consider a simple linear regression model with `birthwt` as the
response and `smoke` as a covariate. As the covariate take only two
values, so does the conditional expectation. In a linear regression
model, @eq-lincondexp will therefore returns two values, $\alpha$ for
$x = 0$ and $\alpha + \beta$ for $x = 1$. $\alpha$ and $\alpha +
\beta$ are therefore the expected weights for babies for respectively
non-smoking and smoking mothers. Note that, as we have as many
parameters as values of the covariate, the linear hypothesis
can't be violated. Natural estimators of the conditional expectations
are the conditional sample means, ie average birth weights in the
sample for non-smoking and smoking women:

```{r }
#| label: smoke_effect
cond_means <- birthwt %>%
    group_by(smoke) %>%
    summarise(birthwt = mean(birthwt))
cond_means
```

```{r }
#| include: false
#| label: smoke_effect_hiden
alpha <- cond_means[1, 2, drop = TRUE]
alpha_beta <- cond_means[2, 2, drop = TRUE]
beta <- alpha_beta - alpha
```

Therefore, our estimation of $\alpha$ is 
$\hat{\alpha} = `r round(alpha, 1)`$ and the estimation of $\alpha +
\beta$ is $`r round(alpha_beta, 1)`$, so that $\hat{\beta} =
`r round(alpha_beta, 1)` - `r round(alpha, 1)` = `r round(beta, 1)`$
ounces, which is the estimation of birth weight's loss caused by smoking during pregnancy.

This can be illustrated using a scatter plot (see @fig-birthwt), with
`smoke` on the horizontal axis and `birthwt` on the vertical axis, all
the points having only two possible abscissa values, 0 and 1. The conditional means
are represented by red points and we also draw in blue the line that
contains these two points. The intercept is therefore the y-value of
the red point for $x = 0$ which is $\hat{\alpha} = `r round(alpha,
1)`$. The slope of the line is the ratio of the vertical and the
horizontal distances between the two points, which are respectively
$\hat{\beta}$ and $1 - 0 = 1$ and is therefore equal to $\hat{\beta}$.

```{r}
#| label: fig-birthwt
#| fig-cap: "Birth Weight for non-smoking and smoking mothers"
#| echo: false
birthwt %>% ggplot(aes(smoke, birthwt)) + geom_point(size = 0.2) +
  geom_point(data = cond_means, size = 2, color = "red") +
  geom_abline(slope = beta, intercept = alpha, color = "blue") + 
  scale_y_continuous(limits = c(50, 200))
```

Consider now the relationship between education and wage. The
`adoptees` data set [@PLUG:04] contains the number of years of education `educ`
and the annual income `income` (in thousands of US$) for 16481
individuals. We restrict the sample to individuals with an education level
between 12 (high school degree) and 15 years (bachelor degree).

```{r }
#| label: adoptees_filter
adoptees <- adoptees %>%
    filter(educ >= 12, educ <= 15)
```

and we compute the mean income for the 4 values of education:

```{r }
#| adoptees_mean_income
adoptees %>% group_by(educ) %>%
    summarise(income = mean(income))
```

The same figure as previously is represented on @fig-adoptees:

```{r }
#| label: fig-adoptees
#| fig-cap: "Education and income"
#| echo: false
cond_means <- adoptees %>% group_by(educ) %>%
    summarise(income = mean(income))
adoptees %>% ggplot(aes(educ, income)) + geom_point(size = 0.1) +
    geom_point(data = cond_means, size = 3, color = "red") +
    scale_y_continuous(limits = c(5E01, 1E02))
```

This time, we have 4 distinct values of the covariate and we can estimate 4 conditional means in the sample, as the there are two parameters to estimate in the simple linear model. Therefore, we can't estimate directly $\alpha$ and $\beta$ using the conditional means, except in the improbable case where the 4 conditional means lie on a straight line, which means that, for one additional year of education, the wage increases exactly by the same amount. We can see on @fig-adoptees that it is not the case: the increase of income is `r round(diff(cond_means$income[1:2]), 1)` for a 13^th^year of education, `r round(diff(cond_means$income[2:3]), 1)` for a 14^th^ year and `r round(diff(cond_means$income[3:4]), 1)` for a 15^th^. We therefore need a formal method of estimation which enable us to obtain values of $\hat{\alpha}$ and $\hat{\beta}$. The remaining of this chapter is devoted to the presentation of this estimator, which is called the **ordinary least squares** (or **OLS**) estimator. 

### Econometric analysis

An econometric analysis has three components:

- a structural model,
- a data set,
- a set of statistical tools that enables to estimate the model using the data and to test some hypothesis.

By **structural** model, we typically mean in microeconometrics than we
start from the rational behavior of an individual or a set of
individuals (for example a household that maximize its utility given
its budget constraint) and we deduce from this behavior a linear
relationship between the two variables (the response and the
covariate), the two parameters of this linear relationship being
directly linked to the parameters of the structural model. 

A **data set** is typically presented as a
rectangular table for which every line is an observation
and every column is a variable. Therefore, in a simple regression
model, we'll have a table with only two columns, the response $y$ and
the unique covariate $x$. We'll denote $n$ the index of the
observations, so that $n=1$ for the first line of the table, $n=2$ for
the second one and $n=N$ for the last one. $N$ is also therefore the
number of observations.

An **estimation method** is required to compute the estimated values of the unknown parameters $(\alpha, \beta)$ as a function of the
values of the response and of the covariate on the sample. Finally,
**tests** are useful to answer questions like: is $\alpha = 0$ or is $\beta = 1$?

## The model and the data set

We'll consider in this chapter the question of mode shares for
inter-urban transportation. More precisely, considering that a trip
can be made using one out of two transport modes (air and rail), how can we modelize
the market share of both modes? We'll use in this section a popular
model in transportation economics which is the price-time model.

`micsr.data::price_time` contains aggregate data about rail
and air transportation between Paris and 13 French towns in 1995, it
is reproduced from @BONN:04 p. 364-366.

```{r }
#| label: price_time
price_time %>% print(n = 3)
```
For the sake of simplicity, We'll use shorter names for the variables:

```{r }
#| label: price_time_names
price_time <- price_time %>%
    set_names(c("town", "qr", "qa", "pr", "pa", "tr", "ta"))
```

Variables are prices (`pr` and `pa`) in euros, transport times (`tr` and
`ta`) in minutes and thousands of trips (`qf` and `qa`) for the two
modes (`r`` for rail and `a` for air).

We first compute the market shares of rail:

```{r }
#| label: shares_computation
price_time <- mutate(price_time, sr = qr / (qr + qa))
price_time %>% pull(sr) %>% summary
```
Rail's market shares exhibit huge variations in the sample, ranging from 
`r round(min(price_time$sr) * 100)` to
`r round(max(price_time$sr) * 100)`%.
For an individual, the relevant cost of a trip is the generalized
cost, which is the sum of the monetary cost and the value of his
travel time. Denoting $h^i$ the time value of individual $i$, in euros
per hour, the generalized cost for the two modes are:

$$
\left\{
\begin{array}{rcl}
c_{a} ^ i &=& p_{a} + 60 h ^ i t_{a}\\
c_{r} ^ i &=& p_{r} + 60 h ^ i t_{r}\\
\end{array}
\right.
$$

Plane is typically faster and more expensive than train, which means
that in the time-value / generalized cost plane, the generalized cost
for rail will be represented by a line with a lower intercept (the price of train is
lower) and with a higher slope (transport time is higher) than the one that corresponds to air. Generalized
cost for both modes and for the two towns of Bordeaux and Nice are
presented in @fig-costmode:

```{r}
#| label: fig-costmode
#| fig-cap: "Generalized cost for train and plane"
#| echo: false
z <- price_time %>% filter(town %in% c("Bordeaux", "Nice")) %>%
    mutate(h = (pa - pr) / ( (tr - ta) / 60))

v <- tibble(h = c(0, 50),
            Bordeaux_Plane = z[1, "pa", drop = TRUE] + z[1, "ta", drop = TRUE] * h / 60,
            Bordeaux_Train = z[1, "pr", drop = TRUE] + z[1, "tr", drop = TRUE] * h / 60,
            Nice_Plane = z[2, "pa", drop = TRUE] + z[2, "ta", drop = TRUE] * h / 60,
            Nice_Train = z[2, "pr", drop = TRUE] + z[2, "tr", drop = TRUE] * h / 60
            )
v %>% pivot_longer(-1, names_sep = "_", names_to = c("town", "mode")) %>%
    ggplot(aes(h, value)) + geom_line(aes(linetype = mode, color = town)) +
    coord_cartesian(xlim = c(0, 40), ylim = c(50, 200)) +
    labs(x = "Value of time", y = "Generalized cost", town = NULL)
```
Every individual will choose the mode with the lowest generalized cost. For
example, $i$ will choose the train if $c_r^i < c_a^i$. This will
depend on the value of time: an individual with a high value of time
will choose the plane as an individual with a lower travel time will
choose the train. For given values of prices and travel times, one can
compute a value of travel time $h^*$ which equates the generalized
costs of the two modes.

$$
h^* = \frac{p_a - p_r}{60(t_r - t_a)}
$$

For Bordeaux and Nice, these time values are respectively 
`r round(z$h[1], 1)` and `r round(z$h[2], 1)` euros per hour.
Nice is actually very far from Paris and only people with a very low
value of time would spend 7.5 hours in the train instead of taking a plane. We now
compute this threshold value for every city:

```{r }
#| label: threshold_value_time
price_time <- mutate(price_time,  h = (pa - pr) / ( (tr - ta) / 60) )
price_time %>% pull(h) %>% summary
```
There are huge variations of the threshold value of time, as it ranges
from `r round(pull(top_n(price_time, 1, h), h))` 
(`r pull(top_n(price_time, 1, h), town)`) to
`r round(pull(top_n(price_time, -1, h), h))` 
(`r pull(top_n(price_time, -1, h), town)`) euros per hour.
Before considering a theoretical model that links the market share of
train with the threshold value of time, let's have a first glance on @fig-hsfsmpl of this relationship using a scatter plot.

```{r}
#| label: fig-hsfsmpl 
#| fig-cap: "Share of rail in function of the threshold time value"
#| echo: false
price_time %>% ggplot(aes(h, sr)) + geom_point() +
    ggrepel::geom_label_repel(aes(label = town))
```

The relationship between the threshold value of time and rail's
market share seems approximately linear, except for cities where the
market share of train is very high (more than 75%). For now, we'll
remove these 4 cities from the sample and plot on @fig-hsfsubsmpl the scatter plot for this restricted sample.

```{r }
#| label: subset_price_time
price_time <- filter(price_time, sr < 0.75)
```

```{r}
#| label: fig-hsfsubsmpl
#| fig-cap: "Share of rail in function of the threshold time value on a sub-sample"
#| echo: false
price_time %>% ggplot(aes(h, sr)) + geom_point() +
    ggrepel::geom_label_repel(aes(label = town))
```

Now, we consider the distribution of the value of time. If $h$ follows
a given distribution between $a$ and $b$, train's market share is the
share of the population for which the value of time is between $a$ and $h^*$
(and plane's market share is the share of the population for which
the value of time is between $h^*$ and $b$). The simplest probability
distribution is the uniform distribution, which is defined by a
constant density equal to $\frac{1}{b-a}$ between $a$ and $b$. It is
represented in @fig-unifdist.

```{r}
#| label: fig-unifdist
#| fig-cap: "Model shares with a uniform distribution"
#| echo: false
library("latex2exp")
xmax <- 4
hmin <- 1
hmax <- 3
hstar <- 2.5
dun <- 1 / (hmax - hmin)
ymax <- dun
plot(x = xmax * c(- 0.2, 1) * 1.2, y = dun * c(-0.2, 1) * 1.1, 
     type = "n", axes = FALSE, ann = FALSE)
polygon(c(hmin, hmin, hstar, hstar), c(0, dun, dun, 0), col = "lightgrey")
arrows(0, 0, 0, dun * 1.1, length = .1, angle = 10)
arrows(0, 0, xmax, 0, length = .1, angle = 10)
text(xmax * 1.05, 0, TeX("$h$"), cex = 1)
text(0, dun * 1.2, "density", cex = 1)
text(hmin + (hstar - hmin) / 2, dun / 2, TeX("$s_f$"))
text(hstar + (hmax - hstar) / 2, dun / 2, TeX("$s_a$"))
lines(c(hmin, hmin, hmax, hmax), c(0, dun, dun, 0), lty = "dotted")
hticks <- dun / 50 * 4
vticks <- xmax / 100
lines(c(hmin, hmin), c(-1, 1) * hticks)
text(hmin, - hticks * 2, TeX("$a$"), cex = 1)
lines(c(hmax, hmax), c(-1, 1) * hticks)
text(hmax, - hticks * 2, TeX("$b$"), cex = 1)
lines(c(hstar, hstar), c(-1, 1) * hticks)
text(hstar, - hticks * 2, TeX("$h^*$"), cex = 1)
lines(c(0, hmin), c(dun, dun), lty = "dotted")
lines(c(-1, 1) * vticks, c(dun, dun))
text(- vticks * 12, dun, TeX("$\\frac{1}{b-a}$"))
```

The area of the rectangle of width $[a,b]$ and height $[0,
\frac{1}{b-a}]$ is 100%, because the whole population has a time value
between $a$ and $b$. This rectangle has two components:

- a first rectangle of width $[a, h ^ *]$ which include people for
  which time value is below  $h^*$ and therefore take the train,
- a second rectangle of width $[h ^*, b]$ which include people for
  which time value is higher than $h^*$ and therefore take the plane.
  
Stated differently : $s_f = \frac{h^* - a}{b - a} = -\frac{a}{b - a} +
\frac{1}{b-a} h^*$ and this model therefore predicts a linear
relationship between $h^*$ et $s_f$, the intercept of this line being
$-\frac{a}{b - a}$ and the slope $\frac{1}{b - a}$. Of course, rail's market share depends on other variables than the threshold value of time, so that the linear relationship concerns the conditional expectation of rail's market share. With $y=s_f$ and $x=h^*$, we therefore have a linear model of the form: $\mbox{E}(y | x) = \alpha + \beta x$. Moreover, the two parameters to be estimated $\alpha$ and $\beta$ are functions of the structural parameters of the model $a$ and $b$ which are the minimum and the maximum of the value of time.

## Computation of the OLS Estimator

The model we seek to estimate is: $\mbox{E}(y_n \mid x_n) = \alpha+\beta x_n$. The difference between the observed value of $y$ and
its conditional expectation is called the **error** for observation
$n$:

$$
y_n - \mbox{E}(y_n \mid x_n) = \epsilon_n
$$

The linear regression model can therefore be rewritten as:

$$
y_n = \mbox{E}(y_n \mid x_n) + \epsilon_n = \alpha + \beta x_n +  \epsilon_n
$$

$\epsilon_n$ is the error for observation $n$ when the values of the
unknown parameters $(\alpha, \beta)$ are set to their true values
$(\alpha_o, \beta_o)$. For given values of $(\alpha, \beta)$, obtained
using an estimation method, $\epsilon_n$ will be called the
**residual** for observation $n$. The residual of an observation is
therefore the vertical distance between the point for observation $n$
and the regression line. 

Heuristically, we seek to draw a straight line that is closest as
possible to all the points of our sample as in @fig-hsfsubsmpl. In the
simple linear regression model, the distance between a
point and the line is defined by the vertical distance, which is the residual for
this observation. For the whole sample, we need to agregate this
individual measures of distance. Summing them is not an issue as there
are positive and negative values of the residuals and the sum may be
very close to zero as the individual residuals may be very high in
absolute values. One solution would be to use the sum of the absolute
values of the residuals,^[This estimator is called the **LAD**
estimator, for least absolute deviations estimator.] but with the OLS estimator, we'll consider the sum of the squares of the
residuals (also called the residual sum of squares **RSS**). Taking the squares, as taking the absolute values,
removes the sign of the individual residuals and it results on an
estimator which has nice mathematical and statistical
properties. We'll therefore consider a function $f$ which
depends on the value of the response and the covariate in the sample
(two vectors $x$ and $y$ of length $N$) and on two unknown parameters
$\alpha$ and $\beta$, respectively the intercept and the slope of the
regression line:

$$
f(\alpha, \beta |x, y)=\sum_{n = 1} ^ N (y_n - \alpha - \beta x_n) ^ 2
$$

Note that we write $f$ as a function of the two unknown parameters
conditional on the values of $x$ and $y$ for a given sample.
First order conditions for the minimization of $f$ are:

$$
\left\{
\begin{array}{rcl}
\displaystyle \frac{\partial f}{\partial \alpha}  &=& 
-2 \sum_{n = 1} ^ N \left(y_n - \alpha - \beta x_n\right) = 0 \\
\displaystyle \frac{\partial f}{\partial \beta}  &=& 
-2\sum_{n=1}^n x_n\left(y_n-\hat{\alpha}-\beta x_n\right)=0 
\end{array}
\right.
$$ {#eq-gradientols}

Or, dividing by $-2$:

$$
\sum_{n = 1} ^ N\left(y_n - \alpha -\beta x_n\right) = \sum_{n = 1} ^
N \epsilon_n = 0
$${#eq-cpoalpha}


$$
\sum_{n = 1} ^ N x_n\left(y_n - \alpha - \beta x_n\right) = \sum_{n =
1} ^ N x_n \epsilon_n = 0
$$ {#eq-cpobeta}

@eq-cpoalpha indicates that the sum (or the mean) of the residuals in the sample is 0. Dividing
this expression by $N$ also implies that, denoting $\bar{y}$ and
$\bar{x}$ the sample means of the response and of the covariate:

$$
\bar{y} = \alpha + \beta \bar{x}, 
$$ {#eq-cpoalpha2}


which means that the sample mean is on the
regression line.


Denoting $\hat{\epsilon}_n$ the residuals of the OLS estimator, @eq-cpobeta states that $\sum_n x_n \hat{\epsilon} / N$, ie that the
average cross-product of the covariate and the residual is 0. But, as
the sample mean of the residuals is 0, this expression is also the
covariance between the covariate and the residuals:

$$
\hat{\sigma}_{x\hat{\epsilon}} = \frac{\sum_{n = 1} ^ N (x_n -
\bar{x})(\hat{\epsilon}_n - \bar{\hat{\epsilon}})}{N} = 
\frac{\sum_{n = 1} ^ N x_n \hat{\epsilon}_n}{N} - \bar{x}\bar{\hat{\epsilon}}=
0
$$

which means that the regression is such that there is no correlation
between the covariate and the residuals in the sample.
Substracting $\bar{y} - \alpha - \beta \bar{x}$, which is
0 (see @eq-cpoalpha2) from @eq-cpobeta, one gets:

$$
\sum_{n = 1}^N x_n\left[\left(y_n - \bar{y}\right)-\beta \left(x_n -
\bar{x}\right)\right] = 0 
$$

Moreover, $\sum_{n = 1} ^ N \bar{x}\left[\left(y_n - \bar{y}\right) -
\beta\left(x_n - \bar{x}\right)\right] = 0$ and so:

$$
\sum_{n=1} ^ N \left(x_n - \bar{x}\right)\left[\left(y_n -
\bar{y}\right) - \beta\left(x_n - \bar{x}\right)\right] = 0 
$$

Solving for $\beta$, we finally get the estimator of the slope:

$$
\hat{\beta} = \frac{\sum_{n = 1} ^ N
(x_n - \bar{x})(y_n - \bar{y})}{\sum_{n = 1} ^ N
(x_n - \bar{x}) ^ 2} = 
\frac{S_{xy}}{S_{xx}}=
\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x ^ 2}=
\hat{\rho}_{xy} \frac{\hat{\sigma}_y}{\hat{\sigma}_x}
$$ {#eq-slrbeta}


We give three formulation for this estimator:

- the first one indicates that it is the ratio of the covariation of
  $x$ and $y$: $S_{xy}=\sum_{n = 1} ^ N (x_n - \bar{x})(y_n -
  \bar{y})$ and the variation of $x$: 
  $S_{xx}= \sum_{n = 1} ^ N (x_n - \bar{x}) ^ 2$,
- the second one is obtained by dividing both sides of the ratio by the sample
  size, so that the estimator is now the ratio of the sample covariance
  between $x$ and $y$ and the sample variance of $x$,
- the third one is obtained by introducing the coefficient of
  correlation of $x$ and $y$: $\hat{\rho}_{xy}
  =\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x\hat{\sigma}_y}$, so that the
  estimator is also expressed as the product of the coefficient of
  correlation and the ratio of the standard deviations. 

This last formulation is particularly intuitive: $\hat{\rho}_{xy}$ is
a pure measure of the correlation between the covariate and the
response. This number has no unit and lies in the -1/+1 interval, a value of -1 (+1)
indicating a perfect negative (positive) correlation and the value of 0 no correlation. The ratio of the standard deviations gives the relevant
unit to the slope, which is the unit of $y$ divided by the unit of $x$.

With this estimator of the slope in hand, we easily get the estimator
of the intercept using @eq-cpoalpha2:

$$
\hat{\alpha}=\bar{y}-\hat{\beta} \hat{x}
$$ {#eq-slralpha}

Consider now that prior the estimation, we deduced from the covariate
and the response their sample means. We then use $\tilde{y}_n = y_n -
\bar{y}$ and $\tilde{x}_n = x_n - \bar{x}$ as the response and the
covariate. In this case, as the mean of these two transformed
variables are zero, the intercept is 0 (from @eq-slralpha) and the
slope is simply $\sum \tilde{y}_n \tilde{x}_n / \sum \tilde{x}_n ^ 2$
and, up to the $1/N$ factor, $\sum \tilde{y}_n \tilde{x}_n$. 

Once the parameters of the regression line have been computed, on can
define the **fitted value** for observation $n$ as the value returned by
the regression line for $x_n$, which is:

$$
\hat{y}_n = \hat{\alpha} + \hat{\beta} x_n
$$

By construction, we have for the fitted model: $y_n = \hat{y}_n +
\hat{\epsilon}_n$. Note that for the "true" model, we have $y_n =
\mbox{E}(y | x = x_n) + \epsilon_n$, so that residuals
are an estimation of the errors and the fitted values $\hat{y}_n$ are an estimation of the
conditional expectations of $y$. Moreover:

$$
\frac{\sum_n (\hat{y}_n - \bar{\hat{y}})(\hat{\epsilon}_n -
\bar{\hat{\epsilon}})}{N}=
\frac{\sum_n \hat{y}_n \hat{\epsilon}_n}{N} -
\bar{\hat{y}}\bar{\hat{\epsilon}}
=
\frac{\sum_n \hat{y}_n \hat{\epsilon}_n}{N}
=
\frac{\sum_n (\hat{\alpha} + \hat{\beta} x_n) \hat{\epsilon}_n}{N}
= 0
$$

Therefore, there is no correlation between $\hat{y}$ and $\hat{\epsilon}$, which is clear as $\hat{y}$ is a linear function of $x$ and $x$ is uncorrelated with $\hat{\epsilon}$.
$(\hat{\alpha}, \hat{\beta})$ correspond to an optimum of the objective function. To check that this optimum is a minimum, we have to compute the second derivatives. From @eq-gradientols, we get: $\frac{\partial^2 f}{\partial \alpha ^ 2}= 2N$, $\frac{\partial^2 f}{\partial \beta ^ 2}= 2\sum_n x_n ^ 2$ and $\frac{\partial^2 f}{\partial \alpha \partial \beta}= 2\sum_n x_n$. We need for a maximum positive direct second derivatives, which is obviously the case, and also that:

$$
D = \frac{\partial^2 f}{\partial \alpha ^ 2}\frac{\partial^2 f}{\partial \beta ^ 2}
- \left(\frac{\partial^2 f}{\partial \alpha \partial \beta}\right)^2 
= 4 N \sum_n x_n ^ 2 - 4  \left(\sum_n x_n\right) ^ 2 = 4N^2\left(\frac{\sum_n x_n ^ 2}{N} - \bar{x}^2\right)
> 0
$$

which is the case as the term in bracket is the variance of $x$ and is therefore positive.

## Geometry of least squares, variance decomposition and coefficient of determination

The OLS estimator relies on variance and covariance of several observable ($y$ and $x$) and computed ($\hat{y}$, $\hat{\epsilon}$) variables. Its properties can be nicely illustrated using vector algebra, each variable being represented by a vector, and by plotting these vectors.^[For a detailed presentation of the geometry of least squares, see @DAVI:MACK:04, chapter 2.]

### Vectors, variance and covariance

Every variable used in a regression is a vector of $\mathcal{R}_N$, ie a
set of $N$ real values. For example $z^\top = (z_1, z_2, \ldots, z_N)$.
Its length, or norm is : $\|z\| = \sqrt{\sum_{n=1}^N z_n ^ 2}$. Remind
that the OLS estimator can always be computed with data measured in
deviations from their sample mean. Then, $\|z\| ^ 2 / N$ is the
variance of the variable, or the norm of the vector is $\sqrt{N}$ times
the standard deviation of the corresponding variable.
The inner (or scalar) product of two vectors is denoted
$z ^ \top w = w ^ \top z = \sum_{n=1} ^ N z_n w_n$ (note that the inner
product is commutative). For corresponding variables expressed in
deviations from their respective means it is, up to the $1/N$ factor, the
covariance between the two variables. Denoting $\theta$ the angle formed
by the two vectors, we also have:^[see @DAVI:MACK:04, page 48.]
$z ^ \top w = \cos \theta \|z\| \|w\|$.

```{r }
#| label: vectors_functions
#| echo: false
x <- c(4, 3)
z <- c(4.5, 6)
w <- c(-6, 4.5)
norm_z <- sqrt(sum(z ^ 2))
norm_x <- sqrt(sum(x ^ 2))
norm_w <- sqrt(sum(w ^ 2))
theta_x <- acos(x[1] / norm_x)
theta_z <- acos(z[1] / norm_z)
theta <- theta_z - theta_x
dg <- function(x) x / pi * 180
prtv <- function(x) paste("(", x[1], ",", x[2], ")")
nrm <- function(x) sqrt(sum(x ^ 2))
```

Consider as an example: $x = `r prtv(x)`$, $z = `r prtv(z)`$ and
$w = `r prtv(w)`$. The three vectors are ploted in @fig-mafig.

```{r }
#| label: fig-mafig
#| fig-cap: "Vector algebra"
#| echo: false
knitr::include_graphics("./tikz/fig/vectors2D.png", auto_pdf = TRUE)
```

The norm of $x$ is
$\|x\|=\sqrt{ `r x[1]` ^ 2 + `r x[2]` ^ 2} = `r nrm(x)`$. Similarly,
$\|z\| = `r nrm(z)`$ and $\|w\| = `r nrm(w)`$.
The cosinus of the angle formed by $x$ and $z$ with the horizontal axis
is $\cos \theta_x = `r x[1]` / `r nrm(x)` = `r round(x[1] / nrm(x), 3)`$
and
$\cos \theta_z = `r z[1]` / `r nrm(z)` = `r round(z[1] / nrm(z), 3)`$.
The angle formed by $x$ and $z$ is therefore:
$\theta = \arccos `r z[1] / nrm(z)` - \arccos `r x[1] / nrm(x)` = `r round(theta_z - theta_x, 3)`$,
with $\cos `r round(theta, 3)` = `r cos(theta)`$. We can then check that
$z ^ \top x = `r x[1]` \times `r z[1]` + `r x[2]` \times `r z[2]` = `r sum(x * z)`$,
which is equal to :
$\cos \theta \|z\| \|w\| = `r cos(theta)` \times `r nrm(z)` \times `r nrm(x)`$.
As the absolute value of $\cos \theta$ is necessary lower or equal than 1,
the inner product of two vectors is lower than the product of the norms
of the two vectors and $\cos \theta = \frac{x ^ \top z}{\|x\| \|z\|}$ is
the ratio of the covariance between $x$ and $z$ and the product of their
standard deviations, which is the coefficient of correlation between the
two underlying variables $x$ and $z$. Consider now $z$ and $w$. Their
inner product is :

$z ^ \top x = `r z[1]` \times `r w[1]` + `r z[2]` \times `r w[2]` = `r sum(z * w)`$

This is because $z$ and $w$ are two orthogonal vectors, which means that
the two underlying variables are uncorrelated.

### The geometry of least squares {#sec-geometry_ols}

The geometry of the simple linear regression model is represented on @fig-smplregmodel.

```{r }
#| label: fig-smplregmodel
#| fig-cap: "Geometry of the simple regression model"
#| echo: false
knitr::include_graphics("./tikz/fig/OLS2D.png", auto_pdf = TRUE)
```

With $N = 2$, $x$ and $y$ are two vectors in a plane. For the "true" model, the $y$ vector is the sum of two vectors: $\beta x$ (which is the conditional expectation of $y$) and $\epsilon$, which is the vector of errors. For the estimated model, $y$ is the sum of the fitted values $\hat{y} = \hat{\beta} x$ and the residuals $\hat{\epsilon}$. Using the **OLS** estimator, we must minimize the sum of squares of the residuals, ie the norm of the $\hat{\epsilon}$ vector. Obviously, this implies that $\hat{\epsilon}$ should be orthogonal to $\hat{y}$ and therefore to $x$, which implies that the residuals are uncorrelated to the fitted values (and to the covariate) in the sample. 

Note also that, except in the unlikely case where $\hat{\beta} = \beta$, $\|\hat{\epsilon}\| < \|\epsilon\|$ which means that the residuals have a smaller variance than the errors. Note finally that what determines $\hat{y}$ and $\hat{\epsilon}$ is not $x$
per se, but the subspace defined by it, in our case a straight line. For
example, consider the regression of $y$ on $z = 0.5 x$; we would then obtain exactly the same values for $\hat{y}$ and $\hat{\epsilon}$, the only difference being that the estimator of $\beta$ would be two times larger.

### Variance Decomposition and the $R^2$

For one observation $n$, we have: 

$$
y_n - \bar{y}=\left(y_n - \hat{y}_n\right)+\left(\hat{y}_n - \bar{y}\right)
$$  {#eq-varobs}

The difference between $y$ for individual $n$ and the sample mean
is therefore the sum of:

- an explained variation: $\hat{y}_n - \bar{y}=\hat{\beta}(x_n -
  \bar{x})$
- a residual variation: $\left(y_n - \hat{y}_n\right) = \hat{\epsilon}_n$

Taking the square of @eq-varobs and summing for all $n$, we get:

$$
\begin{array}{rcl}
\sum_{n = 1} ^ N(y_n - \bar{y})^2 & = & \sum_{n = 1}^ N
\left(\hat{\epsilon}_n + \hat{\beta}(x_n - \bar{x})\right) ^ 2 \\
& = & \sum_{n = 1} ^ N \hat{\epsilon}_n ^ 2 + \hat{\beta} ^ 2 \sum_{n
= 1} ^ N (x_n - \bar{x}) ^ 2 \\
& + & 2 \hat{\beta}\sum_{n = 1} ^ N \hat{\epsilon}_n x_n -
2\hat{\beta}\bar{x}\sum_{n = 1} ^ N \hat{\epsilon}_n
\end{array}
$$

But $\sum_{n = 1} ^ N \hat{\epsilon}_n x_n = 0$ (@eq-cpobeta) and $\sum_{n = 1} ^ N
\hat{\epsilon}_n = 0$ (@eq-cpoalpha), so that:

$$
\sum_{n = 1} ^ N (y_n - \bar{y}) ^ 2 = \hat{\beta} ^ 2
\sum_{n = 1} ^ N (x_n - \bar{x}) ^ 2+\sum_{n = 1} ^ N
\hat{\epsilon}_n ^ 2
$$ {#eq-variance_decomposition}

This equation indicates that the total sum of squares of the response (TSS) equals the sum of the explained sum of squares (ESS) and of the residual sum of squares (RSS). This latter term is also called the **deviance** and it is the objective function for the OLS estimator. Dividing by $N$, we also have on the left-hand size the variance of $y$ and on the right-hand side the sum of the variances of $\hat{y}$ and of $\hat{\epsilon}$. This is the formula of the **variance decomposition** of the response. It can be easily understood using @fig-smplregmodel. It is clear from this figure that $y = \hat{y} + \hat{\epsilon}$ and that $\hat{y}$ and $\hat{\epsilon}$ are orthogonal. Therefore, applying Pythagor theorem, we have $\|y\|^2 = \|\hat{y}\|^2 + \|\hat{\epsilon}\|^2$. Up to the $1/N$ factor, if the response is measured in deviation from its sample mean, we have on the left the total variance of $y$ and on the right the sum of the variances of $\hat{y}$ and $\hat{\epsilon}$. Not that it is a unique feature of the ordinary least square estimator. Any other estimator will generally result with a vector of residuals which won't be orthogonal to $\hat{y}$ (or to $x$) and therefore @eq-variance_decomposition won't apply.

The coefficient of determination, denoted $R^2$, measures the share of the variance of the response which is explained by the model. We then have:

$$
R^2=\frac{\hat{\beta}^2\sum_{n=1}^N(x_n-\bar{x})^2}{\sum_{n=1}^N (y_n-\bar{y})^2}
$$

using @eq-slrbeta, we finally get:

$$
  R^2=\left[\frac{\sum_{n=1}^N(x_n-\bar{x})(y_n-\bar{y})}{\sum_{n=1}^N
    (x_n-\bar{x})^2}\right]^2
\frac{\sum_{n=1}^N(x_n-\bar{x})^2}{\sum_{n=1}^N (y_n-\bar{y})^2} = 
\left[\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x ^ 2}\right] ^ 2 
\left[\frac{\hat{\sigma}_x ^ 2}{\hat{\sigma}_y ^ 2}\right] = \hat{\rho}_{xy} ^ 2
$$

$R^2$ is therefore simply the square of the coefficient of
correlation between $x$ and $y$. We have seen previously that, denoting $\theta$ the angle formed by two vectors, $\cos \theta$ is the coefficient of correlation between the two underlying variables (if they are measured in deviations from their means). Therefore, on @fig-smplregmodel, $R^2$ is represented by the square of the cosinus of the angle formed by the two vectors $\hat{y}$ and $y$. As this angle tends to 0 (the two vectors point almost on the same direction), $R^2$ tends to 1. On the contrary, if this angle tends to $\pi/2$, the two vectors became almost orthogonal and $R^2$ tends to 0.

## Computation with R

The slope of the regression line can easlily be computed "by hand", using any of the
formula indicated in @eq-slrbeta, using the `dplyr::summarise` function. We first compute the variations of $x$ and $y$ and the covariation of $x$ and $y$, the two standard deviations and the coefficient of correlation between $x$ and $y$.

```{r }
#| label: moments_computation
stats <- price_time %>%
    summarise(N = nrow(price_time), xb = mean(h), yb = mean(sr),
              Sxx = sum( (h - xb) ^ 2), Syy = sum( (sr - yb) ^ 2),
              Sxy = sum( (h - xb) * (sr - yb)),
              sx = sqrt(Sxx / N), sy = sqrt(Syy / N),
              sxy = Sxy / N, rxy = sxy / (sx * sy))
```

We then can calculate the slope estimator using any of the three formula:

```{r }
#| label: slope_computation
#| collapse: true
hbeta <- stats$Sxy / stats$Sxx
hbeta
stats$sxy / stats$sx ^ 2
stats$rxy * stats$sy / stats$sx
```

The estimated intercept is obtained using @eq-slralpha:

```{r }
#| label: intercept_computation
halpha <- stats$yb - hbeta * stats$xb
halpha
```

Much simplyer, the OLS estimator can be obtained using the `lm` function (for linear model). It is a very important function in **R**, not only because it implements efficiently the most important estimator used in econometrics, but also because **R** functions that implement other estimators often mimic the `lm` function. Therefore, once one is at ease with using the `lm` function, using other estimating function of **R** will be straightforward. `lm` is a function that has many arguments, but the first two of them are fundamental and almost mandatory^[Actually, the second argument `data` is not mandatory because estimation can be performed without using a data frame.]:

- `formula` is a symbolic description of the model to be
  estimated,
- `data` is a data frame that contains the variables used in the formula.

Here, our formula writes `sr ~ h`, which means `sr` as a function of
`h`. The data frame is `price_time`. The result of the `lm` function
may be directly printed (the result is then lost), or saved in an
object, which can be later printed or manipulated:

```{r }
#| label: lm_function
lm(sr ~ h, price_time)
pxt <- lm(sr ~ h, price_time)
```

writing directly `pxt` is like writing `print(pxt)`, the side effect
is to print a short description of the results, namely a reminding of
the function call and the name and the values of the fitted
coefficients.
`lm` returns an object of class `lm` which is a list of 12 elements; their names can be retrieved using the `names` function:

```{r}
#| label: names_lm
names(pxt)
```

An element of this list can be extracted using the `$` operator. For example:

```{r}
#| label: lm_coefficients
pxt$coefficients
```

returns the named vector of coefficients. `pxt$residuals` and `pxt$fitted.values` return two vectors of length $N$, the vector of residuals and the vector of fitted values. However, it is not advised to use the `$` operator to retrieve the elements of a fitted model. Specific functions, called extractors, should be used instead. For example, to retrieve the coefficients, the residuals and the fitted values as previously, we would use:

```{r }
#| label: using_methods
#| results: false
coef(pxt)
resid(pxt)
fitted(pxt)
```

There are other functions that extract important informations about the model, as the number of observation (`nobs`), the sum of square residuals (`deviance`):

```{r}
#| label: other_lm_methods
#| collapse: true
nobs(pxt)
deviance(pxt)
```

The results of the estimation are presented on @fig-reserror;
we've added to the scatterplot:

- the sample mean, indicated by a red circle,
- the regression line,
- the residuals, represented by arrows: upwards arrows represent
  positive residuals (eg Brest and Toulon) as downwards arrows
  represent negative residuals (eg Strasbourg and Nice).

```{r }
#| label: fig-reserror
#| fig-cap: "Residuals and errors"
#| echo: false
Mpxtps <- price_time %>% summarise(h = mean(h), sr = mean(sr))
dx <- 0
myarrow <- arrow(length = unit(0.025, "npc"), angle = 15, type = "closed")
price_time %>% mutate(hy = fitted(pxt)) %>%
    ggplot(aes(h, sr)) + geom_point() +
    geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
    ggrepel::geom_label_repel(aes(label = town)) + 
        geom_segment(aes(x = h - dx, xend = h - dx, yend = sr, y = hy),
                     arrow = myarrow) +
        geom_point(data = Mpxtps, size = 5, col = "red") 
```

Individual coefficients can be extracted using the `[` operator. As `coef` returns a named vector, one can either indicate the position or the name of the coefficient to extract:

```{r }
#| label: coef_extraction
int <- unname(coef(pxt)[1])
slope <- unname(coef(pxt)["h"])
```

Note that we use the `unname` function in order to remove the name of the extracted coefficient. Once the intercept (`int`) and the slope (`slope`) are extracted, the
structural parameters can be retrieved as the intercept is $\alpha =
-\frac{a}{b-a}$ and the slope $\beta =\frac{1}{b-a}$. 
Therefore, $a = -\frac{\alpha}{\beta}$ and $b =
\frac{1}{\beta} + a$. We finally get:

```{r }
#| label: structural_coefficients
ahat = - int / slope
bhat = 1 / slope + ahat
c(ahat, bhat)
```
Time value therefore lies between `r round(ahat, 2)` and 
`r round(bhat, 2)` euros per hour. The mean (and median) time value is
the mean of the two extrem values, 
which is `r round( (ahat + bhat) / 2, 2)` euros per hour.


## Data generator process and simulations

Inferential statistics rely on the notions of population and sample. The population is a large and exhaustive set of observations and a sample is a small subset of observations drawn in this population. These notions are relevant in medical sciences and often for economic studies. For example, the first data set we used concerned smoking habits and birth weight. The population of interest is all american pregnant women in 1988 and a sample of 1388 of them was drawn from this population. The second data set concerned the relation between education and wage. The population was American labor force in 1992 and a sample of 16481 workers was drawn from this population. On the contrary, our third data set doesn't fit with this notions of population and sample. The sample consists of major towns in France that are connected on regular basis by rail and air to Paris. It contains 13 observations, which are not 13 observations randomly drawn from a large set of cities, but which are more or less all the relevant cities. 

### Data generator process

An interesting alternative is the notion of **data generator process** or **DGP** in short. It describes how the data are assumed to have been generated. We assume in the linear regression model that: 

- $\mbox{E}(y|x = x_n) = \alpha + \beta x_n$: the expected value of $y$ for $x = x_n$ is a linear function of $x$,
- $y_n = \mbox{E}(y|x_n) + \epsilon_n$: the observed value of $y_n$ is obtained by adding to the conditional expectation of $y$ for $x = x_n$ a random variable $\epsilon$ called the error.

From @eq-slrbeta, we can write the estimator of the slope as:

$$
\hat{\beta} = \frac{\sum_{n = 1} ^ N   (x_{n} - \bar{x})(y_{n} - \bar{y})}
{\sum_{n = 1} ^ N   (x_{n} - \bar{x})^ 2}
= \sum_{n = 1} ^ N \frac{(x_{n} - \bar{x})}{\sum_{n = 1} ^ N
(x_{n} - \bar{x}) ^ 2} y_{n}
= \sum_{n} c_{n} y_{n}
$$

with $c_{n} = \frac{x_{n} - \bar{x}}{\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2}$. The OLS estimator is therefore a linear estimator, ie a
linear combination of the values of $y$. The coefficients of this
linear combination $c_n$ are such that $\sum_{n = 1} ^ N  c_{n} = 0$ and that:

$$
\sum_{n = 1} ^ N  c_{n} ^ 2 = \frac{\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2}
{\left(\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2\right) ^ 2}  =
\frac{1}{\sum_{n = 1} ^ N (x_{n} -
\bar{x}) ^ 2} = \frac{1}{N\hat{\sigma}_x^2}
$$
 
Replacing $y_{n}$ by $\alpha + \beta x_{n} + \epsilon_{n}$, we then express $\hat{\beta}$ as a function of $\epsilon_n$:

$$
\hat{\beta}=\sum_{n = 1} ^ N c_{n} (\alpha + \beta
x_{n} + \epsilon_{n}) = \alpha \sum_{n = 1} ^ N c_{n}+\beta\sum_{n = 1}
^ N \frac{x_{n}(x_{n} - \bar{x})}{\sum_{n = 1} ^ N
(x_{n} - \bar{x}) ^ 2}+\sum_{n = 1} ^ N c_{n} \epsilon_{n}
$$ 

As $\sum_{n = 1} ^ N x_{n}(x_{n} - \bar{x}) = \sum_{n = 1} ^ N
(x_{n} - \bar{x}) ^ 2$ 
and $\sum_{n = 1} ^ N c_{n} = 0$, we finally get:

\begin{equation} 
\hat{\beta} = \beta + \sum_{n = 1} ^ N c_{n} \epsilon_{n} 
\end{equation} 

The deviation of the estimator of the slope of the OLS regression line
$\hat{\beta}$ from the true value $\beta$ is therefore a linear
combination of the $N$ errors.
Consider our sample used to estimate the price-time model. From a DGP perspective, this sample has been generated using the formula: $y=\alpha + \beta x + \epsilon$. Consider now that the "true" values of $\alpha$ and $\beta$ are $-0.2$ and $0.032$, we can in this case compute the vector or errors for our sample ($\epsilon = y - \alpha - \beta x$):

```{r}
#| label: error_computation
alpha <- - 0.2
beta <- 0.032
y <- price_time %>% pull(sr)
x <- price_time %>% pull(h)
eps = y - alpha - beta * x
```
We then compute the OLS estimator and we retrieve $\hat{\epsilon}$ and $\hat{y}$.

```{r}
#| label: resid_fitted_extraction
z <- lm(y ~ x)
hat_eps <- z %>% residuals
hat_y <- z %>% fitted
```

The observed data set, in the DGP perspective, is represented on @fig-dgp.

```{r }
#| label: fig-dgp
#| fig-cap: "The Data Generating Process"
#| echo: false
alpha <- - 0.2
beta <- 0.032
price_time <- price_time %>%
    mutate(Eyx = alpha + beta * h,
           hy = fitted(pxt))
Mpxtps <- price_time %>% summarise(h = mean(h), sr = mean(sr))
dx <- 0.05
myarrow <- arrow(length = unit(0.015, "npc"), angle = 15, type = "closed")
price_time %>% ggplot(aes(h, sr)) + geom_point() +
    geom_abline(slope = 0.032, intercept = - 0.2) +
    geom_smooth(method = "lm", se = FALSE, fullrange = TRUE,
                linetype = "dashed", color = "black") +
    geom_segment(aes(x = h - dx, xend = h - dx, yend = sr, y = hy),
                 col = "blue", arrow = myarrow) +
    geom_segment(aes(x = h + dx, xend = h + dx, yend = sr, y = Eyx),
                 col = "red", arrow = myarrow) +
    geom_point(data = Mpxtps, size = 5, col = "red") + 
    ggrepel::geom_label_repel(aes(label = town))
```
The "true" model is represented by the plain line. The errors are
represented by the red arrows (positive errors for upward arrows,
negative errors for downward arrows). Each value of $y_n$ is the sum
of the conditional expectation of $y$ for the value of $x$: $E(y|x = x_n) =
\alpha + \beta x_n$ (the value returned by the plain line for the
given value of $x$) and the error $\epsilon_n$ represented by the red
arrow. For our specific sample, we have a specific vector of
$\epsilon_n$, which means a specific set of points and a specific
regression line, the dashed line on the figure. Each value of $y_n$ is then also the sum of the fitted value (the one returned by the regression line for $x=x_n$) and the residual, represented by a blue arrow. 

We can check that the residuals sums to 0 and are uncorrelated with the covariate and the fitted values:

```{r}
#| collapse: true
#| label: zerosum_uncor_resid
sum(hat_eps)
sum(hat_eps * x)
sum(hat_eps * hat_y)
```
which is not exactly the case for the errors:

```{r}
#| collapse: true
#| label: empirical_moments_errors
sum(eps)
sum(eps * x)
sum(eps * hat_y)
```
We can also check that the residuals are "smaller" than the errors:

```{r}
#| label: smaller_residuals
#| collapse: true
sd(eps)
sd(hat_eps)
```

### Random numbers and simulations

Now consider an other fictive sample, for the same values of $x$, ie consider that the values of the threshold values of time are the same, but that the other factors that influence rail's shares (the $\epsilon$) are different. To generate such a fictive sample, we need to generate the $\epsilon$ vector, using a function that generates random numbers^[More precisely, a function that generates a sequence of numbers that looks like random numbers.]. With **R**, these functions have a name composed by the letter `r` (for random) and the abbreviated name of the statistical distribution: for example `runif` draws numbers in a uniform distribution (by default with a 0-1 range) and `rnorm` draws numbers in a normal distribution (by default with zero expectation and unit standard deviation). These functions have a mandatory argument which is the number of draws. For example, to get 5 numbers drawn from a standard normal distribution:

```{r}
#| include: false
#| label: set_the_seed
set.seed(10L)
```

```{r}
#| label: first_draw_normal
#| collapse: true
rnorm(5)
```

Using the same command once more, we get a completly different sequence:
```{r}
#| label: second_draw_norm
#| collapse: true
rnorm(5)
```


As stated previously, the `rnorm` function doesn't draw random numbers but computes a sequence of numbers that looks like a random sequence. Imagine that `rnorm` actually computes a sequence of thousands of numbers, what is obtained using `rnorm(5)` is 5 consecutive numbers in this sequence, for example from the 5107^th^ to the 5111^th^ number. The position of the first element is called the **seed** and it can be set as an integer using the `set.seed` function. Using the same seed while starting a simulation, we would then get exactly the same pseudo-random numbers and therefore the same results:

```{r}
#| label: setting_seed
#| collapse: true
set.seed(7L)
rnorm(5)
rnorm(5)
set.seed(7L)
rnorm(5)
rnorm(5)
```
The DGP is completely described by specifying the distribution of $\epsilon$. We'll consider here a normal distribution with mean 0 and standard deviation $\sigma_\epsilon = 0.08$. 

A pseudo-random sample can then be constructed as follow:

```{r}
#| label: a_small_sample
N <- price_time %>% nrow
x <- price_time %>% pull(h)
alpha <- - 0.2 ; beta <- 0.032 ; seps <- 0.08
eps <- rnorm(N, sd = seps)
y <- alpha + beta * x + eps
asmpl <- tibble(y, x)
```

which gives the following OLS estimates:

```{r}
#| label: lm_on_small_random_sample
lm(y ~ x, asmpl) %>% coef
```

The notion of DGP enables to perform simulations, which have two
purposes:

- the first one (and the most important in practise) is that, in some
  situations, it is impossible to get analytical results for the
  properties of an estimator and those properties can in this case be
  obtained using simulations,
- the second one is pedagogical, as theoritical results can be confirmed and illustrated using simulations.

A simulation consists on creating a large number of samples, on computing some interesting numbers for every sample and on calculating some relevant statistics for these numbers. For example, we'll compute the slope of the OLS estimate for every sample and we'll calculate the mean and the standard deviation for this slope. 

A minimal simulation is presented on @fig-foursmpls, where we present 4 different samples obtained for 4 different sets of errors.

```{r }
#| label: fig-foursmpls
#| fig-cap: "4 different samples"
#| echo: false
set.seed <- 1
dx <- 0
smpls <- price_time %>% select(h, sr) %>%
    mutate(Ey = alpha + beta * h,
           e_0 = sr - Ey,
           e_1 = rnorm(9, sd = 0.08),
           e_2 = rnorm(9, sd = 0.08),
           e_3 = rnorm(9, sd = 0.08)) %>% 
    select(- sr) %>% 
    pivot_longer(-(1:2), names_to = "sample",
                 values_to = "error") %>% 
    mutate(sr = Ey + error)
Msmpls <- smpls %>% group_by(sample) %>%
    summarise(alpha = coef(lm(formula = sr ~ h))[1],
              beta =  coef(lm(formula = sr ~ h))[2],
              h = mean(h),
              sr = mean(sr),
              coefs = paste("(", round(alpha, 3), ") - (",
                            round(beta, 3), ")", sep = ""))
smpls <- smpls %>% left_join(select(Msmpls, sample, coefs))              
smpls %>% ggplot(aes(h, sr)) + geom_point() +
    geom_abline(intercept = alpha, slope = beta) + 
    geom_smooth(method = "lm", se = FALSE,
                linetype = "dashed", color = "black") +
    geom_point(data = Msmpls, color = "red", size = 4) +
    geom_segment(aes(x = h + dx, xend = h + dx, yend = sr, y = Ey),
                 col = "red", arrow = myarrow) +
    facet_wrap(~ coefs)
```
For every sample, there is a specific set of errors (the red arrows)
and therefore specific data points and regression line. The estimated
intercepts range from
`r round(Msmpls %>% top_n(-1, alpha) %>% pull(alpha), 3)` to
`r round(Msmpls %>% top_n(1, alpha) %>% pull(alpha), 3)` 
and the slopes from
`r round(Msmpls %>% top_n(-1, beta) %>% pull(beta), 3)` to
`r round(Msmpls %>% top_n(1, beta) %>% pull(beta), 3)`.
The important point is that $\beta$ is, in practise, an unkown
fixed parameter. $\hat{\beta}$ depends on the $N$ observations of the sample and therefore the $N$ values of the errors. Each sample is characterized by a specific
vector of errors and therefore by a different value of the estimator.

More generally, we denote $R$ the number of replications and we construct a data frame with $R \times N$ lines. The columns are the 13 values of $x$ (fixed in this simulation), the vector $\epsilon$ drawn in a normal distribution with a standard deviation equal to 0.08 and $y = \alpha + \beta x + \epsilon$, with $\alpha = 0.2$ and $\beta = 0.032$ as previously. Finally we add a variable `smpls` that is a vector containing integers from 1 to $R$ repeated 13 times and that will identifie the sample:

```{r }
#| label: experience_setting
alpha <- - 0.2 ; beta <- 0.032 ; seps <- 0.08
R <- 1E03 ; N <- length(x)
datas <- tibble(id = rep(1:R, each = N),
                x = rep(x, R), 
                eps = rnorm(R * N, sd = seps),
                y = alpha + beta * x + eps)
```

Running regressions on every samples requires to use `lm(y ~ x)` not on the whole data set, but on every subset defined by a value of
`id`. The `lm` has a `subset` argument that must be a logical expression used to select only a subset of the data frame. For example, to get the fitted model for the 1st sample, the logical expression is `id == 1`  and one can use:

```{r }
#| include: false
#| label: simulation_with_lm
lm(y ~ x, data = datas, subset = id == 1)
```
The slope can also be calculated using the formula which shows that the OLS estimator is linear:

```{r}
#| label: simulation_using_manual_computation
datas %>% filter(id == 1) %>% 
  mutate(cn = (x - mean(x)) / sum((x - mean(x)) ^ 2)) %>%
  summarise(slope = sum(cn * y))
```

To get values of the estimator for every sample, we have to loop on this command for `id` equal 1 to `R`. A good practise in **R** is to avoid the use of explicit loops whenever it's possible. The `group_by` / `summarise` couple of functions from the `dplyr` package can be used instead. 

```{r}
#| label: simulations_group_by_summarise
results <- datas  %>% 
  group_by(id) %>% 
  mutate(cn = (x - mean(x)) / sum((x - mean(x)) ^ 2)) %>%
  summarise(slope = sum(cn * y), intercept = mean(y) - slope * mean(x))
results
```

The result is now a tibble with `R` lines, as the computation of the
OLS estimator is performed for every sample. As we now have a large number of values of $\hat{\beta}$, we can analyse its statistical properties, for example its mean and its standard deviation for the 1000 random samples:

```{r}
#| label: simulations_results
results %>% summarise(mean = mean(slope), sd = sd(slope))
```

We'll see in the next chapter that the expected value of $\hat{\beta}$ is $\beta=0.032$ (we'll say that the OLS estimator is unbiased) and that its standard deviation is $\sigma_\epsilon / \sigma_x / \sqrt{N} = 0.0041$. The mean and standard deviations for our 1000 values of $\hat{\beta}$ are estimates of this two theoretical values. We can see that the mean of $\hat{\beta}$ is actually equal to 0.032 and that the standard deviation equals 0.00437, which is close to the theoretical value.

A more general method to perform simulation is to use list-columns of data frames. To conduct such an analysis step by step, we start with a very small example, with $R = 2$ and $N = 3$.

```{r}
#| label: creating_small_data
R <- 2
N <- 3
small_datas <- tibble(id = rep(1:R, each = N),
                      x = rep(x[1:3], R), 
                      eps = rnorm(R * N, sd = seps),
                       y = alpha + beta * x + eps)
small_datas
```

 Starting with `datas`, we nest the data frame using the `id` variable (`.by = id`); the name of the results is by default `data`, but it can be customized using the `.key` argument:

```{r}
#| label: nesting_small_data
datalst <- small_datas %>% nest(.by = id, .key = "smpl")
datalst
```

Each line now corresponds to a value of `id` and `smpl` is a list column that contains a tibble,

```{r}
#| label: extracting_one_tibble
datalst %>% pull(smpl) %>% .[[2]]
```

We now write a function that takes as argument a data frame and returns a one-line tibble that contains the fitted intercept and slope:

```{r}
#| label: ols_function
ols <- function(s){
  x <- s$x ; y <- s$y
  tibble(slope = sum((x - mean(x)) * (y - mean(y))) /  
           sum( ( x - mean(x)) ^ 2),
         intercept = mean(y) - slope * mean(x))
}
```
This function can be applied for a given sample:

```{r}
#| label: ols_on_one_tibble
ols(small_datas[1:3, ])
```

But, as the `smpl` column of `datalst` is a list, the `ols` function can be applied to all the elements of the list, we can be done using the `purrr::map` function which takes as argument a list and a function:

```{r}
#| label: ols_on_all_tibbles
datalst %>% pull(smpl) %>% map(ols)
```

and returns a list. But this can more easily done using `mutate` or `transmute`:

```{r}
#| label: ols_on_all_tibbles_with_mutate
results <- datalst %>% transmute(smpl = map(smpl, ols))
results
```

Finally, `tidyr::unnest` expands the list column containing tibbles into rows and columns.

```{r}
#| label: unnesting_the_result
results %>% unnest(cols = smpl)
```

which returns a standard tibble with one row for each draw and one column for each computed statistic. Going back to the full example with $R = 1000$, we get:

```{r}
#| label: nest_full_example
datas %>% nest(.by = id, .key = "smpl") %>% 
  transmute(smpl = map(smpl, ols)) %>% 
  unnest(cols = smpl) %>% 
  summarise(mean = mean(slope), sd = sd(slope))
```


