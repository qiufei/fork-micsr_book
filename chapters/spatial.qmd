```{r}
#| echo: false
source("../_commonR.R")
library(generics)
st_nb <- function(x, queen = TRUE){
    .crs <- st_crs(x)
    .nb <- poly2nb(x, queen = queen)
    as(nb2lines(.nb, coords = st_centroid(st_geometry(x))), "sf")
}
```

# Spatial econometrics

Spatial econometrics is a very dynamic field in modern econometrics. Geolocated data are now frequently available, even when the data doesn't concern geographic entities like countries, regions or towns, but households or firms. From their geographic coordinates, one is able to define for every observation a set of neighbors and the correlation between neighbors can then be taken into account. There are two very different type of geographical data: vectors and rasters. A vector is a point or a set of point that defines a line. A raster is a grid that contains the value of one variable (for example a numeric indicating the elevation or a factor for land use). Relatively recently, two **R** packages has emerged that provide plenty of function that enables to deal easily with vectors and rasters. These are respectively **sf** (for **simple feature**) and **terra** and supersede the **sp** and the **raster** packages. In this chapter, we'll consider only vectors.

The first two sections are devote to simple features, @sec-simple_features presents the structure of simple features objects and @sec-sf_computation, using the example of a spatial RD design illustrates how to deal with spatial features in a statistical analyzes. @sec-spatial_correlation deals with the detection and the measurement of spatial correlation. Finally, @sec-spatial_models present some popular spatial models, namely the SAM and the SEM models.

## Simple features {#sec-simple_features}

In a spatial statistical analysis, the first task is to get geographical information about the
observations. This is usually done by importing external files in
**R**, for example shapefiles. This task is performed by the **sf**
library and the result is a `sf` object. 

### Structure of a simple feature

To understand what a simple feature is, it is best to
construct a simple one "by hand". The geographical representation of
an observation is a `sfg` for simple feature geometry. It can be a
point, a set of points that forms a line or a polygon if the line ends
where it started.

We first load the **sf** package:

```{r message = FALSE}
library(sf)
```
The message indicates that **sf** uses three important external
libraries:

- **GEOS** to compute topological operations,
- **GDAL** to read external files with a large variety of format,
- **PROJ** to transform the coordinates in a specific **CRS**
  (coordinate reference system). 

We consider in this example the three main cities in
France. For Paris, the latitude^[The latitude is a coordinate that
specifies the north/south position of a point on earth. Its is 0 at
the equator and 90 at the poles. The value is positive in the northern
emispherus and negative in the northern hemisphere and positive in the
southern hemisphere.] is $48.87$ and the longitude^[The longitude is a
coordinate that specifies the east/west position of a point on
hearth. Meridians are lines that connect the two poles and the
longitude is the angular value of the position of a point to the
reference (Greenwhich) meridian.] is $2.33$.
Most of **sf**'s function starts with `st_`. The `st_point` function can be used to construct a `sfg` for Paris, the argument being a numeric vector containing the longitude and the latitude.

```{r }
paris <- st_point(c(2.33, 48.87))
paris
```
We then perform the same operation for Lyon and Marseilles:

```{r }
lyon <- st_point(c(4.85, 45.76))
marseille <- st_point(c(5.37, 43.30))
```

We then construct a `sfc` (simple feature column) which is a set
of `sfg`s, using the `st_sfc` function. The different elements are entered as unnamed arguments of
the function and a `crs` argument can be used to specify the
**CRS**. Three formats exist to describe the **CRS**: **proj4string** (a character string), **EPSG** (an integer), and **WKT**, for well known text (a list).
We can use here either `"+proj=longlat +datum=WGS84 +no_defs"` or
`4326`. The datum is the representation of the earth from which the
latitudes and the longitudes are computed. 

```{r }
cities_coords <- st_sfc(paris, lyon, marseille, crs = 4326)
cities_coords
```
Printing the `sfc` gives interesting information, as the bounding box
(the coordinates of the rectangle that contains the data) and the **CRS**.
Finally, we can construct a `sf` object by putting together a tibble containing information about the cities and the `sfc`. 

```{r }
cities_data <- tibble(pop = c(2.161, 0.513, 0.861),
                      area = c(105.4, 47.87, 240.6))
cities <- st_sf(cities_data, cities_coords)
cities
```

A `sf` is just a data frame with a specific column that contains the
coordinates of the observations. This column (called geometry) can be
extracted using the `st_geometry` function:

```{r }
#| results: "hide"
#| echo: true
cities %>% st_geometry
```

and is "sticky", which means that if some column are selected, the
geometry column is always returned, ie:

```{r }
#| results: false
cities %>% select(pop)
```

returns `pop` **and** `cities_coords`, even if the latter was not selected.
We can then plot our three cities, along with a map of France, which
is obtained using the `geodata::gadm` function.^[This function is an
interface to the `http://www.gadm.org` site which provides the
boundaries of all countries in the world with different administrative
level. The second argument set to 1 means that we want the boundaries
of French regions and the third argument set to `"."` means that the
file is stored in the current directory.]. 

```{r }
france <- geodata::gadm("FR", 1, ".")
```

`france` is not an object of class `sf`,^[It is actually an object of class `SpatVector`, defined in the **terra** library.] so we first coerce it to a
`sf` and then we extract the geometry and the series called `NAME_1` which contains the name of the regions:

```{r }
france <- france %>% st_as_sf %>%
    select(region = NAME_1) %>%
    st_set_crs(4326)
```
Imported vector data are often large and unnecessary detailed:

```{r }
#| collapse: true
france %>% object.size %>% format("MB")
```

and they can be simplified using the `rmapshaper::ms_simplify`
function, with a `keep` argument which indicates the degree of
simplification ($0.01$ means that we seek to obtain a `sf` 100 times
lighter than the initial one).

```{r }
#| collapse: true
france <- france %>%
    rmapshaper::ms_simplify(keep = 0.01)
france %>% object.size %>% format("MB", digits = 2)
```

**sf** provides a `plot` method to get quickly a map of the data. Note that a thematic map is plotted for all the series of the `sf` and it is therefore recommended to select first a unique series. If one is only interested in the vectors, they can be extracted before plotting using `st_geometry`:

```{r}
#| results: hide
#| eval: false
france %>% st_geometry %>% plot
```

For more advanced maps, several specialized packages are available, in particular **tmap** and **mapsf**. In this chapter, we'll only use **ggplot**, which provides a `geom_sf` function. This geom is very special compared to other geoms as the kind of elements that are plotted depends on the geometry column of the `sf`: if `cities` is provided as the data argument, points will be plotted but if `france` is provided, polygons will be drawn. In the following code, we start with `france` as the data argument of `ggplot` and then the call to `geom_sf` results in the drawing of the administrative borders of French regions. Then we use a second time `geom_sf` with this time `cities` as data argument, so that points are drawn for the three cities. We use here `aes(size = pop)` so that the size of the points is related to the population of the cities. The result is presented in @fig-map_france

```{r }
#| fig-cap: "Map of France with its three main cities"
#| label: fig-map_france
france %>% ggplot() + geom_sf() +
    geom_sf(data = cities, aes(size = pop))
```

**sf** provides several functions to compute values of interest, like
distance between two points. For example, to get the distance from
Paris to Lyon and Marseille, we would use:

```{r }
st_distance(cities[1, ], cities[2:3, ])
```
`st_distance` always returns a matrix, the number of elements of the
first (second) argument being the number of rows (columns). If only
one argument is supplied, the result is a square matrix with 0 on the
diagonal. Note that the numbers have a unit of measurement, which is here meters. To define a unit and to convert from one unit to another, the **units** package provides the `set_units` function. Consider the following example: we first provide a numeric ($42.195$), we define its unit as being kilometers, and then we convert it to miles:

```{r}
#| collapse: true
library(units)
d <- 42.195
dkm <- d %>% set_units(km)
dkm
dkm %>% set_units(miles)
```

Even when the conversion is simple, it is advisable to use `set_units` instead of applying the conversion "by hand":

```{r}
#| collapse: true
dm <- dkm %>% set_units(m)
dm
dkm * 1000
```

The numerical values are the same, but in the second case the unit hasn't changed and is still kilometer.

`st_area` computes the area of a polygon.

```{r }
st_area(france) %>% units::set_units(km ^ 2)
```

### Computation on sf objects {#sec-sf_computation}

The regression discontinuity framework can be adapted to consider
geographic discontinuities. Some entities are considered on both sides
of a border and the forcing variable is then the distance to the
border.^[with the convention that the sign of the distance is
different on both sides of the border.]
The `us_counties` data set contains the borders of US counties:

```{r }
#| message: false
us_counties %>% print(n = 3)
```

@KUMA:18 investigates the effect of a legal restriction on home equity
extraction which is specific to Texas on mortgage defaults. He
measures mortgage default rates in Texas and in the bordering states
and compares mortgage default rates for counties which are closed to
the Texas border (on both sides). Let's first plot the map of the
counties, using `ggplot` and `geom_sf` (see @fig-counties):

```{r }
#| label: fig-counties
#| fig-cap: Map of American Counties
#| warning: false
us_counties %>%
    filter(! state %in% c("Alaska", "Hawaii")) %>%
    ggplot() + geom_sf()
```

The polygons can be merged using the `group_by` / `summarise`
functions of **dplyr**. If the `sf` is grouped by states, the
statistics computed by the `summarise` function are performed at the state level and
the `geometry` now contains the coordinates of the states. With a void
call to `summarise`, we get only this new `geometry` and we can use it to
plot a map of states (see @fig-states):

```{r }
#| label: fig-states
#| fig-cap: Map of American States
#| warning: false
states <- us_counties %>%
    filter(! state %in% c("Alaska", "Hawaii")) %>%
    group_by(state) %>%
    summarise()
states %>% ggplot() + geom_sf()
```

To get the border states of Texas, we use spatial indexation, ie we use the `[`
operator. The first argument is used to select rows, it is normaly a
vector of integers (the positions of the lines to extract), a vector
of characters (the names of the lines to extract) or a logical
vector. Here we can index a `sf` by another `sf` and the result is (by
default) a new `sf` containing the elements of the first one which has
any common point with the second one:

```{r }
border_states <- states[filter(states, state == "Texas"), ]
border_states
```

We then compute the Texas border. It is defined by the common points
of the borders of Texas and its border states. This is obtained using the
`st_intersection` function which return 4 lines (one for each border
state) and the border is then obtain by merging these 4 lines using
the `st_union` function. The result is presented on @fig-border.

```{r }
#| fig-cap: Border States and Texas Borders
#| label: fig-border
#| warning: false
texas <- filter(border_states, state == "Texas") %>% st_geometry
border <- filter(border_states, state != "Texas") %>% st_geometry
border <- st_intersection(texas, border) %>%  st_union
border_states %>% ggplot() +
    geom_sf() +
    geom_sf(data = border, color = "red", linewidth = 1) +
    geom_sf_label(aes(label = state)) 
```

Note the use of `geom_sf_label` function which is a specialized version of `geom_label` which is suitable for writing labels on a map.
We then select the counties that belong to any of these states and
we compute the distance to the Texas border. It is defined as the
distance between the centroid of the county and the closest point of
the border. The centroids are obtained using the `st_centroid`
function. We've already used `st_distance` to compute the distance between two points. It can also be used to compute the (shortest) distance between a point and a line:

```{r }
border_counties <- us_counties %>%
    filter(state %in% pull(border_states, state))
centroids <- border_counties %>% st_geometry %>% st_centroid
dists <- st_distance(centroids, border)[, 1] %>% set_units(km)
border_counties <- border_counties %>% add_column(dists)
head(dists)
```

`st_distance` returns a matrix of distance, each column corresponding
to a line of the second argument. As here there is only one line, we
convert this matrix to a vector by taking its first column. The unit of the returned values is meter, we convert it in miles, like in the original article and we add this column to the sf.
As in the original article , we draw on @fig-close_counties_dist counties
which are less than 25, 50, 75 and 100 miles from the border:


```{r }
#| label: fig-close_counties_dist
#| fig-cap: Counties close to the Texan border
#| warning: false
border_counties %>%
    mutate(dist_class = cut(dists, c(0, 25, 50, 75, 100))) %>%
    ggplot() + geom_sf(aes(fill = dist_class)) +
    scale_fill_discrete(na.translate = FALSE) +
    scale_fill_brewer(na.translate = FALSE) + 
    geom_sf(data = border) + theme_minimal()
```

Finally, we select the relevant series from the data set of the
paper, called `mortgage_default`, and we merge it with
`border_counties`:

```{r }
mortgage_defaults <- mortgage_defaults %>%
    full_join(border_counties, by = c("fips")) %>%
    st_sf(agr = "geometry") %>%
    mutate(dists = ifelse(state == "Texas", dists, - dists))
```

Note that the first argument of `dplyr::full_join` is a tibble, so
that the result is also a tibble. To coerce it to a `sf`, we use the
`sf::st_sf` function. We've already used this function in @sec-simple_features and we provided as argument a data frame and a `sfc`.  Here, the `sfc` is a column of the data frame called `"geometry"`. Then, we provide as the `agr` argument of `st_sf` the character `"geometry"`.
Finally, we plot the discontinuity on @fig-disc_texas.

```{r }
#| warning: false
#| fig-cap: "Discontinuity of the mortgage defaults data set"
#| label: fig-disc_texas
mortgage_defaults %>%
    filter(abs(dists) < 50) %>% 
    mutate(state = ifelse(state == "Texas", "Texas", "other")) %>% 
    ggplot(aes(dists, default)) +
    micsr::geom_binmeans(aes(size = after_stat(n)), shape = 21) +
    geom_smooth(aes(linetype = state, weight = loans), 
                method = "lm", se = FALSE)
```

The intercept is a bit more than 7% outside the border and about 4.5% inside the border, so that the effect of the Texas specific regulation seems significant (a reduction of about 2.5% of the mortgage default rate).

## Two examples

To illustrate the techniques presented in the subsequent sections, we'll use to data sets. The first one, from @WHEE:03 is called `agglo_growth` and deals with the topic of economies and diseconomies of agglomeration. The second one, called `sp_growth` is from  @ERTU:KOCH:07 and is used to fit an extension of Solow's grothw model.

### Agglomeration economies and diseconomies

The `agglo_growth` contains data about US counties in 1990, identified by their fips code.

```{r }
agglo_growth
```

`emp_gr` and `pop_gr` are the employment and the population growth in each county between 1980 and 1990 and `emp` and `pop` the level of employement and of the population in 1980. 
@WHEE:03 investigates the existence of:

- agglomeration economies, which implies that the growth of a given
  territory will be positively correlated with its size, 
- agglomeration diseconomies for large cities which experience
  congestion, crime, pollution, etc.

The hypothesis is therefore that the relation between size and growth
should be inverted U shaped, which means that for small territories, the
agglomeration economies effect is dominant, as for large territories,
the agglomeration diseconomies becomes dominant. We'll
reproduce some of his results using the counties of Louisiana. We first
join the tibble to the `sf` called `us_counties` which contains the
geometries of the counties that we've already used in the previous section:

```{r }
louisiana <- us_counties %>%
    right_join(agglo_growth, by = "fips") %>%
    filter(state == "Louisiana")
```
Note the use of the quite unusual `dplyr::right_join` function. By using `right_join` with `us_counties` as the first argument and `agglo_growth` as the second argument, we get an object of the class of the first argument (therefore a `sf`) and we get all the lines of the `agglo_growth` tibble and only thos of `us_counties` that match.

We first plot a thematic map (@fig-map_louisiana), with colors for counties related to the
growth of the population (`pop_gr`). It is best create first a
categorical variable using `base::cut`, because it is easier to visualize a discrete pallette of colors than a continuous one. We use `scale_fill_brewer` to use one of the palettes provided by the **RColorBrewer** package, which provides sequential, diverging and qualitative palettes. The first two are suitable to represent the values of numeric variables. Sequential palettes use a color, light for low values of the variable and dark for high values. Divergent palettes use two colors, with light colors for middle range values and dark colors for low and high values. All the available palettes can be displayed using `RcolorBrewer::display.brewer.all()`

```{r }
#| label: fig-map_louisiana
#| fig-cap: "Population growth and initial population in Louisiana"
louisiana %>%
    mutate(pop_gr = cut(pop_gr, (-3:3) / 10)) %>% 
    ggplot() + geom_sf(aes(fill = pop_gr)) +
    scale_fill_brewer(palette = "Oranges")
```

The inverted U shaped hypothesis between the log of the initial
population and the growth rate can be tested by regressing the growth
rate on the log population and its square if the later coefficient is
negative.

```{r }
mod1 <- lm(pop_gr ~ poly(log(pop), 2, raw = TRUE), louisiana)
mod1 %>% sight
```

```{r}
#| echo: false
b1 <- coef(mod1)[2]
b2 <- coef(mod1)[3]
lpops <- - b1 / 2 / b2
pops <- exp(lpops)
```

The coefficient of the square term is significant and we get a maximum
growth for the fitted model as a value of `log(pop)` equal to $`r round(b1, 4)`
/ `r round(b2, 4)` / 2 = `r round(lpops, 4)`$, ie a population of about `r round(pops / 1E03)`  thousands
inhabitants. 
This result is illustrated in @fig-iushaped_louisiana with a scatterplot with the
growth of population and the logarithm and adding a fitting line with a second degree polynomial:

```{r }
#| label: fig-iushaped_louisiana
#| fig-cap: "Relation between initial population and population growth in Louisiana"
louisiana %>% ggplot(aes(pop, pop_gr)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ poly(x, 2, raw = TRUE), 
                se = FALSE) + 
  scale_x_continuous(trans = "log10")
```

### Solow model

The second example is @ERTU:KOCH:07 who estimated a growth model for
the countries of the world taking spatial correlation into account:

```{r}
data("sp_solow", package = "countries")
sp_solow
```

There are 91 countries in the `sp_solow` data set, the series being the gdp in 1960 and 1995, the saving rate (`saving`) and the growth of the labor force (`labgwth`).
Following @ERTU:KOCH:07, we compute the relevant variables
for the estimation of the growth model:

- the log of the saving rate (`lns`),
- the log of the sum of the growth rate of the labor force, the rate
  of depreciation and the rate of technical progress; as the later two
  variables are difficult to measure, it is assume that their sum is
  equal to 0.05 for all the countries (`lnngd`),
- the annual growth rate, the difference of the logs of the gdp for
  1995 and 1960 divide by 35 (`growth`).

```{r }
sp_solow <- sp_solow %>%
    mutate(lns = log(saving), lnngd = log(labgwth + 0.05),
                    growth = (log(gdp95) - log(gdp60)) / 35)
```

We then need to join this data frame to a sf containing the administrative boundaries of the countries and the coordinate of their capital. Several packages enables to get a world `sf`. For example the **spData** package has a `world` `sf` which is obtained from Natural Earth, **geodata** has a `world` function that enables to download from the gadm site an object of class `SpatVector` (than can be easily converted to a `sf`). We use here the convenient **countries** package that also use Natural Earth and provides a `countries` function with different arguments to select a subset of countries of the world. By default, `countries` return 199 lines, the 193 United Nations' recognized countries, the 2 observer countries (Palestine and Vatican) and 4 not or not fully recognized countries (Kosovo, Somaliland, Northern Cyprus and Taiwan). Each line includes the geometry of the "main" part of the countries. Some countries have "parts" or "dependencies" that can be included using `part = TRUE` and `dependency = TRUE`. A "part" is an area with the same political status that the rest of the country, but is far from the main part of the countries. Examples of parts include Alaska and Hawaii for the United States, Martinique and Guadeloupe for France and Canaries for Spain. A "dependency" is an area with a specific political status. Examples of dependencies are Greenland for Denmark, New Caledonia for France and Gibraltar for the United Kingdom.

Either `name` or `code` can be used to joint `sp_solow` with the
`countries`' object. It is much safer to use `code` as it avoids the
problem of small differences in countries' names. A lot of
countries of the world are not present in `sp_solow` (especially most
of the communist countries). We check whether all the the
countries of `sp_solow` are present in the `countries` object, with the `check_join` function; the `by` argument indicates the series in the `sp_solow` tibble that identified the country:

```{r }
#| collapse: true
library(countries)
countries() %>% check_join(sp_solow, by = "code")
```

The two problems are that the D.R. Congo (`iso3` code `COD`) used to be
called Zaire (`iso3` code `ZAR`) and that Hong Kong, which is a part
of a China in `ne_countries` was considered as a sovereign country by
the time of the study. We then correct the code for the D.R. of Congo, we add Hong Kong using the `include` argument and we remove `Antarctica` with the exclude argument:

```{r eval = TRUE}
sp_solow <- sp_solow %>% 
  mutate(code = ifelse(code == "ZAR", "COD", code))
sps <- countries(include = "Hong Kong", exclude = "Antarctica") %>%
    select(iso3, country, point) %>% 
    left_join(sp_solow, by = "code") %>% select(- name)
```

Note that the resulting `sf` has two geometry columns, polygons for the
borders of the countries (`polygon`) and points for the capitals
(`point`). Note also that the active geometry column is
`polygon`. 
We then draw on @fig-growth_gdp a world map with the color of the countries related to
the annual growth during the 1960-95 period and a point with a size
related to the initial (1960) gdp. `countrie` provides a `plot` method which draws a basic map using **ggplot**. A basic thematic map can be drawn using the `fill` argument to fill the areas of the countries and the `centroid` or the `capital` arguments to draw a point at the position of the centroid or the capital of each country. Any **ggplot** functions can be used to customize the graphic:

```{r eval = TRUE}
#| label: fig-growth_gdp
#| fig-cap: "Growth and initial gdp, 1960-1995"
#| out-width: "100%"
#| fig-width: 7
sps %>% plot(fill = "growth", centroid = "gdp60") +
  scale_size_continuous(range = c(0.5, 3)) + 
  theme(legend.position = "bottom") + 
  guides(size = guide_legend(reverse = TRUE),
         fill = guide_legend(nrow = 2, byrow = TRUE))
```

The basic Solow model, which was given in @sec-solow_equation is then estimated:

```{r }
lm(log(gdp95) ~ lns + lnngd, sp_solow) %>% sight
```

Remind that the structural model implies that $\beta_s = - \beta_v = \kappa / (1 - \kappa)$. This hypothesis can be tested using a reparametrization of the model:

```{r}
lm(log(gdp95) ~ I(lns - lnngd) + lnngd, sp_solow) %>% sight
```
for which the hypothesis is that the coefficient of `lnngd` equals 0. This hypothesis is rejected at the 5% level (but not at the 1% level). Imposing this hypothesis, we get:

```{r}
#| echo: false
z <- lm(log(gdp95) ~ I(lns - lnngd), sp_solow)
b <- unname(coef(z)[2])
kappa <- b / (1 + b)
```

```{r}
lm(log(gdp95) ~ I(lns - lnngd), sp_solow) %>% sight
```

which implies a value of $\kappa$ (the share of the capital in the gdp) equal to $\hat{\beta}_s / (1 + \hat{\beta}_s) = `r round(kappa, 2)`$ which is implausibly high.

## Spatial correlation {#sec-spatial_correlation}

Spatial correlation occurs when one can define a distance relationship
between observations. The notion of distance is broad, but we'll
consider in this section only geographical distance. For each
observation, one can in this case define a set of neighboors. If the
geographical representation of an observation is a **polygon** (which
is the relevant choice for countries or region), two observations can
for example said to be neighboors if they have a common border. If the
geographical representation of an observation is a **point** (for
example for a city), two observations are neighboors if the distance
between them is less than, say, 100 kilometers. Once the set of
neighboors have been defined for every observations, weights can be
computed. The weights can be equal or may depend on the distance
between the observation and its neighbors.

These two operations of defining the set of neighbors and their weights are performed by the **spdep** package. 

### Contiguity and weights

To get the matrix of contiguity, we use the `spdep::poly2nb` function:

```{r }
#| message: false
library(spdep)
nb_louis <- poly2nb(louisiana)
nb_louis
```

```{r}
#| echo: false
nlinks <- sapply(nb_louis, length) %>% sum
ncount <- length(nb_louis)
```

The print method indicates the number of contiguity links
(`r nlinks`). Instead of storing this links in a full matrix (with 1 for
contigus counties and 0 otherwise) which would have $`r ncount`^2 = `r ncount^2`$
cells with only `r nlinks` of them with a value of 1 (about `r round(nlinks / ncount ^ 2 * 100, 1)`%), a `nb`
object is returned. It is a list of `r ncount` vectors which contain, for each
observation the positions of its neighboors. For example:

```{r }
nb_louis %>% head(3)
```
The first two counties have 5 neighboors and the third one has 6. A
`summary` method provides more details. `poly2nb` has a `queen`
argument which is `TRUE` by default. Queen contiguity means that two
polygons which have one common point are neighboors. Setting `queen`
to `FALSE` implies that rook continuity is used. In this case, only
counties which have a common border are neighboors. 

```{r }
nb_louis_rook <- poly2nb(louisiana, queen = FALSE)
```

```{r}
#| echo: false
nlinks_rook <- sapply(nb_louis_rook, length) %>% sum
```

Printing `nb_nc_rook`, one can check that the number of contiguity
links (`r nlinks_rook`) is slightly lower than previously (`r nlinks`). `nb` objects
can't be plotted as is with **ggplot**. We provide a convenient `st_nb`
function which perform this task, as it returns a `sf` object (see @fig-links):

```{r }
#| label: fig-links
#| fig-cap: "Queen and rook links for the counties of Louisiana"
louisiana %>%
    ggplot() +
    geom_sf(fill = NA) +
    geom_sf(data = st_nb(louisiana), color = "red") +
    geom_sf(data = st_nb(louisiana, queen = FALSE), color = "blue") +
    theme_minimal()
```

We first plot queen contiguity links in red and then rook contiguity
links in blue, so that the specific queen contiguity links appears in
red.
A matrix of weight is obtained by using the `nb2listw` function:

```{r }
W_louis <- nb_louis %>% nb2listw
```
A `listw` object is returned, which contains two lists: the first one
(`neighbours`) is the same as the one of the `nb` object. The second
one contains weights:

```{r }
W_louis$weights %>% head(3)
```

We can see that weights of neighbors for a given observation are all
equal and sum to one (0.2 for the first two observations which have 5 neighbors and $1/6$ for the third one which has 6 neighbours). Therefore, premultiplying a vector $(y - \bar{y})$ by $W$
results in a vector with typical value $\sum_m w_{nm}
y_m - \bar{y} = \tilde{y}_n - \bar{y}$
`nb` and a `listw` objects can be coerced to matrices using the
`nb2mat` and `listw2mat` functions.

Instead of defining the neighborhood as common borders, one can consider distance between points, for example the capitals or the centroids of the countries. Remind that `sps` contains two `sf`, `polygon` and `point` and that the active geometry is `polygon`. We first specify `point` as being the active geometry, using the `st_set_geometry` function and we remove all countries for which the data are not available:

```{r }
sp_solow2 <- sps %>% na.omit %>% st_set_geometry("point")
```

Then, we use `dnearneigh` function to compute the set of neighbors for each country: the first argument is the `sf` and the next two arguments, called `d1` and `d2` are mandatory and should be set to the minimum and the maximum distances that should be used to define the neighbors. Note that this distance should be indicated in kilometers if the CRS is geographical, which is the case here.

```{r}
d <- dnearneigh(sp_solow2, 0, 1000)
d
```

```{r}
#| echo: false
nlinks <- sum(sapply(d, function(x) ifelse(length(x) == 1 && x == 0, 0, length(x))))
ncount <- length(d)
```

With a distance of 1000 km, there are `r nlinks` links, an average of `r round(nlinks / ncount, 2)` neighbors per country. Note that 18 countries have no neighbors. We then compute the weights, using `nb2listwdist`. The first two argument are a `nb` and a `sf` objects. The `type` argument indicates how the weights should be computed. Denoting $d_{ij}$ the distance between the two unit $i$ and $j$, with `type = "idw"`, we get $w_{ij} = d_{ij} ^ {- \alpha}$. With `type = "exp"`, the weights are $w_{ij} = \mbox{exp}(- \alpha d_{ij})$. The `alpha` argument controls the value of $\alpha$ which is 1 by default. For example, if `type = "idw"` and `alpha = 2`, the weights are the inverse of the square of the distance. Once the weights have been computed, they can be normalized in different ways, using the `style` argument. It `stype = "raw"` (the default), no normalization is performed. A usual choice is `"W"` where the weights of a unit are normalized so that they sum to 1:

```{r}
w <- nb2listwdist(d, sp_solow2, type = "idw", alpha = 1, 
                  style = "W", zero.policy = TRUE)
```


```{r}
#| include: false
sp_solow2[d[[6]], ]
w$weights[[11]]
w$neighbours[[11]]
```

```{r}
#| include: false
#| eval: false
wa <- countries("Western Africa", crs = 32630, extend = 2, capital  =TRUE, towns = 2E06, lang = "es")
burk <- wa %>% filter(country == "Burkina Faso") %>% pull(point)
burk_buffer <- st_buffer(burk, set_units(1000, km))
wa %>% plot(labels = c("country", "capital", "towns")) + geom_sf(data = burk_buffer, fill = NA)
st_set_geometry(wa, "point")[burk_buffer, ]



west_africa <- countries("Western Africa") %>% st_transform(32630)
west_africa <- countries("Western Africa") %>% st_transform(32630)
west_africa <- west_africa %>% st_set_geometry("point") %>% st_transform(32630)
p1 <- west_africa %>% ggplot + 
  geom_sf(data = st_set_geometry(west_africa, "polygon")) +
  geom_sf(data = west_africa)
burkina <- west_africa %>% 
  filter(country == "Burkina Faso") %>% st_geometry
burkina_buffer <- st_buffer(burkina, set_units(1000, km))
p1 + geom_sf(data = burkina_buffer, fill = NA) + ggrepel::geom_label_repel(aes(label = country, geometry = west_africa$point), stat = "sf_coordinates")
#burkina: 11

burk <- sp_solow2 %>% filter(country == "Burkina Faso")
sf_use_s2(FALSE)
dists <- st_distance(sp_solow2, burk) %>% drop %>% set_units(km)
neigh <- (dists < set_units(1000, km) & dists > set_units(0, km))
which(neigh)
dn <- dists[neigh]
wn <-  1 /dn
s2f <- wn / sum(wn)
```

### Tests for spatial correlation

The spatial correlation hypothesis can be tested either for a specific
variable or using the residuals of a linear model regression. Two
principal tests have been proposed. The first one is the Moran test: denoting $W$ the weights matrix where the weights sum to one for a given line, the statistic is defined as:

$$
I = \frac{(y - \bar{y})^\top W (y - \bar{y})}{(y - \bar{y})^\top (y - \bar{y})} =
\frac{\sum_n (y_n  - \bar{y})(\tilde{y}_n - \bar{y})}{\sum_n (y_n  - \bar{y})^2}
$$

which is simply the ratio of the covariance between $y$ and
$\tilde{y}$ and the variance of $y$. It therefore can be obtained as
the coefficient of $y$ in a linear regression of $\tilde{y}$ on
$y$. If the variance of $\tilde{y}$ and $y$ were equal, it would also
be the coefficient of correlation between the value of $y$ for units and their neighbors.
The Moran test is computed using the `spdep::moran.test` function
which takes as argument a series and a `listw` object:

```{r }
#| collapse: true
moran.test(louisiana$pop_gr, W_louis) %>% sight
```

```{r }
#| include: false
mor <- moran.test(louisiana$pop_gr, W_louis)
stat_mor <- mor$estimate[1] %>% unname
mu_mor <- mor$estimate[2] %>% unname
sd_mor <- mor$estimate[3] %>% unname %>% sqrt
```

```{r include = FALSE}
Y <- louisiana$pop_gr
N <- length(Y)
Wmat <- W_louis %>% listw2mat
DX <- outer(Y, Y, "-") ^ 2
(sum(Wmat * DX) / N) / (sum((Y - mean(Y)) ^ 2) / (N - 1)) / 2 
geary.test(louisiana$pop_gr, W_louis) %>% .[["estimate"]] %>% unname %>% .[1]
mrn <- moran.test(louisiana$pop_gr, W_louis) %>% .[["estimate"]] %>% unname %>% .[1]
(N - 1) / N / 2 * (1 - 2 * mrn + sum( (Y - mean(Y)) ^ 2 * apply(Wmat, 2, sum)) / sum( (Y - mean(Y)) ^ 2))
```

The value of the statistic is $`r round(stat_mor, 3)`$ in our example.  Under the
hypothesis of no correlation, the expected value of this statistic is
$- 1 / (N-1) = `r round(mu_mor, 3)`$ and the standard deviation is $`r round(sd_mor, 3)`$.^[The expression of the variance of the Moran statistic can be found in
@BIVA:WONG:18] The standardized statistic ($`r round((stat_mor - mu_mor) / sd_mor, 3)`$) is asymptotically normal and the hypothesis of no spatial correlation is rejected.
An alternative to Moran's $I$ test is Geary's $C$ test, which is
defined as:

$$
C = \frac{N - 1}{2N}\frac{\sum_n \sum_m w_{nm} (y_n - y_m) ^ 2}{\sum_n
(y_n - \bar{y})^ 2}
$$

Introducing deviations from mean in the previous expression and developing, we get:

$$
C = \frac{1}{2}\frac{N - 1}{N} \left(1 - 2 I + \frac{\sum_n (y_m-
\bar{y}) ^ 2 \sum_m w_{mn}}{\sum_n (y_n - \bar{y}) ^ 2}\right)
$$

If the weight matrix were symetric, as $\sum_n w_{nm} = 1$, we also
would have $\sum_m w_{mn} = 1$, so that the last term is one and $C =
\frac{N - 1}{N}(1 - I)$. Therefore, we can expect $C$ to be close to
$(1 - I)$. Geary's test is implemented in the `spdep::geary.test` function

```{r }
#| collapse: true
geary.test(louisiana$pop_gr, W_louis) %>% sight
```
These tests are unconditional tests of spatial correlation. The same
tests can be performed on the residuals of least squares models.

```{r }
#| collapse: true
model_louisiana <- lm(pop_gr~poly(log(pop),2,raw =TRUE), louisiana)
lm.morantest(model_louisiana, W_louis) %>% sight
```
The hypothesis of no spatial correlation is still rejected at the 5%
level, but the p-value is much higher than the one associated with the
unconditional test. 

@ERTU:KOCH:07 compute the Moran test for the residuals of the OLS estimation of the standard Solow's growth model. The set of neighbors is obtained without setting any maximum distance, so that all countries are neighbors to each other. Two matrices of weights are computed: one with $w_{nm} = d_{nm}^{-2}$ ($W1$) and the other with $w_{nm} = \mbox{exp}(-2 d_{nm})$ ($W2$):

```{r}
#| collapse: true
loglm_solow <- loglm(gdp95 ~ lns + lnngd, sp_solow2)
class(loglm_solow) <- c("micsr", "lm")
lm_solow <- lm(log(gdp95) ~ lns + lnngd, sp_solow2)
d <- dnearneigh(sp_solow2, 0, Inf)
W1 <- nb2listwdist(d, sp_solow2, type = "idw", alpha = 2,
                   style = "W", zero.policy = TRUE)
W2 <- nb2listwdist(d, sp_solow2, type = "exp", alpha = 2,
                   style = "W", zero.policy = TRUE)
lm.morantest(lm_solow, W1) %>% sight
lm.morantest(lm_solow, W2) %>% sight
```

Whatever the weighting matrix, the hypothesis of no spatial correlation is rejected.

```{r}
#| include: false
#| eval: false
locmor <- localmoran(resid(lm_solow), W1)
z_locmor <- locmor %>% as_tibble %>% pull(Ii) %>% as.numeric
z_locmor <- z_locmor %>% cut(c(-1, - 0.5, 0, 0.5, 1, 1.5, 2, 2.5, Inf))
sp_solow2 %>% st_set_geometry("polygon") %>% add_column(z = z_locmor) %>% ggplot + geom_sf(aes(fill = z)) + scale_fill_brewer(palette = "Blues")
```

### Local spatial correlation

A first glance of local spatial correlation can be obtained using the
Moran's plot, implemented in the `spdep::moran.plot` function, where
$y_n - \bar{y}$ is on the $x$ axis and $\tilde{y}_n - \bar{y}$ on the
$y$ axis. Therefore, both variables have zero mean and the intercept
of the fitting line is therefore 0, the slope being Moran's $I$ statistic:

```{r }
moran.plot(louisiana$pop_gr, W_louis)
```
Each observation is situated in one of the 4 quarters of
plane. Observations in the:

- the upper-right quarter are *"high-high"* observations, which means
  that the value of $y$ for observation $n$ and its neighbours are
  higher than the sample mean,
- the lower-left quarter are *"low-low"* observations, which means
  that the value of $y$ for observation $n$ and its neighbours are
  lower than the sample mean,
- the upper-left quarter are *"low-high"* observations, which means
  that the value of $y$ for observation $n$ are lower than the sample
  mean and its neighbours are higher than the sample mean,
- the lower-right quarter are *"high-low"* observations, which means
  that the value of $y$ for observation $n$ are higher than the sample
  mean and its neighbours are lower than the sample mean.

In case of no spatial correlation, the points should be randomly
disposed around the origin and the slope of the regression line should
be 0. On the contrary, in case of positive spatial correlation, a
majority of points should be of the *"low-low"* or *"high-high"*
cathegory and the regression line should have a positive slope. 

@ANSE:95 proposed local versions of Moran's $I$ and Geary's $C$ statistics. We then
have for each observation $I_n$ and $G_n$ so that $\sum_n I_n$ and
$\sum_n G_n$ are respectively proportional to the global Moran and
Geary statistics. Local Moran's statistics are defined as:

$$
I_n = (y_n - \bar{y}) \sum_m w_{nm} (y_m - \bar{y}) = (y_n - \bar{y})(\tilde{y}_n - \bar{y})
$$

and their sum is $(y_n - \bar{y})(\tilde{y}_n - \bar{y})$, which is
the numerator of the Moran's $I$ statistic. Therefore, we have:

$$
I = \frac{\sum_n I_n}{\sum_n (y_n - \bar{y}) ^ 2}
$$

Local Moran's statistics are obtained using the `spdep::localmoran` function:

```{r }
locmor <- localmoran(louisiana$pop_gr, W_louis)
```
which returns a matrix, with a line for every observations and columns
containing the local Moran values, their expectation, variance, the standardized
statistic and the p-value. It's easier to coerce this matrix to a
tibble in order to extract extreme values of the statistic:

```{r }
locmor %>% as_tibble %>% filter(`Pr(z != E(Ii))` < 0.01)
```
There are therefore 11 out of 64 (17%) observations for which the p-value is lower
than 1%, which confirms the presence of spatial correlation. The local
Moran statistic can also be represented in a map, in order to identify
the "hot spots":

```{r }
z_locmor <- locmor %>% as_tibble %>% pull(Ii) %>% as.numeric
z_locmor <- z_locmor %>% cut(c(-1, - 0.5, 0, 0.5, 1, 1.5, 2, 2.5, Inf))
louisiana %>% add_column(z = z_locmor) %>%
    ggplot + geom_sf(aes(fill = z)) + scale_fill_brewer(palette = "Blues")
```

Two hot spots are then identified, in the north-west and the
south-west of the state.

```{r include = FALSE, eval = FALSE}
crossproduct <- function(x, y){
    J <- length(x$weights)
    sapply(1:J, function(j) sum(y[x$neighbours[[j]]] * x$weights[[j]]))
y_yb <- crime_nc$crime_rate - mean(crime_nc$crime_rate)
as.numeric(crossprod(crossproduct(W, y_yb), y_yb)) / sum(y_yb ^ 2)
N <- length(y_yb)
S0 <- N
Wm <- W %>% listw2mat
S1 <- sum((Wm + (t(Wm))) ^ 2) / 2
S2 <- sum( (apply(Wm, 1, sum) + apply(Wm, 2, sum)) ^ 2)
S3 <- (sum(y_yb ^ 4) / N) / (sum(y_yb ^ 2) / N) ^ 2
S4 <- (N ^ 2 - 3 * N + 3) * S1 - N * S2 + 3 * S0 ^ 2
S5 <- (N ^ 2 - N) * S1 - 2 * N * S2 + 6 * S0 ^ 2
V <- (N * S4 - S3 * S5) / ( (N - 1) * (N - 2) * (N - 3) * S0 ^ 2) - 1 / (N - 1) ^ 2
```

## Spatial econometrics {#sec-spatial_models}


#### Spatial models and tests

We consider here linear gaussian models that are extended in order to
integrate the spatial feature of the sample, using the weighting
matrix described in the previous section. Two main models can be
considered. The first one is the **SEM** for spatial error model which
can be written in matrix form as:

$$
y = X \beta + \epsilon \; \mbox{with} \; \epsilon = \lambda W \epsilon + \eta
$$ {#eq-sem}

Therefore, the error for observation $n$ is linearly related to the
errors of its neighbors $\tilde{\epsilon}_n$. **OLS** estimation
gives unbiased and consistent estimators but, as always with
non-spherical errors, it is inefficient and the simple formula of the
covariance matrix of the coefficients ($\sigma ^ 2 (X^\top X) ^ {-1}$)
leads to a biased estimator.

The second model is either called **SAR** (spatial autoregressive
model) or **SLM** (spatial lag model). It extends the basic gaussian
linear model by adding as a regressor the mean value of the response
for the neighbor units. For one observation, the model writes $y_n =
\beta ^ \top x_n + \rho \tilde{y}_n + \epsilon_n$ or, in matrix
form:

$$
y = X \beta + \rho W y + \epsilon
$$
The reduced form of the model is:

$$
y = (I - \rho W) ^ {-1} X \beta + (I - \rho W) ^ {-1}\epsilon
$$  {#eq-sar}

Therefore, the values of $y$ depends on all the values of $\epsilon$
and $\tilde{y}_{n}$ is therefore correlated with $\epsilon_n$. The
**OLS** estimator is therefore biased and inconsistent. Moreover, in
@eq-sar, the spatial dependence in the parameter $\lambda$ feeds back
[@BIVA:MILL:PIRA:21], which is not the case for @eq-sem. This means
that for the **SEM** model, the marginal effect of a covariate is the
corresponding coefficient. This is not the case for the **SAR** model.
Moreover, the matrix $(I - \lambda W) ^ {-1}$ can be written as an
infinite series:

$$
(I - \rho W) ^ {-1} = I + \rho W + \rho^2 W^2 + \rho^3
W^3 + \ldots
$$

Spatial models are usually estimated by maximum likelihood, by
assuming a multivariate normal distribution for the iid errors. For
the **SEM** model, the idiosyncratic errors can be written as a
function of the response:

$$
\eta = (I - \lambda W) y - (I - \lambda W) X\beta
$$ {#eq-etasem}

so that the Jacobian of the transformation is 

$$\left| \frac{\partial \eta}{\partial y} \right| = 
\left| I - \rho W \right|$$. 

the likelihood is similar to the one of the linear
gaussian model except that an extra term, which is the log of the
Jacobian, should be added:

$$
-N/2 (\ln 2\pi + \ln \sigma ^ 2) + \ln \left| I - \lambda W \right|  -
\frac{1}{\sigma ^ 2} \eta^\top \eta
^ 2
$$

where $\eta$ is given by @eq-etasem.
For the **SAR** model, @eq-sar indicates that the Jacobian is the
same, so that the log-likelihood is:

$$
-N/2 (\ln 2\pi + \ln \sigma ^ 2) + \ln \left| I - \rho W \right|  -
\frac{1}{\sigma ^ 2} \epsilon^\top \epsilon
^ 2
$$

with $y - X \beta - \rho Wy$. 

The log likelihood function can be concentrated on $\beta$ and $\sigma
^ 2$ so that it can be expressed as a function of one parameter
($\lambda$ for the **SAR** model and $\rho$ for the **SEM** model).
These two models can be augmented by spatial lags of the covariates, they are in this case called Durbin's models. There is an interseting relationship between the Durbin's SAR model and the SEM model. The former can be written:

$$
y = \rho W y + X\beta +  X W \theta + \epsilon
$$

with $\epsilon$ iid. The **common factor** hypothesis is that $\theta = - \rho \beta$. In this case, we have:


$$
(I - \rho W) y =  (I - \rho W) X\beta + \epsilon
$$
Or finally:

$$
y =  X\beta + (I - \rho W) ^{-1} \epsilon
$$
which is the SEM model, as denoting $\nu = (I -\rho W) ^ {-1} \epsilon$ implies that $\nu = \rho W \nu + \epsilon$. 

Once it has been established that there is spatial correlation, on has to choose the "right" specification. Several tests have been proposed, based on the three tests principle. Lagrange multiplier tests proposed by ANSE:BERA:FLOR:YOON:96, are popular as they only require the OLS estimation. Different flavors of tests have been proposed, testing $\rho = 0$, \lambda = 0$ or $\rho = \lambda = 0$. The basic version of, for example the first test ($\mbox{H}_{0}: \rho = 0$) has, as a maintained hypothesis that $\lambda = 0$. A robust version of the test has been proposed which don't impose this maintained hypothesis. The common factor hypothesis can easily be tested using a likelihood ratio. The Wald statistic is less easy to compute, has a set of non-linear hypothesis $\theta = - \rho \beta$ is tested.

### Application to the growth model

To overcome the irrelevant results of the standard Solow model, @ERTU:KOCH:07 consider an endogenous growth model with spillovers. The production function is as usual a Cobb-Douglas:

$$
Y_n(t) = A_n(t) K_n ^ \kappa(t) L_n ^ {1 -\kappa}(t)
$$
$\kappa$ being the elasticity of the production with the capital and also the share of the profits in the national product. The level of technology is given by:

$$
A_n(t) = \Omega(0) e ^ {\mu t} k_n ^ \phi(t) \prod_{m \neq n} ^ N A_n^{\gamma w_{nm}}(t)
$$
where $k_n(t) = K_n(t) / L_n(t)$ is the physical capital per worker, $\Omega(0)$ is the initial level of the technology and $\mu$ is a constant rate of technological growth, as in the Solow model. The next term takes into account the spillover effect from capital investment, following @ROME:86 and the strength of this spillover effect is measured by $\phi$. The last term is specific the model of @ERTU:KOCH:07, for which the spillovers are not restricted to domestic investment, but concerns also the technology of neighbors countries. The effect of the technology of country $m$ on the technology of country $n$ is the product of a constant parameter $\gamma$ and a specific weight, $w_{nm}$, which is a decreasing function of the distance between both countries. @ERTU:KOCH:07 showed (equation 10, page 1038) that, at the steady state, the output per capita is:

$$
\begin{array}{rcl}
\ln y_n ^ * &=& \frac{1}{1 - \kappa - \phi} \ln \Omega + 
\frac{\kappa + \phi}{1 - \kappa - \phi} \ln s_n -
\frac{\kappa + \phi}{1 - \kappa - \phi} \ln (n_n + g + \gamma) \\
&-& \frac{\kappa  \gamma}{1 - \kappa - \phi} \sum_{m\neq n}^ N w_{nm} \ln s_m + \frac{\kappa  \gamma}{1 - \kappa - \phi} \sum_{m\neq n}^ N w_{nm} \ln (n_n + g + \gamma) \\
&+& \frac{\gamma(1 - \kappa)}{1 - \kappa - \phi} \sum_{m\neq n}^ N w_{nm} \ln y_m ^ *
\end{array}
$$
In matrix form, denoting $y = \ln y^*$, $X = (\ln s \ln v)$, $\beta = \left(\begin{array}{c} (\kappa + \phi) / (1 - \kappa - \phi)\\ - (\kappa + \phi) / (1 - \kappa - \phi)\end{array}\right)$, $\theta = \left(\begin{array}{c} \gamma \kappa / (1 - \kappa - \phi)\\ - \gamma  \kappa  / (1 - \kappa - \phi)\end{array}\right)$ and $\delta = \gamma (1 - \kappa) / (1 - \kappa - \phi)$:

$$
y = \alpha i + X \beta + WX \theta + \delta Wy + \epsilon
$$

if $\gamma = 0$, the model reduces to the model of @ROME:86 and the last three terms disappear. Note that in this case $\kappa$ and $\phi$ are not identified, but only their sum. Moreover, if $\phi$ is also 0, we get the Solow model. An interesting feature of the model of @ERTU:KOCH:07 is that their specification enables the identification of all the structural parameters of the model, $\kappa$, $\phi$ and $\gamma$. 

If $\phi = 0$, $\theta = \gamma \beta$ and $\delta = \gamma$, so that: $y = C + (I - \gamma W)X\beta + \gamma y + \epsilon$ so that the reduce form is:

$$
y = \alpha'i + X \beta + (I - \gamma W) ^ {-1} \epsilon
$$
which is a SEM model without Durbin terms. Therefore, the hypothesis that $\phi = 0$ is in the context of this model the hypothesis of common factors.

The structural growth model imposes that the coefficient of $\ln s_n$ is the opposite of the coefficient of $\ln v_n$. Therefore, to impose this constraint, we can use as a unique covariate $\ln s_n - \ln v_n$.

**spatialreg** provides functions to estimate these flavors of spatial model: `lagsarlm` for SAR models, `sacsarlm` for SEM model and `errorsarlm` for SAR-SEM models. The three first arguments are `formula`, `data` and `listw` which should be a `listw` object containing the weights. The `Durbin` argument enables to estimate the Durbin's versions of the model; this can be done either by setting it to `TRUE` (a spatial lag is then added for all the covariates) or a formula indicating the subset of covariates for which spatial lags had to be added.

```{r}
library(spatialreg)
ols <- loglm(gdp95 ~ lns + lnngd, sp_solow2)
sp_solow3 <- sp_solow2 %>% mutate(lns = lns - lnngd)
olsc <- update(ols, . ~ . - lnngd)
m1 <- lagsarlm(log(gdp95) ~ lns + lnngd, sp_solow2, W1, Durbin = TRUE)
m2 <- errorsarlm(log(gdp95) ~ lns + lnngd, sp_solow2, W1, Durbin = FALSE)
m3 <- sacsarlm(log(gdp95) ~ lns + lnngd, sp_solow2, W1, Durbin = TRUE)
m1c <- update(m1, . ~ . - lnngd, data = sp_solow3)
m2c <- update(m2, . ~ . - lnngd, data = sp_solow3)
m3c <- update(m3, . ~ . - lnngd, data = sp_solow3)
class(ols) <- class(olsc) <- c("micsr", "lm")
```

```{r}
#| echo: false
#| label: tbl-results_ertu_koch
#| tbl-cap: "Results of the growth models"
rawols <- lm(log(gdp95) ~ lns + lnngd, sp_solow2)
rawolsc <- lm(log(gdp95) ~ lns, sp_solow3)
library(modelsummary)
options(modelsummary_get = "broom")
modelsummary(list(OLS = ols, SAR = m1, SEM = m2, SARMA = m3, OLSc = olsc, SARc = m1c, SEMc = m2c, SARMAc = m3c),
             coef_omit = "Intercept",
             coef_map = c("lns" = "$\\ln s$", 
                          "lnngd" = "$\\ln n + g + \\delta$", 
                          "lag.lns" = "$W \\ln s$", 
                          "lag.lnngd" = "$W\\ln (n + g + \\delta)$", 
                          rho = "$\\rho$", 
                          lambda = "$\\lambda$"),
             gof_omit = "R2|AIC|BIC",
             escape = FALSE)
```

The results are presented in @tbl-results_ertu_koch. Starting from the OLS model, the hypothesis of no spatial correlation can be tested using Lagrange multiplier tests. `spdep::lm.LMtests` provides different flavors of these tests. 

```{r}
#| collapse: true
loglm_solow <- rawols
lm.LMtests(loglm_solow, W1, test = "LMerr") %>% sight
lm.LMtests(loglm_solow, W1, test = "RLMerr") %>% sight
lm.LMtests(loglm_solow, W1, test = "LMlag") %>% sight
lm.LMtests(loglm_solow, W1, test = "RLMlag") %>% sight
lm.LMtests(loglm_solow, W1, test = "SARMA") %>% sight
```



```{r}
#| eval: false
#| echo: false
lmtest::lrtest(m1c, m1)
lmtest::lrtest(m2c, m2)
lmtest::lrtest(m3c, m3)
lmtest::lrtest(rawols, rawolsc)
lm.LMtests(rawols, W1, test = "all")
loglm_solow <- loglm(gdp95 ~ lns + lnngd, sp_solow2)
lm_solow <- lm(log(gdp95) ~ lns + lnngd, sp_solow2)
lm.LMtests(lm_solow, W1, test = "LMerr") %>% class
save(lm_solow, W1, file = "~/Bureau/lm_solow.rda")
lmtest::lrtest(m1c, m2c)
```

The reduced form parameters $r$ are:

$$
r ^ \top = (\beta, \theta, \rho) =
\left(\frac{\kappa + \phi}{1 - \kappa - \phi}, - \gamma \frac{\kappa}{1 - \kappa - \phi}, \gamma \frac{1 - \kappa}{1 - \kappa - \phi}\right)
$$
The structural parameters $s$ can be expressed as a function of the reduced form parameters:

$$
s ^ \top = (\kappa, \phi, \gamma) = \left(\frac{\theta}{\theta - \rho}, \frac{\beta}{1 + \beta} - \frac{\theta}{\theta - \rho}, \frac{\rho - \theta}{1 + \beta}\right)
$$
To apply the delta method, we compute the matrix of derivatives of $s$ with respect to $r$:


$$
\Gamma(r) = \frac{\partial s}{\partial r ^ \top}(r)=
\left(
\begin{array}{rcl}
0                                       & - \frac{\rho}{(\theta - \rho) ^ 2} & \frac{\theta}{(\theta - \rho) ^ 2} \\
\frac{1}{(1 + \beta) ^ 2}               & \frac{\rho}{(\theta - \rho) ^ 2}   & - \frac{\theta}{(\theta - \rho) ^ 2} \\
- \frac{\theta - \rho}{(1 + \beta) ^ 2} & - \frac{1}{1 + \beta}                 & \frac{1}{1 + \beta}
\end{array}
\right)
$$
and the asymptotic variance of $s$ is:

$$
\hat{V}(\hat{r}) = \Gamma(\hat{r}) \hat{V}(\hat{s})\Gamma(\hat{r}) ^ \top
$$
We first compute the vector of the estimated structural parameters:

```{r}
beta <- unname(coef(m1c)["lns"])
theta <- unname(coef(m1c)["lag.lns"])
rho <- unname(coef(m1c)["rho"])
kappa <- theta / (theta - rho)
phi <- beta / (1 + beta) - theta / (theta - rho)
gamma <- (rho - theta) / (1 + beta)
s <- c(kappa, phi, gamma)
Vr <- vcov(m1c)[c("lns", "lag.lns", "rho"), c("lns", "lag.lns", "rho")]
G <- c(0, - rho / (theta - rho) ^ 2, theta / (theta - rho) ^ 2,
       1 / (1 + beta) ^ 2, rho / (theta - rho) ^ 2, - theta / (theta - rho) ^ 2,
       (theta - rho) / (1 + beta) ^ 2, - 1 / (1 + beta), 1 / (1 + beta))
G <- matrix(G, nrow = 3, byrow = TRUE)
V <- G %*% Vr %*% t(G)
stdev <- V %>% diag %>% sqrt
cbind(s, stdev) %>% round(5)
```


