# Count data {#sec-count}

```{r }
#| include: false
source("../_commonR.R")
```

It is often the case in economics that the response is a count, ie a
non-negative integer. In this case, fitting a linear model has a number of
drawbacks:

- the integer nature of the response is not taken into account,
- the fitted model can predict negative values for some values of the
  covariates,
- if the distribution of the response is asymmetric, the logarithm
  transformation can't be used in the common case where the response is 0 for a subset of observations.
  
There is therefore the need for specific models that will be presented
in this chapter. In @sec-features_count, we'll illustrate the features of count data, using a survey of empirical studies.
@sec-poisson_model is devoted to the benchmark count model, the Poisson model. @sec-overdisp_zero discusses two problems of count data: over-dispersion and excess of zero. Finally @sec-endog_count deals with the endogeneity problem with a count response. 

## Features of count data {#sec-features_count}

### An empirical survey

We start with the presentation of some data sets for which the
response is a count. All these data are available in the **micsr.count**
package and are summarized in @tbl-empsurvey.

```{r }
#| label: tbl-empsurvey
#| eval: true
#| echo: false
#| cache: true
#| tbl-cap: "Empirical survey of count data sets"
library("micr.count")
library("micsr")
library("tidyverse")
# CAME:TRIV:86 table 3 p. 47
p_doctor_aus <- glm(doctorco ~ sex + age + I(age ^ 2) + income + insurance + illness + actdays + hscore + chcond,
                    data = doctor_aus, family = poisson)

# WINK:04 table 4 p. 468
p_health_reform <- glm(visits ~ I(age / 10) + I(age ^ 2 / 1E03) + male + I(educ / 10) +
                        married + hsize + sport + health + sozh + log(income) + factor(year) +
                        quarter + empl,  data = health_reform, family = poisson)

# KENN:85
#strikes <- strikes %>% select(- duration) %>% distinct
p_strikes <- glm(strikes ~ output, data = strikes, family = poisson)

# TERZ:98 table 2 (pas identique car NLS)
p_trips <- glm(trips ~ workschl + size + dist + smsa + fulltime + distnod +
                   realinc + weekend + car, data = trips, family = poisson)

# MULL:97 (RES) table 1 p. 592
p_cigmales <- glm(cigarettes ~ habit + price + restaurant + income + age + I(age ^ 2) +
                      educ + I(educ ^ 2) + famsize + race, data = cigmales, family = poisson)

# DEB:TRIV:02 table 4 p. 613 (pas de poisson mais negbin two-part et finite mixture)
p_health_ins <- glm(mdu ~ log(1 + coins) + idp + lpi + fmde + log(income) + log(size) +
                        age + sex * child + race + educ + physlim + disease + health,
                    data = health_ins, family = poisson)

# DEB:TRIV:97 table 7 p 330 (pas de poisson mais negbin finite mixture)
p_elderly <- glm(ofp ~ health + numchron + adldiff + region + age + race + sex + marital +
                       school + income + emp + insurance, data = elderly, family = poisson)

# SANT:WIND:01 table 2 p. 82 (negbin_X)
p_doctor_ger <- glm(sdoctor ~ female + single + age + I(age ^ 2) + income + chronic +
                        privins + school+ heavylab + stress + variety + selfdet + control +
                        pop + physdens + unemp + hospm7 + sickm14 + disability,
                    data = doctor_ger, family = poisson)

# GEIL:97
p_hospitalization <- glm(hospital ~ age + I(age ^ 2 / 10E3) + I(age ^ 3 / 1E04) + copayment +
                             (public + voluntary + family) * (additional + aok) +
                             chronic + handicap + I(income / 1E05) + distance + married + children +
                             secondary + apprenticeship + university + healthjob + inlabour + bluecol + whitecol + civil +
                             selfemp + ptjob + western + nationelse,
                         data = hospitalization, family = poisson)

# GURM:97 tableau 6 p.236 ne correspond pas exactement
doctor_cal <- doctor_cal %>% filter(sample == "SSI")
p_doctor_cal <- glm(visits ~ children + age + I(age ^ 2 / 1E02) + I(income / 1E04)  + pc1 + pc2 + I(access / 1E2) +
                        marital + sex + race + I(school / 10) + enroll, data = doctor_cal, family = poisson)

# CAME:JOHA:97 tableau 4 page 215
p_bids <- glm(numbids ~ legrest + realrest + finrest + whtknght + bidprem +
        insthold + size + I(size ^ 2) + regulation,
    data = bids, family = poisson)

# ELLI:SWAN:16 tableau 3 page 1255 (negbin)
p_amc12 <- glm(over100 ~ log(students) + bachelors + graduates + asian + black + hispanic +
                   log(medinc) + freelunch + t1 + urban + female,
               data = amc12, family = poisson)

# KOEN:ZEIL:09 table 2 p. 843
p_asymptotic <- glm(nreg ~ log(nobs), data = asymptotic, family = poisson, weights = 1 / neq)

# MILL:09 table 2 p. 762
p_cartels <- glm(ncaught ~ len +  poly(lent, 5) + dgdp + funds + fines, data = cartels, family = poisson)

# MULL:98 table 3 p. 273 (différents modèles compliqués
p_doctor_us <- glm(doctvisits ~ age + male + white + school + married + health, data = doctor_us, family = poisson)

# SELL:STOL:CHAV:85
p_somerville <- glm(visits ~ quality + ski + income + feesom + costcon + costsom + costhoust,
                    data = somerville, family = poisson)

# FISH:MIGU:07
parking <- parking %>% as_tibble
p_parking <- glm(violations ~ corruption * post + staff + log(gdp) + region, data = parking, family = poisson)
#MASS::glm.nb(violations ~ corruption + post + staff + log(gdp) + region, data = parking)

# AGHI:VANR:ZING:13
p_innovation <- glm(citations ~ shareinstit + log(kl) + log(rdstock) + log(sales) +
                        factor(sector) + factor(year), data = innovation, family = poisson)

# GREE:97
majordrg <- majordrg %>% filter(cardhldr == 1)
p_majordrg <- glm(majordrg ~ age + income + expinc + avgexp + major, data = majordrg, family = poisson)

p_publications <- glm(articles ~ sex + marital + kids + phd + mentor, data = publications, family = poisson)


lrNB1 <- function(x){
    y <- model.response(model.frame(x))
    mu <- fitted(x)
    areg <- lm( I( ((y - mu) ^ 2 - y) / mu) ~ 1)
    coef(summary(areg))[3]
}

za <- tribble(
    ~ data,     ~ response,
    "doctor_aus", "doctorco",
    "health_ins", "mdu",
    "elderly", "ofp",
    "doctor_ger", "sdoctor",
    "hospitalization", "hospital",
    "doctor_cal", "visits",
    "health_reform", "visits",
    "doctor_us", "doctvisits",
    "strikes", "strikes",
    "trips", "trips",
    "cigmales", "cigarettes",
    "bids", "numbids",
    "amc12", "over100",
    "asymptotic", "nreg",
    "cartels", "ncaught",
    "somerville", "visits",
#    "parking", "violations",
#    "innovation", "citations",
    "majordrg", "majordrg",
    "publications", "articles") %>%
    mutate(reg = paste("p", data, sep = "_"))

results <- t(sapply(1:nrow(za),
                    function(i){
                        y <- eval(as.name(za$data[i]))[[za$response[i]]] ;
                        D9 = as.numeric(quantile(y, .9))
                        mu = mean(y)
                        sig2 = var(y)
                        over = sig2 / mu
                        mdel <- eval(as.name(za$reg[i]))
                        test <- lrNB1(mdel)
                        zero <- mean(y == 0)
                        fzero <- mean(dpois(fitted(mdel), x = 0))
                        c(mu = mu, omega = sig2, over = over, D9 = D9, over = over, zero = zero,
                          fzero = fzero, exczero = zero / fzero, test = test)
                    }
                    ))
rownames(results) <- za$reg
library("kableExtra")
results %>% as_tibble(rownames = "data") %>%
    rename("$\\mu$" = mu, "$\\omega$" = omega,
           "$\\frac{\\omega}{\\mu}$" = over,
           "$P(Y = 0)$" = zero,
           "$\\hat{P}(Y = 0)$" = fzero,
           "$\\frac{P(Y = 0)}{\\hat{P}(Y = 0)}$" = exczero) %>%
    mutate(data = str_sub(data, 3),
           data = str_replace(data, "_", ".")) %>%
    arrange(data) %>% 
    knitr::kable(digits = c(0, 0, 1, 1, 1, 2, 2, 1, 1),
                 escape = FALSE, booktabs = TRUE) %>%
    kable_styling()
```

About half of these data sets concern demand for health
care. `doctor_aus`, `doctor_cal`, `doctor_ger` and `doctor_us` concern
annual doctor visits, respectively in Australia, California, Germany
and the United States. `health_ins` focus on the link between health insurance
and doctor visits using a randomized experiment in the United States,
as `health_reform` analyzes the effect of an health insurance reform in
Germany. `elderly` focus on the demand for health care by elderly in
the US and `hospitalization` uses as the response the number of stays at
the hospital.

`amc12` uses the number of students per school who got a very high
score at the AMC12 test (a test on mathematics). `asymptotic` is a
meta analysis of wage equations reported in 156 papers and the
analysis stress on the hypothesis used to get the consistency of an
estimator, ie that the number of regressor is fixed as the number of
observations increases. `bids` analyzes the number of bids received by
126 firms that were targets of tender offers. `cartels` is a time
series of bi-annual observations, the response being the number of
cartels discoveries and the main covariate the pre and post period
when new leniency program was introduced. `cigmales` measure the
number of cigarettes daily smoked by males in the United
States. `majordrg` contain data about the number of major derogatory
reports by cardholders. `publications` analyzes the
scientific production (measured by the number of publications) of
individuals who got a Ph. D. in bio-chemical. `somerville` reports the
number of recreational visits to a lake in Texas. `strikes` is a time
series that reports the number of strikes (and also their length) in
the United States.  `trips` contains the number of trips taken by
members of households the day prior the survey interview.

As we have seen in @sec-poisson_ml, count data can be considered as a random variable that follows a Poisson distribution, which has a unique parameter, this parameter being the mean and the variance of the series. The ML estimator of this parameter is the sample mean, which is indicated in
the first column ($\mu$) of  @tbl-empsurvey. The empirical
variance ($\omega$) is reported in the second column. 
Count data often take small values, and this is the case for almost all
our data sets. We report in @tbl-empsurvey the 9th decile, which is less
than 10 for most of our data sets. Noticeable exceptions are
`asymptotic` (the number of covariates in an econometric equation),
`cigmales` (the number of cigarettes smoked daily).

While comparing real count data to Poisson distribution, one often
face two problems:

- the first one is **overdispersion**, which means that the variance is
  much greater than the mean,
- the second one is the problem of **excess zero**, which means that
  the mean probability of a zero value computed using the Poisson
  distribution is often much less than the observed share of zero in
  the sample.
  
The third column of @tbl-empsurvey contains the mean variance
ratio and shows that overdispersion seems to be the rule. The ratio is
in general much larger than 1, noticeable exceptions being `bids`,
`cartels` and `majordrg`. This overdispersion is important because we
consider that the Poisson parameter is the same for every observation. Adding covariates
will give specific values of the Poisson parameter for every observations and will therefore reduce
the (conditional) variance. Anyway, we'll see in subsequent sections
that the overdispersion problem is often still present even after introducing 
covariates, because of unobserved heterogeneity. 

The excess of zero problem is difficult to distinguish from
overdispersion, as zero is an extreme value of the distribution and
therefore, an excess of zero leads to overdispersion. The excess of
zero can in part be explained by the fact that 0 is a special value
that may not be correctly explained by the same process that explain
strictly positive values. An example is `cigmales`, where 0 means no
smoking and smoking vs non-smoking is quite different in nature than
smoking a few or a lot of cigarettes. The 8th column of @tbl-empsurvey returns the
ratio between the empirical share of zero and the Poisson probability of 0 and, unsurprisingly, this ratio is huge for `cigmales`. Some models have been proposed that have a special
treatment for the value of 0 and allow to obtain predicted
probabilities of 0 greater that the one implied by the Poisson model.

### Analyzing the number of trips

Throughout this chapter, we'll use the `trips` data set. It was used by @TERZ:98 and the response is the number of trips taken the day before the
interview. 
We first consider the unconditional distribution of the number of
`trips`, computing the first two moments and the share of 0:

```{r }
#| collapse: true
trips %>% summarise(m = mean(trips), v = var(trips), 
                    s0 = mean(trips == 0))
```
This series exhibits important overdispersion, as the variance is
about 5 times larger than the mean. The ML estimator is the sample
mean and therefore, the predicted probability of 0 is:

```{r }
#| collapse: true
dpois(0, 4.551)
```

which is much lower than the actual percentage of 0 which is
18.5\%. There is therefore a large excess of 0, which also can be
associated with overdispersion as 0 is a value far from the mean of
4.551.
The **countreg** package^[The **countreg** package is not on CRAN, it can be installed using `install.packages("countreg", repos="http://R-Forge.R-project.org")`.] provides an interesting plotting function
called `rootogram` to compare the actual and the fitted distribution
of a count. The first one is represented by bars and the second one by
 a curve. The most simple representation is obtained
by setting the `style` and the `scale` parameters respectively to
`"standing"` and `"raw"` (see @fig-tripsuncond). With the first one, the bottom of all
the bars are on the horizontal axes, with the second one, the absolute
frequencies are displayed on the vertical axes. For example, for
$Y=0$, the absolute frequency is `r sum(trips$trips == 0)` and, as
seen previously, the fitted probability is $0.01056$ which implies, as
the number of observations is `r nrow(trips)`,  a fitted
frequency of $577 \times 0.01056 = 6.1$. The frequencies are clearly
over-estimated for values of the response close to the mean (3, 4, 5,
6) and under-estimated for extreme values. This is particularly the
case for 0, which indicates that a specific problem of excess of zero
may be present.

```{r }
#| label: fig-tripsuncond
#| fig-cap: "Unconditional distribution of trips"
countreg::rootogram(trips$trips, fitted = "poisson", plot = FALSE,
                    style = "standing", scale = "raw") %>%
  autoplot + coord_cartesian(xlim = c(0, 15))
```

## The Poisson model {#sec-poisson_model}

### Derivation of the Poisson model

The basic count data model is the Poisson model. It relies on the
hypothesis that the response is a Poisson variable and therefore that the probability of a given value $y$ is:

$$
\mbox{P}(Y = y, \mu) = \frac{e^{-\mu}{\mu} ^ y}{y !}
$$

The Poisson distribution is defined by a unique parameter $\mu$,
which is the mean and the variance of the distribution. This is a
striking feature of the Poisson distribution which differs for
example from the normal distribution which has two separate position
and dispersion parameters (respectively the mean and the standard
deviation). The Poisson probability can also be written as:

$$
\mbox{P}(Y = y, \mu) = e^{y \ln \mu-\mu- \ln y !}
$$
and therefore, the Poisson model belongs to the generalized linear model family (see @sec-glm), with $\theta = \ln \mu$, $b(\theta) = \mu = e^ \theta$, $\phi = 1$ and $c(y, \phi) = - \ln y !$. The mean is $b'(\theta) = e ^ \theta = \mu$ and the variance $\phi b''(\theta) = e ^ \theta  = \mu$. The link defines the relation between the linear predictor $\eta$ and the parameter of the distribution $\mu$: $\eta = g(\mu)$. The canonical link is such that $\theta = \eta$ and it is therefore, for the Poisson model, the log link: $\eta = \ln \mu$. For a given set of covariates, the Poisson parameter for observation $n$ is $\mu_n = g ^ {-1}(\eta_n)$, with $\eta_n = \alpha + \beta ^ \top x_n = \gamma ^ \top z_n$ the linear predictor. Therefore:

$$
\mu_n = \mbox{E}(y \mid x_n) = e^{\alpha + \beta^\top x_n} = e^{\gamma ^ \top z_n}
$$ {#eq-condexp_poisson}

and the log-likelihood function is:

$$
\ln L (\gamma, y, Z) = \sum_{n=1} ^ N \ln \frac{e^{-e^{\gamma^\top z_n}}
e ^{y_n\gamma ^ \top z_n}}{y_n !} = \sum_{n=1} ^ N -e^{-\gamma^\top z_n} +
y_n \gamma^\top z_n - \ln y_n !
$$

The first order conditions for a maximum are:

$$
\frac{\partial \ln L}{\partial \gamma} = \sum_{n = 1} ^ N \left(y_n -
e^{\gamma^\top z_n}\right)z_n = 0
$$ {#eq-foc_poisson}

which states, like in the OLS model, that the difference between the actual value of the response $y$
and the prediction of the model (which can be called the residual) should be
orthogonal to every covariate. It is a very important property as it
implies that the Poisson model is consistent if the conditional
expectation function is correctly specified, whether or not the
hypothesis of the Poisson distribution is correct. Moreover, as the first element of $z_n$ is 1, the first element of @eq-foc_poisson implies that:

$$
\bar{y} = \frac{1}{N} \sum_{n=1} ^N e ^ {\hat{\gamma}^\top z_n}
$$ {#eq-mean_me_poisson}

which means that the mean of the predictions equals the mean value of $y$ in the sample.

For the `trips` data set, the covariates are the share of trips for work or school
(`workschl`), the number of individuals in the household (`size`), the
distance to the central business district (`dist`), a dummy (`smsa`)
for large urban area, the number of full time
worker in household (`fulltime`), the distance from home to nearest
transit node, household income divided by the median income of the
census tract (`realinc`), a dummy if the survey period is Saturday or
Sunday (`weeekend`) and a dummy for owning at least a car (`car`). The Poisson model is fitted using the `glm` function and setting the `family` argument to `poisson`. As for the binomial model, the `family` argument can be a character, a function without argument (in this case, the default link is chosen) or a function with a link argument.^[The other possible other links for the Poisson model are `"identity"` and `"sqrt"` but the default `"log"` link is almost always used.] For the sake of comparison, we also estimate a linear model using `lm`:

```{r}
ftrips <- trips ~ workschl + size + dist + smsa + fulltime + distnod + 
  realinc + weekend + car
pois_trips <- glm(ftrips, family = poisson, trips)
lm_trips <- lm(ftrips, trips)
```

```{r }
#| include: false
#| eval: false
#| label: tbl-regtrips
#| tbl-cap: "Regressions for the trips data set"
     # gof_map: list of lists
     f1 <- function(x) format(round(x, 3), big.mark=",")
     f2 <- function(x) format(round(x, 0), big.mark=",")
     gm <- list(
         list("raw" = "nobs", "clean" = "N", "fmt" = f2),
         list("raw" = "AIC", "clean" = "aic", "fmt" = f1))

pois_trips <- glm(trips ~ workschl + size + dist + smsa + fulltime + distnod +
                   realinc + weekend + car, data = trips, family = poisson)
qpois_trips <- glm(trips ~ workschl + size + dist + smsa + fulltime + distnod +
                       realinc + weekend + car, data = trips, family = quasipoisson)
lm_trips <- lm(trips ~ workschl + size + dist + smsa + fulltime + distnod +
                   realinc + weekend + car, data = trips)
modelsummary::msummary(list("linear model" = lm_trips,
                            "Poisson model" = pois_trips,
                            "quasi-Poisson model" = qpois_trips),
                       estimate = "{estimate} ({std.error}){stars}",
                       statistic  = NULL,
                       gof_omit = 'DF|Deviance|R2|AIC|BIC|F|RMSE',
                       fmt = 2)
```

### Interpretation of the coefficients

As for any non-linear models, the marginal effects are not equal to the
coefficients. Taking the derivative of the conditional expectation
function (@eq-condexp_poisson) with the k^th^ covariate, we get the following marginal effect for observation $n$:

$$
\frac{\partial \mbox{E}(y\mid x_n)}{\partial x_{nk}} = \beta_k e^{\gamma^\top z_n}
$$

which can be consistently estimated by $\hat{\beta}_k
e^{\hat{\gamma}^\top z_n}$. Note that the ratio of the marginal effects
of two covariates $k$ and $l$ is $\beta_k / \beta_l$ and therefore
doesn't depend on the values of the covariates for a specific
observation. The average marginal effect (AME) for the $N$ observations is:

$$
\hat{\beta}_k \frac{\sum_{n=1}^N e^{\hat{\gamma}^\top z_n}}{N}
$$

But, if the regression contains an intercept, the sample mean of the
predictions is the sample mean of the response (see @eq-mean_me_poisson). Therefore, the
average marginal effect is $\hat{\beta}_k
\bar{y}$. Therefore, we can for example compare the estimate slopes
in a linear model (which are directly the marginal effects) to the
Poisson estimators multiplied by the mean of the response. 

We now print the coefficients obtained for the Poisson and for the linear model:

```{r}
rbind(lm = coef(lm_trips), poisson = coef(pois_trips))
```

An increase of 1 of `realinc` means that household's
income increases by an amount equal to the local median income. The
linear models indicates that the number of trips then increases by $0.133$
trips (the sample mean for trips being 4.55). The Poisson model indicates that the relative increase of trips $dy /y$ is equal to $0.019$, ie 2%. At the sample mean, it corresponds to
an increase of $0.019 \times 4.55 = 0.086$ trips.

If the interview concerns a week-end day, the OLS coefficient is
$-0.48$, which indicates that the number of trips taken during the week-end
are about one half less compared to a week day. For such a dummy
variable, denoting $\beta_w$ and $z_0$ a given vector of covariates such that $x_w = 0$,  $\gamma^\top z_0$ is the linear predictor for a week-day and $\gamma^\top z_0 + \beta_w$ the linear predictor for a week-end day. Therefore, the relative
difference $\tau$ of the number of trips between a week-end day and a week day is:

$$
\tau = \frac{e^{\gamma^\top z_0 + \beta_w}- e^{\gamma^\top
z_0}}{e^{\gamma^\top z_0}} = e^{\beta_w} - 1 = e^{-0.0738} - 1 = - 0.071
= 7.1\%
$$

The previous expression indicates also that $\beta_w = \ln (1 +
\tau) \approx \tau$. For small values of $\beta_w$, the coefficient can be
interpreted as the relative difference of the response. In terms of
absolute value, at the sample mean, the absolute difference is $-
0.071 \times 4.55  = -0.323$, which is close to the OLS coefficient.
Of course the approximation is only relevant for small values of the
coefficient. If we consider the coefficient of `cars`, the
value for the Poisson model is $1.413$ which implies an increase of
$e^{1.413} - 1 = 3.1 = 310$% much larger that the value of the coefficient.

### Computation of the variance of the estimator

The matrix of the second derivatives is:

$$
\frac{\partial \ln L}{\partial \gamma\partial \gamma ^
\top}=-\sum_{n=1}^N e^{\gamma^\top z_n}z_n z_n^\top
$$

The information matrix is either the opposite of the hessian (as it
doesn't depend on $y$, it is equal to its expected value) or the
variance of the gradient. The latter is:

$$ 
\sum_{n=1} ^ N \mbox{E}\left( (y_n - e^{\gamma^\top z_n})^2 z_n
z_n^\top\right) = \sum_{n=1} ^ N e^{\gamma^\top z_n} z_n
z_n^\top
$$

as $\mbox{E}\left( (y_n - e^{\gamma^\top z_n}) ^2 \right)$ is the
conditional variance which equals the conditional expectation for a
Poisson distribution. The estimated variance of the estimator is therefore:

$$
\hat{\mbox{V}}(\hat{\beta}) = \left(\sum_{n=1} ^ N e^{\hat{\gamma}^\top z_n} z_n
z_n^\top\right) ^ {-1} = \left(\sum_{n=1} ^ N \hat{\mu}_n z_n
z_n^\top\right) ^ {-1}
$$

This estimator is consistent only if the distribution of the response is Poisson, 
and therefore if the conditional variance equals the conditional mean. 
A more general estimator is based on the sandwich formula:

$$
\left(\sum_{n=1} ^ N \hat{\mu}_n z_n
z_n^\top\right) ^ {-1}
\left(\sum_{n=1} ^ N (y_n - \hat{\mu}_n)^2 z_n
z_n^\top\right)
\left(\sum_{n=1} ^ N \hat{\mu}_n z_n
z_n^\top\right) ^ {-1}
$$ {#eq-sandpois}

One alternative is to assume that the variance is proportional to the
mean: $\mbox{V}(y|x) = \phi \mbox{E}(y|x)$. $\phi$ can then be consistently estimated by:

$$
\hat{\phi} = \frac{1}{N}\sum_{n=1} ^ N \frac{(y_n - \hat{\mu}_n) ^ 2}{\mu_n}
$$ {#eq-qpoisvar}

which leads to the third estimator of the covariance matrix:

$$
\hat{\mbox{V}}(\hat{\gamma}) = \hat{\phi}\left(\sum_{n=1} ^ N \hat{\mu}_n z_n z_n^\top\right) ^ {-1}
$$ {#eq-quasi-poisson}

The "quasi-Poisson" model is obtained by using the log link and @eq-quasi-poisson for the variance. It leads to the same estimator than the Poisson model and a variance that is
greater (respectively lower) that the one of the Poisson model if
there is overdispersion (respectively underdispersion). It can be fitted using `glm` by setting the `family` argument to `quasipoisson`:

```{r}
qpois_trips <- update(pois_trips, family = quasipoisson)
```

Compared to the Poisson model, the standard
deviations of the quasi-Poisson model are inflated by a factor which
is the square root of the estimate of the variance-mean ratio. This
factor can be estimated using @eq-qpoisvar, which makes use
of the residuals of the Poisson regression. 

```{r }
y <- pois_trips %>% model.frame %>% model.response
mu <- pois_trips %>% fitted
e_resp <- pois_trips %>% resid(type = "response")
e_pears <- pois_trips %>% resid(type = "pearson")
```
The response residual is $y_n - \hat{\mu}_n$ and we get the Pearson residual
by dividing the response residual by the standard deviation, which is $\sqrt{\hat{\mu}_n}$.
Therefore, the estimation of the variance-mean ratio is simply the sum
of square of the Pearson residuals divided by the sample size (the
number of degrees of freedom can be used instead).

```{r }
#| collapse: true
phi <- sum(e_pears ^ 2) / df.residual(pois_trips)
c(phi, sqrt(phi))
```
The estimated variance-mean ratio is more than 3, so that the standard
deviations of the quasi-Poisson model are almost 2 times larger than
those of the Poisson model (1.835). The sandwich estimator can be constructed by extracting from the fitted Poisson model the model matrix ($Z$), the fitted values ($\hat{\mu}_n$) and the response residuals ($y_n - \hat{\mu}_n$):

```{r }
mu <- pois_trips %>% fitted
e <- pois_trips %>% residuals(type = "response")
Z <- pois_trips %>% model.matrix
```
We can then compute the "bread" (`B`) and the "meat" (`M`):

```{r }
B <- crossprod(mu * Z, Z)
M <- crossprod(e ^ 2 * Z, Z)
```
Applying @eq-sandpois, and taking the square root of the
diagonal elements, we get:

```{r }
#| collapse: true
sand_vcov <- solve(B) %*% M %*% solve(B)
sand_vcov %>% stder %>% head(4)
```
The `sandwich` package provides the `vcovHC` function which computes
different flavors of the sandwich estimator of the variance. The one
that corresponds to @eq-sandpois is obtained by setting `type`
to `HC`.

```{r }
#| collapse: true
sandwich::vcovHC(pois_trips, type = "HC") %>% stder %>% head(4)
```

The sandwich standard errors can also be extracted from the Poisson model using `stder` with the `vcov` and `type` arguments (respectively equal to `sandwich::vcovHC` and `"HC"`):

```{r}
#| collapse: true
stder(pois_trips, vcov = sandwich::vcovHC, type = "HC") %>% head(4)
```

Taking the ratio of the simple standard deviations of the coefficients
and the sandwich estimator, we get:

```{r }
stder(sand_vcov) / stder(pois_trips)
```

As previously, the robust standard deviations are about twice the
basic ones in average, but the ratio now depends on the coefficient.

### Overdispersion

A more general expression for the variance is the following Negbin
specification:

$$
\sigma_n ^ 2 = \mu_n + \alpha \mu_n ^ p
$$

with two special cases:

- $p = 1$ (**NB1**) which implies that the variance is proportional to the
  expectation, $\sigma_n ^ 2 = (1 + \alpha) \mu_n$,
- $p = 2$ (**NB2**) which implies that the variance is quadratic in the
  expectation, $\sigma_n ^ 2 = \mu_n + \alpha \mu_n ^ 2$. 

Tests for overdispersion can easily be implemented using one the three
test principles: Wald, likelihood ratio and score tests. The first
two require the estimation of a more general model that doesn't impose
the equality between the conditional mean and the conditional variance. The score test requires only the estimation of the constrained (Poisson) model. It can takes different form, we'll just
present here the test advocated by @CAME:TRIV:86 who use an
auxiliary regression.^[Score tests that are not obtained from auxiliary regressions are
presented by @DEAN:92.] The null hypothesis is:

$$
\mbox{E}\left( (y_n - \mu_n) ^ 2 - y_n\right) = \sigma_n ^ 2 - \mu_n = 0
$$

With the NB1 specification, the alternative is:

$$
\mbox{E}\left( (y_n - \mu_n) ^ 2 - y_n\right) = (1 + \alpha)\mu_n -
\mu_n = \alpha \mu_n
$$

or:

$$
\frac{\mbox{E}\left( (y_n - \mu_n) ^ 2 - y_n\right)}{\mu_n} = \alpha
$$ {#eq-aregnb1}

The hypothesis of equidispersion (ie $\alpha = 0$) can be tested by
regressing $\frac{ (y_n - \hat{\mu}_n) ^ 2 - y_n}{\hat{\mu}_n}$ on a constant and
comparing the student statistic to the relevant critical value.
With the NB2 specification, the alternative is:

$$
\mbox{E}\left( (y_n - \mu_n) ^ 2 - y_n\right) = \mu_n + \alpha\mu_n ^
2 - \mu_n = \alpha \mu_n ^2
$$

or:

$$
\frac{\mbox{E}\left( (y_n - \mu_n) ^ 2 - y_n\right)}{\mu_n} = \alpha \mu_n
$$ {#eq-aregnb2}

The hypothesis of equidispersion (ie $\alpha = 0$) can then be tested by
regressing $\frac{ (y_n - \hat{\mu}_n) ^ 2 - y_n}{\hat{\mu}_n}$ on $\hat{\mu}_n$ without an intercept and comparing the student statistic to the relevant critical value. Note that the response in these auxiliary regression can be written as: $(y_n - \hat{\mu}_n) ^ 2 / \hat{\mu}  - y_n/\hat{\mu}_n$, which is the difference between the square of the Pearson residual and the ratio of the actual and the fitted value of the response:

```{r }
#| collapse: true
hmu <- fitted(pois_trips)
y <- model.response(model.frame(pois_trips))
e_pears <- residuals(pois_trips, type = "pearson")
resp_areg <- e_pears ^ 2 - y / hmu
lm(resp_areg ~ 1) %>% gaze(coef = 1)
lm(resp_areg ~ hmu - 1) %>% gaze
```

In both regression, the t statistic is much higher than 1.96, the
p-value is very close to zero so that the equi-dispersion hypothesis
is strongly rejected. 

To see in what extent the introduction of covariates has reduced the
over-dispersion problem, we can produce once again a rootogram, this
time on the fitted Poisson model. We use the default style
which is `"hanging"`, which means that the bars are vertically aligned
on the point that display the fitted frequency and the default scale
which is `"sqrt"`; the vertical axis indicates the square root of the
frequencies, to increase the readability of small frequencies.
Rootograms for the Poisson model are presented on
@fig-rootograms along with the previous rootogram for the
unconditional distribution of `trips`. The two figures are first
created without being plotted by setting the `plot` argument to
`FALSE`, and are then assembled in one figure using
`ggpubr::ggarrange`. The limits on the vertical axis are set
to the same values for the two plots so that the frequencies for the
two figures can be easily compared.

```{r }
#| label: fig-rootograms
#| fig-cap: "Rootograms for Poisson models with and without covariates"
fig_raw <- countreg::rootogram(trips$trips, fitted = "poisson", 
                                 plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
fig_covar <- countreg::rootogram(pois_trips, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
figure <- ggpubr::ggarrange(fig_raw, fig_covar,
                           labels = c("No covariates", "Covariates"),
                           ncol = 2, nrow = 1)
figure
```

One can see that the bottom of the bars are closer to the horizontal axe
for the right figure (Poisson model with covariates), which indicates
that the inclusion of covariates reduces slightly the overdispersion
problem, which anyway still remains. Moreover, The excess of zero
largely remains, which suggest that a model that take the specificity
of zero values should be used.
The over-dispersion phenomena can also be represented by plotting the
square of the residuals against the fitted values, the straight
line (NB1 hypothesis) and the parabola (NB2 hypothesis) that fit the
data. The result is presented in @fig-overpts. 

```{r }
#| label: fig-overpts
#| fig-cap: "Squared residuals and fitted values"
tb <- tibble(e2 = residuals(pois_trips, type = "response") ^ 2,
             mu = fitted(pois_trips))
tb %>% ggplot(aes(mu, e2)) + geom_point() +
    coord_cartesian(xlim = c(0, 15), ylim = c(0, 150)) +
    geom_smooth(method = lm, se = FALSE, formula = y ~ x - 1, 
                color = "red") +
    geom_smooth(method = lm, se = FALSE, formula = y ~ x + I(x ^ 2) - 1)
```

## Overdispersion and excess of zero {#sec-overdisp_zero}

We describe in this section models that have been proposed to overcome the limits of the
Poisson model, namely the fact that it doesn't deal correctly with
overdispersion and the excess of zero.

### Mixing model

For the Poisson model with the log-link, the parameter of the Poisson distribution for individual $n$ is $\mu_n = e^ {\gamma ^ \top z_n}$. The problem of overdispersion occurs when the variance of $y$ is greater than its expectation, although the hypothesis underlying the use of the Poisson model is that they are equal. On solution is to define the Poisson parameter for individual $n$ as : $\lambda_n = \nu_n \mu_n = \nu_n e^ {\gamma ^ \top z_n}$. $\nu_n$ is an unobserved positive term that inflates the variance of $y$. Actually, for the same set of covariates $z$, two individuals $n$ and $m$  will be characterized by two different values of their Poisson parameters if $\nu_n \neq \nu_m$. For a given value of $\nu$, the probability of $y$ is still Poisson:

$$
P(y_n \mid x_n, \nu_n ; \gamma) = \frac{e^{- \nu_n e^{\gamma ^ \top z_n}} \left(\nu_ne^{\gamma ^ \top z_n}\right) ^ y_n}{y_n !}
$$
but, as $\nu_n$ is unobserved, this probability can't be used to estimate the parameters of the model.
To get the distribution of $y$ conditional on $x$ only, we need to
integrate out $\nu_n$, assuming a specific distribution for $\nu$, called the **mixing distribution**. Two
popular choices are the gamma and the log-normal distributions. The gamma density is:

$$
g(\nu; \delta, \gamma) = \nu ^ {\delta - 1} e ^ {- \gamma \nu} \gamma ^ \delta / \Gamma(\delta)
$$

where $\delta$ and $\gamma$ are respectively called the shape and the
intensity parameters and $\Gamma(z) = \int_0 ^ {+\infty}t ^ {z-1}e^{-t} dt$ is the Gamma function. The expected value and the variance are
respectively $\delta / \gamma$ and $\delta / \gamma ^ 2$. As $\gamma$ contains an intercept $\alpha$, $\alpha$ and the expected value of $\nu$ can't be identified and a simple choice of normalization is to set $\delta = \gamma$, so that the expected value of $\nu$ is set to 1 and the variance is then $1 / \gamma$. The density of $\nu$ is then:

$$
g(\nu; \delta) = \nu ^ {\delta - 1} e ^ {- \delta \nu} \delta ^ \delta / \Gamma(\delta)
$$

The distribution of $y$ conditional on $x_n$ only, called the **negative binomial** distribution, is then:

$$
\begin{array}{rcl}
P(y_n \mid x_n ; \gamma, \delta) &=& \displaystyle \int_0 ^ {+\infty} P(y_n \mid
x_n, v ; \gamma) g(\nu; \delta) d\nu \\
&=& \displaystyle \int_0 ^ {+\infty} 
\frac{e^{- \mu_n \nu} (\mu_n\nu) ^{y_n}}{y_n !}
\nu ^ {\delta - 1} e ^ {- \delta \nu} \delta ^ \delta / \Gamma(\delta)
d\nu
\end{array}
$$
Rearranging terms, we get:

$$
P(y_n \mid x_n ; \gamma, \delta) = 
\frac{\mu_n ^ {y_n} \delta ^ \delta}{y_n !\Gamma(\delta)}
\int_0 ^ {+\infty} 
e^{- (\mu_n + \delta) \nu} \nu ^ {y_n + \delta - 1}
d\nu
$$
with the change of variable $t = (\mu_n + \delta)\nu$, we finally get:

$$
P(y_n \mid x_n ; \gamma, \delta) = 
\frac{\mu ^ {y_n} \delta ^ \delta}{y_n !\Gamma(\delta)}
\frac{\Gamma(y_n + \delta)}{(\mu + \delta) ^{y_n + \delta}}
$$

and finally, as, for integer values of $x$: $\Gamma(x + 1) = x!$:

$$
P(y_n \mid x_n ; \gamma, \delta) = 
\frac{\Gamma(y_n + \delta)}{\Gamma(y_n + 1)\Gamma(\delta)}
\left(\frac{\mu_n}{\mu_n + \delta}\right) ^ {y_n} \left(\frac{\delta}{\mu_n +
\delta}\right) ^ \delta
$$

Conditional on $x_n$ and $\nu_n$, $\mbox{E}(y_n \mid x_n, \nu_n) = \mbox{V}(y_n \mid x_n, \nu_n) = \nu_n \mu_n$. Conditional on $x_n$ only, $\mbox{E}(y_n | x_n)= \mbox{E}_\nu(\nu \lambda_n) = \mu_n$ (as
$\mbox{E}(\nu) = 1$). To get the variance, we use the formula of
variance decomposition:

$$
\begin{array}{rcl}
\mbox{V}(y_n |x_n) &=& \mbox{E}_\nu\left(\mbox{V}(y_n |x_n,
\nu_n)\right) + \mbox{V}_\nu\left(\mbox{E}(y_n |x_n,
\nu_n)\right) = 
\mbox{E}_\nu(\lambda_n) +
\mbox{V}_\nu(\lambda_n)\\
&=&
\mu_n + \mu_n ^ 2 \mbox{V}(\nu_n) = 
\mu_n(1 + \mu_n/ \delta)
\end{array}
$$
Different parametrizations result in different flavors of the **Negbin model**. Replacing $\delta$ by $\sigma = 1 / \delta$ we get the NB2 model, with a quadratic conditional variance
function:

$$
\mbox{V}(y_n|x_n) = \mu_n + \sigma \mu_n ^ 2 = (1 + \sigma \mu_n)\mu_n
$$ {#eq-var_negbin2}

Replacing $\delta$ by $\mu_n / \sigma$, we get the NB1 model, with a
linear conditional variance function:

$$
\mbox{V}(y_n\mid x_n) = (1 + \sigma) \mu_n
$$ {#eq-var_negbin1}

Note that as $\delta$ is positive so is $\sigma$ in the versions of the Negbin model so that 
that the Negbin model exhibits necessarily overdispersion, with the
special case of equidispersion if $\sigma = 0$. Actually, in this case, $\delta \rightarrow \infty$, so that the variance of $\nu$ tends to 0 and the Poisson model is therefore obtained as
a limited case. 

Another choice for the mixing distribution is the log-normal distribution, with $\ln \nu \sim \mathcal (0, \sigma)$. The first parameter of the distribution is set to 0. For a log-normal distribution, the exponential of this parameter is the median of the log-normal. Therefore, the choice of normalization for the log-normal distribution is to set the median (and not the mean as in the gamma distribution) to 0. The log-normal density is $\frac{1}{\sigma\nu}\phi(\ln \nu / \sigma)$ where $\phi()$ is the standard normal density. The probabilities are then given by:

$$
P(y_n \mid x_n ; \gamma, \delta) =
 \int_0 ^ {+\infty} 
\frac{e^{- e ^ {\gamma ^ \top z_n} \nu_n} (e ^ {\gamma ^ \top z_n}\nu_n) ^ y}{y !}
\frac{1}{\sqrt{2\pi}\sigma\nu} e ^ {-\frac{1}{2}\left(\frac{\ln \nu}{\sigma}\right) ^ 2} d\nu
$$
using the change of variable $t = \ln \nu / (\sqrt{2}\sigma)$, we get:

$$
P(y_n \mid x_n ; \gamma, \delta) =\frac{1}{\sqrt{\pi} y_n !}
 \int_{- \infty} ^ {+\infty} 
e^{- e ^ {\gamma ^ \top z_n + \sqrt{2}\sigma t}} e ^ {y(\gamma ^ \top z_n + \sqrt{2}\sigma t)}
 e ^ {-t^2} dt
$$

There is no closed form for this integral, but it can be approximated using Gauss-Hermite quadrature:

$$
\int_{-\infty} ^ {+\infty}f(t) e ^ {-t ^ 2}dt \approx \sum_{r = 1} ^ R \omega_r f(t_r)
$$
where $R$ is the number of points for which the function is evaluated, $t_r$ are called the nodes and $\omega_r$ the weights.
For a log-normal variable, $\mbox{E}(x) = e ^ {\mu + 0.5 \sigma ^ 2}$ and $\mbox{V}(x) = e ^ {2\mu + \sigma ^ 2}(e ^ {\sigma ^ 2} - 1)$. As, we set $\mu$ to 0, $\mbox{E}(y_n\mid x_n) = \mbox{E}_\nu\left(\mbox{E}(y_n\mid, x_n, \nu)\right) = e^{\frac{1}{2}\sigma ^ 2}\mu_n$. Moreover, $\mbox{E}_\nu\left(\mbox{V}(y_n\mid, x_n, \nu)\right) = e^{\frac{1}{2}\sigma ^ 2}\mu_n$ and $\mbox{V}_\nu(\mbox{E}(y_n\mid, x_n, \nu)) = \mu_n ^ 2 e ^ {\sigma ^ 2}(e ^ {\sigma ^ 2} - 1)$ so that:

$$
\mbox{V}(y_n \mid x_n) = e^{\frac{1}{2}\sigma ^ 2}\mu_n + e ^ {\sigma ^ 2}(e ^ {\sigma ^ 2} - 1) \mu_n ^ 2 =
\left(1 + (e ^ {\sigma ^ 2} - 1)\mbox{E}(y \mid  x_n)\right) \mbox{E}(y_n \mid  x_n)
$$ {#eq-var_lognorm}

We then estimate the Poisson model and the 3 mixed models we've just described. The `micsr::poisreg` function enables the estimation of this mixed models. It has a `mixing` argument. If `mixing = "none"` (the default), the basic Poisson model is estimated. Setting `mixing` to `"gamma"` and `"lognorm"` result respectively in the Negbin and the Log-normal-Poisson models. For the Negbin model, the two different flavors are obtained by setting `vlink` either to `"nb1"` (the default) or to `"nb2"` to get respectively the Negbin1 and the Negbin2 models.^[The NB2 model is implemented in `MASS::glm.nb`.] The results are presented in @tbl-mixed_models.

```{r}
pois <- poisreg(trips ~ workschl + size + dist + smsa + fulltime + distnod + 
                 realinc + weekend + car, trips)
nb1 <- update(pois, mixing = "gamma", vlink = "nb1")
nb2 <- update(pois, mixing = "gamma", vlink = "nb2")
ln <- update(pois, mixing = "lognorm")
```

The most striking result is the improvement of the fit. The coefficient of $\sigma$ in the three specifications is highly significant and the AIC or the BIC conclude that the mixed models are much better than the Poisson model. Using these indicators, it is difficult to discriminate between these three models because the AIC and the BIC are almost the same.
The standard errors are much higher for the mixed models compared to the Poisson model, but remind that, when overdispersion is present, the standard errors of the Poisson model are downward biased.
Using respectively @eq-var_negbin1, @eq-var_negbin2 and @eq-var_lognorm, we compute the ratio of the variance and the expectation at the sample mean for the three models, ie using $\bar{y}$ instead of $\mbox{E}(y \mid x_n)$ in the three formula:

```{r}
#| label: tbl-mixed_models
#| tbl-cap: "Mixed models for the demand for trips"
#| echo: false
modelsummary::msummary(list(poisson = pois, Negbin1 = nb1, Negbin2 = nb2, `Log-normal` = ln))
```


```{r}
#| collapse: true
sig_nb1 <- coef(nb1)["sigma"]
sig_nb2 <- coef(nb2)["sigma"]
sig_ln <- coef(ln)["sigma"]
yb <- mean(trips$trips)
1 + sig_nb1
1 + sig_nb2 * yb
(1 + (exp(sig_ln ^ 2) - 1) * yb)
```

Therefore, the three mixing models predict about the same level of overdispersion, the conditional variance being about 3.5 times larger than the conditional expectation. This value should be compared to the ratio of the sample variance (`r round(var(trips$trips), 1)`) and the sample mean (`r round(mean(trips$trips), 1)`), which is (`r round(var(trips$trips)/mean(trips$trips), 1)`). @fig-rootograms_mixed presents the rootogram for the Poisson and the Negbin model. We can see that taking into account the overdispersion using a mixing model resolves most of the overdispersion problem.

```{r }
#| label: fig-rootograms_mixed
#| fig-cap: "Rootograms for the Poisson and the Negbin model"
#| echo: false
pois_glm <- glm(formula(pois), family = poisson, trips)
nb2_mass <- MASS::glm.nb(formula(pois), trips)
fig_pois <- countreg::rootogram(pois_glm, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
fig_nb2 <-countreg::rootogram(nb2_mass, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
figure <- ggpubr::ggarrange(fig_pois, fig_nb2,
                           labels = c("Poisson", "Negbin2"),
                           ncol = 2, nrow = 1)
figure
```


### Hurdle and ZIP models

Has we have seen previously for the `trips` data set, it seems that
the Poisson model is unable to model correctly the probability of $y =
0$. Different phenomena can explain zero values:

- some people may never (or almost never) take a trip out of the house
  because, for example, of a bad physical condition,
- some people may take a trip infrequently (for example twice a week)
  so that a 0 value may be reported if the survey concerns a Tuesday
  as the individual has taken a trip on Monday and on Thursday.
  
This question is closely related to the analysis of consumption
expenditures based on individual data for which the value is 0 for large part of the sample. For example, a large share of the sample will
report a null consumption on tobacco simply because they don't
smoke. Some other individuals will report a null consumption of fish
because fish is bought infrequently (once a month for example) and the
length of the survey is two weeks. 

#### Hurdle models

For the first situation, @CRAG:71 proposed the hurdle model (presented in @sec-tobit2), for which
the level of consumption of a good is taken into account by two
different models:

- the first model is a binomial model that explains the fact that the
  expenditure is zero or positive,
- the second model explains the level of the expenditure if it is positive.

This hurdle model has been adapted to count data by @MULL:86. The first model
returns the probability that $y=0$. Therefore, the response is $d_n=\mbox{I}(y_n > 0)$, which takes the two values of 0 ($y_n$ is 0) and 1 ($y_n$ is strictly positive). A set of covariates $z_1$ is used for this model, with corresponding coefficients denoted $\gamma_1$. For example, using a probit model, we would have $\mbox{P}(d_n = 1) = \Phi(\gamma_1 ^ \top z_1)$. Another choice is to use a count distribution to modelize the probability that $d_n = 0$. For example, using the Poisson distribution ($\mbox{P}(y) = e^ {-\theta}\theta ^ y / y!$) and the log link ($\theta = \exp(\gamma_1 ^ \top z_1$) we have $\mbox{P}(d_n = 0) = \exp(e ^ {-\gamma_1 ^ \top z_1})$. Therefore the first model writes:

$$
\left(\exp\left(- e^{\gamma_1 ^ \top z_{n1}}\right)\right)^{d_n}
\left(1 - \exp\left(-e^{\gamma_1 ^ \top z_{n1}}\right)\right) ^ {1 - d_n}
$$ {#eq-hurdle_part1}

The second model concerns the zero left-truncated sample, ie the subsample of individuals for which $y>0$. Any count model can be used, but the probabilities returned by the chosen distribution for every positive values of $y$ should be divided by their sum, so that they sum to 1. If once again, the Poisson distribution is used, the probabilities for positive values of $y$ are:

$$
\displaystyle\frac{\exp\left(- e^{\gamma_2 ^ \top z_{n2}}\right) e^{y_n\gamma_2^\top z_{n2}} /
y_n !}{1 - \exp\left(- e^{\gamma_2 ^ \top z_{n2}}\right)}
$$ {#eq-hurdle_part2}

Th hurdle model is a **finite mixture** obtained by combining the two
distributions, one that generate the zero values and the other one
that generate the positive values. 
As the two components of the hurdle model depends on a specific
parameters vector ($\gamma_1$ and $\gamma_2$), the estimation can be
performed by independently fitting the two models. The general expression for the probability of
one observation is the product of the two previous probabilities:

$$
\left(\exp\left(- e^{\gamma_1 ^ \top z_{n1}}\right)\right)^{1 - d_n}
\left(\frac{1 - \exp\left(-e^{\gamma_1 ^ \top z_{n1}}\right)}
{1 - \exp\left(-e^{\gamma_2 ^ \top z_{n2}}\right)}
\right) ^{d_n}
\left(\frac{\exp\left(- e^{\gamma_2 ^ \top z_{n2}}\right) e^{y_n\gamma_2^\top z_{n2}}}{y_n !}\right)^{1 - d_n}
$$

This expression illustrates the interest of using the same
distribution (here the Poisson distribution) for the two parts of the model. In this
case, if $z_1 = z_2$, ie if the same set of covariates is used in the two models the hurdle model is nested in the simple count model, the null
hypothesis of the simple model being: $\mbox{H}_0: \gamma_1 = \gamma_2$.

The **zero-inflated Poisson** (or **ZIP** model), also called *with zeros*
model, proposed by @LAMB:92 starts from a count model, for example a Poisson with a set of covariates denoted $z_2$ and corresponding coefficients $\gamma_2$. The probability of 0 ($e^{-\gamma_2^\top
z_{n2}}$ is inflated using a further parameter $\psi_n$ between 0 and 1:

$$
P(Y = 0 | x_n) = \psi_n + (1 - \psi_n) e^{-\gamma_2^\top
z_{n2}} = e^{-\gamma_2^\top z_{n2}} + \psi_n \left(1 - e^{-\gamma_2^\top
z_{n2}}\right) > e^{-\gamma_2^\top z_{n2}}
$$
The Poisson's probabilities for positive values of $y$ are then deflated by
$(1 - \psi_n)$, so that the probabilities for all the possible values of $y$ (0 and positive)
sums to 1:

$$
\mbox{P}(y_n | z_{n2}) = (1 - \psi_n) \frac{e^{-\mu_n}\mu_n ^y}{y!} \mbox{ for
} y_n > 0
$$

$\phi_n$ can be a constant, or a function of covariates ($z_1$) that returns a
value between 0 and 1. For example, a logistic specification can be
used:

$$
\psi_n = \frac{e^{\gamma_1^\top z_{n1}}}{1 + e^{\gamma_1^\top z_{n1}}}
$$
Hurdle and ZIP models are implemented in package **countreg**. `countreg::Hurdle` and `countreg::zeroinfl` both have a `formula` and a `dist` argument. The first one is a two-part formula where the first part describes $x_1$ and the second one $x_2$. If a one-part formula is provided, the same set of covariates is used for the two parts of the model. The second one a character indicating the count model family (`"poisson"` and `"negbin"` are the most common choices). `countreg::hurdle` has a `zero.dist` argument which is a character indicating the distribution for the binomial part of the model (`"poisson"`, `"negbin"` or `"binomial"`; in the latter case, the link can be indicated using the `link` argument). The link used for $\psi_n$ is indicated in `countreg::zeroinfl` by the `link` argument. 

```{r}
hdl_pois <- countreg::hurdle(trips ~ workschl + size + dist + smsa + 
                               fulltime + distnod + realinc + weekend + 
                               car, dist = "poisson", 
                             zero.dist = "poisson", trips)
hdl_nb2 <- update(hdl_pois, dist = "negbin", zero.dist = "negbin", trips)
zip_pois <- countreg::hurdle(trips ~ workschl + size + dist + smsa + 
                               fulltime + distnod + realinc + weekend + 
                               car, dist = "poisson", trips)
zip_nb2 <- update(zip_pois, dist = "negbin")
```
 
The rootograms for the Hurdle and the ZIP models are presented in @fig-root_nb2_hurdle_zip.
Using the AIC criteria, we can now compare all the models estimated in this section. The AIC of the basic model is:

```{r}
#| collapse: true
AIC(pois)
```

Adding either overdispersion (using a Negbin model) or an excess of 0 (using either a hurdle or a ZIP model) leads to:

```{r}
#| collapse: true
AIC(nb2)
AIC(hdl_pois)
AIC(zip_pois)
```

The performance of the hurdle and the ZIP's Poisson models are similar. The AIC is much larger than the one of the Poisson model, but much less than the one of the Negbin2 model.

```{r }
#| label: fig-root_nb2_hurdle_zip
#| fig-cap: "Rootograms for the Negbin2, the Hurdle and the ZIP models"
#| echo: false
nb2_mass <- MASS::glm.nb(formula(pois), trips)
fig_nb2 <-countreg::rootogram(nb2_mass, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
fig_hdl <-countreg::rootogram(hdl_pois, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
fig_zip <-countreg::rootogram(zip_pois, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
figure <- ggpubr::ggarrange(fig_nb2, fig_hdl, fig_zip,
                           labels = c("Negbin2", "Hurdle", "ZIP"),
                           ncol = 3, nrow = 1)
figure
```

Finally the Negbin2 hurdle or zip model can be compared either to the Negbin2 Poisson model (from which the treatment of excess of 0 is added) or to the hurdle or zip Poisson model (from which the treatment of over-dispersion is added):

```{r}
#| collapse: true
AIC(hdl_nb2)
AIC(zip_nb2)
```

The AIC of these two models are quite similar and much lower than the ones of their particular cases (`hdl_pois` and `nb2` for the first one `zip_pois` and `nb2` for the second one). Therefore, the conclusion of this analysis is that either the ZIP or the hurdle Negbin model should be favored. The rootograms for these two models are presented on @fig-root_exc0_nb.

```{r }
#| fig-cap: "Rootograms for the Negbin Hurdle and ZIP models"
#| label: fig-root_exc0_nb
#| echo: false
fig_hdl <-countreg::rootogram(hdl_nb2, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
fig_zip <-countreg::rootogram(zip_nb2, plot = FALSE) %>%
    autoplot + coord_cartesian(xlim = c(0, 15), ylim = c(-3, 12))
figure <- ggpubr::ggarrange(fig_hdl, fig_zip,
                           labels = c("Hurdle", "ZIP"),
                           ncol = 2, nrow = 1)
figure
```



<!-- ## Panel data -->

<!-- ### Fixed effects model -->

<!-- Fixed effects Poisson and NegBin models are proposed by @HAUS:HALL:GRIL:84 . -->

<!-- #### Poisson model -->

<!-- The fixed effects Poisson model is very specific as it doesn't suffer -->
<!-- from the incidental parameter problem and can therefore be obtained -->
<!-- either by estimating the individual effects or by using a sufficient -->
<!-- statistic^[[see chap. 9 @CAME:TRIV:13].]. -->

<!-- In a panel context, the Poisson parameter for individual $\n$ in -->
<!-- period $\t$ is written: -->

<!-- $$ -->
<!-- \poispar_{\n \t}=\id_\n\poiscov_{\n \t}=\id_\n e^{\coef^\top \x_{\n \t}} -->
<!-- $$ -->

<!-- which means that the individual effect is multiplicative. For a given -->
<!-- value of the individual effect, the probability of observing -->
<!-- $\y_{\n \t}$ is: -->

<!-- $$ -->
<!-- \PR(\y_{\n \t} \mid x_{\n \t},\id_\n,\beta)= -->
<!-- \frac{e^{-\poispar_{\n \t}}\poispar_{\n \t}^{\y_{\n \t}}}{\y_{\n \t}!}  = -->
<!-- \frac{e^{-\id_\n\poiscov_{\n \t}}(\id_\n\poiscov_{\n \t})^{\y_{\n \t}}}{\y_{\n \t}!} -->
<!-- $$ -->

<!-- Let $\sumy_\n=\sum_{\t=1}^\T \y_{\n \t}$ be the sum of all the values of -->
<!--   the response for individual $\n$ and -->
<!--   $\sumpois_\n=\sum_{\t=1}^\T \poiscov_{\n \t}$ the sum of the Poisson -->
<!--   parameters. A sum of Poisson variables follows a Poisson -->
<!--   distribution with parameter equal to the sum of the parameters of -->
<!--   the summed variables. We therefore have: -->


<!-- $$ -->
<!-- \PR(\sumy_\n \mid x_\n,\id_\n,\beta)= -->
<!-- \frac{e^{-\id_\n\sumpois_\n}(\id_\n \sumpois_\n)^{\sumy_\n}}{\sumy_\n!} -->
<!-- $$ {#eq-sumy} -->

<!--   Let $\y_\n=(\y_{i1},\y_{i2},\ldots,\y_{\n \t})$ be the vector of values -->
<!--   of $\y$ for individual $\n$. We then have: -->


<!-- $$ -->
<!-- \PR(\y_\n \mid x_\n,\id_\n,\beta) = -->
<!-- \frac{e^{-\id_\n\sum_{\t=1}^\T\poiscov_{\n \t}}\prod_{\t=1}^\T(\id_\n\poiscov_{\n \t})^{\y_{\n \t}}}{\prod_{\t=1}^\T \y_{\n \t}!} -->
<!-- = -->
<!-- \frac{e^{-\id_\n\sumpois_{i}}\id_\n^{\sumy_\n}\prod_{\t=1}^\T\poiscov_{\n \t}^{\y_{\n \t}}}{\prod_{\t=1}^\T \y_{\n \t}!} -->
<!-- $$ {#eq-vecty} -->

<!-- Applying Bayes' theorem, we obtain: -->

<!-- $$ -->
<!-- \PR(\y_\n\mid x_\n, \id_\n, \beta)=\PR(\y_\n\mid x_\n, -->
<!-- \id_\n,\beta, \sumy_\n)\PR(\sumy_\n\mid x_\n, \id_\n, \beta) -->
<!-- $$ -->
<!--   ie the joint probability of the components of $\y_\n$ is -->
<!--   the product of the conditional probability of $\y_\n$ given $\sumy_\n$ -->
<!--   and the marginal distribution of $\sumy_\n$. This conditional -->
<!--   probability is: -->

<!-- $$ -->
<!-- \PR(\y_\n\mid x_\n, \id_\n, \beta, \sumy_\n) -->
<!-- =\frac{\PR(\y_\n\mid x_\n,\id_\n,\beta)} -->
<!-- {\PR(\sumy_\n\mid x_\n,\id_\n,\beta)} -->
<!-- $$ -->

<!-- which implies: -->

<!-- $$ -->
<!-- \PR(\y_\n\mid x_\n,\beta,\sumy_\n)= -->
<!-- \frac{\sumy_\n!}{\sumpois_\n^{\sumy_\n}}\prod_{\t=1}^\T\frac{\poiscov_{\n \t}^{\y_{\n \t}}}{\y_{\n \t}!} -->
<!-- $$ {#eq-pwith} -->

<!--   As for the logit model, $\sumy_\n$ is a sufficient statistic, which -->
<!--   means that it allows to get rid of the individual effects. Taking -->
<!--   the logarithm of this expression and summing over all individuals, -->
<!--   we obtain the within Poisson model: -->

<!-- $$ -->
<!-- \ln L (\y \mid x, \beta, \sumy)= -->
<!-- \sum_{\n=1}^\N\left(\ln \sumy_\n! - \sumy_\n \ln\sum_{\t=1}^\T\poiscov_{\n \t}+\sum_{\t=1}^\T\left(\y_{\n \t}\ln \poiscov_{\n \t}-\ln \y_{\n \t}!\right)\right) -->
<!-- $$ {#eq-loglwa} -->

<!--  or: -->


<!-- $$ -->
<!-- \begin{array}{rcl} -->
<!--   \ln L (\y \mid x, \beta, \sumy)&=& -->
<!--                                 \sum_{\n=1}^\N\left(\ln \sumy_\n! - \sum_{\t=1}^\T \ln \y_{\n \t}! + \sum_{\t=1}^\T \y_{\n \t} \ln\frac{\poiscov_{\n \t}}{\sum_{\t=1}^\T\poiscov_{\n \t}}\right) \\ -->
<!--                             &\propto& \sum_{\n=1}^\N\left(\sum_{\t=1}^\T \y_{\n \t} \ln\frac{\poiscov_{\n \t}}{\sum_{\t=1}^\T\poiscov_{\n \t}}\right) -->
<!-- \end{array} -->
<!-- $$ {#eq-loglw} -->

<!-- As stated previously, the Poisson model is not affected by the -->
<!--   incidental parameter problem, as the same estimator may be obtained -->
<!--   by estimating the individual effects. To show this result, we -->
<!--   take the logarithm of the joint probability for the $\T$ -->
<!--   observations of $\y$ for individual $\n$ (@eq-vecty), -->
<!--   in order to obtain the log likelihood function: -->

<!-- $$ -->
<!-- \ln \PR(\y_\n \mid x_\n,\id_\n,\beta) =-\id_\n\sum_t\poiscov_{\n \t} -->
<!-- + \sum_t\y_{\n \t}\ln(\id_\n\poiscov_{\n \t}) -\sum_t\ln \y_{\n \t}! -->
<!-- $$ {#eq-lvecty} -->

<!--   The first order condition for $\id_\n$ to maximise the log -->
<!--   likelihood function is: -->

<!-- $$ -->
<!-- \frac{\partial \ln \PR_\n}{\partial \id_\n} = -->
<!-- -\sum_t\poiscov_{\n \t}+\frac{1}{\id_\n}\sum_t\y_{\n \t} = 0 -->
<!-- $$ -->

<!--   which implies that: $\id_\n=\frac{\sum_t \y_{\n \t}}{\sum_t \poiscov_{\n \t}}$. -->

<!--   Introducing this expression in (@eq-lvecty) and summing over all -->
<!--   $\n$, we obtain the concentrated log-likelihood function: -->

<!-- $$ -->
<!-- \begin{array}{rcl} -->
<!-- \ln L_{\mbox{conc}}(\y \mid x,\beta) &=&\sum_\n\left(-\sumy_\n + \sumy_\n\ln \sumy_\n +\sum_t \y_{\n \t}\frac{\poiscov_{\n \t}}{\sum_t \poiscov_{\n \t}} - \sum_t \ln \y_{\n \t}!\right) \\ -->
<!-- &\propto& \sum_{\n=1}^\N\left(\sum_{\t=1}^\T \y_{\n \t} \ln\frac{\poiscov_{\n \t}}{\sum_{\t=1}^\T\poiscov_{\n \t}}\right) -->
<!-- \end{array} -->
<!-- $$ {#eq-llconc} -->

<!--   The two log-likelihood functions @eq-loglw} and -->
<!--   @eq-llconc are proportional, they therefore lead to the same -->
<!--   estimators of $\beta$. Moreover, if a logarithmic link is chosen, we -->
<!--   have: $\poiscov_{\n \t}=e^{\coef^\top \x}$. The likelihood is in -->
<!--   this case proportional to: -->

<!-- $$ -->
<!-- \Pi_{\n=1}^\N\Pi_{\t=1}^\T\left(\frac{e^{\coef^\top \x_{\n \t}}}{\sum_\t -->
<!--     e^{\coef^\top \x_{\n \t}}}\right)^{\y_{\n \t}} -->
<!-- $$ -->

<!--   which is similar to the likelihood of a multinomial logit -->
<!--   model for which $\N$ individuals -->
<!--   must choose one among $L$ exclusive alternatives. The difference is -->
<!--   that in this latter model $\y_{\n \t}$ is either equal to 0 or to 1 -->
<!--   and $\sum_{\t} \y_{\n \t}=1$, as in our context each $\y_{\n \t}$ is a -->
<!--   natural integer. -->

<!-- #### Negbin models -->
<!--   @HAUS:HALL:GRIL:84 also propose a fixed effects NegBin -->
<!--   model. We just present below without demonstration the joint -->
<!--   probability for individual $\n$: -->

<!-- $$ -->
<!-- \PR(\y_\n\mid x_\n, \beta, \sumy_\n) =  -->
<!-- \left(\prod_{\t=1}^\T\frac{\Gamma(\poiscov_{\n \t}+\y_{\n \t})}{\Gamma(\poiscov_{\n \t})\Gamma(\y_{\n \t}+1)}\right) -->
<!-- \frac{\Gamma(\sumpois_\n)\Gamma(\sumy_\n+1)}{\Gamma(\sumpois_\n+\sumy_\n)} -->
<!-- $$ {#eq-pnbwith} -->
<!-- \end{equation} -->

<!-- ### Random effects models -->

<!-- #### Poisson models -->

<!--   @HAUS:HALL:GRIL:84 also proposed a \emph{between} and a random -->
<!--   effects Poisson model, integrating out the relevant probabilities -->
<!--   (@eq-sumy and @eq-vecty respectively). A gamma distribution -->
<!--   hypothesis is made for the individual effects, with the following -->
<!--   density: -->

<!-- $$ -->
<!-- f(x,a,b)=\frac{a^b}{\Gamma(b)}e^{-ax}x^{b-1} -->
<!-- $$ -->

<!-- with: -->

<!-- $$ -->
<!-- \Gamma(z)=\int_0^{+\infty}t^{z-1}e^{-t}dt -->
<!-- $$ -->


<!--   the gamma function. The expected value and the variance of $x$ are -->
<!--   respectively: -->

<!-- $$\EV(x)=\frac{b}{a} \mbox{ and }  \mbox{V}(x)=\frac{b}{a^2}$$ -->

<!--   If the model contains an intercept, the expected value is not -->
<!--   identified and we can then suppose, whithout restriction, that it is -->
<!--   equal to 1, which implies $a=b$. We then obtain a gamma distribution -->
<!--   with one parameter (denoted $\gampar$): -->

<!-- $$ -->
<!-- f(\poisint)=\frac{\gampar^\gampar}{\Gamma(\gampar)}e^{-\gampar -->
<!--   \poisint}\poisint^{\gampar-1} -->
<!-- $$ -->

<!-- Integrating out the conditional probabilities (@eq-sumy and -->
<!--   @eq-vecty}), we obtain the unconditional probabilities for the -->
<!--   between and the random effects models: -->

<!-- $$ -->
<!-- \PR(\sumy_\n\mid \xp_\n, \beta) =  -->
<!-- \int_0^{+\infty}\PR(\sumy_\n,\xp_\n,\poisint,\coefp)f(\poisint)d\poisint -->
<!-- =\frac{{\sumpois_\n}^{\sumy_\n}}{\sumy_\n!}\frac{\gampar^\gampar}{\Gamma(\gampar)} -->
<!-- \frac{\Gamma(\sumy_\n+\gampar)}{(\sumpois_\n+\gampar)^{\sumy_\n+\gampar}} -->
<!-- $$ -->

<!-- $$ -->
<!-- \PR(\y_\n,\x_\n,\coef)= -->
<!-- \int_0^{+\infty}\PR(\y_\n,x_\n,\poisint,\beta)f(\poisint)d\poisint -->
<!-- =\prod_{\t=1}^\T\frac{\poiscov_{\n \t}^{\y_{\n \t}}}{\y_{\n \t}!} -->
<!-- \frac{\gampar^\gampar}{\Gamma(\gampar)} -->
<!-- \frac{\Gamma(\sumy_\n+\gampar)}{(\sumpois_\n+\gampar)^{\sumy_\n+\gampar}} -->
<!-- $$ -->

<!--   which leads to the following log likelihood functions: -->

<!-- $$ -->
<!-- \begin{array}{rcl} -->
<!-- \ln L (\sumy \mid \xp, \coefp)&=& -->
<!-- \sum_{\n=1}^\N\left[\sumy_\n\ln \sum_t \poiscov_{\n \t} - \ln \sumy_\n! +\gampar \ln \gampar\right. \\ -->
<!-- &-& \ln \Gamma(\gampar)+\ln \Gamma(\sumy_\n+\gampar)\\ -->
<!-- &-&\left. (\sumy_\n+\gampar)\ln \left(\sum_{\t=1}^\T\poiscov_{\n \t}+\gampar\right)\right] -->
<!-- \end{array} -->
<!-- $$ {#eq-loglb} -->
<!-- \end{equation} -->

<!-- $$ -->
<!-- \begin{array}{rcl} -->
<!--   \ln L (\y \mid \xp, \coefp)&=& -->
<!--   \sum_{\n=1}^\N\left[\sum_t\left(\y_{\n \t}\ln \poiscov_{\n \t} - \ln \y_{\n \t}!\right) +\gampar \ln \gampar\right. \\ -->
<!--   &-& \ln \Gamma(\gampar)+\ln \Gamma(\sumy_\n+\gampar) \\ -->
<!--    & -& \left.(\sumy_\n+\gampar)\ln \left(\sum_{\t=1}^\T\poiscov_{\n \t}+\gampar\right)\right] -->
<!-- \end{array} -->
<!-- $$ {#eq-loglr} -->

<!-- #### Negbin models -->
<!--   In addition to the Poisson, @HAUS:HALL:GRIL:84 also proposed -->
<!--   \emph{between} and random effects NegBin models. We just present -->
<!--   below without demonstration the joint probability for individual -->
<!--   $\n$. -->


<!-- $$ -->
<!-- \PR(\sumy_\n\mid \x_\n, \coefp)= -->
<!-- \frac{\Gamma(\sumpois_\n+\sumy_\n)}{\Gamma(\sumpois_\n)\Gamma(\sumy_\n+1)} -->
<!-- \frac{\Gamma(a+b)\Gamma(a+\sumpois_\n)\Gamma(b+\sumy_\n)} -->
<!-- {\Gamma(a)\Gamma(b)\Gamma(a+b+\sumpois_\n+\sumy_\n)} -->
<!-- $$ {#eq-pnbbet} -->

<!-- $$ -->
<!-- \PR(\y_\n,\x_\n,\coefp)= -->
<!-- \frac{\Gamma(a+b)\Gamma(a+\sumpois_\n)\Gamma(b+\sumy_\n)} -->
<!-- {\Gamma(a)\Gamma(b)\Gamma(a+b+\sumpois_\n+\sumy_\n)} -->
<!-- \left(\prod_{\t=1}^\T\frac{\Gamma(\poiscov_{\n \t}+\y_{\n \t})}{\Gamma(\poiscov_{\n \t})+\Gamma(\y_{\n \t}+1)}\right) -->
<!-- $$ {#eq-pnbre} -->

<!-- ```{r } -->
<!-- #| eval: true -->
<!-- #| cache: true -->
<!-- #| echo: false -->
<!-- form <- doctorco ~ sex + age + I(age ^ 2) + income + insurance + illness + actdays + hscore + chcond -->
<!-- ols <- lm(form, doctor_aus) -->
<!-- pois <- glm(form, doctor_aus, family = poisson) -->
<!-- qpois <- glm(form, doctor_aus, family = quasipoisson) -->
<!-- nb1 <- MASS::glm.nb(form, doctor_aus) -->
<!-- modelsummary::msummary(list(ols = ols, poisson =  pois, quasipois = qpois, "Negbin 2" = nb1), estimate = "{estimate} ({std.error})", statistic = NULL) -->
<!-- ``` -->




<!-- ```{r } -->
<!-- #| eval: false -->
<!-- #| echo: false -->
<!-- mdel <- p_majordrg -->
<!-- tb <- tibble(y = model.response(model.frame(mdel)), -->
<!--              mu = fitted(mdel), -->
<!--              e2 = (y - mu) ^ 2, -->
<!--              z = ( e2 - y) / mu) -->
<!-- nb1 <- lm(e2 ~ mu - 1, tb) -->
<!-- nb2 <- lm(e2 ~ mu + I(mu  ^2) - 1, tb) -->
<!-- ggplot(tb, aes(mu, e2)) + geom_point() + -->
<!-- #   scale_y_continuous(limits = c(0, 100)) + -->
<!--     geom_smooth(method = lm, formula = y ~ x - 1, se = FALSE) + -->
<!--     geom_smooth(method = lm, formula = y ~ x + I(x ^ 2) - 1, se = FALSE, color = 'red') -->

<!-- ggplot(tb, aes(mu, z)) + geom_point() + -->
<!-- #    coord_cartesian(ylim = c(-10, 50)) + -->
<!--     geom_smooth(method = lm, formula = y ~ 1, se = FALSE) + -->
<!--     geom_smooth(method = lm, formula = y ~ x - 1, se = FALSE, color = 'red') -->


<!-- countreg::disptest(cam_triv_86, type = "scoreNB1") -->
<!-- countreg::disptest(cam_triv_86, type = "scoreNB2") -->
<!-- mu <- fitted(cam_triv_86) -->
<!-- y <- model.response(model.frame(cam_triv_86)) -->
<!-- N <- nobs(cam_triv_86) -->

<!-- sum( ( (y - mu) ^ 2 - y) / mu) / sqrt(2 * N) -->
<!-- sum( ( (y - mu) ^ 2 - y)     ) / sqrt(2 * sum(mu ^ 2)) -->

<!-- lm( I( ((y - mu) ^ 2 - y) / mu) ~ mu - 1) %>% summary %>% coef %>% .[3] -->
<!-- ``` -->



<!-- ```{r } -->
<!-- N <- 1E05 -->
<!-- pz <- 0.3 -->
<!-- mu <- 2.5 -->
<!-- v <- dpois(0, mu) -->
<!-- M <- 20 -->
<!-- Probs <- c(pz, (1 - pz) * dpois(1:M, mu) / (1 - dpois(0, mu))) -->
<!-- S <- sample(0:M, size = N, replace = TRUE, prob = Probs) -->
<!-- c(mean(S), var(S)) -->

<!-- Eyp <- mu / (1 - v) -->
<!-- Vyp <- mu * (1 - v) - v * mu ^2 -->
<!-- c(Eyp, Vyp) -->
<!-- c(mean(S[S > 0]), var(S[S > 0])) -->

<!-- Ey <- (1 - pz) * Eyp -->
<!-- c(Eyp, Ey) -->

## Endogeneity and selection {#sec-endog_count}

### Instrumental variable estimators for count data

We've seen in the @sec-general_iv section that, denoting $W$ the matrix of instruments, the **IV** estimator is $\hat{\gamma} = (Z^\top P_W Z) ^ {-1} (Z^\top P_W y)$ (@eq-overidentified_iv) and the **GMM** estimator is  $\hat{\gamma} = \left(Z^\top W \hat{S}^{-1} W ^ \top Z\right)^{-1}Z^\top W \hat{S}^{-1} W ^ \top y$ (@eq-two_steps_iv), with
$\hat{S} = \sum_n \hat{\epsilon}_n ^ 2 w_n w_n^\top$, $\hat{\epsilon}_n$ being the residual of a consistent estimation. 

#### The exponential linear conditional mean model

The linear model is often inappropriate if the conditional
distribution of $y$ is asymmetric. In this case, a common solution is
to use $\ln y$ instead of $y$ as the response: $\ln y_n = \gamma ^ \top z_n + \epsilon$.
This is of course possible only if $y_n > 0\, \forall n$, which is usually not the case with count data. An alternative is to use an exponential linear conditional mean model [@MULL:97], with additive ($y_n = e^{\gamma^\top z_n} + \epsilon_n$) or multiplicative errors ($y_n = e^{\gamma^\top z_n} \nu_n$).

With additive errors, if all the covariates are exogenous, the theoretical moment conditions are $\mbox{E}\left(y - e^{\gamma^\top z} \mid x\right) = 0$ and the corresponding empirical moments are: $Z^ \top \left(y - e ^ {Z \gamma}\right) / N = 0$, which define a non-linear system of $K+1$ equations with $K+1$ unknown parameters ($\gamma$). This corresponds to the first order conditions for the Poisson model with a log link, but it can be used with any non-negative response. If some of the covariates are endogenous, an IV
estimator can be defined. The empirical moments are then:

$$
\frac{1}{N}\sum_{n=1} ^ N (y_n  - e^{\gamma^\top z_n})w_n = W^\top (y  -
e^{Z\gamma}) / N
$$

If the errors are spherical, the variance of the empirical moments vector is: $\sigma_{\epsilon} ^ 2 (W ^ \top W) / N ^2$ and the IV estimator minimizes:

$$
\epsilon ^ \top W \left(\sigma ^ 2 W ^ \top W\right) ^ {-1} W ^ \top \epsilon/ \sigma ^ 2
= \epsilon ^ \top P_W \epsilon / \sigma ^  2
$$

Denoting $\hat{\epsilon}$ the residuals of this regression, the optimal weighting matrix 
$\hat{S} = \sum_{n = 1} ^ N \hat{\epsilon}_n ^ 2 w_n w_n ^ \top$  matrix can be constructed and used in a second step to get the more efficient GMM estimator. 
With additive errors, the only difference with the linear case is that
the minimization process results in a set of non linear equations, so
that some numerical methods should be used. 
With multiplicative errors, we have: $\nu_n = y_n / e^{\gamma^\top
z_n}$. The moment conditions are then: $\mbox{E}\left((y_n/e^{\gamma ^ \top z_n} - 1)^\top w_n\right)=0$
which leads to the following empirical moments:

$$
\frac{1}{N}\sum_{n=1} ^ N (y_n/e^{\gamma^\top z_n} - 1)w_n = W^\top (y/
e^{Z^\top\gamma}- 1) / N = Z^\top \tau_n / N
$$
with $\tau_n = y_n/e^{\gamma^\top z_n} - 1$. 
Minimizing the quadratic form of these empirical moments with $(W^\top
W) ^{-1}$ or $\left(\sum_{n=1} ^ N \hat{\tau}_n ^ 2 w_n
w_n^\top\right) ^ {- 1}$ leads respectively to the IV and the GMM
estimators.

When the number of external instruments is greater that the number of
endogenous variables, the Sargan test enables to test the hypothesis of exogeneity of all the instruments. The
value of the objective function at convergence times the size of the
sample is, under the null hypothesis that all the instruments are
exogenous a $\chi^2$ with a number of degrees of freedom equal to
the difference between the number of instruments and the number of
covariates.

#### Cigarette smoking behaviour

@MULL:97 estimates a demand function for cigarettes which depends on
the stock of smoking habits. This variable is quite similar to a
lagged dependent variable and is likely to be endogenous as the
unobservable determinants of current smoking behaviour should be
correlated with the unobservable determinants of past smoking
behavior. The data set, called `cigmales`, contains observations of 6160 males in 1979
and 1980 from the smoking supplement to the 1979 National Health
Interview Survey. The response `cigarettes` is the number of
cigarettes smoked daily. The covariates are the habit "stock" `habit`,
the current state-level average per-pack price of cigarettes `price`,
a dummy indicating whether there is in the state of residence a
restriction on smoking in restaurants `restaurant`, the age `age` and the
number of years of schooling `educ` and their squares, the number of
family members `famsize`, and a dummy `race` which indicates whether
the individual is white or not. The external instruments are cubic
terms in `age` and `educ` and their interaction, the one-year lagged
price of a pack of cigarettes `lagprice` and the number of years the
state's restaurant smoking restrictions had been in place.

The starting point is a basic count model, ie a Poisson model with a
log link and robust standard errors:

```{r }
#| message = FALSE
cigmales <- cigmales %>%
    mutate(age2 = age ^ 2, educ2 = educ ^ 2,
           age3 = age ^ 3, educ3 = educ ^ 3,
           educage = educ * age)                                 
pois_cig <- glm(cigarettes ~ habit + price + restaurant + income + 
                  age + age2 + educ + educ2 + famsize + race, 
                data = cigmales, family = quasipoisson)
```

The IV and the GMM estimators are estimated using the `micsr::expreg`
function. Its main argument is a two-part formula, where the first
part indicates the covariates and the second part the instruments. The `method` argument can be set to `"iv"` or `"gmm"` to estimate respectively the instrumental variable and the general method of moments estimators and the `error` argument can be equal to `"mult"` (the default) or `"add"` to select respectively multiplicative or additive errors.

```{r }
iv_cig <- expreg(cigarettes ~ habit + price + restaurant + income + 
                   age + age2 + educ + educ2 + famsize + race | 
                   . - habit + age3 + educ3 + educage + 
                   lagprice + reslgth, 
                 data = cigmales, method = "iv")
gmm_cig <- update(iv_cig, method = "gmm")
```
the `twosteps` argument is a logical which default value is `TRUE`
(the GMM estimator); setting this argument to `FALSE` leads to the IV estimator.
The results are presented in @tbl-cigarettes_iv. For the two flavors of instrumental variable estimators, the coefficient of `habit` is about half of the one of the Poisson model, indicating that this covariate is positively correlated with the unobserved determinants of smoking. 

```{r }
#| label: tbl-cigarettes_iv
#| tbl-cap: "Cigarette consumption and smoking habits"
#| echo: false
modelsummary::msummary(list(ML = pois_cig, IV = iv_cig, GMM = gmm_cig),
                       fmt = 4, statistic = 'statistic', coef_omit = "(Intercept)")
```

Computing the Sargan test:

```{r }
# collapse: true
sargan(gmm_cig) %>% gaze
```

we conclude that the exogeneity of the instruments hypothesis is not rejected.

### Sample selection and endogenous switching for count data

In two seminal papers, @HECK:76 and @HECK:79 considered the case where
two variable are jointly determined, a binomial variable and an
outcome continuous variable. For example, the continuous variable can
be the wage and the binomial variable the labor force
participation. In this case, the wage is observed only for the
sub-sample of the individuals who work. If the unobserved
determinants of labor force participation are correlated with the
unobserved determinants of wage, estimating the wage equation only on
the subset of individuals who work will result in an inconsistent
estimator. This case is called the **sample selection** model and has been presented in @sec-tobit2. Consider now the case where the binomial variable is a a dummy
for private vs public sector. In this case the wage is observed for
the whole sample (for the individuals in the public and in the private
sector) but, once again, if the unobserved determinants of the
chosen sector are correlated with those of the wage,
estimating the wage equation only will lead to an inconsistent
estimator. This case is called the **endogenous switching** model. 
Two consistent method of estimation can be used in this context:

- the first one is a two-step method where, in a first step, a probit
  model for the binomial variable is estimated and, in a second step,
  the outcome equation is estimated by OLS with a supplementary
  covariate which is a function of the linear predictor of the
  probit,^[More precisely the inverse Mills ratio.]
- the second one is the maximum likelihood estimation of the system of
  the two equations, assuming that the two error are jointly
  normally distributed.

Let $y$ be a count response (for sake of simplicity a Poisson
variable) and $w$ a binomial variable. The value of $w_n$ is given by
the sign of $\gamma ^ \top z_{n1} + \nu_{n}$, where $\nu_n$ is a standard normal
deviate, $z_{n1}$ a vector of covariates and $\gamma_1$ the associated vector
of unknown parameters. The distribution of $y_n$ is Poisson with
parameter $\lambda_n$, given by $\lambda_n = e ^{\gamma_2 ^ \top z_{n2}} + \epsilon_n$ where $z_{n2}$ is a second set of covariates (which can overlap
with $z_{n1}$), $\gamma_2$ is the corresponding set of unknown parameters and
$\epsilon_n$ is a random normal deviate with 0 mean and a standard
deviation equal to $\sigma$. $\epsilon$ and $\nu$ being potentially
correlated, their joint distribution has to be considered:

$$
\begin{array}{rcl}
f(\epsilon, \nu ; \sigma, \rho)  &=& 
\frac{1}{2\pi\sqrt{1 - \rho ^ 2}\sigma}
e^{-\frac{1}{2} \frac{\left(\frac{\epsilon}{\sigma}\right) ^ 2 + \nu ^
2 - 2 \rho \left(\frac{\epsilon}{\sigma}\right)\nu}{1 - \rho ^2}} \\
&=& \frac{1}{\sqrt{2\pi}}\sigma e ^
{-\frac{1}{2}\left(\frac{\epsilon}{\sigma}\right) ^ 2}
\frac{1}{\sqrt{2\pi}\sqrt{1 - \rho ^ 2}}
e^{-\frac{1}{2}\left(\frac{(\nu - \rho \epsilon / \sigma)}{\sqrt{1 -
\rho ^ 2}}\right) ^ 2}
\end{array}
$$ {#eq-joint_dist_normal}

The second expression gives the joint distribution of $\epsilon$ and $\nu$ as the
product of the marginal distribution of $\epsilon$ and the conditional
distribution of $\nu$.
The conditional distribution of $y$, given $\epsilon$ is:

$$
g(y_n \mid z_{n2}, \epsilon_n;\gamma_2) = \frac{e ^
{-\exp(\gamma_2 ^ \top z_{n2} + \epsilon_n)}e^{y_n(\gamma_2^\top z_{n2} +
\epsilon_n)}}{y_n!}
$$
For $w=1$, the unconditional distribution of $y$ is obtained by
integrating $g$ with the two random deviates $\epsilon$ and $\nu$, using their joint distribution given by @eq-joint_dist_normal:

$$
\begin{array}{rcl}
P(y_n \mid z_{n1}, z_{n2}, w_n = 1) &=& \displaystyle\int_{-\infty} ^ {+\infty}\int_{-\gamma_1 ^ \top
z_{n1}} ^ {+ \infty} g(y_n \mid z_{n2}, \epsilon_n;\gamma_2)
f(\epsilon, \nu;\sigma, \rho) d\epsilon d\nu\\
&=& \displaystyle\int_{-\infty} ^ {+\infty}
g(y_n \mid z_{n2}, \epsilon_n)
\left(\int_{-\gamma_1 ^ \top z_{n1}} ^ {+ \infty} 
\frac{1}{\sqrt{2\pi}\sqrt{1 - \rho ^ 2}}
e^{-\frac{1}{2}\left(\frac{(\nu - \rho \epsilon / \sigma)}{\sqrt{1 -
\rho ^ 2}}\right) ^ 2} 
d\nu\right)\\
&\times&\displaystyle \frac{1}{\sqrt{2\pi}\sigma} e ^
{-\frac{1}{2}\left(\frac{\epsilon}{\sigma}\right) ^ 2}
d\epsilon
\end{array}
$$

By symetry of the normal distribution, the term in braquet is: 

$$
\Phi\left(\frac{\gamma_1 ^ \top z_{n1} + \rho / \sigma \epsilon}{\sqrt{1 -
\rho ^ 2}}\right)
$$

which is the probability that $w = 1$ for a given value of
$\epsilon$. The density of $y$ given that $w = 1$ is then:

$$
\begin{array}{rcl}
P(y_n \mid z_{n1}, z_{n2}, w_n = 1) &=& \displaystyle\int_{-\infty} ^ {+\infty} \frac{e ^
{-\exp(\gamma_2 ^ \top z_{n2} + \epsilon_n)}e^{y_n(\gamma_2^\top z_{n2} +
\epsilon_n)}}{y_n!}\\
&\times&\Phi\left(\frac{\gamma_1 ^ \top z_{n1} + \rho / \sigma \epsilon}{\sqrt{1 -
\rho ^ 2}}\right)\frac{1}{\sqrt{2\pi}\sigma} e ^
{-\frac{1}{2}\left(\frac{\epsilon}{\sigma}\right) ^ 2} d\epsilon
\end{array}
$$ {#eq-probyw1}

By symmetry, it is easily shown that $P(y_n \mid z_{n1}, z_{n2}, w_n = 0)$ is
similar except that $\Phi\left( (\gamma_1 ^ \top z_{n1} + \rho / \sigma \epsilon) / \sqrt{1 - \rho ^ 2}\right)$ is replaced by $\Phi\left( -(\gamma_1 ^ \top z_{n1} + \rho / \sigma \epsilon) / \sqrt{1 - \rho ^ 2}\right)$, so that a general formulation of the
distribution of $y_n$ is, denoting $q_n = 2 w_n -1$:

$$
\begin{array}{rcl}
P(y_n \mid z_{n1}, z_{n2}) &=& \displaystyle\int_{-\infty} ^ {+\infty} \frac{e ^
{-\exp(\gamma_2 ^ \top z_{n2} + \epsilon_n)}e^{y_n(\gamma_2^\top z_{n2} +
\epsilon_n)}}{y_n!}\\
&\times&\Phi\left(q_n\frac{\gamma_1 ^ \top z_{n1} + \rho / \sigma \epsilon}{\sqrt{1 -
\rho ^ 2}}\right)\frac{1}{\sqrt{2\pi}\sigma} e ^
{-\frac{1}{2}\left(\frac{\epsilon}{\sigma}\right) ^ 2} d\epsilon
\end{array}
$$ {#eq-proby}

There is no closed form for this integral but, using the change of
variable $t = \epsilon / \sqrt{2} / \sigma$, we get:

$$
\begin{array}{rcl}
P(y_n \mid z_{n1}, z_{n2}) &=& \displaystyle\int_{-\infty} ^ {+\infty} \frac{e ^
{-\exp(\gamma_2 ^ \top z_{n2} + \sqrt{2} \sigma  t)}e^{y_n(\gamma_2^\top x_{n2} +
\sqrt{2}\sigma t)}}{y_n!}\\
&\times&\Phi\left(q_n\frac{\gamma_1 ^ \top z_{1n} + \sqrt{2} \rho  t_n}{\sqrt{1 - \rho ^ 2}}\right)\frac{1}{\sqrt{\pi}} e ^
{-t ^ 2} dt
\end{array}
$$

which can be approximated using Gauss-Hermite quadrature. Denoting
$t_r$ the nodes and $\omega_r$ the weights:

$$
\begin{array}{rcl}
P(y_n \mid z_{n1}, z_{n2}) &\approx& 
\sum_{r = 1} ^ R
\omega_r \frac{e ^
{-\exp(\gamma_2 ^ \top z_{n2} + \sqrt{2} \sigma  t_r)}e^{y_n(\gamma_2^\top z_{n2} +
\sqrt{2}\sigma t_r)}}{y_n!}\\
&\times&\Phi\left(q_n\frac{\gamma_1 ^ \top z_{n1} + \sqrt{2} \rho  t_r}{\sqrt{1 - \rho ^ 2}}\right)\frac{1}{\sqrt{\pi}} e ^ {-t_r ^ 2}
\end{array}
$$

For the exogenous switching model, the contribution of one observation
to the likelihood is given by @eq-proby. 
For the sample selection model, the contribution of one observation to
the likelihood is given by @eq-probyw1 if $w_n = 1$. If $w_n =
0$, $y$ is unobserved and the contribution of such observations to the
likelihood is the probability that $w_n = 0$, which is:

$$
P(x_n = 0 \mid z_{n1}) = \int_{-\infty} ^ {+\infty}
\Phi\left(q_n\frac{\gamma_1 ^ \top z_{n1} + \rho / \sigma \epsilon}{\sqrt{1 -
\rho ^ 2}}\right)\frac{1}{\sqrt{2\pi}\sigma} e ^
{-\frac{1}{2}\left(\frac{\epsilon}{\sigma}\right) ^ 2} d\epsilon
$$

The ML estimator is computing intensive as the integral has no closed
form. One alternative is to use non-linear least squares, by first
computing the expectation of $y$. @TERZ:98 showed that this expectation is:

$$
\mbox{E}(y_n\mid z_{n1}, z_{n2}) = \exp\left(\gamma_2 ^ \top z_{n2} + \ln \frac{\Phi\left(q_n(\gamma_1 ^\top
z_{n1} + \theta)\right)}{\Phi\left(q_n(\gamma_1 ^\top z_{n1})\right)}\right)
$$
for the endogenous switching case and:
$$
\mbox{E}(y_n\mid z_{n1}, z_{n2}, w_n = 1) = \exp\left(\gamma_2 ^\top z_{n2} + \ln \frac{\Phi(\gamma_1 ^\top z_{n1} + \theta)}{\Phi(\gamma_1 ^ \top z_{n1})}\right)
$$
for the sample selection case, where $\theta = \sigma\rho$.
@GREE:01 noted that, taking a first order Taylor series of $\ln
\frac{\Phi(\gamma_1 ^\top z_{n1} + \theta)}{\Phi(\gamma_1 ^ \top z_{n1})}$
around $\theta = 0$ gives: $\theta \phi(\gamma_1 ^ \top z_{n1}) /
\Phi(\gamma_1 ^ \top z_{n1})$, which is the inverse mills ratio that is
used in the linear case in order to correct the inconsistency due to
sample selection. As $\gamma_1$ can be consistently estimated by a
probit model, the NLS estimator is obtained by minimizing with respect
to $\gamma_2$ and $\theta$ the sum of squares of the following residuals:

$$
y_n - e^{\gamma_2 ^ \top z_{n2} + \ln
\frac{\Phi(\hat{\gamma_1} ^ \top z_{n1} + \theta)}{\Phi(\hat{\gamma_1}
^ \top z_{n1})}}
$$

As it is customary for a two-steps estimator, the covariance
matrix of the estimators should take into account the fact that
$\gamma_1$ has been estimated in the first step. Moreover, only $\theta
= \rho \sigma$ is estimated. To retrieve an estimator of $\sigma$,
@TERZ:98 proposed to insert into the log-likelihood function the
estimated values of $\gamma_1$, $\gamma_2$ and $\theta$ and then to
maximize it with respect to $\sigma$.^[These once again requires the use of
Gauss-Hermite quadrature, but the problem is considerably simpler as
the likelihood is maximized with respect with only one parameter.]

The `micsr::escount` function estimates the endogenous switching and the
sample selection model for count data. The first one is obtained by
setting the `model` argument to `'es'` (the default) and the second
one to `'ss'`. The estimation method is selected using the `method`
argument, which can be either `"twosteps"` for the two-steps non-linear
least squares model (the default) or `"ml"` for maximum
likelihood. The model is described by a two-part formula, on the left and on the right side. On the left side, the outcome and the binomial responses are indicated. On the right side, the first part contains the covariates for the outcome equation and  the second part the covariates of the selection equation. Relevant only for the ML method, the
`hessian` argument is a boolean: if `TRUE`, the covariance matrix of
the coefficients is estimated using the numerical hessian, which is
computed using the `hessian` function of the **numDeriv**
package, otherwise, the outer product of the gradient is used and `R` is an integer that indicates the number of points used for the
Gauss-Hermite quadrature method.
`escount` returns an object of class `escount` which inherits from `micsr`. 

#### Physician advice and alcohol consumption

@KENK:TERZ:01 investigate the effect of physician's advice on alcohol
consumption, the data set is called `drinks`. The outcome variable `drinks` is the number of drinks in
the past two weeks and the selection variable `advice` is a dummy based
on the respondents' answer to the question "Have you ever been told
by a physician to drink less". The unobserved part of the equation
indicating the propensity to receive an advice form the physician can
obviously be correlated with the one of the alcohol consumption
equation. The covariates are monthly income in thousands of dollars
(`income`), `age` (a factor with six 10 years categories of age),
education in years (`educ`), `race` (a factor with levels `white`,
`black` and `other`), the marital status (`marital`, a factor with
levels `single`, `married`, `widow`, `separated`), the employment
status (a factor `empstatus` with levels `other`, `emp` and `unemp`)
and the region (`region`, a factor with levels `west`, `northeast`,
`midwest` and `south`). For the binomial part of the model, the same
covariates are used (except of course `advice`) and 10 supplementary
covariates indicating the insurance coverage and the health status are
added.

@tbl-drinksres presents the results of a Poisson
model and of the endogenous switching model, estimated by the
two-steps non-linear least squares and by the maximum likelihood estimators.^[The
coefficients of `marital`, `empstatus` and `region` are omitted to
save place.]
The coefficient of `advice` in the alcohol demand equation is positive
in the Poisson model, which would imply that a physical advice have a positive effect of on alcohol consumption. The estimation of the
endogenous switching model shows that this positive coefficient is
due to the positive correlation between the error terms of the two
equations (the unobserved propensities to drink and to receive an
advice from a physician are positively correlated).

```{r }
#| label: drinksest
#| warning: false
#| cache: true
kt_pois <- glm(drinks ~ advice + income + age + educ + race + marital + 
                 empstatus + region, data = drinks, family = poisson)
kt_ml <- escount(drinks | advice ~ advice + income + age + educ + race + 
                   marital + empstatus + region | . - advice + medicare + 
                   medicaid + champus + hlthins + regmed + dri + limits + 
                   diabete + hearthcond + stroke,
                 data = drinks, method = "ml")
kt_2s <- update(kt_ml, method = "twosteps")
```

```{r }
#| label: tbl-drinksres
#| echo: false
#| warning: false
#| tbl-cap: "Poisson and endogenous switching models for alcohol demand"
modelsummary::msummary(list("Poisson" = kt_pois, "2 steps" = kt_2s, "ML" = kt_ml),
         gof_omit = "AIC|BIC|Log.Lik.",
         label = "tbl-drinksres",
         coef_omit = "region|marital|empstatus|age")
```

