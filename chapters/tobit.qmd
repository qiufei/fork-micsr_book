<!-- precportfolio food trafficcitations + charitable dans tobit1 -->

```{r }
#| include: false
source("../_commonR.R")
```

```{r }
#| include: false
library("micsr")
library("tidyverse")
```

# Censored and truncated models

## Introduction

We'll discuss in this chapter models for which the value of the
response is continuous and observed only in a certain range. This
variables are truncated for a certain value, which can be on the left
side of the distribution ($a$), on the right side ($b$) or on both
sides. Therefore, the distribution of such a variable is a mix of a
discrete and of a continuous distribution:

- the value of $y$ is continuous on the $]a, b[$ interval, and its
  distribution can be described by a density function $f(y)$,
- there is a mass of probability on $a$ or/and on $b$, which is
  described by a probability $P(y = a)$ or/and $P(y = b)$.
  
This kind of response are observed in different context in
economics. The first one is a corner solution, the second one is a
problem of missing data, also called censoring and the last one is a
problem of selection, also called incidental truncation.

### Corner solution

Consider a consumer who can buy two goods, food ($z$) and vacations
($y$). Assume that the preferences of the consumer can be represented by
the following utility function:

$$
U(y,z) = (y + \mu) ^ \alpha z ^ {1 - \alpha}
$$

where $0 < \alpha < 1$ and $\mu > 0$. The consumer seeks to maximize
its utility subject to its budget constrain, which writes $x=p_z z +
p_y y$, where $x$ is the total expense and $p_z$ and $p_y$ are the
unit prices. For an interior solution, the consumer should equate the
marginal rate of substitution to the price ratio\:

$$
\frac{\alpha}{1-\alpha}\frac{z}{y + \mu} = \frac{p_y}{p_z}
$$

We therefore have $p_z z = \frac{1 - \alpha}{\alpha} p_y(y +
\mu)$. Replacing in the budget constraint and solving for $z$ and then
for $y$, we finally get the demand functions.

$$
\left\{
\begin{array}{rcl}
z &=& (1-\alpha)\frac{x}{p_z} + (1-\alpha)\frac{p_y}{p_z} \mu \\
y &=& \alpha \frac{x}{p_y} - (1-\alpha)\mu \\
\end{array}
\right.
$$

Note that the demand function for $y$ can return negative values,
which of course is impossible. Therefore, the pseudo demand function
previously written are only suitable for an interior solution, ie when
both goods are consumed. This is only the case for a sufficient level
of income, namely $\bar{x} = x > \frac{1 - \alpha}{\alpha} p_y
\mu$. For a lower level of income, we have $y = 0$ and therefore $z =
x / p_z$. 


```{r }
#| label: fig-cornersol
#| echo: false
#| fig.cap: "Internal and corner solution"
library("tidyverse")
library("ggplot2")
library("micsr")
Rs <- c(0.5, 0.9, 1, 2, 3)
xs <- ifelse(Rs > 1, 0.5 * (Rs - 1), 0)
ys <- ifelse(Rs > 1, 0.5 * (Rs + 1), Rs)
pts <- tibble(x = xs, y = ys, label = LETTERS[1:length(Rs)])
ci <- function(x, R) ifelse(R > 1, 0.5 * (R + 1), sqrt(R)) ^ 2 / (1 + x)
cols <- c("blue", "red", "green", "orange", "purple", "purple")

A <- ggplot() +
    scale_x_continuous(limits = c(-1, 3)) +
    scale_y_continuous(limits = c(0, 3)) +
    geom_vline(xintercept = -1, linetype = "dotted") +
    geom_vline(xintercept = 0) +
    geom_hline(yintercept = 0) +
    ggrepel::geom_label_repel(data = pts, aes(x, y, label = label)) + 
    theme_void()

for (i in 1:6)
    A <- A + geom_function(fun = ci, args = list(R = Rs[i]),
                           linetype = "dashed", color = cols[i]) +
        geom_segment(aes(x = Rs[i], y = 0, xend = 0, yend = Rs[i]))      
A
```

This situation is depicted on figure @fig-cornersol. Points $D$
and $E$ correspond to interior solutions for large values of the
income. On the contrary, $A$ and $B$ are corner solutions for low
income households. We then have $y=0$ and the value of the marginal
rate of substitution (the slope of the indifference curve) is lower
than the price ratio. The $C$ point corresponds to the level of income
that leads to a corner solution but for which the marginal rate of
substitution equals the price index. The consumption of $y$ starts
when the income is greater than this level.

The expression simplifies by taking as the response the expense for
the good, and not the quantity. We then have: $p_y y = \alpha x - (1 -
\alpha)p_y\mu$ or, replacing $p_y \mu$ by it's expression in terms of
$\bar{x}$\:

$$
p_y y = \alpha x - \alpha \bar{x}
$$

In a linear regression context, the slope is therefore the marginal
propensity to consume the specific good (for 1 more $ of income, the
expense increase by $\alpha$ $) and the intercept is the opposite of
$\alpha$ times the minimum income ($\bar{x}$) for which the
consumption of the good starts.

The consumption of good $y$ is an example of a truncated variable,
more precisely a left-zero truncated variable. Note that, as stressed
by @WOOL:10, 0 is a relevant value for the variable. There is a
positive probability to get the 0 value, but all the 0 values don't
have the same meaning. For example, at point $B$, $y = 0$, but a small
increase of the income would lead the household to start consuming the
good. On the contrary, at point $A$, $y$ also equals 0, but even with
a large increase of income, the household would still consume only
good $z$ and not good $y$.

### Data censoring and truncation

Data censoring occurs when the value of the variable is reported only
in a certain range $]a,b[$ and is set to $a$/$b$ otherwise. For
example, @CROM:PALM:URBA:97 estimate the demand for food using
households survey data in the Netherlands. For the upper five
percentiles (13030 Dfl), the expenditure is not reported, but is
replaced by the average value (17670 Dfl). Therefore, the response is
right-truncated with $b =13030$. In this case, the data censoring
process takes the top-coding form. 

Sometimes, the censoring process leads to a truncation sample, which
means that only observations for which the response is in the
continuous observed range are selected. A classic example is
@HAUS:WISE:76 and @HAUS:WISE:77 who used data from the New Jersey
negative income tax experiment, for which families with incomes above
one and a half times the poverty level were excluded.

### Sample selection

The last data generating process is the one of sample selection. It
means that the observation of the response in a particular sample is
not random because of a self selection process. This kind of process
was first analyzed by @GRON:73 in the context of women participation
to the labor force. The response is the wage offered to women, who can
be considered as a linear function of the age, the education and other
covariates. The wage is only observed for women which participate to
the labor market and this decision to participate is based on the
comparison between the offered wage and the reservation wage. More
precisely, with a utility function of the form $U(c, L)$, where $c$ is
units of consumption and $L$ hours not worked and with a budget
constrained written as $R + w (\bar{T} - L) = p c$ with $R$ the non labor
income, $\bar{T}$ the total number of daily hours available, $w$ the
hourly wage and $p$ the unit price of consumption.



Although it is clearly a different form of truncation compared to the
case of a corner solution or data censoring, the models that deal with
this kind of response are much alike, it therefore make sens to
consider these two cases in the same chapter.

<!-- Data censoring often occurs when the response is a duration, for -->
<!-- example an unemployment spell. If the interview starts in january 2020 -->
<!-- and ends in december 2020 and that every individual reports mouthly -->
<!-- their situation, 3 situations can be observed: -->

<!-- - the job is lost in march and another job is found in september, -->
<!--   therefore the unemployment spell is 7 month, -->
<!-- - the individual was unemployed when the survey started and found a -->
<!--   job in june: in this case, the response is left-censored and all we -->
<!--   can say about the unemployment spell (that ended) is that it is at -->
<!--   least equal to 6 month, -->
<!-- - the individual lost his job in september and is still unemployed at -->
<!--   the end of the survey. The unemployment spell is therefore ongoing -->
<!--   and will length at least 4 months. -->
  
### Truncated and censored samples

The responses in this chapter are **truncated variables**, but the
sample used can be either truncated or censored. For the demand for
vacations\:

- a **censored sample** consists on households for which the demand is
  strictly positive and on households which never take vacations,
- a **truncated sample** consists only on households for which the demand
  is strictly positive. 
  
Samples used in consumer expenditure surveys are censored. A
representative sample of households is surveyed and this includes
households that don't have any expense on vacations during the survey.

On the contrary, sample that consists on individual surveyed in a
travel agency or in an airport are truncated samples, ie sample for
which the variable of interest (vacation expenses) is strictly
positive. 

Estimation of models using censored samples are called **censored
regression models** or tobit models. The tobit name comes from James
Tobin, who is the first economist that proposed this model in
econometrics (@TOBI:58) and was proposed by @GOLD:64 by similarity
with the probit and the logit models.

Estimation on truncated samples leads to the **truncated regression
models** (@CRAG:71 and @HAUS:WISE:76).

In a classic paper, @AMEM:84 surveyed different flavors of the tobit
model and proposed a typology of 5 categories, that he called tobit1,
tobit2, ..., tobit5. We'll concentrate on this chapter on the first
two categories:

- the **tobit1** model, which is a model with one equation which
  explains jointly the probability that the value is in the observable
  range and the level of the variable if it is observed,
- the **tobit2** model, which is a bivariate model, the first response
  being an unobserved latent variable whose sign determines whether
  the variable is observed or not and the second response indicating
  the value of the response when it is observed.


## Tobit1 model

We'll denote a tobit1 (or tobit for short)  model a linear model of the usual form:

$$
y = \alpha + \beta x + \epsilon
$$

where $y$ is only observed in a certain range, say $y \in ] a,
b[$ and is assumed to follow a normal distribution.

Several example may help to understand what kind of real situations
are included in this setting:

- @CROM:PALM:URBA:97 estimate the demand for food using households
  survey data in the Netherlands. For the upper five percentiles
  (13030 Dfl), the expenditure is not reported, but is replaced by the
  average value (17670 Dfl). Therefore, we have $b = 13030$,
- @MINI:PAST:10 and @HOCH:03 estimate the share of risk-less assets,
  which can be either an internal solution, or a corner solution with
  the share equal to 0 or 1 ($a = 0$ and $b=1$).

In a semi-parametric setting, no hypothesis are made on the
distribution of $\epsilon$. On the contrary, a fully  parametric model
will specify the distribution of $\epsilon$, for example it will
impose that $\epsilon \sim N (0, \sigma^2)$, ie that the errors of the
model are normal and homoscedastic. In the context of the linear
regression model, violation of these assumptions are not too severe,
as the estimator is still consistent. This is not the case for the
model studied in this chapter, as wrong assumptions of
homoscedasticity and normality will lead to biased and inconsistent
estimators.

  
### Truncated normal distribution, truncated and censored sample


Early model in this field assume that the errors of the model are
normal. But the fact that the response is truncated implies that the
conditional distribution of $y$ (or the distribution of the errors) is
truncated normal. This distribution is represented in
@fig-normal2Trunc.


```{r }
#| label: fig-normal2Trunc
#| fig.cap: "Truncated normal distribution"
#| echo: false
knitr::include_graphics("./tikz/fig/normal2Trunc.png", auto_pdf = TRUE)
```

Starting from a normal distribution
$\frac{1}{\sigma}e^{-\frac{1}{2}\left(\frac{y-\mu_y}{\sigma_y}\right)^2}$,
we first compute the probability that $a < y < b$ and we divide
the normal density by this probability, which is:

$$
\Phi\left(\frac{b-\mu_y}{\sigma_y}\right) - \Phi\left(\frac{b-\mu_y}{\sigma_y}\right)
$$

The density of $y$ is therefore:

$$
f(y) = \frac{1}{\sigma_y}
\frac{\phi\left(\frac{y - \mu_y}{\sigma_y}\right)}{\Phi\left(\frac{b-\mu_y}{\sigma_y}\right) -
\Phi\left(\frac{b-\mu_y}{\sigma_y}\right)}
$$

so that $\int_{a}^{b} f(y) dy = 1$.

As $y$ is truncated, its expected value and its variance are not
$\mu_y$ and $\sigma_y ^ 2$. More precisely, left(-right) truncation will lead
to an expected value greater(-lower) than $\mu_y$. On
@fig-normal2Trunc, the expected value is greater than $\mu_y$
because the truncation is more severe on the left. Obviously,
reducing the range of the values of $y$ implies a reduction of the
variance, so that $\mbox{V}(y) < \sigma_y^2$. 

To compute the first two moments of this distribution, it's easyer to consider
first a truncated standard normal deviates $z$. We then have:

$$
\mbox{E}(z) = \frac{\int_{l}^{u} z\phi(z) dz}{\Phi(u) -
\Phi(l)}=\frac{\left[ -\phi(z)\right]_{z_l}^{z_u}}{\Phi(u) - \Phi(l)}
=\frac{\phi(l) - \phi(u)}{\Phi(u) - \Phi(l)}
$$

$$
\begin{array}{rcl}
\mbox{V}(z) &=& \frac{\int_{z_l}^{z_u} z ^ 2\phi(z) dz}{\Phi(z_u) -\Phi(z_l)}-E(z) ^ 2=
\frac{\left[\Phi(z) -z\phi(z)\right]_{z_l}^{z_u}}{\Phi(u) - \Phi(l)} -
E(z) ^ 2 \\
&=& 
1 - \frac{z_u \phi(z_u) - z_l \phi(z_l)}{\Phi(z_u) - \Phi(z_l)} - \frac{(\phi(z_u) -
\phi(z_l)) ^ 2}{(\Phi(z_u) - \Phi(z_l)) ^ 2}
\end{array}
$$

Let $r(x) = \frac{\phi(x)}{\Phi(x)}$.  $r(x)$ is called the inverse
Mills ratio, it is the ratio of the normal density and the normal
cumulative density function. This function is represented on @fig-mills.

```{r }
#| label: fig-mills
#| fig.cap: "Inverse Mills ratio"
#| echo: false
knitr::include_graphics("./tikz/fig/mills_ratio.png", auto_pdf = TRUE)
```

$r$ is a decreasing function, with $\displaystyle \lim_{x\rightarrow
-\infty}= - x$ and $\displaystyle \lim_{x\rightarrow +\infty} = 0$.

The derivative of $r$ is:

$$
r'(x) = - r(x)\left[r(x) + x\right]
$$

With these notations, the first two moments of the truncated normal
distribution are:

$$
\begin{array}{rcl}
\mbox{E}(z) &=& \left[r(z_l) - r(z_u)\right]
\frac{\Phi(z_l)}{\Phi(z_u) - \Phi(z_l)} - r(z_u) \\
\mbox{V}(z) &=&  1 + \left[z_l r(z_l) - z_u r(z_u)\right] 
\frac{\Phi(z_l)}{\Phi(z_u) - \Phi(z_l)}
 - z_u r(z_u) - \mbox{E}(z) ^ 2
\end{array}
$$

If $z$ is only left-truncated, $z_u \rightarrow +\infty$,
$r(z_u)\rightarrow 0$, $z_u \times r(z_u)\rightarrow 0$ and
$\frac{\Phi(z_l)}{\Phi(z_u) - \Phi(z_l)} r(z_l) \rightarrow
\frac{\phi(z_l)}{1 - \Phi(z_l)} = \frac{\phi(z_l)}{\Phi(-z_l)} =
r(-z_l)$. The first two moments then reduce to:

$$
\left\{
\begin{array}{rcl}
\mbox{E}(z) &=&  r(-z_l) \\
\mbox{V}(z) &=&  1 - r(-z_l)\left[r(-z_l) - z_l\right] = 1 + r'(-z_l)
\end{array}
\right.
$$


If $z$ is only right-truncated\:


$$
\left\{
\begin{array}{rcl}
\mbox{E}(z) &=& - r(z_u) \\
\mbox{V}(z) &=&  1 - r(z_u)\left[r(z_u) + z_u\right] = 1 + r'(z_u)
\end{array}
\right.
$$

Consider now the linear model: $y=\alpha + \beta x + \epsilon$. For
the most common case where $y$ is zero-left truncated, $\epsilon$ is
left truncated at $-\beta x$ and we get, denoting $v = \beta x / \sigma$:

$$
\left\{
\begin{array}{rcl}
\mbox{E}(y\mid x) &=& \alpha + \beta x + \mbox{E}(\epsilon\mid x) =
\sigma_\epsilon r\left(\frac{\alpha + \beta  x}{\sigma}\right)\\
\mbox{V}(y\mid x) &=& \sigma_\epsilon ^ 2 \left[1 +
r'\left(\frac{\alpha + \beta x}{\sigma}\right)\right]
\end{array}
\right.
$$

Therefore, truncation have two consequences for the linear regression
model:

- the conditional expectation of $y$ is now longer equal to
  $\alpha+\beta x$ or, stated differently, the errors of the model are
  correlated with the covariate ($\mbox{E}(\epsilon\mid x) \neq 0$),
- the conditional variance depends on $x$ so that the errors of the
  model are heterosckedastic.
  
The second point implies that ordinary least squares estimators are
inefficient, the first that they are biased and inconsistent. For
$\beta > 0$, this correlation is shown on figure @fig-normtrunc4
which presents the distribution of $y$ for different values of $x$. The
mode of the distribution is $\alpha + \beta x$ (and it would also be
$\mbox{E}(y\mid x)$ if the response weren't
truncated). $\mbox{E}(y\mid x)$ is obtained by adding
$\mbox{E}(\epsilon\mid x)$ to $\alpha + \beta x$ .

```{r }
#| label: fig-normtrunc4
#| fig.cap: "Truncated normal distribution for $y$"
#| echo: false
#| out.width: "100%"
knitr::include_graphics("./tikz/fig/normTrunc4.png", auto_pdf = TRUE)
```

As $x$ increases, $\alpha + \beta x$ increase, which reduce
$\mbox{P}(y < 0)$ and makes the truncated normal density closer to the
untruncated one. As we can see on the figure, the distance between the
mode of the distribution and $\mbox{E}(y|x)$, which is
$\mbox{E}(\epsilon\mid x)$ decreases with higher values of $x$.

This situation is illustrated, using simulated data on figure
@fig-simulbias.

```{r }
#| label: fig-simulbias
#| echo: false
#| fig.cap: "OLS bias in a truncated sample"
library("tidyverse")
set.seed(1)
alpha <- 1
beta <- 1
sigma <- 0.5
R <- 1E03
x <- rnorm(R, - 1)
eps <- rnorm(R, 0, sigma)
ys <- alpha + beta * x + eps
y <- pmax(ys, 0)
d <- tibble(ys = ys, y = y, x = x)
Eytrunc <- function(x){
    z <- (alpha + beta * x) / sigma
    (alpha + beta  * x) + sigma * dnorm(z) / pnorm(z)
}

Eycens <- function(x){
    z <- (alpha + beta * x) / sigma
    (alpha + beta * x) * pnorm(z) + sigma * dnorm(z)
}
d %>% filter(y > 0) %>% ggplot(aes(x, y)) + geom_point() +
    geom_smooth(method = "lm", se = FALSE, linetype = "dotted") +
    geom_function(fun = Eytrunc, color = "red") +
    geom_abline(intercept = 1, slope = 1)
```

The black line denotes the "true" relation between $x$ and $y^*$, ie
it is the straight line defined by $\beta ^ \top x$. The blue dotted
line is the regression line for this sample. Its slope is slightly
lower than $\beta$, which illustrate the fact that the OLS estimator
of the slope is downward biased (if $\beta > 0$, which is the case
here). The red line depicts $\mbox{E}(y\mid x, y > 0)$. For large
values of $x$, it is indistinct from the $\beta^\top x$ straight
line, which indicates than in this range of values for $x$,
$\mbox{E}(y\mid x, y > 0)$ is almost equal to $\beta^ \top x$, which
means than the correlation between $x$ and $\epsilon$ almost
vanishes. On the opposite, for low values of $x$, the gap between
$\mbox{E}(y\mid x, y > 0)$ (the red line) and $\beta ^ \top x$
increases. This gap is $\mbox{E}(\epsilon\mid x, y > 0)$, it is
positive and is particularly high for very low values of $x$. As
$x\rightarrow -\infty$, $\mbox{E}(y\mid x, y > 0)\rightarrow 0$
and therefore $\mbox{E}(\epsilon\mid x, y > 0)\rightarrow -\beta^\top x$.


By now, we have considered a truncated sample, which is a sample
containing only observed values of $y$. Consider now the following
variable $\tilde{y}$ defined by:

$$
y\sim N(\mu_y, \sigma_y)
$$

and:

\begin{equation}
\left\{
\begin{array}{rclccc}
\tilde{y}&=&a &\mbox{ if }& y < a\\
\tilde{y}&=&y  &\mbox{ if }& a \leq y \geq b\\
\tilde{y}&=&b &\mbox{ if }& y > b\\
\end{array}
\right.
\end{equation}

or, for the most common case where $y$ is zero-left truncated:

\begin{equation}
\left\{
\begin{array}{rclccc}
\tilde{y}&=&0 &\mbox{ if }& y < 0\\
\tilde{y}&=&y  &\mbox{ if }& y \geq 0\\
\end{array}
\right.
\end{equation}

$\tilde{y}$ is used for censored samples, which means samples where we
have observations for the whole range of $y$ values, the values of $y$
being set to the truncation point when they are outside the observable
range.

In this case, the conditional expected value of $\tilde{y}$ can be
computed as the weighted average of the expected values given that $y$
is greater or lower than 0, the first one being the expected value of
$y$ in the truncated sample and the second one being 0:

$$
\begin{array}{rcl}
\mbox{E}(\tilde{y}\mid x) &=& \left[1 - \Phi\left(\frac{\beta^\top
x}{\sigma}\right)\right] \times 0 + \Phi\left(\frac{\beta^\top
x}{\sigma}\right) \times \mbox{E}(\tilde{y}\mid x, y > 0) \\
&=& 
(\alpha + \beta x) \Phi\left(\frac{\beta^\top x}{\sigma}\right) + 
\sigma_\epsilon \phi\left(\frac{\beta^\top x}{\sigma}\right)
\end{array}
$$

As for the previous case, the conditional expected value of
$\tilde{y}$ still depends on $x$ which implies that the OLS estimator
is biased and inconsistent. 

$$
\mbox{E}(y\mid x)=\Phi\left(\frac{\beta^\top x}{\sigma}\right)\beta^\top x + \sigma \phi\left(\frac{\beta^\top x}{\sigma}\right)
$$

Least squares estimation is illustrated, using simulated data, on figure
@fig-simulbiascens.

```{r }
#| label: fig-simulbiascens
#| echo: false
#| fig.cap: "OLS bias in a censored sample"
d  %>% ggplot(aes(x, y)) + geom_point() +
    geom_smooth(method = "lm", se = FALSE, linetype = "dotted") +
    geom_function(fun = Eycens, color = "red") +
    geom_abline(intercept = 1, slope = 1)
```

The downward bias of the slope seems more severe than for the
truncated sample because there are much more observations for very low
values of $x$, ie in the range of the values of $x$ where the
correlation between $x$ and $\epsilon$ is severe.


### Bias of the OLS estimator


We have seen in the previous section that OLS estimation, either with
a truncated or censored sample leads to a biased estimator. In this
section, we'll present the asymptotic bias, using the hypothesis that
$x$ is normally distributed^[The bias of the OLS estimator were first
computed by @GOLD:81 for the truncated sample and by
@GREE:81 for the censored sample.]. For sake of simplicity,
we'll consider the simple linear regression model, but the analysis
can be easily extended to the multiple linear regression model.

Estimating the model by OLS on the untruncated sample, we obtain:
$\hat{\beta}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma_{x}^2}}$ which
converges to the "true" value, which is 
$\beta=\frac{\sigma_{xy}}{\sigma_{x}^2}=\rho\frac{\sigma_y}{\sigma_x}$. The
demonstration consists on computing the probability limit of the OLS
estimator for the truncated and for the censored sample and to express
the resulting value, $\beta^*=\frac{\sigma^*_{xy}}{\sigma_x^{*2}}$ for
the truncated sample and
$\tilde{\beta}=\frac{\tilde{\sigma_{xy}}}{\sigma_x^2}$ for the
censored sample as a function of $\beta$.

Suppose that $y$ and $x$ are drawn from a bivariate normal
distribution with a coefficient of correlation equal to $\rho$.

$$
\left( \begin{array}{c} x \\ y \end{array} \right) \sim
N \left( \left( \begin{array}{c} \mu_x \\ \mu_y \end{array} \right) ,
\left(  \begin{array}{cc} \sigma_x ^ 2 & \rho \sigma_x \sigma_y \\ 
                          \rho \sigma_x \sigma_y & \sigma_y ^ 2
						  \end{array} \right)
\right)
$$

Using well-known results about the bivariate normal distribution, the
first two moments of the conditional distribution of $x$ are:

$$
\mbox{E}(x\mid y) = \mu_x + \rho \frac{\sigma_x}{\sigma_y}(y - \mu_y)
$$

$$
\mbox{V}(x\mid y) = (1 - \rho ^ 2)\sigma_x ^ 2
$$

The unconditional expectation of $x$ is obtained as
$\mbox{E}_y\left[\mbox{E}(x\mid y)\right]=\mu_x$ and the unconditional
variance by using the variance decomposition formula:

$$
\mbox{V}(x) = \mbox{V}_y\left[\mbox{E}(x\mid y)\right] + \mbox{E}_y\left[\mbox{V}(x\mid y)\right]=\sigma_x^2
$$

Now consider that $y$ is truncated. Denote:
$\sigma_y^{*2}=\theta \sigma_y ^ 2$. $\sigma_y^{*2}$ is the variance
of $y$ in the observable range. It is necessary lower than
$\sigma_y^2$, so that $0\leq \theta \leq 1$.


The conditional distribution of $x$ is unchanged, but the marginal
distribution of $y$ is. Therefore, to compute the unconditional
moments of $x$, the expectations are computed using the truncated
density function of $y$ and we'll denote $\mbox{E}_y^*$ and
$\mbox{V}_y^*$ the expected value and the variance operators that
use this density function. Denoting $\mu_y^*$ and $\sigma_y^{2*}$ the
mean and variance of $y$ in the observed range, we get:

$$
\mu_x ^ * = E^*_y\left(\mbox{E}(x\mid y\right) = \mu_x + \rho
\frac{\sigma_x}{\sigma_y}(\mu_y ^ * - \mu_y)
$$

$$
\begin{array}{rcl}
\sigma_x^{*2} &=& 
V^*_y\left(\mbox{E}(x\mid y\right) + E^*_y\left(\mbox{V}(x\mid
y\right)\\
&=&
\rho ^ 2 \frac{\sigma_x ^ 2}{\sigma_y ^ 2} \sigma_y ^ {*2} + (1 - \rho
^ 2) \sigma_x ^ 2 \\
&=& 
\sigma_x ^ 2 \left(1 - \rho ^ 2 (1 - \theta)\right)
\end{array}
$$

with $\theta=\frac{\sigma^{*2}_y}{\sigma^2_y}$.

Finally, to compute the covariance, we use the fact that the
covariance between $x$ and $y$ equals the covariance between
$\mbox{E}(x\mid y)$ and $y$:

$$
\begin{array}{rcl}
\sigma_{xy} ^ * &=&  \mbox{cov} ^ * \left(\mbox{E}(x\mid y), y\right) \\
&=& \rho \frac{\sigma_x}{\sigma_y} \sigma_y ^{*2} \\
&=& \rho \theta\sigma_x\sigma_y
\end{array}
$$

The OLS estimator on a truncated sample $\beta^*$ is the ratio of the
population covariance and variance of $x$ on the observable range.

$$
\beta ^ * = \frac{\sigma_{xy} ^ *}{\sigma_{x} ^ {*2}} = \beta
\frac{\theta}{1 - \rho ^ 2(1 - \theta)} = \lambda \beta
$$

with $\lambda = \frac{\theta}{1 - \rho ^ 2(1 - \theta)} =
\frac{\theta}{\theta + (1-\theta)(1-\rho^2)}$. As $\rho$ and $\theta$
are in the $[0,1]$ interval, so is $\lambda$. The probability limit of
the least squares estimator is therefore proportional to the real
value $\beta$. Moreover, this result applies to the multiple linear
model, for which the probability limits of the vector of estimated
slopes are proportional to the vector $\beta$. We are therefore in a
situation of an attenuation bias, which means that the absolute value
of all the estimated slopes are proportionally lower compared to the
true values. 

<!-- For the intercept, we get: -->



<!-- $$ -->
<!-- \begin{array}{rcl} -->
<!-- \alpha ^ * - \alpha &=& (\mu_y ^ * - \mu_y) - \beta \lambda (\mu_x ^ * - -->
<!-- \mu_x) + \beta (1 - \lambda) \mu_x \\ -->
<!-- &=& (\mu_y ^ * - \mu_y)(1 - \lambda \rho ^ 2) + -->
<!-- (1 - \lambda) \rho \frac{\sigma_y}{\sigma_x} \mu_x -->
<!-- \end{array} -->
<!-- $$ -->

For the special case of a zero-left truncated response, denoting
$\nu_y = \frac{\mu_y}{\sigma_y}$, we have $\sigma_y^{*2} =
\sigma_y^2 \left[1 + r'(\nu_y)\right]$, so that $\theta = \left[1 +
r'(\nu_y)\right]$ and:

$$
\lambda = \frac{1 + r'\left(\frac{\mu_y}
{\sigma_y}\right)}{1 + \rho ^
2 r'(\nu_y)}
$$

<!-- $$ -->
<!-- \left\{ -->
<!-- \begin{array}{rcl} -->
<!-- \mu_y^* &=& \mu_y + \sigma_y r(\nu_y) \\ -->
<!-- \sigma_y^{*2} &=& \sigma_y^2 \left[1 + r'(\nu_y)\right] -->
<!-- \end{array} -->
<!-- \right. -->
<!-- $$ -->


<!-- $$ -->
<!-- \mu_x^* = \mu_x + \rho \sigma_x r(\nu_y) -->
<!-- $$ -->


<!-- $$ -->
<!-- \alpha ^ * - \alpha = (1 - \lambda \rho ^ 2) \sigma_y -->
<!-- r(\nu_y) -->
<!-- + -->
<!-- (1 - \lambda) \rho \frac{\sigma_y}{\sigma_x} \mu_x -->
<!-- $$ -->

For the zero-left censored sample case, we compute the uncentered
moments as a weight average of $0$ (with probability $1 -
\Phi(\nu_y)$) and of observable values of $y$ (with probability
$\Phi(\nu_y)$), which means that $\mbox{E}(\tilde{y}) = \Phi(\nu_y)
\mu_y ^ *$ and that $\mbox{E}(x\tilde{y})=\Phi(\nu_y)(\sigma_{xy}^* +
\mu_x^*\mu_y^*)$. The covariance between $x$ and $\tilde{y}$ is then:

<!-- $$ -->
<!-- \tilde{\sigma}_y ^ {2} = \mbox{E}(\tilde{y} ^ 2)  - \tilde{\mu}_y^{2} =  -->
<!-- \Phi(\nu_y) \mbox{E}(y ^ 2\mid y > 0) - -->
<!-- \tilde{\mu}_y ^ {2} =  -->
<!-- \Phi(\nu_y) \left(\sigma_y ^{*2} + \mu_y ^ {*2} \right) - \tilde{\mu}_y ^ {2} -->
<!-- $$ -->

<!-- $$ -->
<!-- \tilde{\sigma}_y ^ {2} = \sigma_y ^  2 -->
<!-- \Phi(\nu_y)  -->
<!-- \left[1 +  -->
<!-- \left( -->
<!-- \Phi(\nu_y) -->
<!-- r'(\nu_y) -->
<!-- - \eta__y\right) - -->
<!-- r'(\nu_y) -->
<!-- \right] -->
<!-- $$ -->

$$
\begin{array}{rcl}
\tilde{\sigma}_{xy} &=& \mbox{E}(x\tilde{y}) - \mu_x\tilde{\mu}_y \\
&=& \Phi(\nu_y) \left(\sigma_{xy} ^ * + \mu_x ^ * \mu_y ^ * \right) -
\mu_x\tilde{\mu}_y \\
&=& \Phi(\nu_y) \rho \sigma_x \sigma_y
\end{array}
$$

Dividing this covariance by $\sigma^2_x$, we finally obtain the
probability limit of the OLS estimator for the censored sample:

$$
\tilde{\beta} = \Phi(\nu_y) \beta
$$

Once again, the probability limit of the OLS estimator is proportional
to $\beta$. The remarkable result for the censored sample is that the
coefficient of proportionality is just the probability of drawing a
censored observation, which can be consistently estimated by the share
of censored observations in the sample. 

<!-- $$ -->
<!-- \tilde{\alpha} = \tilde{\mu}_y - \tilde{\beta} \mu_x = \Phi(\nu_y) (\mu_y + -->
<!-- \sigma_y r(\nu_y)) - \Phi \beta \mu_x = \Phi(\nu_y)(\alpha + \sigma_y r(\nu_y)) -->
<!-- $$ -->


### Interpretation of the coefficients

As the conditional expectation of $y$ is a non-linear function of $x$,
$\beta_k$ is not the marginal effect of the corresponding covariate
$x_k$.

The effect of a change in $x_k$ is twofold:

- firstly, it changes the probability that the value of $y$ is in the
observable range (positive for the left-zero truncated case),
- secondly, it changes the expected value of $y$ in the observable
range ($\mbox{E}(y\mid x, y > 0)$.

The probability that $y$ is positive and the conditional expectation
for positive values of $y$ are:

$$
\left\{
\begin{array}{rcl}
\mbox{P}(y > 0\mid x) &=& \Phi\left(\frac{\beta^\top x}{\sigma}\right)\\
\mbox{E}(y\mid x, y > 0) &=& \beta ^ \top x + \sigma
r\left(\frac{\beta^\top x}{\sigma}\right)
\end{array}
\right.
$$

and the unconditional expectation of $y$ is just the product of these
two expressions:

$$
\mbox{E}(y\mid x) = \mbox{P}(y > 0\mid x) \times \mbox{E}(y\mid x, y > 0)
$$

Its derivative with respect to $x_k$ gives:

$$
\begin{array}{rclrcl}
\frac{\displaystyle\partial \mbox{E}(y\mid x)}{\displaystyle\partial x_k} &=& 
\frac{\displaystyle\partial\mbox{P}(y > 0\mid x)}{\displaystyle\partial x_k} &\times&
\mbox{E}(y\mid x, y > 0)\\ 
&+& \mbox{P}(y > 0\mid x) &\times& \frac{ \displaystyle\partial \mbox{E}(y\mid x, y >
0)}{\displaystyle\partial x_k}
\end{array}
$$

with:

$$
\left\{
\begin{array}{lcl}
\frac{\displaystyle\partial\mbox{P}(y > 0\mid
x)}{\displaystyle\partial x_k} &=& \frac{\beta_k}{\sigma} \phi\left(\frac{\beta^\top x}{\sigma}\right) \\
\frac{\displaystyle\partial \mbox{E}(y\mid x)}{\displaystyle\partial
x_k} &=& \beta_k \left[1 + r'\left(\frac{\beta ^ \top x}{\sigma}\right) \right]
\end{array}
\right.
$$


The effect of a change of a covariate is represented on figure
@fig-normal2Trunc.


```{r }
#| label: fig-mfx
#| fig.cap: "Change of $x$"
#| echo: FALSE
knitr::include_graphics("./tikz/fig/mfx.png", auto_pdf = TRUE)
```
When the value of $x$ increases from $x_1$ to $x_2$, the untruncated
normal density curve moves to the right, the mode increasing from
$\mu_1=\alpha+\beta x_1$ to $\mu_2=\alpha+\beta x_2$. The increase of the probability that $y > 0$
is represented by the gray area, as it is the area between the two
density curves from 0 to $+\infty$ which reduce to the area between
the two curves between $\mu_1+\mu_2$ and $+\infty$.^[as the area
between 0 and the intersection point $\frac{\mu_1+\mu_2}{2}$ and the
one between $\frac{\mu_1+\mu_2}{2}$ and $\mu_1+\mu_2$ are the same
with the opposite sign.]

This is the first source of change of $\mbox{E}(y\mid x)$ which, for a
small variation of $x$ is equal to $\Delta \Phi\left(\frac{\alpha + \beta
x}{\sigma}\right) \times \mbox{E}(y\mid x, y > 0)$.

The second source of change is the increase of the conditional
expectation of $x$, which is multiplied by the probability that $y$ is
observed: $\Delta \mbox{E}(y\mid x, y > 0) \times \Phi\left(\frac{\alpha +
\beta x}{\sigma}\right)$.

The first one can be considered as an increase of $y$ on the *extensive
margin*, ie due to the fact that for more people, we observe $y >
0$. The second one is an increase of $y$ on the *intensive margin*,
which means that for people for which $y$ was already positive, the
value of $y$ increases^[This decomposition of marginal effects for
tobit models was first proposed by @MCDO:MOFF:80.]. 

The sum of these two components gives the marginal effect of a
variation of $x$ on the unconditional expected value of $y$, which is
simply:

$$
\frac{\partial \mbox{E}(y\mid x)}{\partial x_k} = 
\Phi\left(\frac{\beta ^ \top x}{\sigma}\right)\beta_k
$$


## Methods of estimation

Several consistent estimators are available for the truncated and the
censored model. We'll start by inefficient estimators (non-linear
least squares, probit, two-steps estimators). We'll then present
the maximum likelihood estimator which is asymptotically efficient if
the conditional distribution of $y$ is normal and homoskedastic. We'll
finally develop the symmetrically trimmed least squares estimator,
which is consistent even if the distribution of $y$ is not normal and
heteroskedastic.

### Non-linear least squares

The conditional expected value of $y$ 
($\mbox{E}(y\mid x) = \beta^\top x + \sigma r\left(\frac{\beta^\top
x}{\sigma}\right)$)
is non-linear in $x$. Therefore, the parameters can be consistently
estimated using non-linear least squares, by minimizing:

$$
\sum_{n=1} ^ N \left[y_n - \beta^\top x + \sigma r\left(\frac{\beta^\top
x}{\sigma}\right)\right] ^ 2
$$

### Probit and two-steps estimators

The probability that $y$ is positive is $\Phi\left(\frac{\beta ^ \top
x}{\sigma}\right)$, therefore, a probit model can be used to estimate the
vector of coefficient $\frac{\beta}{\sigma}$. Therefore, using the
probit model, $\sigma$ is not identified, and each element of the
$\beta$ vector is only estimated up to a $1/\sigma$ factor.

To estimate the probit model, we first have to compute a binary
response from the observed response which is equal to 1 if $y > 0$ and
0 if $y = 0$. We then obtain a vector of estimated coefficients
$\hat{\gamma}$ which are related to the structural coefficients of the
model by the relation $\gamma = \frac{\beta}{\sigma}$. Obviously the
probit estimation can only be performed for the censored sample, and
not for the truncated sample for which all the values of $y$ are
positive.

Remind that the expected value of $y$ is:

$$
\mbox{E}(y\mid x) = \beta^\top x + \sigma r\left(\frac{\beta^\top x}{\sigma}\right)
$$

if $\frac{\beta}{\sigma}$ were known and denoting $r_n =
r\left(\frac{\beta^\top x_n}{\sigma}\right)$, estimating the equation:

$$
y_n=\alpha + \beta ^\top x_n + \sigma r_n + \nu_n
$$

by least squares would lead to consistent estimates of $\beta$ and
$\sigma$ as $\mbox{E}(y_n\mid x_n, r_n) = \beta^\top x_n + \sigma r_n$
or $\mbox{E}(\nu_n\mid x_n, r_n) = 0$. $r_n$ is obviously unknown as
it depends of the parameters we seek to estimate, but it can be
consistently estimated, using the probit estimator, by $\hat{r}_n =
r\left(\hat{\gamma}^\top x_n\right)$. This idea leads to the
**two-steps estimator** first proposed by @HECK:76:

- first estimate the coefficient of the probit model $\hat{\gamma}$
  and estimate $r_n$ by $\hat{r}_n = r(\hat{\gamma}^\top x_n)$,
- then regress $y$ on $x$ and $\hat{r}$ and estimate $\hat{\beta}$ and
  $\hat{\sigma}$.
  

Denoting $Z = (X, \hat{r})$ the matrix of covariates for the second
  step and $\delta ^ \top = (\beta ^ \top, \sigma)$ the associated
  vector of parameters. The covariance matrix of the parameters
  reported by the OLS estimation is $\hat{\sigma} ^ 2 (Z^\top Z) ^
  {-1}$. It is inconsistent for two reasons:
  
- the first one is that the errors of the model are heteroscedastic,
  their variance being $\mbox{V}(\epsilon_n) = \sigma_\epsilon ^ 2 (1 +
  r'(\beta^\top x_n / \sigma)$,
- the second one is that the supplementary covariate is $\hat{r}(\beta
  ^ \top x_n)$ and the fact that this differs from the true value of
  $r(\beta ^ \top x_n)$ inflates the variance of the estimators.
  
A consistent estimate of the covariance matrix of the two-steps
estimator is^[See for example @AMEM:84 equation 28 page 13.]:

$$
\sigma (Z ^ \top Z) ^ {-1} Z \left[\Sigma + (I - \Sigma) X
\hat{V}_{\mbox{probit}} (I - \Sigma) X ^ \top \right] Z ^ \top (Z ^
\top Z) ^ {-1}
$$
  
The first matrix in the bracket takes into account the
heteroscedasticity, it is a diagonal matrix that contains either
$\hat{\sigma}_\epsilon ^ 2 (1 + r'(\hat{\beta}^\top x_n / \hat{\sigma}))$ 
or, following White's idea, the square of the residual
$y_n - \hat{\beta} ^ \top x_n -  r(\hat{\beta} ^ \top x_n /
\hat{\sigma}_n)$.
The second matrix take into account the fact that $\hat{r}_n$ is
  introduced in place of $r_n$.

###  Maximum Likelihood estimation

Without loss of generality, we'll assume that observations from 1 to
$N_o$ are censored as those from $N_o+1$ to $N$ are not. Estimating
the model on the truncated sample, we obtain the likelihood by
multiplying the truncated density of $y$ for all the individuals from
$N_o+1$ to $N$:

$$
L^T(\beta, \sigma \mid y, x) = \prod_{n = N_o + 1}^N
\frac{1}{\sigma \Phi\left(\frac{\beta^\top
x_n}{\sigma}\right)}\phi\left(\frac{y_n - \beta^ \top x_n}{\sigma}\right)
$$

or, taking the logarithm:

$$
\ln L^T(\beta\mid y, x) = 
-\frac{N -N_o}{2}(\ln \sigma^2 + \ln 2\pi) - 
\frac{1}{2\sigma ^ 2}\sum_{n = N_o + 1}^N (y_n -\beta^\top x)^2 - 
\sum_{n = N_o + 1} ^ N \ln \Phi\left(\frac{\beta^\top x}{\sigma}\right)
$$


For the censored sample, the individual contribution to the likelihood
sample will depend on whether $y=0$ or not:

- if $y = 0$, this is the probability that $y=0$, which is 
  $1 - \Phi\left(\frac{\beta^ \top x}{\sigma}\right)$,
- if $y > 0$, this is the product of the probability that $y > 0$ and
  the density of the truncated distribution of $y$, which is:
  $\Phi\left(\frac{\beta^\top
  x}{\sigma}\right)\frac{1}{\sigma\Phi\left(\frac{\beta^\top
  x}{\sigma}\right)} \phi\left(\frac{y - \beta^ \top
  x}{\sigma}\right)$.
  
The likelihood function is therefore:

$$
L^C(\beta, \sigma  | y,x)=\prod_{n=1}^{N_o}
\left[1 - \Phi\left(\frac{\beta^ \top x}{\sigma}\right)\right]
\prod_{n = N_o + 1}^{N}\Phi\left(\frac{\beta^\top x}{\sigma}\right)
\prod_{n = N_o + 1}^{N}\frac{1}{\sigma\Phi\left(\frac{\beta^\top
x}{\sigma}\right)}
\phi\left(\frac{y_n-\beta^\top x_n}{\sigma}\right)
$$

which is simply the product of:

- the likelihood of a probit model which explains that $y=0$ or $y >
  0$ (the first two terms),
- the likelihood of $y$ for the truncated sample (the last term). 


Denoting $L^P$ the likelihood of the probit model, we then have:

$$
L^C(\beta, \sigma \mid y,x)=L^P(\beta,\sigma\mid y, x) \times L^T(\beta, \sigma \mid y, x)
$$

Taking logs and re-arranging terms, we finally get:

$$
\begin{array}{rcl}
\ln L^C(\beta, \sigma \mid y,x)&=&\sum_{n=1}^{N_o}
\ln \left[1 - \Phi\left(\frac{\beta^ \top x}{\sigma}\right)\right]
-\frac{N - N_o}{2}\left(\ln \sigma ^ 2 + \ln 2 \pi\right)\\
&-&\frac{1}{2\sigma^ 2}\sum_{n = N_o + 1}^{N}\left(y_n-\beta^\top
x_n\right)^2
\end{array}
$$

The gradient and especially the hessian are rather tricky, but their
expression can be greatly simplified using a re-parametrization, due
to @OLSE:78: $\gamma = \beta / \sigma$ and $\theta = 1 / \sigma$.
@OLSE:78 showed that the log-likelihood of the censored model
expressed in terms of $\gamma$ and $\sigma$ is globally concave and
therefore admit a unique optimum which is a maximum. 

### Semi-parametric estimators

In a semi-parametric approach, only the regression function, ie
$y=\beta^\top x$ is parametrically specified, while the rest of the
model (especially the conditional distribution of $y$) is not. This
approach is therefore much more generally applicable. Compared to the
estimators presented in the previous two sections, which are only
consistent if the conditional distribution of $y$ is normal and
homoskedastic, the semi-parametric estimator presented in this section
is consistent in a much broader context, as it requires only the
symmetry of the conditional distribution of the response.

In the context of truncated response, the semi-parametric estimator
is based on a very simple idea. For the left-zero truncated response
case, the OLS estimator is biased because the conditional distribution
of $y$ is asymmetric, as the observations on the left tail of the
distribution ($y < 0$) are either missing (the case of a truncated
sample) or set to 0 (the case of a censored sample). For the case of a
truncated sample, trimming the observations for which 
$y > 2\beta ^ \top x$, ie observations that lie in the upper tail, would restore
the symmetry and OLS estimation on this trimmed sample would be
consistent. This situation is depicted on figure @fig-symmetric.


```{r }
#| label: fig-symmetric
#| fig.cap: "Symmetricaly trimmed truncated distribution"
#| echo: FALSE
knitr::include_graphics("./tikz/fig/symetric.png", auto_pdf = TRUE)
```

The plain line represents the distribution of the not-truncated
response, the dashed line the corresponding distribution of the
zero-left truncated response. This distribution is asymmetric and the
expected value of y ($\mbox{E}(y\mid x)$) is above $\beta ^ \top x$
because of the left truncation. The dotted line represents the
two-sided truncated distribution of $y$, truncated at 0 on the left side
and at $2\beta^\top x$ on the right side. As the untruncated distribution
of $y$ is symmetric, so is the two-sided truncated distribution, for
which the conditional expected value of $y$ is now equal to
$\beta^\top x$. Therefore, if we were able to remove from the sample
all the observations for which $y > 2 \beta^\top x$, the OLS estimator
on this trimmed sample would be consistent. Of course, the problem is
that the right truncation point is unknown for every observation as
it depends on $\beta$ which is the parameter that we seek to
estimate. Define $\textbf{1}(x)$ the indicator function, which is
equal to 1 if $x$ is true and 0 otherwise. The subset of observations
that should be kept in the symmetrically truncated estimator should
verify $0 < y_n < 2\beta^\top x_n$, or $-\beta^\top x_n < \epsilon_n <
\beta^\top x_n$, the first inequality being always verified for a
truncated sample. Note that this condition will remove all the
observations for which $\beta^\top x_n < 0$.

The first order conditions to minimize the sum of squares
of the residuals for the trimmed sample is then similar to the normal
equations ($\sum_{n=1}^N (y_n - \beta ^x_n) x_n = 0$) on the relevant
subset of the sample:

$$
\sum_{n = 1} ^ N \textbf{1}(y_n < 2 \beta^\top x_n) (y_n - \beta ^
\top x_n)  x_n = 0
$$

Note that the left-hand side of this equation is a discontinuous
function of $\beta$, as even a small change of $\beta$ may, for some
observations, turn the condition $y_n < 2 \beta^\top x_n)$ from true
to false (or the contrary). Moreover, note that $\beta=0$ is a trivial
solution as, in this case, $\textbf{1}(y_n < 2 \beta^\top
x_n)=0\;\forall n$. Therefore, it is safer to consider the estimator
as the result of a minimization problem instead of solving the set of
non-linear equations. By direct integration, we get that the minimizer
of the following function:

$$
R_T = \sum_{n=1}^N \left[y_n - \max\left(\frac{y_n}{2}, \beta^\top
x_n\right)\right] ^ 2
$$

is this set of non-linear equations.

This idea of symmetrically truncated sample easily extend to the case of
censored samples. In this case, negative values of $y$ are unobserved
and the value of the response is set to 0. A symmetricaly censored
sample is obtained by setting the response, for observations for which
the observed value of $y$ is greater than $2\beta^\top x$ to
$2\beta^\top x$. This means that as the response is zero-left
censored, we also right-censore it, with a truncation value that is
specific to the observation and that depends on the set of unknown
parameters $\beta$ we seek to estimate. The resulting first-order
conditions to minimize the sum of squares of the "symmetrically
censored sample" is:

$$
\sum_{n = 1} ^ N \textbf{1}(y_n < 2 \beta^\top x_n)
\left[\min\left(y_n, 2 \beta ^ \top x_n\right) - \beta ^ \top x_n\right]  x_n = 0
$$

which is the minimizer of the following function:

$$
\begin{array}{rcl}
R_C &=& \sum_{n=1}^N \left[y_n - \max\left(\frac{y_n}{2}, \beta^\top x_n\right)\right] ^ 2\\
&+& \sum_{n=1}^N \textbf{1}(y_n < 2 \beta^\top x_n)
\left[\left(\frac{y_n}{2}\right) ^ 2 - \max(0, \beta ^ x_n) ^ 2\right]
\end{array}
$$

## Estimation with the tobit1 package

### Left-truncated response

We'll reproduce here some results obtained by @WILH:08 using a data
set which deals with charitable giving. The data set is shipped with
the `tobit1` package and can be accessed as soon as this package is
attached.

```{r }
library("tidyverse")
```

```{r }
charitable %>% print(n = 5)
```

The response is called `donation`, it measures annual charitable
givings in $US. This variable is left-censored for the value of 25, as
this value corresponds to the item "less than 25 $US
donation". Therefore, for this value, we have households who didn't
make any charitable giving and some which made a small giving (from 1
to 24 $US).

The covariates used are the donation made by the parents
(`donparents`), two factors indicating the educational level and
religious beliefs (respectively `education` and `religion`), annual
income (`income`) and two dummies for living in the south (`south`)
and for married couples (`married`). 

@WILH:08 consider the value of the donation in logs and subtract $\ln
25$, so that the response is 0 for households who gave no donation or
a small donation.


```{r }
charitable <- charitable %>% mutate(logdon = log(donation) - log(25))
```

The tobit model can be estimated by maximum likelihood using
`AER::tobit`, `censReg::censReg` or with the `tobit1` package. 


```{r }
library("censReg")
library("survival")
char_form <- logdon ~ log(donparents) + log(income) +
    education + religion + married + south
ml_aer <- AER::tobit(char_form, data = charitable)
ml_creg <- censReg(char_form, data = charitable)
ml <- tobit1(char_form, data = charitable)
```

`tobit1` provide a rich set of estimation methods, especially the
**SCLS** (symmetrically censored least squares) estimator proposed by
@POWE:86. We also, for pedagogical purposes, estimate the ols
estimator although it is known to be inconsistent.


```{r }
scls <- update(ml, method = "trimmed")
ols <- update(ml, method = "lm")
```

The results of the three models are presented in table
@tbl-models).


```{r }
#| label: tbl-models
#| results: 'asis'
#| tbl-cap: "Estimation of charitable giving models"
modelsummary::msummary(list("OLS" = ols, "ML" = ml, "SCLS" = scls))
               #, single.row = TRUE, digits = 2)
```

The last two columns of table @tbl-models match exactly the
first two columns of [@WILH:08, table 3 page 577]. Note that the OLS
estimators are all lower in absolute values than those of the two
other estimators, which illustrate the fact that OLS estimators are
biased toward zero when the response is censored. The maximum
likelihood is consistent and asymptotically efficient if the
conditional distribution of $y^*$ (the latent variable) is
homoscedastic and normal. The **SCLS** estimator consistency relies
only the hypothesis that the errors are symmetrical around 0. However,
if they are also normal and homoscedastic, it is less efficient than
the maximum likelihood estimator. Therefore, the strong distributional
hypothesis of the maximum likelihood estimator can be addressed using a
Hausman test:

```{r }
haustest(scls, ml, omit = "(Intercept)")
```

### Right-truncated response

The data used by @CROM:PALM:URBA:97 to estimate the demand for food 
  in the Netherlands is available as `food` in the `edf.tobit`
  package:

```{r }
data("Food", package = "edf.tobit")
Food
```

Two surveys are available, for 1980 and 1988. Let's concentrate on the
1980 survey. Food expenses are top-coded at 13030 Dfl for 1980, which
means that all the expenses greater than 13030 are reported as being
equal to 13030. 

```{r }
Food %>% filter(year == 1980) %>%
    summarise(n = sum(food == 13030),
              f = mean(food == 13030))
```
We can see than the data set is weakly censored, as less than 5% of
the households have an expense greater than the threshold. As `tobit1`
only allows the estimation of left-truncated response models, we are
going to use the `AER::tobit` function and compare the tobit and the
OLS estimator. For the tobit estimator, as the response is expressed
in logarithms, the right threshold is set to `log(13030)` and the left
one to `- Inf` as the response is not right-truncated.

```{r }
food_tobit <- AER::tobit(log(food) ~ log(income) + log(hsize) + midage,
                         data = Food, subset = year == 1980,
                         left = -Inf, right = log(13030))
summary(food_tobit)
```

The main coefficient of interest is the one associted with the
`log(income)` covariate. The value indicates that the income
elasticity of food is estiamted to be 0.34.

The OLS estimator is:

```{r }
food_ols <- lm(log(food) ~ log(income) + log(hsize) + midage,
               Food,  subset = year == 1980)
food_ols
```
As expected, it is very close to the tobit estimator, as the share of
censored observations is low. 


### Two-sided tobit models
  
@HOCH:03 estimate the share of risk-less assets, which can be either
  an internal solution, or a corner solution with the share equal to 0
  or 1 ($a = 0$ and $b=1$). He therefore estimate a two-sided tobit
  model. The paper seeks to explain the low share of risky assets in
  portfolios of Dutch households.  The data set is available as
  `PrecPortfolio` in the `edf.tobit` package.
  
```{r }
data("PrecPortfolio", package = "edf.tobit")
```
This data set is a panel of annual observations from 1993 to 1998. In
    his paper, @HOCH:03 use panel and cross-section estimators (the
    latter being obtained on the whole data set by pooling the 6
    time-series). The two covariates of main interest
    are `uncert` and `expinc`.

- `uncert` indicates the degree of uncertainty felt by the household,
  it is a factor with levels `low`, `moderate` and `high`.
- `expinc` indicates the prediction of the household concerning the
  evolution of their income in the next 5 years, it is a factor with
  levels `increase`, `constant` and `decrease`. 
  

We'll use a simpler specification than his, which contains a lot of
covariates, we only add to the two covariates preceedingly described
the net worth, the age of household's head and its square and a dummy
for households for which the head is a woman. We use the `AER::tobit`
function and we set the `left` and the `right` arguments respectively
to 0 and 1.

```{r }
prec_ml <- AER::tobit(share ~ uncert + expinc + networth +
                          age + I(age ^ 2 / 100) + female,
                      left = 0, right = 1, data = PrecPortfolio)
summary(prec_ml)
```

As expected, high uncertainty and pessimist expectations about future
income increase the share of riskless assets. Portfolio of richer
houselholds and households headed by a woman are less risky. Finally
the effect of age has an inverse U-shaped.

Actually, @HOCH:03 don't estimate this model, as they suspect the
presence of heteroscedascticity. Therefore, they estimate a model for
which $\sigma$ is replaced by $\sigma_n = e^{\alpha + \gamma^\top z_n}$,
where $z_n$ are a set of covariates and $\gamma$ a set of further
parameters to be estimated. The `crch::crch` enables the estimation of
such models. The model is described using a two-part formula, the
second one containing the covariates that are used to estimate
$\sigma_n$. By default, a logistic link is used ($\ln \sigma_n =
\alpha + \gamma ^ \top z_n$) but other links can be selected using the
`link.scale` argument. Moreover, departure from normality can be taken
into account using the `dist` argument for, for example, switching
from a gaussian distribution (the default) to a student or a
logistic. For the scedasticity function, we use all the covariates
used in the main equation except `uncert` and `expinc` which appear to
be insignificant. 

```{r }
prec_ht <- crch::crch(share ~ uncert + expinc + networth +
        age + I(age ^ 2 / 100) + female | networth +
        age + I(age ^ 2 / 100) + female, left = 0, right = 1,
        data = PrecPortfolio)
summary(prec_ht)
```

Without conducting a formal test, it is clear that the heteroscedastic
specification is supported by the data, the log-likelihood value of
the second specification being much larger than the one of standard
tobit model. The value of some coefficients are strikingly
different. For example, the coefficient of `networth` is
`r round(coef(prec_ml)["networth"], 3)` for the tobit model, bu
`r coef(prec_ht)["networth"]` for the heteroscedastic model, as this
covariate also has a huge effect on the conditional variance of the
response. 

## Specification tests for the tobit1 model

The most popular method of estimation for the tobit1 model is the
fully parametric maximum likelihood method. Contrary to the ols model,
the estimators are only consistent if the gdp process is perfectly
described by the likelihood function, ie if the $\epsilon_n \mid N(0,
\sigma)$. In particular, the consistency rests on the hypothesis of
normality and homoscedasticity.

One more hypothesis that should be tested is the one of omitted
variable. Even in the ols model, estimators are inconsistent if some
relevant covariates that are correlated with the errors are omitted
from the regression. Therefore, three kind of specification tests can
be performed, which are based on the following theoretical moment
conditions:

- **omitted variable**: $\mbox{E}(w_n \epsilon_n \mid x_n) = 0$,
- **homoscedasticity**: $\mbox{E}(w_n (\epsilon_n ^ 2 - \sigma ^
  2)\mid x_n)
  = 0$,
- **normality**: $\mbox{E}(\epsilon_n ^ 3 \mid x_n) = 0$ and
  $\mbox{E}(\epsilon_n ^ 4 - 3\sigma ^ 4 \mid x_n) = 0$
  
Conditional moment tests are obtained by replacing those theoretical
moment conditions by their empirical counterparts, obtained by
substituting in the previous expressions the residuals to the
errors. For the tobit1 model, there is a further complication, as the
residual is not observed for censored observations. Denoting $I_n$ a
dummy variable equal to 1 if $y_n > 0$ and 0 if $y_n = 0$, a
generalized residual can be defined by using, for $I_n = 0$ the sample
counterpart of the expectation of $\epsilon_n$, which is $-\sigma
r(\beta^\top x_n / \sigma)$. We therefore have the following
expression for the generalized residual of the tobit1 model:

$$
\hat{\eta}_n = I_n \hat{\epsilon}_n - (1 - I_n) \hat{\sigma}
r(\hat{\beta}^\top x_n / \hat{\sigma})
$$

This leads to the following test statistics, denoting $z_n = \hat{\beta}^\top x_n / \hat{\sigma}$:

- **omitted variable**: $\sum_n w_n \left[I_n \hat{\epsilon}_n - (1 - I_n) \hat{\sigma}
r(z_n)\right]$,
- **homoscedasticity**: $\sum_n w_n
  \left[I_n(\hat{\epsilon}_n^2-\hat{\sigma} ^ 2) +
  (1-I_n)\hat{\sigma}\hat{\beta} ^ \top x_n r(z_n)\right]$,
- **normality**: $\sum_n \left[I_n \hat{\epsilon}_n ^ 3 -
  (1-I_n)(z_n^2 + 2)\hat{\sigma}^3
  r(z_n)\right]$ and $\sum_n
  \left[I_n(\hat{\epsilon}_n ^ 4 - 3 \hat{\sigma} ^ 4) + (I -
  I_n)(z_n ^ 2 + 3)\hat{\sigma} ^
  4 r(z_n) z_n\right]$.



Specification tests for the maximum likelihood can also be conducted
using conditional moments tests. This can easily be done using the
`cmtest::cmtest` function, which can take as input a model fitted by
either `AER::tobit`, `censReg::censReg` or `tobit1::tobit1`:


```{r }
cmtest(ml)
```

```{r include = FALSE}
cmtest(ml_aer)
cmtest(ml_creg)
```

`cmtest` has a `test` argument with default value equal to
`normality`. To get a heteroscedasticity test, we would use:

```{r }
cmtest(ml, test = "heterosc")
```
Normality and heteroscedasticity are strongly rejected. The values are
different from @WILH:08 as he used the "outer product of the gradient"
form of the test. These versions of the test can be obtained by
setting the `OPG` argument to `TRUE`.

```{r }
cmtest(ml, test = "normality", OPG = TRUE)
cmtest(ml, test = "heterosc", OPG = TRUE)
```

Non-normality can be further investigate by testing separately
the fact that the skewness and kurtosis indicators are respectively
different from 0 and 3.

```{r }
cmtest(ml, test = "skewness")
cmtest(ml, test = "kurtosis")
```
The hypothesis that the conditional distribution of the response is
mesokurtic is not rejected at the 1% level and the main problem seems
to be the asymmetry of the distribution, even after taking the
logarithm of the response. 

This can be illustrated (see figure\@ref(fig:histnorm)) by plotting the
(unconditional) distribution of the response (for positive values) and
adding to the histogram the normal density curve.


```{r }
#| label: fig-histnorm
#| fig.cap: "Empirical distribution of the response and normal approximation"
moments <- charitable %>% filter(logdon > 0) %>% summarise(mu = mean(logdon), sigma = sd(logdon))
ggplot(filter(charitable, logdon > 0), aes(logdon)) +
    geom_histogram(aes(y = after_stat(density)), color = "black", fill = "white", bins = 10) +
    geom_function(fun = dnorm, args = list(mean = moments$mu, sd = moments$sigma)) +
    labs(x = "log of charitable giving", y = NULL)
```


## Simultaneous equation tobit model

Let's consider the following system of simultaneous equations:


$$
\left\{
\begin{array}{rcl}
y^* &=& \gamma ^ \top z + u \\
w  &=& \Pi ^ \top x + v \\
\end{array}
\right.
$$

and we observe as usual $y = y ^ *$ if $y ^ * > 0$ and 0 otherwise. $z
^ \top = (1, x_1 ^ \top, w ^ \top)$ and $x ^ \top = (1, x_1 ^ \top, x_2 ^
\top)$.

$w$ is a vector of $G$ variables that enter the first equations. These
variables are endogenous if $v$ is correlated with $u$. In this case,
the standard tobit model is inconsistent.

We assume that the distribution of $(y ^ \top, w ^ \top)$ is
multivariate normal:

$$
\left(\begin{array}{c} y \\ w\end{array}\right) \sim
N\left(
\left(\begin{array}{c} \gamma ^ \top z\\ \Pi ^ \top
x\end{array}\right); 
\left(\begin{array}{cc} 
\sigma_u ^ 2 & \sigma_{12} ^ \top \\
\sigma_{12}  & \Sigma_v \end{array}\right)
\right)
$$

Writing the $(G+1)$ multivariate density as the product of the
conditional density of $y$ given $w$ and the joint marginal density of
$w$, we get:

$$
y ^ * \mid w \sim N(\gamma ^ \top z_n + \rho ^ \top v_n ; \sigma)
$$

with $\rho = \Sigma_v ^ {-1} \sigma_{12}$ and $\sigma = \sigma_u ^ 2 -
\sigma_{12} ^ \top \Sigma_v ^ {-1} \sigma_{12}$.

The equation of interest can therefore be re-writen :

$$
y ^ * = \gamma ^ \top z_n + \rho ^  \top v + e \mbox{ with } e \sim
N(0, \sigma)
$$

This equation, with the observing rule of $y$ defines a standard tobit
model, with a vector of covariates $(z ^ \top, v ^ \top) = (1, w ^
\top, x_1 ^ \top, v ^ \top)$. Therefore, if $v$ where observed, a
standard maximum likelihood estimator would result in consistent
estimates. 

### Test of exogeneity and two-steps estimator

$v$ is unobserved, but a constitent estimate can be obtained by taking
the residuals of an OLS regression of each element of $w$ on the
complete set of explanatory variables $(1, x_1 ^ \top, x_2 ^
\top)$. This kind of regression was first proposed by @SMIT:BLUN:86
and provides:

- a test of exogeneity which is H~0~ : $\rho = 0$ ; this can be
  conducted by using the student statistic of the unique $\hat{\rho}$
  if there is only one endogenous variables or by chi-square statistic
  if there are more than one,
- a consistent estimation of the parameter associated with the
  endogenous variable. Note that the standard errors reported by the
  tobit regression are inconsistent as we use $\hat{v}$ and not $v$ as
  covariate. @SMIT:BLUN:86 provide a consistent estimate of the
  covariance matrix in this case.


### Maximum-likelihood estimator

The log Likelihood can be writen as a sum of $N$ contributions, which
are the sum of the log of the conditional density of $y$ and the
marginal distribution of $w$:

$$
\ln l_n = \ln f(y_n \mid w_n) + \ln f(w_n)
$$

with:

$$
\left\{
\begin{array}{lcl}
\ln f(y_n \mid w_n) &=& (1 - I_n) \ln \left[1 -\phi\left(\frac{\delta
^ \top z_n + \rho ^ \top v_n}{\sigma}\right)\right] -
I_n\left[ \frac{1}{2} \ln 2\pi + \ln \sigma + \frac{1}{2\sigma ^ 2}
\left(y_n - \delta ^ \top z_n - \rho ^ \top v_n\right)\right] \\
\ln f(w_n) &=& -\frac{G}{2}\ln 2\pi - \frac{1}{2}\ln \mid \Sigma_v
\mid - \frac{1}{2} v_n ^ \top \Sigma_v ^ {-1} v_n
\end{array}
\right.
$$

The marginal density can be concentrated respective to $\Sigma_v$ as:

$$
\frac{\partial \sum_n \ln f(w_n)}{\partial \Sigma_v} = 0 \Rightarrow
\hat{\Sigma}_v = \frac{1}{N}v_n v_n^\top \Rightarrow \mid
\hat{\Sigma}_v \mid = \frac{1}{N} \sum_n v_n v_n^\top
$$


$$
\sum_n \ln f(w_n) =  - \frac{N}{2} \left[G \ln 2\pi +1 + 
\ln \frac{1}{N}\sum v_n^\top v_n\right]
$$


The individual contribution to the log-likelihood therefore simplifies
to:

$$
\begin{array}{rcl}
\ln l_n &=& - \frac{1}{2} \left[G \ln 2\pi +1 + 
\ln \frac{1}{N}\sum v_n^\top v_n\right] \\
&+&(1 - I_n) \ln \left[1 -\phi\left(\frac{\delta
^ \top z_n + \rho ^ \top v_n}{\sigma}\right)\right] \\
&-& I_n\left[ \frac{1}{2} \ln 2\pi + \ln \sigma + \frac{1}{2\sigma ^ 2}
\left(y_n - \delta ^ \top z_n - \rho ^ \top v_n\right)\right] 
\end{array}
$$


## Tobit2 model

Tobit2 models are bivariate models, were the first equation is the
*selection equation* and the second one the *outcome equation*. The model
can be written as follow:

$$
\left\{
\begin{array}{rcl}
y_1 ^ * &=& \beta_1^\top x_1 + \epsilon_1 \\
y_2 ^ * &=& \beta_2^\top x_2 + \epsilon_2 \\
\end{array}
\right.
$$

and the observation rule is:

$$
\left\{
\begin{array}{rcl}
y_2  &=& y_2^* \mbox{ if } y_1^* > 0\\
y_2  &=& 0 \mbox{ if } y_1^* \leq 0\\
\end{array}
\right.
$$

Note that the tobit2 model reduce to the tobit 1 model if $x_1=x_2$
and $\beta_1=\beta_2$. In the outcome equation, $y_2 ^ *$ can be
replaced by $\ln y_2 ^*$, so that predicted values of $y_2$ are
necessarily positive. 

The tobit2 model is more general than the tobit 1 as it allows the
economic mechanisms that explain the fact that the response is
observed to be different from those that explain the value of the
response.

The seminal papers about the tobit2 model are @GRON:73 with his model
of female labor supply and @HECK:79 who precisely describe the
statistical properties of this model and proposed a two-steps
estimator.

Hurdle models, proposed by @CRAG:71 can also be considered as tobit2
models. These models describe the level of consumption of a good as a
two steps process: first, the good should be selected (selection
equation) and then the level of the consumption is set (outcome
equation). 

The tobit2 model is also widely used to measure the treatment effect
of public programs. If the selection of individuals in a particular
program is not purely random, the selection equation describe as a
binomial model the process of getting hired in the program and the
outcome equation a measure of the effectiveness of the program, eg
wage one year after leaving the program.


### Two-part models

With the assumption of uncorrelation between $\epsilon_1$ and
$\epsilon_2$ (which means uncorrelation of the unobserved part of the
two responses), this model is called the **two-part** model, and the
two equations can be estimated independently, the first one by any
binomial model (very frequently but unnecessarily a probit) with the
response equal to 0/1 for positive/negative values of $y_1^*$ and the
second one by least squares (with either $y_2$ or $\ln y_2$ as a
response). The main advantage of this model is that it can be
estimated without further hypothesis about the conditional
distribution of $y_2$ and can therefore be viewed as a semi-parametric
estimator, which would be consistent in a wide variety of context (eg
heteroskedasticity and non-normality).

### Hurdle models

Hurdle models, proposed by @CRAG:71 share with the two-part models the
fact that the errors of the two equations are uncorrelated. He
proposed three flavors of hurdle models: 

- simple hurdle models with a log-normal or a truncated normal
distribution for the outcome response,
- double hurdle models with a normal distribution for the outcome
  response.
  
The *log-normal simple hurdle* model is a two-step model for
which the outcome equation consist on a linear regression of the
logarithm of the outcome equation on the set of covariates $x_2$,
which is consistent even without normality and homoscedasticity.

The *truncated normal simple hurdle* model shares with
two-steps model the fact that the two equations can be estimated
independently, but the outcome equation can't be estimated by ordinary
least squares, but using methods of estimation (for example maximum
likelihood) that require the hypothesis of normality, and the
consistency of the estimator relies on this hypothesis. 

Finally, the *normal double hurdle model*, best suited for left-zero
truncated responses as consumption of certain goods is not per se a
tobit2 model because zero observations may appear not only because
$y_1^* < 0$, but also because $y_2^* < 0$, which means that the good
is selected, but is not consumed because the solution of the consumer
problem is a corner solution. 


In this case, we have $\mbox{P}(y_1 ^ * > 0) = 1 -
\Phi\left(-\beta_1^\top x_1\right) =\Phi\left(\beta_1^\top
x_1\right)$, 
$\mbox{P}(y_2 ^ * > 0) = 1 - \Phi\left(-\frac{\beta_2^\top
x_2}{\sigma}\right)=\Phi\left(\frac{\beta_2^\top x_2}{\sigma}\right)$,
so that, given the hypothesis of independence of the two errors:

$$
\mbox{P}(y > 0) = \Phi\left(\beta_1^\top x_1\right)  \Phi\left(\frac{\beta_2^\top x_2}{\sigma}\right)
$$

The density of $y$ for positive values of $y$ is:

$$
f(y \mid x_2, y > 0) = \frac{\frac{1}{\sigma}\phi\left(\frac{y -
\beta^2 x_2}{\sigma}\right)}{\Phi\left(\frac{y -
\beta^2 x_2}{\sigma}\right)}
$$

So that finally, the likelihood is:

$$
\begin{array}{rcl}
L^{2H}(\beta | y,x)&=&\prod_{n=1}^{N_o}
\left[1 - 
\Phi\left(\beta_1^ \top x_{1n}\right)
\Phi\left(\frac{\beta_{2n}^ \top x_{2n}}{\sigma}\right)
\right]
\prod_{n = N_o + 1}^{N}\Phi\left(\frac{\beta_{2n}^ \top x_{2n}}{\sigma}\right)
\Phi\left(\beta_1^ \top x_{1n}\right) \\
&\times& \prod_{n = N_o + 1}^{N}\frac{1}{\sigma\Phi\left(\frac{\beta_{2n}^\top
x_{2n}}{\sigma}\right)}
\phi\left(\frac{y_n-\beta_2^\top x_{2n}}{\sigma}\right)
\end{array}
$$

Or more simply:

$$
L^{2H}(\beta | y,x)=\prod_{n=1}^{N_o}
\left[1 - 
\Phi\left(\beta_1^ \top x_{1n}\right)
\Phi\left(\frac{\beta_{2n}^ \top x_{2n}}{\sigma}\right)
\right]
\prod_{n = N_o + 1}^{N}\frac{1}{\sigma}\Phi\left(\beta_1^ \top x_{1n}\right)
\phi\left(\frac{y_n-\beta_2^\top x_{2n}}{\sigma}\right)
$$

### Correlated models

Finally, we consider the errors of the two equations are
correlated. In this case, the model should be fully parametrized and
a natural way to do it is to suppose that $y_1^*$ and $y_2^*$ (or $\ln
y_2 ^ *$) follow a bivariate normal distribution:



$$
\left( \begin{array}{c} y_1 ^ * \\ y_2 ^ * \end{array} \right) \sim
N \left( \left( \begin{array}{c} \beta_1^\top x_1 \\ \beta_2^\top x_2 \end{array} \right) ,
\left(  \begin{array}{cc} \sigma_x ^ 2 & \rho \sigma_x \sigma_y \\ 
                          \rho \sigma_x \sigma_y & \sigma_y ^ 2
						  \end{array} \right)
\right)
$$

Note that as $y_1^*$ is not observed (but only its sign), its variance
is unidentified and arbitrarily set to 1. 

$$
f(y_1^*, y_2^* ; \theta) = \frac{1}{2\pi\sigma\sqrt{1 - \rho ^ 2}} 
e^{-\frac{1}{2}\frac{\left((y_1 ^ * - \beta_1  x_1)^2 +
      \left(\frac{y_2 ^ * - \beta_2  x_2}{\sigma}\right)^2 -
      2 \rho (y_1 ^ * - \beta_1 x_1)\left(\frac{y_2 ^ * - \beta_2  x_2}{\sigma}\right)\right)}{1 - \rho ^ 2}}
$$

with $\theta = (\beta_1^\top x_1, \beta_2^\top x_2, \rho)$.

Or more simply, denoting $z_1 = y_1 ^ * - \beta_1  x_1 = \epsilon_1$ and
$z_2 = \frac{y_2 ^ * - \beta_2 x_2}{\sigma} =
\frac{\epsilon_2}{\sigma}$:

$$
f(y_1^*, y_2^* ; \theta) = \frac{1}{2\pi\sigma\sqrt{1 - \rho ^ 2}} 
e^{-\frac{1}{2}\frac{z_1 ^ 2 + z_2 ^ 2 - 2 z_1 z_2}{1 - \rho ^ 2}}
  = \frac{1}{\sigma}\phi_b\left(y_1^*-\beta_1x_1, \frac{y_2 ^ * - \beta_2 x_2}{\sigma};\rho\right)
$$

where $\phi_b$ denotes the standard bivariate normal distribution. It
is well known that this density can be rewritten as the product of a
marginal and a conditional densities, both of them being normal: 

$$
\phi_b(z_1, z_2 ; \rho) = 
\phi(z_1) \frac{1}{\sqrt{1 - \rho ^ 2}} \phi\left(\frac{z_2 - \rho
z_1}{\sqrt{1 - \rho ^ 2}}\right)
$$

The probability that $y_1^* < 0$ is

$$\mbox{P}(y_1^* < 0) = \displaystyle\int_{-\infty} ^ {0}
\int_{-\infty}^{+\infty}\frac{1}{\sigma}\phi_b\left(y_1^*-\beta_1x_1,
  \frac{y_2 ^ * - \beta_2 x_2}{\sigma};\rho\right)dy_1^*dy_2^*$$

which simplifies to:

$$
\mbox{P}(y_1^* < 0)=\Phi(-\beta_1x_1) = 1 - \Phi(\beta_1x_1)
$$

For observed values of $y_2^*$ (which means $y_1^* > 0$), the density
is obtained by summing for all the positive values of $y_1^*$ the
bivariate density, truncated for $y_1^* < 0$:

$$
f^+(y_2^*
; \theta) = \frac{1}{\Phi(\beta_1x_1)}\int_{0}^{+\infty}\frac{1}{\sigma}\phi_b\left(y_1^*-\beta_1x_1,
  \frac{y_2 ^ * - \beta_2 x_2}{\sigma};\rho\right)dy_1^*
$$


$$
f^+(y_2^*; \theta) = \frac{1}{\sigma\sqrt{1-\rho ^ 2}}\frac{\Phi\left(\frac{\beta_1x_1 +
      \rho\frac{y_2^* -
        \beta_2x_2}{\sigma}}{\sqrt{1-\rho^2}}\right)}{\Phi(\beta_1x_1)}
\phi\left(\frac{y_2^*-\beta_2x_2}{\sigma}\right)
$$


The contribution of an observation to the likelihood is either
$P(y_1^* < 0)$ if $y$ is not observed and $P(y_1^* > 0) \times
f^+(y_2^*; \theta)$ if it is, which leads to the following likelihood
function:


$$
L(\beta | y,x)=\prod_{n=1}^{N_o}
\left[1 - 
\Phi\left(\beta_1^ \top x_{1n}\right)
\right]
\prod_{n = N_o + 1}^{N}
\frac{1}{\sigma\sqrt{1-\rho ^ 2}}
\Phi\left(\frac{\beta_1x_1 +
    \rho\frac{y_2^* -
      \beta_2x_2}{\sigma}}{\sqrt{1-\rho^2}}\right)
\phi\left(\frac{y_2^*-\beta_2x_2}{\sigma}\right)
$$


Consider now the conditional expectation of $y_2^*$:

$$
\mbox{E}(y_2\mid x_2, y_1 ^ * > 0) = 
\mbox{E}(y_2\mid x_2, z_1 ^ * > -\beta_1'x_1)=
\beta_2'x_2 + \mbox{E}(\epsilon_2\mid \epsilon_1  > -\beta_1'x_1)
$$

$$
\begin{array}{rcl}
\mbox{E}(\epsilon_2\mid \epsilon_1 > -\beta_1'x_1) &= &
\frac{\displaystyle\int_{-\beta_1'x_1}^{+\infty}\int_{-\infty}^{+\infty}
\epsilon_2/\sigma\phi_b(\epsilon_1, \epsilon_2 / \sigma, \rho)d\epsilon_1d\epsilon_2}
{\displaystyle\int_{-\beta_1'x_1}^{+\infty}
  \phi(\epsilon_1)d\epsilon_1}\\
&=&
\frac{\displaystyle\int_{-\beta_1'x_1}^{+\infty}\int_{-\infty}^{+\infty}
z_2\phi_b(\epsilon_1, z_2, \rho)d\epsilon_1dz_2}
{\Phi(\beta_1'x_1)}\\
&=& 
\frac{\displaystyle\int_{-\beta_1'x_1}^{+\infty}\int_{-\infty}^{+\infty}
z_2\phi(z_1) \frac{1}{\sqrt{1 - \rho ^ 2}} \phi\left(\frac{z_2 - \rho
z_1}{\sqrt{1 - \rho ^ 2}}\right)dz_1dz_2}
    {\displaystyle\int_{-\beta_1'x_1}^{+\infty} \phi(z_1)dz_1}
\end{array}
$$

With the change of variable $v = \frac{z_2-\rho
z_1}{\sqrt{1-\rho ^ 2}}$, which implies $dv = \frac{dz_2}{\sqrt{1-\rho ^ 2}}$:

$$
\int_{-\infty}^{+\infty}
z_2 \phi\left(\frac{z_2 - \rho
z_1}{\sqrt{1 - \rho ^ 2}}\right)dz_2=
\int_{-\infty}^{+\infty} \left(\rho z_1 + \sqrt{1-\rho ^ 2} v\right) \phi(v)dv=
\rho z_1
$$

and then:

$$
\mbox{E}(z_2\mid \epsilon_1 > -\beta_1'x_1) = 
\frac{\displaystyle\int_{-\beta_1'x_1}^{+\infty}
\rho z_1\phi(z_1)dz_1}
{1-\Phi\left(\beta_1'x_1\right)}
=\rho\frac{\phi(\beta_1'x_1)}{\Phi(\beta_1'x_1)}=\rho r(\beta_1 ^\top x_1)
$$

with, as usual, $r(z)$ denoting the inverse mills ratio. Finally, the
expected value of $y_2$ it then:

$$
\mbox{E}(y_2\mid x_2, y_1 ^ * > 0) = \beta_2'x_2 + 
\sigma \rho r(\beta_1 ^\top x_1)
$$

As for the tobit1 model, the conditional expectation is not equal to
$\beta^\top x_2$ because of the supplementary term $\sigma\rho
r(\beta_1^\top x_1)$. The linear estimator is therefore biased as in
the case of omitted variable if $r(\beta_1^\top x_1)$ is correlated
with $x_2$, which is obviously the case if there are common covariates
in $x_1$ and $x_2$. This expression also directly leads to an
alternative estimator to maximum likelihood estimator. This estimator
is a two-steps estimator, sometimes denoted the "heckit" estimator. It
consists on first regressing $1(y_1^* > 0)$ on $x_1$ using the probit
estimators, and then estimating $r(\beta^\top x_1)$ by
$r(\hat{\beta}^\top x_1)$, $\hat{\beta}^\top x_1$ being the linear
predictor of the probit model. In a second step, regressing $y_2$ on
$x_2$ and the supplementary covariate $r(\hat{\beta}_1^\top x_1)$
leads to a consistent estimates of $\beta_2$.

As previously seen (@fig-mills), $r(z)$ is almost linear
in $z$ for a wide range of values of $z$. Therefore, if $x_1=x_2$, the
coefficients of the second steps of the heckit estimator are only
identified by the non-linearity of $r$. The high correlation between
$x_2$ and $r(\hat{\beta}^\top_1x_1)$ will lead in this case to very
imprecise estimates. It is therefore recommended to impose exclusion
conditions, ie to exclude at least on covariate of the $x_1$ set for
the second step of the estimator. This (or these) excluded covariates
must be relevant for the selection equation, but not for the outcome
equation. In practice, it is often  difficult to provide theoretical
basements to such exclusion conditions. 


### Applications

@MAKO:STRA:09 analyse the behovior of policemen in terms of fees
because of excessive speed. A policeman has to take two decisions:

- the first one is whether to five a fee or not,
- if a fee is given, a its amount should be set, with the
  recomendation of applying the folowing formula: 
  50$+(speed-(max speed allowed + 10))

The authors make the hypothese that policemen's behavior will depends
on their institutional and political environment. First, there are two
kind of policemen, belonging to the municipality or the the state of
Massashussetts. Municipality agents are headed by a chief who is
nominated by the municipality authorities and can be fired at any
time. One can suppose, that contrary to the state policemen, these
local policement will act in the interest of the local
authorities, for which the fees policy has two aspects;

- firstly, fees is a source of income, which can be particulary
  important for municipalities with budget problems,
- secondly, fees given to drivers from the municipality can make them
  unhappy and unwilling to vote for the current authorities, which is
  not an issue for drivers from other municipalities.

`TrafficCitations` from the `edf.package` contains the data from this
article. The data set includes:

- two responses, `fine` which indicates whether a fee has been given
or not, and `amount` which is the amount of the fine when it has been
issued,
- covariates that describe local fiscal conditions; `propvalue` is the
  average property value in the municipality, which is the base of the
  main local tax and `overloss` which indicates that an override
  referendum (which indicates that the municipal anticiaptes
  insufficient revenues) fails to pass and `hospemp`, the percentage
  of employment in the hospitality sector,
- characteristics of the driver; `com` indicates if he is black or
hispanic, and `sex` his sex, `residence` indicates whether he lives in
the municipality (`local`), in another municipality of the State
(`outtown`) or in another State (`outstate`), `courtdist` is the
distance to the courts where the driver can appeal the citation,
`mphover` is the difference between the speed of the driver and the
legal limit and `cdl` is a dummy for commercial driving licence.
- characteristics of the policeman; `police` indicates whether the
policeman belongs to the municipality (`local`) or the State
(`state`). 

```{r}
data("TrafficCitations", package = "edf.tobit")
TrafficCitations <- TrafficCitations %>%
    rename(fem = female, pv = prval)
sel <- fine ~ log(mph) + black + hispanic + fem * log(age) +
    stpol * (res + log(pv) + oloss) + cdl
out <- update(sel, log(amount) ~ . - cdl)
probit <- glm(sel, data = TrafficCitations,
              family = binomial(link='probit'))
lp <- probit$linear.predictor
mls <- dnorm(lp) / pnorm(lp)
summary(probit)
```

Computing the heckit by hand
```{r}
lm <- lm(out, TrafficCitations)
heck <- update(lm, . ~ . + mls)
texreg::screenreg(list(lm, heck))
```

```{r}
library("sampleSelection")
heck <- heckit(sel, out, data = TrafficCitations,
               method = "2step")
summary(heck)
ml <- update(heck, method = "ml")
```


